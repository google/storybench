[{"texts": ["A person wearing denim jeans is standing and mixing something in a white cup with a hand mixer while a person whom hand is visible is pouring something in the white cup with the white mug.", "The white cup starts rolling, and the person lifts the hand mixer from the white cup while the person takes a white bowl towards the white cup and he puts aside the white mug."], "durations": null, "exact_frames_per_prompt": [32, 48], "background": "In the background, there are bottles, a white wall, and a countertop with some stuff, and the sound of a hand mixer is audible.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/--7qK_w-g3Y.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_dense_val_0_0"}, {"texts": ["A person is putting liquid in a cup with a white mug while a person wearing a blue jeans is standing and holding a hand blander and blends some liquid in a cup the he steps back.", "The person is moving his hand backward.", "The person puts something in a cup with a white bowl while a person stands on the backside."], "durations": null, "exact_frames_per_prompt": [41, 5, 18], "background": "In the background, there are bottles, a white wall, and a countertop with some stuff, and the sound of a hand mixer is audible.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/--7qK_w-g3Y.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_dense_val_0_1"}, {"texts": ["A person whose only hand is visible is holding a spoon while a person standing on the right side holding a hug and filling water to cup", "The person moves his hand toward the cup while holding a spoon while a person moves hand towards the left side and holding a jug"], "durations": null, "exact_frames_per_prompt": [70, 10], "background": "In the background, there are bottles, a white wall, and a countertop with some stuff, and the sound of a hand mixer is audible.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/--7qK_w-g3Y.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_dense_val_0_2"}, {"texts": ["A man wearing a black jacket and white cap is standing and tightening the knot of the fish wire."], "durations": null, "exact_frames_per_prompt": [80], "background": "In the background there are yachts, sky, red pole, black pole, rock wall, trees, a person speaking sound and people speaking sound is audible in the background.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/-b-YkpzFphk.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_dense_val_101_0"}, {"texts": ["A lady wearing blue clothes is sitting on a dark brown horse and driving the buffaloes towards the left side on a soil surface while a man in a blue shirt rides a brown horse to the right on the soil surface."], "durations": null, "exact_frames_per_prompt": [80], "background": "In the background, there is a soil surface, iron fencing, and people speaking voices are audible.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/5ft5ebGnMFU.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_dense_val_104_1"}, {"texts": ["A man wearing blue clothes is sitting on a horse and coming from the left side and going toward the right side on a brown soil surface while a woman wearing blue jeans is riding a horse towards the left side on a soil surface, and a black animal is walking on a soil surface."], "durations": null, "exact_frames_per_prompt": [18], "background": "In the background, there is a soil surface, iron fencing, and people speaking voices are audible.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/5ft5ebGnMFU.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_dense_val_104_2"}, {"texts": ["A dark brown horse carrying a lady on his back is driving the buffaloes toward the left on a brown soil surface while a group of buffaloes is moving here and there on the soil surface, some people are standing, and a man is riding a brown horse towards the right."], "durations": null, "exact_frames_per_prompt": [80], "background": "In the background, there is a soil surface, iron fencing, and people speaking voices are audible.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/5ft5ebGnMFU.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_dense_val_104_4"}, {"texts": ["A man wearing a white beekeeping suit is holding a beehive frame.", "The man is then tapping the beehive frame behind the green plants.", "The man starts walking to the left side."], "durations": null, "exact_frames_per_prompt": [31, 32, 14], "background": "In the background, the man is speaking, birds are chirping, a sound of tapping, there are green plants, an orange object, and a white beehive box.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/0M1SkaJJcV0.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_dense_val_105_0"}, {"texts": ["A man wearing a white t-shirt is juggling with the bottles while a group of people are sitting and looking at the glasses."], "durations": null, "exact_frames_per_prompt": [80], "background": "In the background, there is a countertop, glasses, and the bottles.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/1VfEeERUGKM.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_dense_val_108_0"}, {"texts": ["A person wearing a white t-shirt first sits behind the man.", "The person wearing a white t-shirt bends forward.", "The person again sits back."], "durations": null, "exact_frames_per_prompt": [63, 13, 4], "background": "In the background, there is a countertop, glasses, and the bottles.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/1VfEeERUGKM.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_dense_val_108_1"}, {"texts": ["A man wearing a grey t-shirt is sitting on a red chair and eating food."], "durations": null, "exact_frames_per_prompt": [80], "background": "In the background there is white cupboards, brown wall, white table, sink, yellow wooden basket, white napkin, white container, glasses, people speaking and laughing sound is audible in the background.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/-2kJpg1NIzM.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_dense_val_115_0"}, {"texts": ["A man wearing blue-black cloth is standing and holding a golf stick.", "The man is hitting a golf ball with his golf stick."], "durations": null, "exact_frames_per_prompt": [30, 50], "background": "In the background, a person is speaking, there is a green grass surface, white sky, buildings, trees, roads, a golf stick, golf ball and a golf bag.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/0u4c8Cel91U.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_dense_val_11_0"}, {"texts": ["A kid wearing a red-white striped t-shirt is sitting while a person whose hand is visible is holding a feeding bottle.", "The kid is drinking water given by a person."], "durations": null, "exact_frames_per_prompt": [38, 42], "background": "In the background, there is a grey floor, a bottle, a dark background, and the man and woman laughing and other miscellaneous sound is audible.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/0JFH2o8xhOc.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_dense_val_121_0"}, {"texts": ["A person whose hand is visible is holding the baby's bottle while a baby wearing a pink and white t-shirt sitting on the left side looking left again and again"], "durations": null, "exact_frames_per_prompt": [80], "background": "In the background, there is a grey floor, a bottle, a dark background, and the man and woman laughing and other miscellaneous sound is audible.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/0JFH2o8xhOc.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_dense_val_121_2"}, {"texts": ["A boy wearing a red t-shirt is standing and making a sushi roll on a table in front of a group of children and speaking."], "durations": null, "exact_frames_per_prompt": [80], "background": "In the background, there are shelves filled with books, tables, a white board, a white container, a green wall, a gray rug, some other stuff, and the boy is speaking, and children's laughing sounds are audible.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/-C9gnkkNI6Y.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_dense_val_124_0"}, {"texts": ["A man wearing a black t-shirt is playing golf inside a room while holding a golf stick.", "The man wearing a black t-shirt hits a golf ball with that stick.", "The ball goes into the golf putting cup."], "durations": null, "exact_frames_per_prompt": [49, 27, 4], "background": "In the background, there is a clock, white walls, a rug, golf sticks, chairs, a door, slippers, a brown surface, and people's voices are audible.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/1QNmZ23rVzU.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_dense_val_125_0"}, {"texts": ["A girl wearing a red-purple top is walking on a gray floor while a girl in a red t-shirt is standing and opening a present."], "durations": null, "exact_frames_per_prompt": [72], "background": "In the background, people are laughing. There is a gray floor, a white-green wall, a door, a white bucket, clothes, a speaker, TV, a cabinet with some stuff, and boxes.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/-1Gh2yyYY7M.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_dense_val_127_0"}, {"texts": ["A girl wearing a red top is standing on a gray floor, holding a box and unwrapping it and looking at it while another baby wears a red top and is walking around her."], "durations": null, "exact_frames_per_prompt": [80], "background": "In the background, people are laughing. There is a gray floor, a white-green wall, a door, a white bucket, clothes, a speaker, TV, a cabinet with some stuff, and boxes.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/-1Gh2yyYY7M.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_dense_val_127_1"}, {"texts": ["A man wearing dark gray shorts is standing on a soil surface, holding a snake.", "The man is letting the snake go."], "durations": null, "exact_frames_per_prompt": [40, 28], "background": "In the background, a man is speaking. There is a soil surface.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/-g5emSW-SOg.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_dense_val_12_0"}, {"texts": ["A black snake is getting held by the man.", "The snake is crawling on the soil surface while a man wearing a grey short is releasing a snake on the soil surface."], "durations": null, "exact_frames_per_prompt": [40, 27], "background": "In the background, a man is speaking. There is a soil surface.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/-g5emSW-SOg.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_dense_val_12_1"}, {"texts": ["A person whose hand is visible is caressing a brown-white cat."], "durations": null, "exact_frames_per_prompt": [80], "background": "In the background, there is a miscellaneous sound, a person speaks, there is a black printed surface.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/24cgfaG8WI0.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_dense_val_133_0"}, {"texts": ["A brown-white cat is lying on a black printed surface and getting caressed by the person."], "durations": null, "exact_frames_per_prompt": [80], "background": "In the background, there is a miscellaneous sound, a person speaks, there is a black printed surface.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/24cgfaG8WI0.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_dense_val_133_1"}, {"texts": ["A boy wearing a blue outfit is standing on the grey surface and feeding a brown baby goat while a group of goats is standing at the back.", "The boy starts moving towards the left."], "durations": null, "exact_frames_per_prompt": [38, 29], "background": "In the background, there are fences, grey surface, trees, wooden shed, animal sound is audible and woman sound is audible.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/-3cl74lG6T4.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_dense_val_135_0"}, {"texts": ["The brown baby goat is standing on the grey surface in front of the first boy and is being fed by the boy while a group of goats and a group of people are standing and moving at the back."], "durations": null, "exact_frames_per_prompt": [67], "background": "In the background, there are fences, grey surface, trees, wooden shed, animal sound is audible and woman sound is audible.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/-3cl74lG6T4.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_dense_val_135_1"}, {"texts": ["A person wearing an orange t-shirt and grey shorts is standing and holding a black snake by its tail.", "The person wearing an orange t-shirt and grey shorts is letting it free on the ground while the snake is moving slowly on the surface."], "durations": null, "exact_frames_per_prompt": [40, 28], "background": "In the background, there is a brown surface, plants, wooden log, sticks, and a boy is speaking.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/-g5emSW-SOg.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_dense_val_13_0"}, {"texts": ["A black snake is held by a person with its tail.", "The snake starts moving on the brown surface."], "durations": null, "exact_frames_per_prompt": [39, 22], "background": "In the background, there is a brown surface, plants, wooden log, sticks, and a boy is speaking.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/-g5emSW-SOg.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_dense_val_13_1"}, {"texts": ["A kid wearing a brown vest is eating the cake with a fork and then turns back."], "durations": null, "exact_frames_per_prompt": [69], "background": "In the background there is grey surface, white plate, a person speaking sound and people speaking sound is audible in the background.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/0pQACKjlllc.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_dense_val_142_0"}, {"texts": ["A kid wearing pink-white clothes is walking while holding the leash of a puppy.", "The kid drops the leash and walks toward the right."], "durations": null, "exact_frames_per_prompt": [44, 21], "background": "In the background, there are black tires, grass, a brown surface, a white door, and some other stuff, and the sound of birds chirping is audible.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/-0Y6xqWo_kM.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_dense_val_144_0"}, {"texts": ["A brown puppy is walking on the grass while it's leash is held by a kid.", "Then the kid drops the leash and the puppy walks forward."], "durations": null, "exact_frames_per_prompt": [45, 21], "background": "In the background, there are black tires, grass, a brown surface, a white door, and some other stuff, and the sound of birds chirping is audible.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/-0Y6xqWo_kM.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_dense_val_144_1"}, {"texts": ["A person whose only hands are visible is making a knot with the white thread."], "durations": null, "exact_frames_per_prompt": [64], "background": "In the background, there is a brown wooden surface, a white thread, and the music playing sound is audible.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/-54Bs-0kdhA.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_dense_val_145_0"}, {"texts": ["A man wearing a red vest and black pants is standing on the right side of the sheep while a woman wearing a grey hoodie is shearing the sheep with a wool shearing machine."], "durations": null, "exact_frames_per_prompt": [47], "background": "In the background, there is a beige colored wall, a concrete surface, a hair trimming machine, and a sheep's voice. The trimming machine sound and some miscellaneous sounds are audible.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/1WqYcl4nNCE.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_dense_val_146_0"}, {"texts": ["A woman wearing a gray jacket and black pants is leaning forward and trimming the hairs of the sheep while a man in a red vest is holding it with his legs."], "durations": null, "exact_frames_per_prompt": [47], "background": "In the background, there is a beige colored wall, a concrete surface, a hair trimming machine, and a sheep's voice. The trimming machine sound and some miscellaneous sounds are audible.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/1WqYcl4nNCE.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_dense_val_146_1"}, {"texts": ["A sheep is lying on the concrete surface and getting trimmed by the woman while a man in a red vest and black pants stands holding the sheep from his leg."], "durations": null, "exact_frames_per_prompt": [47], "background": "In the background, there is a beige colored wall, a concrete surface, a hair trimming machine, and a sheep's voice. The trimming machine sound and some miscellaneous sounds are audible.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/1WqYcl4nNCE.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_dense_val_146_2"}, {"texts": ["A girl wearing a pink dress is riding on a baby elephant while a man wearing a blue t-shirt is holding and moving with the baby elephant, other people are sitting and moving."], "durations": null, "exact_frames_per_prompt": [80], "background": "In the background there is grey surface, trees, sky, chairs, tables, birds chirping sound, people speaking sound, a person speaking sound and wind sound is audible in the background.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/-4jhRyZILBc.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_dense_val_14_0"}, {"texts": ["A man on the left side wearing a blue t-shirt is walking on the grey surface and holding the baby elephant on which a girl is riding while a group of people in which some are walking and some are sitting on the backside."], "durations": null, "exact_frames_per_prompt": [80], "background": "In the background there is grey surface, trees, sky, chairs, tables, birds chirping sound, people speaking sound, a person speaking sound and wind sound is audible in the background.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/-4jhRyZILBc.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_dense_val_14_1"}, {"texts": ["A baby elephant is held by a man and walking on the grey surface while carrying a girl on its back."], "durations": null, "exact_frames_per_prompt": [80], "background": "In the background there is grey surface, trees, sky, chairs, tables, birds chirping sound, people speaking sound, a person speaking sound and wind sound is audible in the background.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/-4jhRyZILBc.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_dense_val_14_2"}, {"texts": ["A girl on the left side wearing an orange top is sitting and using her mobile phone while another girl in the middle wearing a white t-shirt is sitting and eating something, the third girl on the right side wearing a black top is sitting and pointing at something and two men are standing behind on the brown surface."], "durations": null, "exact_frames_per_prompt": [80], "background": "In the background there is brown table, grey surface, a flower, a plant with pot, and people speaking sound is audible in the background.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/-7784_WqE1E.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_dense_val_158_0"}, {"texts": ["A girl in the centre wearing a grey t-shirt is sitting and picking up the food while a woman on the left is using her phone and another woman on the right is moving her hand and two boys are standing on the backside and talking with each other.", "The girl takes a bite."], "durations": null, "exact_frames_per_prompt": [30, 50], "background": "In the background there is brown table, grey surface, a flower, a plant with pot, and people speaking sound is audible in the background.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/-7784_WqE1E.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_dense_val_158_1"}, {"texts": ["A lady wearing blue clothes is sitting on a dark brown horse and driving buffaloes towards the left side on a soil surface.\n while a man is riding another horse."], "durations": null, "exact_frames_per_prompt": [80], "background": "In the background, there is a soil surface, iron fencing, and people speaking voices are audible.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/5ft5ebGnMFU.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_dense_val_159_1"}, {"texts": ["A man wearing blue clothes is sitting on a horse and coming from the left side and going toward the right on a soil surface while a woman wearing a blue outfit is riding on a horse from right to left and a black animal runs towards the left  on a soil surface."], "durations": null, "exact_frames_per_prompt": [18], "background": "In the background, there is a soil surface, iron fencing, and people speaking voices are audible.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/5ft5ebGnMFU.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_dense_val_159_2"}, {"texts": ["A dark brown horse carrying a lady on his back is driving the buffaloes toward the left on a soil surface while a group of buffaloes is moving here and there on the soil surface, some people are standing, and a man is riding a brown horse towards the right."], "durations": null, "exact_frames_per_prompt": [80], "background": "In the background, there is a soil surface, iron fencing, and people speaking voices are audible.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/5ft5ebGnMFU.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_dense_val_159_4"}, {"texts": ["A man wearing blue shorts is sitting on the couch while a cat putted leg on the bed and moving leg towards the first person", "The man is pampering a cat while a cat watching the first person"], "durations": null, "exact_frames_per_prompt": [32, 30], "background": "In the background, there is a laptop, a mobile phone, a table, a couch, a wooden surface, a white object, a rack, and the music playing sound is audible.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/5G94o6UdX2w.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_dense_val_15_0"}, {"texts": ["A cat is walking under the table while a man wearing blue shorts is sitting on the sofa.", "The cat stands near a man.", "The cat gets pampered by a man."], "durations": null, "exact_frames_per_prompt": [20, 13, 47], "background": "In the background, there is a laptop, a mobile phone, a table, a couch, a wooden surface, a white object, a rack, and the music playing sound is audible.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/5G94o6UdX2w.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_dense_val_15_1"}, {"texts": ["A girl wearing a purple t-shirt is standing and clapping", "The girl picks something from the table and puts it into her pocket.", "The girl starts walking."], "durations": null, "exact_frames_per_prompt": [28, 25, 10], "background": "In the background, there is a red wall, a red carpet, a white ceiling, a table covered with white cloth, a window, a blue wall, and the music playing sound is audible.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/-1GsilN0qjU.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_dense_val_164_2"}, {"texts": ["A woman wearing black clothes is massaging her cheeks with her finger."], "durations": null, "exact_frames_per_prompt": [80], "background": "In the background, there is a white wall, and a mirror.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/0cONsRVrLbU.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_dense_val_167_0"}, {"texts": ["A girl wearing a light blue and black dress is lying upside down on the bed and laughing while rolling to the right side."], "durations": null, "exact_frames_per_prompt": [80], "background": "In the background, there are yellow walls, and off-white door, and a mirror.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/0gR5FP7HpZ4.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_dense_val_170_0"}, {"texts": ["A person wearing black clothes is riding on a white mole on the rock surface.\n while the girl wearing a yellow top is sitting on a mule and riding on the rock surface."], "durations": null, "exact_frames_per_prompt": [45], "background": "In the background, there are trees, a blue-cloudy sky, a rock surface, rocks, and the sound of miles walking is audible.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/0xTPF7ePX5c.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_dense_val_174_0"}, {"texts": ["A person wearing yellow clothes is riding on a brown mule after the first person on the rock surface."], "durations": null, "exact_frames_per_prompt": [45], "background": "In the background, there are trees, a blue-cloudy sky, a rock surface, rocks, and the sound of miles walking is audible.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/0xTPF7ePX5c.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_dense_val_174_1"}, {"texts": ["A white mule is walking on the rock surface while carrying a person on its back while a woman in a yellow jacket is riding mule behind the white mule."], "durations": null, "exact_frames_per_prompt": [46], "background": "In the background, there are trees, a blue-cloudy sky, a rock surface, rocks, and the sound of miles walking is audible.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/0xTPF7ePX5c.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_dense_val_174_2"}, {"texts": ["A brown mule is walking on the rock surface while carrying another person on its back while a white mule is walking on the rock surface in front of first mule and a person wearing black outfit is sitting on its back."], "durations": null, "exact_frames_per_prompt": [49], "background": "In the background, there are trees, a blue-cloudy sky, a rock surface, rocks, and the sound of miles walking is audible.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/0xTPF7ePX5c.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_dense_val_174_3"}, {"texts": ["A man wearing blue pants is holding a newspaper and turning its pages."], "durations": null, "exact_frames_per_prompt": [80], "background": "In the background, there is a newspaper, and a white background.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/-8oPwToqArE.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_dense_val_177_0"}, {"texts": ["A woman wearing a waitress dress is standing holding a tray and a bowl filled with food and she is serving the food with the tongs to the woman while speaking while a group of people is standing, sitting, and moving at the back.", "The woman walks aside."], "durations": null, "exact_frames_per_prompt": [66, 10], "background": "In the background, there is an inside view of a restaurant, chandeliers, tables, a wine bottle, a window, and people's voices are audible.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/-vOrVT1CiPQ.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_dense_val_17_0"}, {"texts": ["A woman wearing a green dress is sitting, and she is getting served food by the first woman, and then the second woman moves her hand towards the plate as a group of people is standing and sitting on the backside."], "durations": null, "exact_frames_per_prompt": [43], "background": "In the background, there is an inside view of a restaurant, chandeliers, tables, a wine bottle, a window, and people's voices are audible.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/-vOrVT1CiPQ.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_dense_val_17_1"}, {"texts": ["A woman wearing a pink dress is standing.", "The woman is moving her hand toward the screen showing weather forecast."], "durations": null, "exact_frames_per_prompt": [55, 25], "background": "In the background, there is a blue-yellow background, a screen showing the weather forecast and the voice of a woman speaking, and the sound of music is audible.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/-kahgmRD-4g.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_dense_val_186_0"}, {"texts": ["A man whose half body is visible is holding a fish.", "The man left the fish into the ice fishing hole."], "durations": null, "exact_frames_per_prompt": [33, 11], "background": "In the background, there is a snowy surface, an ice fishing hole, buildings, trees, a gray bag, ice fishing equipment, and the voices of the people and some miscellaneous sounds are audible.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/-0N5cgaMCfY.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_dense_val_188_0"}, {"texts": ["A man whose only half body is visible is wearing a blue-white jacket and pants is holding a fishing rod and doing ice fishing while the other person sitting opposite to him"], "durations": null, "exact_frames_per_prompt": [35], "background": "In the background, there is a snowy surface, an ice fishing hole, buildings, trees, a gray bag, ice fishing equipment, and the voices of the people and some miscellaneous sounds are audible.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/-0N5cgaMCfY.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_dense_val_188_2"}, {"texts": ["A man wearing a white sumo uniform is standing on the right side and holding a child while another man on the left side wearing a sumo uniform is standing while holding a child and the third man on the right side wearing a golden kimono is standing and looking at the child."], "durations": null, "exact_frames_per_prompt": [59], "background": "In the background, people are speaking, there is a maroon-white house, banners, green trees, and a grey speaker.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/-C7WEhlgf7w.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_dense_val_189_2"}, {"texts": ["A man wearing a black cap is standing on the right side and watching the children while two men are holding the babies in their hands."], "durations": null, "exact_frames_per_prompt": [62], "background": "In the background, people are speaking, there is a maroon-white house, banners, green trees, and a grey speaker.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/-C7WEhlgf7w.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_dense_val_189_4"}, {"texts": ["A child wearing a purple cloth is crying and held by the first man while a group of people are roaming around the man, and one of them is holding a baby."], "durations": null, "exact_frames_per_prompt": [80], "background": "In the background, people are speaking, there is a maroon-white house, banners, green trees, and a grey speaker.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/-C7WEhlgf7w.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_dense_val_189_5"}, {"texts": ["A child wearing a pink t-shirt is crying and held by the second man while another man wearing a white cloth is standing facing backwards and is carrying another child, and other man wearing a black hat is looking at the child and smiling then stands on the right and looks at another child."], "durations": null, "exact_frames_per_prompt": [63], "background": "In the background, people are speaking, there is a maroon-white house, banners, green trees, and a grey speaker.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/-C7WEhlgf7w.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_dense_val_189_6"}, {"texts": ["A person whose only hands are visible is pouring the engine oil into a car engine with the help of a green funnel."], "durations": null, "exact_frames_per_prompt": [80], "background": "In the background, there is a car, engine parts of a car, a black bottle, a green funnel, a grey wall, and stones.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/--rC9Wkh6HA.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_dense_val_195_0"}, {"texts": ["A man wearing a black t-shirt is standing and playing cello."], "durations": null, "exact_frames_per_prompt": [80], "background": "In the background, there are bottles, a table, a window, a cloth, a wall, and some other stuff and the sound of musical instruments is audible.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/-fbscFfkh4M.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_dense_val_196_0"}, {"texts": ["A man wearing a shirt is sitting and playing the guitar while another man in a white shirt is drinking beer, a third man in a black shirt is playing guitar, and a fourth man in a gray shirt is playing banjo and then drinking beer."], "durations": null, "exact_frames_per_prompt": [80], "background": "In the background, there are bottles, a table, a window, a cloth, a wall, and some other stuff and the sound of musical instruments is audible.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/-fbscFfkh4M.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_dense_val_196_1"}, {"texts": ["A man on the right side is sitting, playing banjo while another man wearing black and blue outfit is standing on the backside and playing cello.", "The man lifts a drink bottle, starts drinking while third man wearing brown and white checkered shirt and blue pant, holding a guitar is sitting on the backside and he is playing guitar.", "The man puts the bottle on the table while fourth man wearing white outfit, holding a violin in his left hand and a bottle in his right hand is sitting beside third man and he is drinking and moving his head."], "durations": null, "exact_frames_per_prompt": [34, 33, 13], "background": "In the background, there are bottles, a table, a window, a cloth, a wall, and some other stuff and the sound of musical instruments is audible.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/-fbscFfkh4M.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_dense_val_196_3"}, {"texts": ["A man wearing a gray sweatshirt and spectacles is sitting on the red seat and eating the food."], "durations": null, "exact_frames_per_prompt": [80], "background": "In the background, there is a red seat, a light yellow pillar, a window like structure, red plates, papers, food, the sound of the song and a miscellaneous sound is audible.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/-AMpy1HyBfk.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_dense_val_199_0"}, {"texts": ["A boy wearing a green t-shirt and grey shorts is moving on the left side of a girl on the red floor. He is holding a doll and a red object and puts his other hand in it.\n while the girl move towards the boy."], "durations": null, "exact_frames_per_prompt": [35], "background": "In the background, there is white wall, a table, door, red floor, a cabinet, a white object, and white mat.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/1f0Nce2SgOE.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_dense_val_200_0"}, {"texts": ["A girl wearing a pink dress is on the right side of the first boy and she is standing and holding objects in her hands.", "The girl wearing a pink dress starts moving on the red floor."], "durations": null, "exact_frames_per_prompt": [15, 65], "background": "In the background, there is white wall, a table, door, red floor, a cabinet, a white object, and white mat.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/1f0Nce2SgOE.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_dense_val_200_1"}, {"texts": ["A woman wearing an apron is cooking food in a frying pan using a spatula.", "The woman wearing an apron starts speaking."], "durations": null, "exact_frames_per_prompt": [49, 32], "background": "In the background, there are wooden cabinets, a gas stove with a microwave, a white countertop, utensils, food, and the woman's speaking sound is audible.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/-DhAghDNh60.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_dense_val_206_0"}, {"texts": ["A man wearing a blue jeans is trimming the hoof of the sheep while another man is trimming the hair of the other sheep."], "durations": null, "exact_frames_per_prompt": [80], "background": "In the background, there is a muddy surface, wool, and a fence.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/-5o48YgRlkQ.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_dense_val_20_0"}, {"texts": ["A man wearing a blue t-shirt is shearing the sheep and same is done by the second man wearing blue jeans"], "durations": null, "exact_frames_per_prompt": [80], "background": "In the background, there is a muddy surface, wool, and a fence.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/-5o48YgRlkQ.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_dense_val_20_1"}, {"texts": ["A sheep on the left side is getting its hooves trimmed by the man in blue jeans while a man wearing a blue t-shirt on the right side is trimming the hooves of a sheep."], "durations": null, "exact_frames_per_prompt": [80], "background": "In the background, there is a muddy surface, wool, and a fence.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/-5o48YgRlkQ.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_dense_val_20_2"}, {"texts": ["A sheep on the right side is getting sheared by the man in blue t-shirt as another sheep on the left side is getting its hooves cleaned by the man in blue jeans."], "durations": null, "exact_frames_per_prompt": [80], "background": "In the background, there is a muddy surface, wool, and a fence.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/-5o48YgRlkQ.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_dense_val_20_3"}, {"texts": ["A man wearing an olive t-shirt is standing and putting chips in his mouth which are snatched by a bird."], "durations": null, "exact_frames_per_prompt": [80], "background": "In the background, there is a sky, a drum, a river, and a rock, and the grey road surface and the people speaking and birds chirping sound is audible.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/1G2zxrAC7Q8.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_dense_val_210_0"}, {"texts": ["A person wearing a light grey t-shirt is standing and unscrewing the air filter."], "durations": null, "exact_frames_per_prompt": [24], "background": "In the background, there is a rack with some stuff, a black mat, a barbell, a light, a white wall, gym equipments, a white basket, a brown floor, a silver object, air filters, a black car engine, chairs, and the man speaking sound is audible.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/1C4Q1Xjyeig.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_dense_val_213_1"}, {"texts": ["A man is bending towards the counter-top and eating food from the plate through his mouth directly.\n while the woman wearing black t-shirt is drinking milk from the glass and chewing the food"], "durations": null, "exact_frames_per_prompt": [80], "background": "In the background, there is a black countertop, white wall, a rack, some stuff, white cupboard, black chair, white cloth, white plates, food, glasses, the voices of the people and a miscellaneous sound is audible.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/2WtmYtTUr_c.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_dense_val_214_0"}, {"texts": ["A woman wearing a black t-shirt is bending down and drinking white liquid from the glass.", "The woman is putting the glass back on the counter-top while a man on the right is bent down and eating something from the plate."], "durations": null, "exact_frames_per_prompt": [26, 25], "background": "In the background, there is a black countertop, white wall, a rack, some stuff, white cupboard, black chair, white cloth, white plates, food, glasses, the voices of the people and a miscellaneous sound is audible.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/2WtmYtTUr_c.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_dense_val_214_1"}, {"texts": ["A man on the right side is bent and eating from the white plate on the black table.\n while the other woman is also bent drinking and eating"], "durations": null, "exact_frames_per_prompt": [80], "background": "In the background, people are speaking, there is a black countertop, white plates, transparent glasses, white walls, white cupboard, a metal frame, and some other stuff.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/2WtmYtTUr_c.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_dense_val_215_0"}, {"texts": ["A kid is sitting near the person and eating watermelon."], "durations": null, "exact_frames_per_prompt": [50], "background": "In the background, there is a brick wall, a glass window, a sitting bench, a dustbin, a watermelon, a blue object, a reflection of a table, and green grass, and the woman speaking sound is audible.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/-xA_1WuydMU.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_dense_val_217_0"}, {"texts": ["A woman wearing blue clothes is holding the baby on her lap and talking and smiling while looking at the baby."], "durations": null, "exact_frames_per_prompt": [80], "background": "In the background, there is a sofa, and a piece of paper.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/-Ovwq0kVUx4.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_dense_val_219_0"}, {"texts": ["A baby wearing red clothes is sitting on the lap of the woman and watching the person tearing a piece of paper."], "durations": null, "exact_frames_per_prompt": [80], "background": "In the background, there is a sofa, and a piece of paper.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/-Ovwq0kVUx4.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_dense_val_219_1"}, {"texts": ["A man on the left, wearing a light purple shirt and jeans, is leaning forward and cutting the hoof of the sheep.\n while another man on the right wearing blue and black outfit is also leaning forward and cutting the hoof of the sheep and the sheep is lying on the ground and getting its hoof cut by the man."], "durations": null, "exact_frames_per_prompt": [80], "background": "In the background, there is a wooden fencing, sheep hairs, brown surface, muddy surface, some other stuff and trimming machine sound and people's voices and some miscellaneous sounds are audible.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/-5o48YgRlkQ.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_dense_val_21_0"}, {"texts": ["A man wearing a blue t-shirt and black pants, is leaning forward and trimming the hair of another sheep with a trimming machine while another man wearing a lilac shirt is leaning forward and is cutting something on the sheep's foot."], "durations": null, "exact_frames_per_prompt": [80], "background": "In the background, there is a wooden fencing, sheep hairs, brown surface, muddy surface, some other stuff and trimming machine sound and people's voices and some miscellaneous sounds are audible.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/-5o48YgRlkQ.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_dense_val_21_1"}, {"texts": ["A sheep on the left side is sitting on the brown surface and getting its hoof cut by the first man while another sheep on the right side is also being shaved by the man wearing a blue t-shirt."], "durations": null, "exact_frames_per_prompt": [80], "background": "In the background, there is a wooden fencing, sheep hairs, brown surface, muddy surface, some other stuff and trimming machine sound and people's voices and some miscellaneous sounds are audible.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/-5o48YgRlkQ.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_dense_val_21_2"}, {"texts": ["A sheep on the right side is sitting on the brown surface and getting trimmed by the second man while the other man with blue jeans holding the sheep's leg"], "durations": null, "exact_frames_per_prompt": [80], "background": "In the background, there is a wooden fencing, sheep hairs, brown surface, muddy surface, some other stuff and trimming machine sound and people's voices and some miscellaneous sounds are audible.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/-5o48YgRlkQ.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_dense_val_21_3"}, {"texts": ["A person wearing a blue t-shirt is sitting inside the car which is moving on the grey road surface and reading a newspaper.\n"], "durations": null, "exact_frames_per_prompt": [80], "background": "In the background there is grey road surface, sky, trees, bushes, houses, and people speaking and laughing sound is audible in the background.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/0NZewzomYbA.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_dense_val_222_0"}, {"texts": ["A man wearing red and blue clothes is holding the sheep between his legs and shearing the sheep."], "durations": null, "exact_frames_per_prompt": [80], "background": "In the background, there is a wooden pole, a wooden platform, and the fence.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/3qW7LQxH4fo.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_dense_val_223_0"}, {"texts": ["A sheep is being held by the man and getting sheared."], "durations": null, "exact_frames_per_prompt": [80], "background": "In the background, there is a wooden pole, a wooden platform, and the fence.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/3qW7LQxH4fo.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_dense_val_223_1"}, {"texts": ["A man wearing a blue and red jumpsuit is shaving the wool off the sheep.\n while a group of sheep are standing behind the wooden fence."], "durations": null, "exact_frames_per_prompt": [80], "background": "In the background, people are speaking and miscellaneous sounds are audible. There is a brown tree, a brown wooden wall, a brown surface, fencing, a black bag and a white object.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/3qW7LQxH4fo.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_dense_val_224_0"}, {"texts": ["A sheep lying on the brown surface is being shaved by the man while a person whose hands are visible is holding a phone in the direction of the wooden stage and other sheep are moving behind the wooden structure."], "durations": null, "exact_frames_per_prompt": [80], "background": "In the background, people are speaking and miscellaneous sounds are audible. There is a brown tree, a brown wooden wall, a brown surface, fencing, a black bag and a white object.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/3qW7LQxH4fo.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_dense_val_224_1"}, {"texts": ["A person whose hand is visible is picking up an electric kettle.", "The person is putting water into the disposable glass from the kettle."], "durations": null, "exact_frames_per_prompt": [27, 36], "background": "In the background, there is an electric kettle, a glass with a tea bag, water, plants in pots, a window, trees, a white sky, a white wall, a white table, and some music is audible.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/-0r6NmrdKCU.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_dense_val_225_0"}, {"texts": ["A brown-white dog attached to a leash held by a person is moving on one side of a road", "The dog starts running towards the other side of the road."], "durations": null, "exact_frames_per_prompt": [29, 33], "background": "In the background, there is a grey road, houses, parked cars, trees, a blue sky, and a drumbeat sound is audible.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/-dMaV5Il324.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_dense_val_226_0"}, {"texts": ["A person wearing white clothes is playing golf on the green surface while another person wearing white pants is also playing golf."], "durations": null, "exact_frames_per_prompt": [80], "background": "In the background, there are golf sticks, a green surface, golf balls, a mat, a wall, and some other stuff, and the sound of music is audible.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/6Cwq13Oaays.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_dense_val_229_0"}, {"texts": ["A person wearing white trousers is standing and playing golf on the green surface."], "durations": null, "exact_frames_per_prompt": [41], "background": "In the background, there are golf sticks, a green surface, golf balls, a mat, a wall, and some other stuff, and the sound of music is audible.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/6Cwq13Oaays.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_dense_val_229_1"}, {"texts": ["A man wearing a yellow t-shirt is standing in one knee position and holding a fishing rod with a girl."], "durations": null, "exact_frames_per_prompt": [80], "background": "In the background, there is a miscellaneous sound, people are speaking, a sound of wind, there is a sand beach, a sea, and a sky.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/-2eq0citAJg.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_dense_val_233_0"}, {"texts": ["A girl wearing a brown t-shirt is sitting on a red-blue cloth and holding a fishing rod with the man wearing yellow t-shirt while a woman wearing a green skirt comes running from the right and grabs a stick from the man then moves back, and another man wearing t-shirt is holding a fishing net and runs towards the water."], "durations": null, "exact_frames_per_prompt": [80], "background": "In the background, there is a miscellaneous sound, people are speaking, a sound of wind, there is a sand beach, a sea, and a sky.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/-2eq0citAJg.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_dense_val_233_2"}, {"texts": ["A girl wearing a green dress is running on the beach and taking a fishing rod from the second man then goes to the left side while a boy and a girl are holding a fishing rod and watching the another girl."], "durations": null, "exact_frames_per_prompt": [36], "background": "In the background, there is a miscellaneous sound, people are speaking, a sound of wind, there is a sand beach, a sea, and a sky.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/-2eq0citAJg.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_dense_val_233_3"}, {"texts": ["A person whom hands are visible is mixing the vegetables in a glass bowl with a spoon."], "durations": null, "exact_frames_per_prompt": [80], "background": "In the background, a person is speaking and music is playing. There is a white surface, a glass bowl, a spoon and vegetables.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/1bihmEt95PI.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_dense_val_235_0"}, {"texts": ["A baby whose bare body is sitting on the white baby feeding chair and eating food with his hand."], "durations": null, "exact_frames_per_prompt": [80], "background": "In the background there is grey surface, white baby feeding chair, white wall, and people speaking sound is audible in the background.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/-AEUGxV9mpg.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_dense_val_241_0"}, {"texts": ["A girl wearing a pink cap is standing on the right and moving her hand towards a group of pigeons.\n"], "durations": null, "exact_frames_per_prompt": [69], "background": "In the background, the music is playing, there is a grey surface, shops, and brown boxes.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/264oaD4orK0.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_dense_val_243_0"}, {"texts": ["A woman wearing a black sweatshirt is standing and tossing food in a frying pan."], "durations": null, "exact_frames_per_prompt": [62], "background": "In the background, there is a wooden cabinet, a kitchen counter-top with some stuff, a door, a towel, a key holder, a gas stove, a frying pan, and a peach wall, some other stuff and the people speaking sounds are audible.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/-yv8c2CDbR8.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_dense_val_244_0"}, {"texts": ["A man wearing a black jacket is holding a bundle of paper money in his hand and counting it.", "The man stops and speaks while doing the hand movement.", "The man moves his hand in the right direction to take another bundle of paper money."], "durations": null, "exact_frames_per_prompt": [28, 39, 13], "background": "In the background, there is an off-white wall, a monitor, an electronic machine, and a bundle of paper money. There is the sound of a male voices in the background.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/-02UO1KSdZ0.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_dense_val_248_0"}, {"texts": ["A person is holding a bowl of rice. The person pours an egg on the rice.", "The person starts mixing the egg with the red chopsticks."], "durations": null, "exact_frames_per_prompt": [40, 40], "background": "In the background, there is white table, bowls, spoon, rice, raw egg, red chopsticks, white texts are visible and music is playing.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/1koxtPz76MU.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_dense_val_253_0"}, {"texts": ["A woman wearing black and blue clothes is holding a piece of cloth.", "The woman folds the clothes.", "The woman puts the clothes on the left side."], "durations": null, "exact_frames_per_prompt": [18, 41, 5], "background": "In the background, there is a cloth, countertop, glasses, and the wooden floor.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/-QRY9rDrJf0.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_dense_val_254_0"}, {"texts": ["A child wearing a grey t-shirt is held by a woman while a group of people are are moving here and there.", "The child gets put on the left by the woman and starts crawling on the floor."], "durations": null, "exact_frames_per_prompt": [26, 34], "background": "In the background, people are speaking, there is a white-blue printed wall, some toys, a red surface, a purple wall, a red-yellow inflatable ball pit with colourful balls, and a green surface.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/2WowmnRTyqY.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_dense_val_259_0"}, {"texts": ["A person wearing a white outfit is standing and sprinkling salt on the meat.", "The person pours the substance in the bowl with the help of a glass cup.", "Then the person mixes them with a black spoon."], "durations": null, "exact_frames_per_prompt": [14, 44, 22], "background": "In the background, there is a brown table, a bowl, meat, green tray, a black spoon, black color texts are visible , green color texts are visible and music is playing.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/1CjON-2Oqxc.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_dense_val_264_0"}, {"texts": ["A grey-white husky is sitting in the front and getting blow dried given by a person."], "durations": null, "exact_frames_per_prompt": [80], "background": "In the background, the music is playing, there is a green grass lawn, a wooden barrier, a brick house with a white door, a brown chair, a grey surface, and green trees.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/-9BGMU4WYMg.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_dense_val_267_0"}, {"texts": ["A man wearing white clothes is standing in a kitchen and holding a frying pan in his hand.", "The man at first scatters the food kept on the plate with a wooden spatula.", "The man looks in the left direction while speaking.", "The man again starts scattering the food."], "durations": null, "exact_frames_per_prompt": [20, 40, 10, 10], "background": "In the background, there is a kitchen, plates, spatula, a frying pan, food and a spoon. There is the sound of male voice in the background.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/-5RvkhL92b4.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_dense_val_269_0"}, {"texts": ["A girl whose half face is visible is filling her eyebrow with an eyebrow pencil.", "The girl whose half face is visible is brushing her eyebrow."], "durations": null, "exact_frames_per_prompt": [16, 64], "background": "In the background, the girl is speaking, the music is playing, there is a black table, a vase with orange flowers, a picture frame, a black object, and a white wall.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/-BlV342BAM4.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_dense_val_270_0"}, {"texts": ["A person whose hands are visible is folding a white paper."], "durations": null, "exact_frames_per_prompt": [80], "background": "In the background, miscellaneous sounds are audible. There is a wooden floor, a white paper, and a light brown wall.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/-35wittelPs.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_dense_val_274_0"}, {"texts": ["A man wearing a white shirt is tying a black and white neck tie around his neck."], "durations": null, "exact_frames_per_prompt": [80], "background": "In the background, the man is speaking and there is a light brown wall.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/3I5Nf4z4XrU.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_dense_val_279_0"}, {"texts": ["A kid wearing a red t-shirt is sitting on the tile floor while the boy in grey t-shirt sits back side of him.", "The kid is eating something from a plastic cup with the help of a spoon along with grey t-shirt kid."], "durations": null, "exact_frames_per_prompt": [19, 61], "background": "In the background, there is a tiled floor, stairs, a cabinet, and people speaking sounds are audible.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/0fCDlKYkRxc.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_dense_val_282_0"}, {"texts": ["A kid behind the first kid is also eating something from the plastic cup with the help of a spoon."], "durations": null, "exact_frames_per_prompt": [80], "background": "In the background, there is a tiled floor, stairs, a cabinet, and people speaking sounds are audible.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/0fCDlKYkRxc.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_dense_val_282_1"}, {"texts": ["A woman wearing a black top is sitting, holding and knotting a tie on the table.\n"], "durations": null, "exact_frames_per_prompt": [80], "background": "In the background, there are white curtains, a table with a white table cloth, a blue surface, a window, and the sound of music is audible.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/-0byZyStAYQ.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_dense_val_283_0"}, {"texts": ["A man wearing a white t-shirt is holding a brown-yellow snake wrapped in his hand and showing it."], "durations": null, "exact_frames_per_prompt": [80], "background": "In the background, there is a white wall, white ceiling, graphic white wall, photo frames, a white object and the sound of the music is audible.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/02L6JyleSLE.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_dense_val_284_0"}, {"texts": ["A brown-yellow snake is wrapped in the hand of the man.", "The snake is crawling towards the bottom.  while the man is moving his hands."], "durations": null, "exact_frames_per_prompt": [23, 57], "background": "In the background, there is a white wall, white ceiling, graphic white wall, photo frames, a white object and the sound of the music is audible.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/02L6JyleSLE.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_dense_val_284_1"}, {"texts": ["A man wearing a white t-shirt is holding a yellow-orange snake in his hands."], "durations": null, "exact_frames_per_prompt": [80], "background": "In the background, there is a white designer wall, picture frames, and some music is audible.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/02L6JyleSLE.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_dense_val_285_0"}, {"texts": ["A yellow-orange snake is held by the man, and the snake is moving in his hands."], "durations": null, "exact_frames_per_prompt": [80], "background": "In the background, there is a white designer wall, picture frames, and some music is audible.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/02L6JyleSLE.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_dense_val_285_1"}, {"texts": ["A woman wearing a purple pink top is sitting in the back and milking a statue cow then a man wearing a white t-shirt is sitting and milking a statue cow and another man man is standing on the the right side and watching the the first man and the woman."], "durations": null, "exact_frames_per_prompt": [80], "background": "In the background, people are speaking. There is a green surface, a gray surface, a silver bucket, and a black-white statue cow.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/-0-s1eu4sF4.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_dense_val_28_0"}, {"texts": ["A man wearing a white t-shirt is sitting and watching the woman while another man whose head is visible is watching the man.", "The man is milking the statue cow and the woman stands up and puts her hand on the man's shoulder and another man moves aside."], "durations": null, "exact_frames_per_prompt": [53, 27], "background": "In the background, people are speaking. There is a green surface, a gray surface, a silver bucket, and a black-white statue cow.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/-0-s1eu4sF4.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_dense_val_28_1"}, {"texts": ["A man wearing a black t-shirt is standing in the front and watching the other two people."], "durations": null, "exact_frames_per_prompt": [70], "background": "In the background, people are speaking. There is a green surface, a gray surface, a silver bucket, and a black-white statue cow.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/-0-s1eu4sF4.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_dense_val_28_2"}, {"texts": ["A person wearing orange clothes is holding a brown fish with a fish landing net."], "durations": null, "exact_frames_per_prompt": [81], "background": "In the background, there is a fish landing net, water, pebbles, and the voice of a person speaking and some miscellaneous sounds are audible.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/-2yTc7WxGMM.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_dense_val_290_0"}, {"texts": ["A brown fish is moving in the fish landing net while a person wearing an orange outfit is holding the fish landing net."], "durations": null, "exact_frames_per_prompt": [81], "background": "In the background, there is a fish landing net, water, pebbles, and the voice of a person speaking and some miscellaneous sounds are audible.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/-2yTc7WxGMM.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_dense_val_290_1"}, {"texts": ["A brown fish is moving in the fish landing net."], "durations": null, "exact_frames_per_prompt": [81], "background": "In the background, there is a fish landing net, water, rocks, and the voice of a person speaking is audible.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/-2yTc7WxGMM.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_dense_val_291_1"}, {"texts": ["A man wearing a white-red shirt is standing behind a counter and doing flair."], "durations": null, "exact_frames_per_prompt": [80], "background": "In the background, there is a black counter-top a camera, a mobile phone, glasses, a transparent glass wall, a monitor, a printer, lights, and the people cheering and music playing sound is audible.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/1-qCqfG-iMg.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_dense_val_293_0"}, {"texts": ["A woman wearing a multi-color t-shirt is sitting in the back and milking a statue cow while the other men are seeing towards her."], "durations": null, "exact_frames_per_prompt": [47], "background": "In the background, miscellaneous sounds and people are audible. There is a green carpet, a gray surface, a silver bucket, and a black-white statue cow.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/-0-s1eu4sF4.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_dense_val_29_0"}, {"texts": ["A man wearing a white t-shirt is sitting in the middle while a woman wearing a printed top is milking a statue cow, another man wearing a black T-shirt is standing and leaning towards the woman and the man.", "The man wearing a white t-shirt starts milking the statue cow while a woman wearing printed top and a man wearing black t-shir move backward."], "durations": null, "exact_frames_per_prompt": [54, 26], "background": "In the background, miscellaneous sounds and people are audible. There is a green carpet, a gray surface, a silver bucket, and a black-white statue cow.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/-0-s1eu4sF4.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_dense_val_29_1"}, {"texts": ["A man wearing a black t-shirt is standing and watching the other two people."], "durations": null, "exact_frames_per_prompt": [71], "background": "In the background, miscellaneous sounds and people are audible. There is a green carpet, a gray surface, a silver bucket, and a black-white statue cow.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/-0-s1eu4sF4.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_dense_val_29_2"}, {"texts": ["A man wearing a t-shirt is standing while holding bottles in his left hand and then moves his right hand while holding a glass.", "The man puts the glass.", "The man is pouring a drink into the glass with the bottles.", "The man puts the bottles on the table.", "The man throws up the glass from his right hand to left hand."], "durations": null, "exact_frames_per_prompt": [18, 10, 13, 10, 27], "background": "In the background, there are posters, a table with some stuff, and a digital clock, and the sound of music is audible.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/-M3_7e_exd8.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_dense_val_32_0"}, {"texts": ["A baby is lying on the floor near a couch and crying."], "durations": null, "exact_frames_per_prompt": [71], "background": "In the background, there is a couch, a footrest, a wardrobe, and a baby's crying sound and people laughing sound is audible.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/-1Te0BM0oU8.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_dense_val_38_0"}, {"texts": ["A lady wearing a black and white cloth is standing and trying to open a bottle with a piece of cloth then the man in a black outfit is holding the bottle.", "The lady is giving the bottle to a man."], "durations": null, "exact_frames_per_prompt": [47, 10], "background": "in the background, there is a brown ceiling, brown walls, a kitchen with utensils and some other stuff, and people speaking voices are audible.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/-4szNHl4P0c.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_dense_val_39_0"}, {"texts": ["A man wearing black clothes is standing and taking the bottle from the lady while a woman wearing grey outfits standing on the right side holding the towel and talking something"], "durations": null, "exact_frames_per_prompt": [58], "background": "in the background, there is a brown ceiling, brown walls, a kitchen with utensils and some other stuff, and people speaking voices are audible.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/-4szNHl4P0c.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_dense_val_39_1"}, {"texts": ["A man wearing black-white clothes is speaking and presenting the monsoon forecast."], "durations": null, "exact_frames_per_prompt": [48], "background": "In the background, man is speaking, there is a blue screen with monsoon forecast.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/0KRZpDL6rWQ.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_dense_val_42_0"}, {"texts": ["A person whose only hands are visible is tearing papers in front of a baby and making him laugh."], "durations": null, "exact_frames_per_prompt": [80], "background": "In the background, there is a rug, toys, a baby walker, a door, a book, papers, and a person and the babies laughing sounds are audible.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/2Vzh7XO912Q.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_dense_val_48_0"}, {"texts": ["A baby wearing a diaper is sitting on the rug, holding a piece of paper and playing with it, and laughing when the person tears the papers."], "durations": null, "exact_frames_per_prompt": [80], "background": "In the background, there is a rug, toys, a baby walker, a door, a book, papers, and a person and the babies laughing sounds are audible.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/2Vzh7XO912Q.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_dense_val_48_1"}, {"texts": ["A man wearing a black t-shirt and hat is sitting in front and drinking from a green-grey jar.", "The man stops and cleans his mouth with his hand."], "durations": null, "exact_frames_per_prompt": [41, 39], "background": "In the background, there is white wall, a door, a jar, a black hat, a black object, drinking sound is audible and a man is speaking.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/-A7PrCTRvOY.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_dense_val_51_0"}, {"texts": ["A man wearing a t-shirt is sitting and tightening a bolt of the vehicle.\n"], "durations": null, "exact_frames_per_prompt": [80], "background": "In the background, there are trees, a white vehicle, a road surface, a sky, and a tire, and the voice of people speaking and laughing is audible.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/2-M5TagOHMw.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_dense_val_52_0"}, {"texts": ["A person wearing red upper and black lower is standing and holding a food and rolling it.", "The person is dropping it in a white powder."], "durations": null, "exact_frames_per_prompt": [45, 35], "background": "In the background, music is playing and a person is speaking, there is a gray-black countertop, white plate, food, and white wall.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/-3LxDH4ZusU.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_dense_val_53_0"}, {"texts": ["A man wearing a printed black t-shirt standing on a brown surface is speaking on a mic while moving his left hand."], "durations": null, "exact_frames_per_prompt": [80], "background": "In the background, there is a brown surface, trees, a blue sky, a speaker, a house, and people's voices are audible.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/5U0O1P6qgDI.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_dense_val_56_0"}, {"texts": ["A person wearing blue jeans is sitting on the left side of the brown surface, holding a scissor and cutting the bandage and an animal's leg is visible, which is bandaged.", "The person is pasting it on the animal's leg while another person whose hand is visible is holding the animal's leg."], "durations": null, "exact_frames_per_prompt": [23, 23], "background": "In the background, there is a brown surface, a green blanket, bandage, scissors, and a woman is speaking.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/3nTMc2nK_W8.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_dense_val_57_0"}, {"texts": ["An animal whose leg is only visible is held by the second person and pasting a bandage while a person on the left side is sitting and cutting the bandage with a scissor."], "durations": null, "exact_frames_per_prompt": [80], "background": "In the background, there is a brown surface, a green blanket, bandage, scissors, and a woman is speaking.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/3nTMc2nK_W8.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_dense_val_57_2"}, {"texts": ["A person wearing a blue t-shirt is riding on a camel and looking here and there while another person wearing a white t-shirt is also riding on a camel, and a group of people are standing and walking at the back."], "durations": null, "exact_frames_per_prompt": [80], "background": "In the background, there is a black sky, a gray surface, and building structures, and the voice of people speaking is audible.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/-5fqwdtpSOY.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_dense_val_59_0"}, {"texts": ["A person wearing white clothes is riding a camel in the front while another person wearing a blue t-shirt is also riding the camel, and a group of people are standing and walking at the back."], "durations": null, "exact_frames_per_prompt": [80], "background": "In the background, there is a black sky, a gray surface, and building structures, and the voice of people speaking is audible.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/-5fqwdtpSOY.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_dense_val_59_1"}, {"texts": ["A camel is walking on the gray surface and carrying people on its back."], "durations": null, "exact_frames_per_prompt": [80], "background": "In the background, there is a black sky, a gray surface, and building structures, and the voice of people speaking is audible.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/-5fqwdtpSOY.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_dense_val_59_2"}, {"texts": ["A man wearing a green shirt is standing and holding a baby and man's finger is locked by the baby."], "durations": null, "exact_frames_per_prompt": [31], "background": "In the background there is dry grass surface, brown wall, and people speaking sound is audible in the background.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/1168w8ZwiVk.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_dense_val_60_1"}, {"texts": ["A white goat is standing on the dry grass surface and touched by two people while a kid wearing a pink dress is moving and standing on the dry grass.", "The goat's back is brushed by a kid."], "durations": null, "exact_frames_per_prompt": [39, 6], "background": "In the background there is dry grass surface, brown wall, and people speaking sound is audible in the background.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/1168w8ZwiVk.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_dense_val_60_4"}, {"texts": ["A man whose hands are visible lifts the lid of the sandwich maker.", "The man uses a spatula to remove the burger from the sandwich maker."], "durations": null, "exact_frames_per_prompt": [27, 53], "background": "In the background, a man is speaking. There is a tile wall, a gray counter, a spatula, a sandwich maker, and a kitchen mat.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/0dUkWf8gBkU.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_dense_val_61_0"}, {"texts": ["A person whose hands are visible is opening the upper section of a burger maker.", "The person picks up a burger using his hands and a spatula.", "The person drags the burger machine aside and starts placing the burger on the surface."], "durations": null, "exact_frames_per_prompt": [15, 51, 14], "background": "In the background, there is a burger making machine, a burger, a yellow cloth, a grey counter-top, a tile wall, and a person's voice is audible.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/0dUkWf8gBkU.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_dense_val_62_0"}, {"texts": ["A man wearing a graphic black t-shirt is peeling an apple on the apple peeler corer machine and making faces."], "durations": null, "exact_frames_per_prompt": [70], "background": "In the background, there is an apple peeler corer machine, a peeled apple, peel, white cupboard, a toy owl, white window, gray wall and some miscellaneous sounds are audible.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/-aLdPxMzGhg.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_dense_val_64_0"}, {"texts": ["A woman wearing a blue checked shirt is standing and writing on a white board with a marker.", "The woman turns and starts speaking."], "durations": null, "exact_frames_per_prompt": [44, 36], "background": "In the background, a woman is speaking. There is a white board with something written on it, and a marker.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/-HutuMqTAPw.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_dense_val_65_0"}, {"texts": ["A woman wearing a blue patterned shirt is standing and writing on a white board  with a marker.", "The woman turns and then starts speaking."], "durations": null, "exact_frames_per_prompt": [15, 65], "background": "In the background, miscellaneous sounds and woman speaking are audible, there is a white board, and a marker.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/-HutuMqTAPw.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_dense_val_66_0"}, {"texts": ["A man wearing a red t-shirt, is using a funny face filter with huge eyeballs, and  speaking", "The man wearing a red t-shirt starts eating a chip very fast in a funny way."], "durations": null, "exact_frames_per_prompt": [38, 42], "background": "In the background, there is a yellow wall, lights, a towel hanger with a towel, some other stuff, and the man is speaking and some miscellaneous sounds are audible.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/-Cfmw6L4cI0.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_dense_val_67_0"}, {"texts": ["An old man wearing a dark blue t-shirt is sitting and looking at the kids."], "durations": null, "exact_frames_per_prompt": [73], "background": "In the background, people are speaking and laughing. There is a table with a tablecloth, brown chairs, a brown wall, two white objects, two white plates, two white spoons, a tissue paper, a fruit cake, a curtain and a window.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/2mFclrl0wo8.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_dense_val_78_0"}, {"texts": ["A boy sitting besides person one is eating cake from the white plate while another kid on the right side is also  eating cake from the white plate.."], "durations": null, "exact_frames_per_prompt": [73], "background": "In the background, people are speaking and laughing. There is a table with a tablecloth, brown chairs, a brown wall, two white objects, two white plates, two white spoons, a tissue paper, a fruit cake, a curtain and a window.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/2mFclrl0wo8.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_dense_val_78_1"}, {"texts": ["A girl sitting besides person two is also eating the cake from another white plate.\n while an old man on the left side in a blue t-shirt is sitting."], "durations": null, "exact_frames_per_prompt": [56], "background": "In the background, people are speaking and laughing. There is a table with a tablecloth, brown chairs, a brown wall, two white objects, two white plates, two white spoons, a tissue paper, a fruit cake, a curtain and a window.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/2mFclrl0wo8.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_dense_val_78_2"}, {"texts": ["A man wearing a blue t-shirt is sitting on the left while a boy and a girl is sitting on the chair and eating food."], "durations": null, "exact_frames_per_prompt": [29], "background": "In the background, people and miscellaneous sounds are audible. There is a table, a cake, white plates, food, forks, a white curtain, chairs, a window, and a wooden wall.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/2mFclrl0wo8.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_dense_val_79_0"}, {"texts": ["A boy wearing a red-black t-shirt is sitting on a chair and eating food while an old man is looking at him and a girl is sitting on the left is also eating food."], "durations": null, "exact_frames_per_prompt": [73], "background": "In the background, people and miscellaneous sounds are audible. There is a table, a cake, white plates, food, forks, a white curtain, chairs, a window, and a wooden wall.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/2mFclrl0wo8.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_dense_val_79_1"}, {"texts": ["A girl wearing a white t-shirt is sitting on a chair and eating her food.\n and a boy wearing blue t-shirt is sitting on her right is also eating food while the old man wearing blue t-shirt is watching them"], "durations": null, "exact_frames_per_prompt": [62], "background": "In the background, people and miscellaneous sounds are audible. There is a table, a cake, white plates, food, forks, a white curtain, chairs, a window, and a wooden wall.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/2mFclrl0wo8.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_dense_val_79_2"}, {"texts": ["A boy is sitting beside old man, is eating cake from the white plate along with the girl."], "durations": null, "exact_frames_per_prompt": [73], "background": "In the background, people are speaking and laughing. There is a table with tablecloth, brown chairs, a brown wall, two white object, two white plate, two white spoon, a fruit cake, a curtain and a window.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/2mFclrl0wo8.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_dense_val_80_1"}, {"texts": ["A girl is sitting beside the boy, is also eating cake from the white plate while an old man wearing a grey t-shirt is sitting on the left side."], "durations": null, "exact_frames_per_prompt": [60], "background": "In the background, people are speaking and laughing. There is a table with tablecloth, brown chairs, a brown wall, two white object, two white plate, two white spoon, a fruit cake, a curtain and a window.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/2mFclrl0wo8.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_dense_val_80_2"}, {"texts": ["A boy wearing a red-blue-white t-shirt is sitting on a wooden chair and eating cake with a fork from a white plate.\n while a girl wearing a white t-shirt is sitting on a wooden chair and eating cake with a fork from a white plate, and a man wearing a blue t-shirt is sitting on the left side."], "durations": null, "exact_frames_per_prompt": [73], "background": "In the background, people are speaking, there is a white table, white plates with pieces of cake, a brown wall, white curtains, wooden chairs, a box, and a cake.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/2mFclrl0wo8.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_dense_val_81_0"}, {"texts": ["A girl wearing a white printed t-shirt is sitting on a wooden chair and eating cake from a white plate with a fork while the boy beside wearing black, red and white t-shirt eating the food"], "durations": null, "exact_frames_per_prompt": [62], "background": "In the background, people are speaking, there is a white table, white plates with pieces of cake, a brown wall, white curtains, wooden chairs, a box, and a cake.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/2mFclrl0wo8.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_dense_val_81_1"}, {"texts": ["A man wearing a blue t-shirt is sitting on the left side while two kids on the right are sitting and eating cake."], "durations": null, "exact_frames_per_prompt": [28], "background": "In the background, people are speaking, there is a white table, white plates with pieces of cake, a brown wall, white curtains, wooden chairs, a box, and a cake.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/2mFclrl0wo8.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_dense_val_81_2"}, {"texts": ["A boy wearing a green t-shirt is holding a white glass with some brown liquid in it while the boy in black t-shirt drink the liquid and turn back.", "The boy starts drinking the liquid from the glass."], "durations": null, "exact_frames_per_prompt": [54, 26], "background": "In the background, there is a grey surface, tables, chairs, a beige wall, and people's voices are audible.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/15FiZ48tTUU.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_dense_val_85_0"}, {"texts": ["A boy wearing a navy blue t-shirt is moving on a grey surface behind the first boy while a boy wearing grey t-shirt is sitting on the table is holding a white glass and another boy is standing on the left."], "durations": null, "exact_frames_per_prompt": [72], "background": "In the background, there is a grey surface, tables, chairs, a beige wall, and people's voices are audible.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/15FiZ48tTUU.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_dense_val_85_1"}, {"texts": ["A person is straightening the white wrapping paper.\n"], "durations": null, "exact_frames_per_prompt": [50], "background": "In the background there is brown table, chairs, a blue box, white wrapping paper, grey wall, and person speaking sound is audible in the background.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/-0WZKTu0xNk.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_dense_val_88_0"}, {"texts": ["A person wearing a black t-shirt is moving.", "The person wearing a black t-shirt is giving the poly bag to the first person."], "durations": null, "exact_frames_per_prompt": [45, 35], "background": "In the background there is brown door, white wall, mince grinder machine, a person speaking sound and music is audible in the background.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/-hFa8jTSaJw.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_dense_val_99_1"}, {"texts": ["A person on the right side is standing on a white surface and eating something with his hand.\n and the man wearing white t-shirt facing the first man along with the man wearing black t-shirt who is dancing"], "durations": null, "exact_frames_per_prompt": [80], "background": "In the background there is a curtain, a door, shoes, some beds, a white surface, yellow wall, hanging objects and a person's screaming and people's voices are audible.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/13-k56ie2wQ.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_1002_0"}, {"texts": ["A person wearing a white t-shirt is chewing something, and standing on a white surface and two are also chewing something, and one of them is jumping while chewing it on a white surface."], "durations": null, "exact_frames_per_prompt": [80], "background": "In the background there is a curtain, a door, shoes, some beds, a white surface, yellow wall, hanging objects and a person's screaming and people's voices are audible.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/13-k56ie2wQ.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_1002_1"}, {"texts": ["A person wearing a black t-shirt is moving his hands, and standing on a white surface while a man wearing a white t-shirt is standing beside the person and chewing something, another man is standing on the right and eating something, and another person is standing at the back holding the door."], "durations": null, "exact_frames_per_prompt": [80], "background": "In the background there is a curtain, a door, shoes, some beds, a white surface, yellow wall, hanging objects and a person's screaming and people's voices are audible.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/13-k56ie2wQ.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_1002_2"}, {"texts": ["A girl wearing a grey t-shirt is feeding milk to the goats with a baby feeding bottle while a group of people are walking in different directions on the grey road and a group of goats are standing on the soil surface and a wooden platform."], "durations": null, "exact_frames_per_prompt": [80], "background": "In the background, people are speaking, there is a brown surface, a grey-brown surface, a yellow-white building, red barriers, wired barriers, trees, a grey container, a wooden platform.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/0-9PkPLBeB4.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_1003_0"}, {"texts": ["A black goat on the right is drinking milk with a baby feeding bottle while some other goats are also standing near the fence and some people are walking in the back."], "durations": null, "exact_frames_per_prompt": [31], "background": "In the background, people are speaking, there is a brown surface, a grey-brown surface, a yellow-white building, red barriers, wired barriers, trees, a grey container, a wooden platform.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/0-9PkPLBeB4.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_1003_2"}, {"texts": ["A woman whose hand is visible is putting a tea maker on the blue cup.", "The woman is pointing and touching her finger on the jar."], "durations": null, "exact_frames_per_prompt": [25, 17], "background": "In the background, there is a light brown counter-top, a tea maker, a white wire, a red object, a blue cup, and the woman speaking sound is audible.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/01WVMe1Vrsw.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_1004_0"}, {"texts": ["A girl wearing a black coat is standing, holding a peeled pineapple.", "The girl is cutting the pineapple with a knife."], "durations": null, "exact_frames_per_prompt": [26, 54], "background": "In the background, there is a soil surface, green plants, green grass, tree roots, bunch of sticks, wooden blocks, letters, a pineapple, a knife, and people speaking and some miscellaneous sounds are audible.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/-rvS_b-OUpo.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_1005_0"}, {"texts": ["A baby wearing a white dress with a blue and white teddy picture on it and lying on the white-brown clothes."], "durations": null, "exact_frames_per_prompt": [80], "background": "In the background, there are white-brown clothes, a lady is speaking, and the babies laughing voice is coming.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/0XKR4zE_cdQ.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_1010_0"}, {"texts": ["A girl wearing a black dress is standing in front and speaking."], "durations": null, "exact_frames_per_prompt": [64], "background": "In the background there is a blue graphic wall and something is written using graphics and a girl is speaking.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/1CbmYck1MO4.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_1011_0"}, {"texts": ["A boy wearing a blue school uniform is sitting and reading a newspaper."], "durations": null, "exact_frames_per_prompt": [80], "background": "In the background, there is a green wall, photo frames, electrical fitting, newspapers, brown door, book holder stands, books, and the boy's voice is audible.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/5FxmDqFj-oA.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_1017_0"}, {"texts": ["A man wearing a black t-shirt is standing.", "The man wearing a black t-shirt is writing on the white board with a black pen."], "durations": null, "exact_frames_per_prompt": [23, 57], "background": "In the background, the man is speaking, there is a white board and a black pen.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/-Jr4hagX4fY.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_1022_0"}, {"texts": ["A girl wearing a pink top is sitting on a chair.", "The girl is eating a donut from a white black plate."], "durations": null, "exact_frames_per_prompt": [19, 60], "background": "In the background, a woman and a girl are speaking. There is a white wall, white curtains, a white table, a black chairs white and black plate with a donut in it, a white object, a light brown floor and a black surface.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/16CDSpDlU1c.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_1023_0"}, {"texts": ["A person wearing black t-shirt and pants is leaning forward holding a lug wrench.", "The person attached the wrench into a car stepney nut."], "durations": null, "exact_frames_per_prompt": [50, 30], "background": "In the background, there is a car, a lug wrench, sunlight and a person's voice is audible.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/4QE6LGVB30A.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_1025_0"}, {"texts": ["A girl wearing a blue-white t-shirt and blue shorts is standing on the left side while holding a hand mixer machine in her right hand and mixing the yellow paste with a hand mixer machine in the bowl."], "durations": null, "exact_frames_per_prompt": [80], "background": "In the background, children are laughing; there is the sound of the hand mixer machine, a stainless steel bowl with the yellow paste, boxes and bottles on the counter-top, another bowl on the counter-top, a white countertop, and a hand mixer machine.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/2aEDSBdE-c0.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_1026_0"}, {"texts": ["A person in greyish-white clothing is preparing the sushi roll."], "durations": null, "exact_frames_per_prompt": [80], "background": "In the background, there is a sushi roll, meat, wooden wall, and the platform.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/-qNOmqblmyQ.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_1033_0"}, {"texts": ["A baby wearing a white t-shirt is standing in a baby bed and watching the first person and laughing."], "durations": null, "exact_frames_per_prompt": [80], "background": "In the background, the baby is laughing, the tearing sound of paper is audible and people are speaking. There is a baby bed, a white paper, a white wall, and a multi colored cloth.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/-tuKSaADF3c.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_1036_0"}, {"texts": ["A person whose hands are visible is tearing the paper and a baby is laughing while looking here and there."], "durations": null, "exact_frames_per_prompt": [80], "background": "In the background, the baby is laughing, the tearing sound of paper is audible and people are speaking. There is a baby bed, a white paper, a white wall, and a multi colored cloth.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/-tuKSaADF3c.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_1036_1"}, {"texts": ["A man on the left side wearing a green jacket is sitting on his knee holding a fish accompanied by the man wearing red jacket", "The man is holding a drone in his hands and speaking and the man wearing red jacket is holding the fish"], "durations": null, "exact_frames_per_prompt": [44, 36], "background": "In the background, there is a snow surface, a camera with a stand, a drone, some equipment, dried trees, a fish lure, a scissor, white sky and the voice of the people and some music is audible.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/0lxip_X2LmY.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_1037_0"}, {"texts": ["A gray fish is held by the first man while the man in red hoodie watches the fish.", "The fish lure wire is cut in her mouth.", "The fish is lifted up by the second man while the other man picks up the drone."], "durations": null, "exact_frames_per_prompt": [21, 24, 35], "background": "In the background, there is a snow surface, a camera with a stand, a drone, some equipment, dried trees, a fish lure, a scissor, white sky and the voice of the people and some music is audible.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/0lxip_X2LmY.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_1037_2"}, {"texts": ["A man wearing a white shirt and black pants is playing golf.\n"], "durations": null, "exact_frames_per_prompt": [43], "background": "In the background, there is a grass mat, green flooring, green ground, brown surface, a golf club, a ball, and the sound of a golf club hitting the ball is audible.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/-mmRPduUwZg.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_1040_0"}, {"texts": ["A man wearing a black suit is standing in front of the digital screen.", "The man wearing a black suit is turning towards the screen, speaking and moving his hand."], "durations": null, "exact_frames_per_prompt": [63, 12], "background": "In the background, there is a digital screen, music is playing and a man's voice is audible.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/-do4D3M20aM.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_1041_0"}, {"texts": ["A woman wearing a top is standing, pouring the egg into the pan, and then starts mixing it."], "durations": null, "exact_frames_per_prompt": [80], "background": "In the background, there are wooden cabinets, a counter-top with some stuff, a wooden spatula, a black pan, a white wall, a gas stove, wooden cardboard, a bowl, containers, a microwave, and the voice of a woman speaking, and some miscellaneous sounds are audible.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/4p3zFOelr4Q.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_1045_0"}, {"texts": ["A woman wearing black clothes is cleaning a steel pan with a tissue wipe."], "durations": null, "exact_frames_per_prompt": [26], "background": "In the background there are gas stoves, some pans, bowls, white walls, some bottles, some clothes, a beige counter-top, meat loaves and a person's voice and some miscellaneous sounds are audible.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/-BptOURMao8.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_1049_1_ms"}, {"texts": ["A woman on the extreme right is standing with the first woman.", "The woman is showing a victory gesture."], "durations": null, "exact_frames_per_prompt": [20, 5], "background": "In the background there are gas stoves, some pans, bowls, white walls, some bottles, some clothes, a beige counter-top, meat loaves and a person's voice and some miscellaneous sounds are audible.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/-BptOURMao8.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_1049_2_ms"}, {"texts": ["A man wearing blue red clothes is standing and holding a cycle rim.", "And then showing a cycle tube wall."], "durations": null, "exact_frames_per_prompt": [42, 38], "background": "In the background, music is playing. There are cycles, cycle rims, tripods, a blue bag, a small container, a red wall, black surface, a black wooden wall, a bike, and some other objects.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/2OSUhsn1klQ.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_104_0"}, {"texts": ["A woman wearing a blue top and dark blue jeans is sitting and checking the dog from ears to mouth."], "durations": null, "exact_frames_per_prompt": [80], "background": "In the background, there is a gray wall with a painting, yellow walls white pillar, wooden objects, white surface and the voices of the people are audible.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/1stnFgiop8U.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_1052_0"}, {"texts": ["A black dog is sitting and getting his face, ears and mouth checked by the woman."], "durations": null, "exact_frames_per_prompt": [80], "background": "In the background, there is a gray wall with a painting, yellow walls white pillar, wooden objects, white surface and the voices of the people are audible.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/1stnFgiop8U.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_1052_1"}, {"texts": ["A woman wearing a blue top is sitting on the floor and checking a dog."], "durations": null, "exact_frames_per_prompt": [80], "background": "In the background, two women are audible. There is a brown wooden furniture, a yellow wall, a window, a black surface, and a beige floor.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/1stnFgiop8U.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_1053_0"}, {"texts": ["A black dog is sitting and getting checked by a woman."], "durations": null, "exact_frames_per_prompt": [80], "background": "In the background, two women are audible. There is a brown wooden furniture, a yellow wall, a window, a black surface, and a beige floor.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/1stnFgiop8U.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_1053_1"}, {"texts": ["A white and black horse is standing on a gray surface while the woman in blue jeans tries to tie some cloth on thee leg of the horse"], "durations": null, "exact_frames_per_prompt": [80], "background": "In the background, the girl is speaking, birds chirping sounds are audible. There is a gray surface, a wooden and metal grill stable, ropes with carabiners, green clothes, a red cloth and lights.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/1g_AV6ocuAY.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_1055_0"}, {"texts": ["A girl wearing a black top is sitting on a gray floor.", "The girl wearing a black top wrapping a white cloth in a horse leg."], "durations": null, "exact_frames_per_prompt": [13, 67], "background": "In the background, the girl is speaking, birds chirping sounds are audible. There is a gray surface, a wooden and metal grill stable, ropes with carabiners, green clothes, a red cloth and lights.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/1g_AV6ocuAY.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_1055_2"}, {"texts": ["A woman wearing a black t-shirt is sitting on the brown surface a white-black horse is standing on the brown surface.", "The woman is rolling a white cloth on the leg of the horse."], "durations": null, "exact_frames_per_prompt": [13, 67], "background": "In the background, there are white railings, brown ceiling, white light, brown wall, green mats, a red mat, ropes, brown surface, a woman is speaking and birds chirping sounds are audible.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/1g_AV6ocuAY.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_1056_0"}, {"texts": ["A black-white horse is standing, and the leash of the horse is tied on the white railing while a woman wearing a black top is sitting and is wrapping a white object around the horse's leg."], "durations": null, "exact_frames_per_prompt": [80], "background": "In the background, there are white railings, brown ceiling, white light, brown wall, green mats, a red mat, ropes, brown surface, a woman is speaking and birds chirping sounds are audible.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/1g_AV6ocuAY.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_1056_1"}, {"texts": ["A man wearing white clothes is standing and holding a pineapple.", "The man is cutting a pineapple with a knife."], "durations": null, "exact_frames_per_prompt": [39, 42], "background": "In the background, there are utensils, cardboard, a knife, walls, a counter top, and the voice of a man speaking, and some miscellaneous sounds are audible.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/1jgnab5xbhI.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_105_0"}, {"texts": ["A person whom hands are visible wearing a black glove in the right hand while holding a knife in the right hand and holding a potato in the left hand, is peeling the potato with the knife.", "The person whom hands are visible puts knife on the table.", "The person whom hands are visible puts aside the potato peel."], "durations": null, "exact_frames_per_prompt": [58, 10, 12], "background": "In the background, there is the sound of a fan, the sound of putting a knife on the table, a potato, and a remote, a knife and potato peel on the table, and a white sheet-covered table.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/5Tq5X4A9ahM.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_1062_0"}, {"texts": ["A man wearing a shirt is standing on the right side.", "The man is going towards the left side."], "durations": null, "exact_frames_per_prompt": [16, 18], "background": "In the background, there is a milking machine, blue surface, red metal frames, metal railing, a man is speaking, and some miscellaneous sounds are audible.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/-ptMoBcBe7Y.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_1067_0"}, {"texts": ["A brown cow is standing on the blue surface getting milked by the milking machine while a man wearing a red-white checked shirt stands up and walks away toward the left."], "durations": null, "exact_frames_per_prompt": [81], "background": "In the background, there is a milking machine, blue surface, red metal frames, metal railing, a man is speaking, and some miscellaneous sounds are audible.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/-ptMoBcBe7Y.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_1067_1"}, {"texts": ["A woman on the left side is folding a yellow t-shirt with a cardboard and then she puts the t-shirt on the cardboard."], "durations": null, "exact_frames_per_prompt": [45], "background": "In the background, there is a cardboard, a yellow printed t-shirt, floor, and the floor mat.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/21XG9zZZqDU.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_1074_0"}, {"texts": ["A person whose hands are visible is folding a green top using brown cardboard on a gray surface."], "durations": null, "exact_frames_per_prompt": [32], "background": "In the background, people are speaking and miscellaneous sounds are audible. There is a gray surface, a black bed, a brown cardboard, a black cloth, a blue and white box, a blue book and a green top.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/21XG9zZZqDU.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_1075_0"}, {"texts": ["A woman whose only hands are visible is folding a green t-shirt using cardboard.", "The woman flips the t-shirt on the cardboard."], "durations": null, "exact_frames_per_prompt": [21, 24], "background": "In the background, there is a green printed t-shirt, a cardboard, marble floor, a mat, a black bed, a white-blue object, and the voice of a woman is audible.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/21XG9zZZqDU.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_1076_0"}, {"texts": ["A woman wearing a blue top is standing behind the countertop.", "The woman wearing a blue top is chopping the watermelon with the help of the knife."], "durations": null, "exact_frames_per_prompt": [38, 42], "background": "In the background, there is brown wooden shelves, a white refrigerator with colourful stickers, a white chopping board, a grey counter-top, small brown-black objects, and the people speaking sound is audible.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/9-nZy24oAr4.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_1080_0"}, {"texts": ["A girl wearing a blue top is sitting on the bed and is crying."], "durations": null, "exact_frames_per_prompt": [80], "background": "In the background, music is playing and the girl is audible. There is a brown bed, a floral bed sheet, white walls, a wooden wall frame and blue curtains.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/01Sy8Hh6i-s.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_1087_0"}, {"texts": ["A woman wearing dark blue pants is holding some papers and moving on a grey surface inside a room."], "durations": null, "exact_frames_per_prompt": [80], "background": "In the background there are yellow walls, a brown gate, cubicles, a brown notice board, a glass wall, a mic and a woman's voice is audible.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/5bTbbfEHypQ.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_1090_0"}, {"texts": ["A man wearing a white t-shirt and black pants is standing and drinking from a glass while a man wearing a black jacket is standing and drinking beer from a beer bottle, and another man wearing a black t-shirt is standing at the back near the bar."], "durations": null, "exact_frames_per_prompt": [44], "background": "In the background, there is a bar, a table, lights, bottles, glass hanging rack, brick walls, wall decors, counters, white walls, switch boards, black objects, a window, wall clock, photo frame, a white rack, grey ceiling, and the sound of music playing and the miscellaneous sounds are audible.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/3cOlaoWmXxA.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_1092_0"}, {"texts": ["A man wearing black clothes is standing and drinking from a bottle while the other man wearing grey t-shirt drinks along with him."], "durations": null, "exact_frames_per_prompt": [80], "background": "In the background, there is a bar, a table, lights, bottles, glass hanging rack, brick walls, wall decors, counters, white walls, switch boards, black objects, a window, wall clock, photo frame, a white rack, grey ceiling, and the sound of music playing and the miscellaneous sounds are audible.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/3cOlaoWmXxA.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_1092_1"}, {"texts": ["A woman wearing a gray top is sitting on a brown wooden floor and folding a white cloth."], "durations": null, "exact_frames_per_prompt": [24], "background": "In the background, a person is speaking. There is a brown wooden floor, a black sofa, black chairs, brown doors, a brown object, a table with some stuff on it, poly bags, and a white wall.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/2GviuVuJDiI.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_1093_0"}, {"texts": ["A brown camel is walking while carrying three girls, another camel is standing and people are sitting on it, and a group of people are standing and few are sitting in the audience area."], "durations": null, "exact_frames_per_prompt": [80], "background": "In the background, miscellaneous sounds and people are audible. There is a gray floor, red seats, yellow walls, lights, silver railings, and a black roof with a metal structure.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/0yy1Df51_QE.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_1095_1"}, {"texts": ["A boy wearing a white t-shirt is sitting.", "The boy blowing air on noodles.", "The boy starts eating."], "durations": null, "exact_frames_per_prompt": [14, 11, 55], "background": "In the background, there is a white plate, noodles, breads, a basket, a white cup, a brown wall, a brown chair, a mirror, a fork, a boy's voice and some miscellaneous sound is audible.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/-FYZWo0YmtE.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_1096_0"}, {"texts": ["A man wearing a red t-shirt is standing and making a sushi roll."], "durations": null, "exact_frames_per_prompt": [80], "background": "In the background, miscellaneous sounds are audible. There is a red cutting board, a white surface, a sushi roll mat, sushi roll ingredients, a white wall, and a few miscellaneous things.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/37MLRj0s4ok.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_109_0"}, {"texts": ["A woman wearing black clothes is standing and dancing on a green surface."], "durations": null, "exact_frames_per_prompt": [80], "background": "In the background, music is playing and people are speaking, there is a green surface, green wall and a screen.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/1qNpl-VZjM0.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_10_0"}, {"texts": ["A woman wearing a pink t-shirt and printed pant is holding a baby with the green carrier wrap and and it's pouring the butter on bread", "The woman is putting the bread on the grill sandwich maker machine and then picking up the napkins."], "durations": null, "exact_frames_per_prompt": [39, 41], "background": "In the background, there are breads with butter, a tea kettle, electric gas stove, a white chair, a refrigerator, a baby cot with clothes, and a white plate, a white bowl with butter, a chopping board, a white small box, a camera stand are on the right black counter, and boxes, and other stuff on the left side counter, a sandwich maker machine on the right counter, a clock on the right counter, white curtains hanging on the glass window, white walls and a wooden tile floor.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/0kArTUqGOAI.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_1109_0"}, {"texts": ["A horse is walking in the front while carrying a person on his back, and two other people are moving behind the first horse while sitting on their horses."], "durations": null, "exact_frames_per_prompt": [80], "background": "In the background, the wind sounds are audible, There are trees, white sky, road surface, soil surface and bushes.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/4x6djgqGXp4.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_1120_3"}, {"texts": ["A horse is walking in the middle while a horse is walking in front on a soil surface, another horse is walking at the back on a soil surface, and three people are riding horses."], "durations": null, "exact_frames_per_prompt": [80], "background": "In the background, the wind sounds are audible, There are trees, white sky, road surface, soil surface and bushes.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/4x6djgqGXp4.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_1120_4"}, {"texts": ["A man wearing a blue-orange shirt is standing on the left side with his thumb up while the girl wearing blue top is eating watermelon", "The man goes behind the other girls and does a pose with thumb up while the second and third girl both wearing white top and the second girl throws something", "The man starts putting a green sticker in his mouth second and third girl are eating watermelon"], "durations": null, "exact_frames_per_prompt": [42, 30, 8], "background": "In the background, people are speaking, there is a grey surface, a grey shed, a window, and tables.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/0CV5LDRWwJ8.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_1121_0"}, {"texts": ["A girl wearing a black t-shirt is sitting on the right side while the boy behind wearing blue shirt smiling and showing the sign thumbs up", "The girl wearing a black t-shirt is eating watermelon while the boy walks to the back of another two girls and puts a green and white colored paper in his mouth"], "durations": null, "exact_frames_per_prompt": [22, 58], "background": "In the background, people are speaking, there is a grey surface, a grey shed, a window, and tables.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/0CV5LDRWwJ8.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_1121_1"}, {"texts": ["A man wearing white trousers is standing a man in a black coat and pants enters the stage and receives something from the man on the right in a purple shirt.", "The man is shaking hands with the first man and looking here and there."], "durations": null, "exact_frames_per_prompt": [47, 16], "background": "In the background, there is a green mat, a tree, and a black background, and the voice of a person speaking and the sound of music is audible.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/0z10mPSYfB0.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_1125_2"}, {"texts": ["A man wearing a suit is standing while the other person comes to receive award", "The man is clapping, shaking hands with the first man and looking here and there while others watching him."], "durations": null, "exact_frames_per_prompt": [32, 31], "background": "In the background, there is a green mat, a tree, and a black background, and the voice of a person speaking and the sound of music is audible.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/0z10mPSYfB0.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_1125_3"}, {"texts": ["A baby wearing a white-pink dress is sitting on the gray baby chair and eating food."], "durations": null, "exact_frames_per_prompt": [80], "background": "In the background, there is a blue-green-orange baby walker, wooden floor, windows, red drapes, brown wall, a dark brown pillar like structure, white ceiling, red wall, a blue lamp, a gray baby chair, the voices of the people and a miscellaneous sound is audible.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/3b08kaRvXUc.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_1131_0"}, {"texts": ["A woman wearing black coat is sitting on a chair and speaking on a mike."], "durations": null, "exact_frames_per_prompt": [37], "background": "In the background, there are chairs, mike's, books, and registers, a pen, water bottle, and the glass.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/2jwiuVTwaS0.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_1134_0_ms"}, {"texts": ["A man wearing a black suit and blue tie is speaking on a mike while a group of people sat on either side of him."], "durations": null, "exact_frames_per_prompt": [40], "background": "In the background, there are chairs, mike's, books, and registers, a pen, water bottle, and the glass.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/2jwiuVTwaS0.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_1134_1"}, {"texts": ["A woman wearing a black coat and a pink top is sitting on the wooden chair and speaking on the mic while a group of people are also sitting on wooden chairs."], "durations": null, "exact_frames_per_prompt": [38], "background": "In the background, there is a bottle, glasses, books, open, mics, wooden chairs, wooden tables, wooden wall, white wall, the voice of the woman, the voice of the man and a miscellaneous sound is audible.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/2jwiuVTwaS0.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_1135_0"}, {"texts": ["A man wearing a white shirt, black coat and spectacles is sitting in the middle of another group of people and speaking on the mic."], "durations": null, "exact_frames_per_prompt": [40], "background": "In the background, there is a bottle, glasses, books, open, mics, wooden chairs, wooden tables, wooden wall, white wall, the voice of the woman, the voice of the man and a miscellaneous sound is audible.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/2jwiuVTwaS0.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_1135_2_ms"}, {"texts": ["A group of people, including some who are sitting on the right side of the man, some who are sitting on the left side of the man, and one who is standing away, are watching and listening."], "durations": null, "exact_frames_per_prompt": [41], "background": "In the background, there is a bottle, glasses, books, open, mics, wooden chairs, wooden tables, wooden wall, white wall, the voice of the woman, the voice of the man and a miscellaneous sound is audible.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/2jwiuVTwaS0.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_1135_3_ms"}, {"texts": ["A woman wearing a black coat is sitting in a wooden chair and speaking while the man wearing black coat is also speaking on the mic."], "durations": null, "exact_frames_per_prompt": [80], "background": "In the background, a man and a woman are audible. There are mice, brown wooden tables, wooden chairs, yellow chairs, papers, a water bottle, cups, a few miscellaneous items, a brown wooden wall, and a white wall.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/2jwiuVTwaS0.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_1136_0"}, {"texts": ["A man wearing a black suit is sitting and speaking while other people are also sitting next to him."], "durations": null, "exact_frames_per_prompt": [41], "background": "In the background, a man and a woman are audible. There are mice, brown wooden tables, wooden chairs, yellow chairs, papers, a water bottle, cups, a few miscellaneous items, a brown wooden wall, and a white wall.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/2jwiuVTwaS0.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_1136_1"}, {"texts": ["A group of people is sitting in the back and a person is standing."], "durations": null, "exact_frames_per_prompt": [41], "background": "In the background, a man and a woman are audible. There are mice, brown wooden tables, wooden chairs, yellow chairs, papers, a water bottle, cups, a few miscellaneous items, a brown wooden wall, and a white wall.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/2jwiuVTwaS0.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_1136_2_ms"}, {"texts": ["A person wearing blue clothes is riding on the head of an elephant while a man and a woman are also riding on an elephant."], "durations": null, "exact_frames_per_prompt": [46], "background": "In the background, there are trees and bushes, muddy surface, and the sky.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/0Pb4ghR0COE.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_1138_0"}, {"texts": ["An elephant is walking on the muddy surface behind the other elephant, and giving a ride to the person in blue clothes and the another elephant is walking in the front while carrying people on its back."], "durations": null, "exact_frames_per_prompt": [80], "background": "In the background, there are trees and bushes, muddy surface, and the sky.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/0Pb4ghR0COE.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_1138_2"}, {"texts": ["An elephant who is walking on the head of another elephant is giving a ride to the group of people."], "durations": null, "exact_frames_per_prompt": [80], "background": "In the background, there are trees and bushes, muddy surface, and the sky.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/0Pb4ghR0COE.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_1138_3"}, {"texts": ["A person wearing blue shorts is standing, holding and moving the black tire on the gray surface."], "durations": null, "exact_frames_per_prompt": [80], "background": "In the background, there are black tires, a gray surface, and the voice of people speaking, and some miscellaneous sounds are audible.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/-He1Ivgwyxc.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_1140_0"}, {"texts": ["A person  wearing black clothes is sitting on the couch and petting a black-white cat."], "durations": null, "exact_frames_per_prompt": [80], "background": "In the background, there is a couch, a green wall, a white door, and picture frames.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/-QlQrBZOY0w.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_1141_0"}, {"texts": ["A black-white cat is sitting on the couch and getting petted by a person."], "durations": null, "exact_frames_per_prompt": [80], "background": "In the background, there is a couch, a green wall, a white door, and picture frames.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/-QlQrBZOY0w.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_1141_1"}, {"texts": ["A man wearing a light gray t-shirt is sitting on a brown sofa holding the baby and speaking and the baby wearing a pink shirt is laughing."], "durations": null, "exact_frames_per_prompt": [51], "background": "In the background, the man is speaking, the baby is laughing and the television audio is audible. There is a white wall and brown sofa.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/0GsiopyhDgI.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_1142_0"}, {"texts": ["A baby wearing a light pink cloth is sitting on the man's lap and laughing while man doing hand gestures and looking on the right side."], "durations": null, "exact_frames_per_prompt": [51], "background": "In the background, the man is speaking, the baby is laughing and the television audio is audible. There is a white wall and brown sofa.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/0GsiopyhDgI.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_1142_1"}, {"texts": ["A man wearing a light gray t-shirt is sitting on a sofa, holding a baby.", "The man is talking a baby wearing pink shirt is laughing."], "durations": null, "exact_frames_per_prompt": [24, 27], "background": "In the background, a man and a baby are audible. There is a brown sofa, and a white wall.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/0GsiopyhDgI.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_1143_0"}, {"texts": ["A baby wearing a pink dress is sitting and laughing while being held by a man who is sitting and talking."], "durations": null, "exact_frames_per_prompt": [51], "background": "In the background, a man and a baby are audible. There is a brown sofa, and a white wall.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/0GsiopyhDgI.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_1143_1"}, {"texts": ["A man wearing a green-blue checked shirt and blue trousers is leaning forward and shaving the sheep."], "durations": null, "exact_frames_per_prompt": [66], "background": "In the background, a person and miscellaneous sounds are audible. There is a trimmer, a brown wooden floor, a brown wooden wall, and a wooden door.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/0PhpTs9ttHc.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_1144_0"}, {"texts": ["A white sheep is lying on the floor and being shaved by the man."], "durations": null, "exact_frames_per_prompt": [66], "background": "In the background, a person and miscellaneous sounds are audible. There is a trimmer, a brown wooden floor, a brown wooden wall, and a wooden door.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/0PhpTs9ttHc.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_1144_1"}, {"texts": ["A boy wearing a white shirt is sitting behind the black table and drinking from the glass.", "The boy is sliding a green packet towards himself and starts drinking again."], "durations": null, "exact_frames_per_prompt": [53, 27], "background": "In the background, there is a gray-white wall, a window, a black table, a wooden object, a packet, a green packet, a glass, a white straw, the voices of the people and a miscellaneous sound is audible.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/3bhDPbdMbp4.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_1146_0"}, {"texts": ["A man wearing a black coat is sitting and moving his hands a man wearing a white shirt is sitting and looking towards the right side."], "durations": null, "exact_frames_per_prompt": [80], "background": "In the background, there are photo frames, white light, a bottle, two glasses of wine, a brown table, a white wall, and a man's voice is audible.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/-ATBr1X9HRQ.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_1154_0"}, {"texts": ["A man wearing a white shirt is sitting and keeps his hand on the brown table, taking a glass of wine and putting it on the table while a man wearing glasses is sitting on the right side and speaking while moving his hands."], "durations": null, "exact_frames_per_prompt": [80], "background": "In the background, there are photo frames, white light, a bottle, two glasses of wine, a brown table, a white wall, and a man's voice is audible.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/-ATBr1X9HRQ.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_1154_1"}, {"texts": ["A man wearing black clothes is folding and flipping a sheet kept on the wooden table."], "durations": null, "exact_frames_per_prompt": [80], "background": "In the background, there is a seat, wooden table, greenish wall, and the sheet. A piece of music is playing in the background.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/-8ieGoxXh0A.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_1158_0"}, {"texts": ["A girl wearing a blue cloth, is sitting on the right side and carry a rabbit while a girl wearing a black top is bends towards the soil surface and touching something then she stands and starts touching the rabbit."], "durations": null, "exact_frames_per_prompt": [80], "background": "In the background, there is a iron barrier, storage box, banner, a blue box, a red iron barrier, a plastic cage, a blue cloth, a soil surface and the people speaking and also man laughing sound is audible.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/1CN2-8KGLKo.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_1161_0"}, {"texts": ["A girl wearing a black t-shirt is standing and leaning forward while a woman on the right side wearing a blue vest is sitting while carrying a rabbit on her lap.", "The girl is picking up something.", "The girl starts rubbing rabbit's head."], "durations": null, "exact_frames_per_prompt": [47, 18, 15], "background": "In the background, there is a iron barrier, storage box, banner, a blue box, a red iron barrier, a plastic cage, a blue cloth, a soil surface and the people speaking and also man laughing sound is audible.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/1CN2-8KGLKo.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_1161_1"}, {"texts": ["A girl wearing a printed t-shirt opens a packet of bread.", "The girl wearing a printed t-shirt put two slices of bread on a plate.", "The girl wearing a printed t-shirt lifts the peanut butter jar.", "The girl wearing a printed t-shirt tries to open it."], "durations": null, "exact_frames_per_prompt": [38, 14, 5, 23], "background": "In the background, there is slices of bread, a butter knife, a peanut butter jar, a table, a table cloth, and a white door with clear glass windows.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/1hjtUQTTJag.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_1163_0"}, {"texts": ["A black cat is standing and being caressed by a person."], "durations": null, "exact_frames_per_prompt": [80], "background": "In the background, there is a wooden door, posters, red curtain, rails, television, a flower vase, some stuff, a brown teddy bear, a white cloth, pillow, bed, wooden cabinet rack and some miscellaneous sounds are audible.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/2-3Yv8ws3ys.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_1170_1"}, {"texts": ["A man wearing a black suit, a beige shirt, and a red tie is standing in front and holding a pen in his right hand."], "durations": null, "exact_frames_per_prompt": [80], "background": "In the background, there is a pan, a big digital screen showing news, a man is coughing, and a man is speaking.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/3-t_qpear60.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_1171_0"}, {"texts": ["A woman wearing a blue shirt and black shorts is moving and firing coals into the grill with a flame lighter on the grass surface."], "durations": null, "exact_frames_per_prompt": [80], "background": "In the background, there is a grill, black coals, a red flame lighter, white table, white chairs plastic bowl, green grass, plants, a black tire, and there is a miscellaneous sound.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/2qzMxBJ5hGo.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_1172_0"}, {"texts": ["A man wearing black clothes is standing and speaking and moving his hands."], "durations": null, "exact_frames_per_prompt": [33], "background": "In the background, a man is speaking. There are cooked vegetables mixed with rice, white plate, a wooden counter-top, a white wall, and big containers.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/0EISyzfzELo.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_1173_0"}, {"texts": ["A baby wearing a white-pink frock is feeding a baby goat and another baby and some goats are standing on the backside.", "The baby wearing a white-pink frock is standing, and moving."], "durations": null, "exact_frames_per_prompt": [13, 39], "background": "In the background, miscellaneous sounds and people are audible. There is a square wire fencing with wooden planks and a brown soil surface.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/08u3cb1wyvg.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_1174_0"}, {"texts": ["A white baby goat is standing behind the square wire fencing and eating a carrot while a baby wearing a dress is feeding the goat and then moves her hand, a kid wearing an orange top is feeding other goats and then turns back."], "durations": null, "exact_frames_per_prompt": [52], "background": "In the background, miscellaneous sounds and people are audible. There is a square wire fencing with wooden planks and a brown soil surface.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/08u3cb1wyvg.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_1174_2"}, {"texts": ["A baby wearing a jacket is playing with a newspaper inside a moving car."], "durations": null, "exact_frames_per_prompt": [80], "background": "In the background, a man is speaking and miscellaneous sounds are audible. There is a car with windows, gray seats, black and gray doors, a newspaper and a seat belt.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/6ZwFwfDcwH8.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_1175_0"}, {"texts": ["A man wearing a red t-shirt is playing golf in the golf course.", "The man wearing a red t-shirt moves backward."], "durations": null, "exact_frames_per_prompt": [57, 16], "background": "In the background, the sound of the wind is audible. There is a green surface, green trees, flags, a golf stick, golf balls and a blue sky with white clouds.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/1gPc7aoFcuc.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_1176_0"}, {"texts": ["A man wearing a red t-shirt, brown jeans, and white shoes is standing while holding a golf stick.", "The man wearing a red t-shirt, brown jeans, and white shoes is playing golf while hitting the golf ball with the golf stick to the golf goal hole.", "The man wearing a red t-shirt, brown jeans, and white shoes looking in the front while turning his body back."], "durations": null, "exact_frames_per_prompt": [18, 36, 19], "background": "In the background, there is the sound of air, green trees, a pole, golf flags on the ground, a golf stick, golf balls, a sky with white clouds, and a green grass surface.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/1gPc7aoFcuc.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_1177_0"}, {"texts": ["A person whose only hands are visible is folding a printed white t-shirt using a shirt folding board."], "durations": null, "exact_frames_per_prompt": [80], "background": "In the background there is a red coloured shirt folding board, a white t-shirt, a brown cover on the mattress, some other clothes and a miscellaneous sound is audible.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/1xzRU8OHyEo.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_1180_0"}, {"texts": ["A person wearing a white cloth is peeling sweet potatoes in a big wicker basket while another person sitting behind the first person is cutting something."], "durations": null, "exact_frames_per_prompt": [80], "background": "In the background, there is a carpet, soil surface, slippers, wicker baskets, sweet potatoes, cabbages, a black container, a blue-green tub, raw bananas, a brown wall, a wooden bench, a knife, a blue basket, wooden sticks and the sound of music is audible.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/1isKrzrZTMw.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_118_0"}, {"texts": ["A person wearing a red-white t-shirt and green shorts is sitting beside the first person while another person whose legs are visible is standing and walking towards the second person.", "The person is chopping cabbage."], "durations": null, "exact_frames_per_prompt": [71, 9], "background": "In the background, there is a carpet, soil surface, slippers, wicker baskets, sweet potatoes, cabbages, a black container, a blue-green tub, raw bananas, a brown wall, a wooden bench, a knife, a blue basket, wooden sticks and the sound of music is audible.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/1isKrzrZTMw.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_118_1"}, {"texts": ["A man whose upper half-body is visible wearing a green shirt is standing on the left side while holding a white cap in his left hand while a group of people are sitting in front of him.", "The man is showing the cap to the group of people while a man wearing a black suit is speaking into the mic and moving here and there."], "durations": null, "exact_frames_per_prompt": [18, 62], "background": "In the background, people are speaking. There is the sound of laughing, creamy walls, a white ceiling with white light fixtures, a big speaker on the left side, some stuff on the left wall slab, cream color curtains hanging with a metal bar, a brown boundary, a dark blue t-shirt hanging, a wooden brown wardrobe on the right side, a blue cloth-covered object on the wardrobe, a white cap, a counter with stuff, and a microphone.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/-qBi-XmpFXU.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_1190_1"}, {"texts": ["A man wearing a black shirt and black pants is standing on the right side and pointing his right hand at the group of people while a man wearing a brown shirt is standing on the left side and picking up an object, and another man wearing black suit is standing on the right side."], "durations": null, "exact_frames_per_prompt": [80], "background": "In the background, people are speaking. There is the sound of laughing, creamy walls, a white ceiling with white light fixtures, a big speaker on the left side, some stuff on the left wall slab, cream color curtains hanging with a metal bar, a brown boundary, a dark blue t-shirt hanging, a wooden brown wardrobe on the right side, a blue cloth-covered object on the wardrobe, a white cap, a counter with stuff, and a microphone.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/-qBi-XmpFXU.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_1190_2"}, {"texts": ["A man wearing a black suit, is standing in the middle of the first and second man while holding a microphone in his left hand and speaking on the mic while pointing his right hand to the first man while the first man on the left side wearing a green shirt is raising his hand while holding a white object and showing it to the people."], "durations": null, "exact_frames_per_prompt": [80], "background": "In the background, people are speaking. There is the sound of laughing, creamy walls, a white ceiling with white light fixtures, a big speaker on the left side, some stuff on the left wall slab, cream color curtains hanging with a metal bar, a brown boundary, a dark blue t-shirt hanging, a wooden brown wardrobe on the right side, a blue cloth-covered object on the wardrobe, a white cap, a counter with stuff, and a microphone.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/-qBi-XmpFXU.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_1190_3"}, {"texts": ["A man wearing a green jacket, goggles, and green pants is sitting with his knees while holding a black metal object and is speaking.", "The man is putting the metal object on the snow surface."], "durations": null, "exact_frames_per_prompt": [50, 10], "background": "In the background, a man is speaking, two transparent boxes with stuff on the left side; machines on the left side of the snow surface, a black metal object and a white snowy surface.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/1YUEFVWWCpc.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_1191_0"}, {"texts": ["A man wearing a brown hoodie is standing on a green golf mat and playing golf.", "The man wearing a brown hoodie is adjusting his hoodie.", "The man wearing a brown hoodie again starts playing golf."], "durations": null, "exact_frames_per_prompt": [37, 19, 24], "background": "In the background, there is a miscellaneous sound, a man is speaking, there is a green golf mat, a black barrier, a golf stick, a golf ball, a stadium, lights, and a dark background.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/0Vq-tbqCPWY.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_1194_0"}, {"texts": ["A person wearing a sky-blue t-shirt is standing and cutting the pineapple on the white cardboard."], "durations": null, "exact_frames_per_prompt": [80], "background": "In the background, there are wooden drawers, white cardboard, a countertop, a white door, a knife, and the voice of a person speaking, and music is audible.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/0W47UXcH2-o.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_1195_0"}, {"texts": ["A boy wearing gray clothes is tearing the white papers and speaking."], "durations": null, "exact_frames_per_prompt": [81], "background": "In the background, there is a maroon-light pink wall, and the voice of a boy speaking and some miscellaneous sounds are audible.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/1effP13TOtA.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_1196_0"}, {"texts": ["A girl wearing a black t-shirt is standing, holding a potato and peeling it while putting the peels on the black counter-top.\n"], "durations": null, "exact_frames_per_prompt": [80], "background": "In the background, there is a white wall, a soil pitcher, a metal pitcher, a tap, a pink paper, a blue stand, a wash basin, a white boil, potatoes, peels, a black counter-top, a bottle, a white soap case, black handle, spoon, and sizzling voices are audible.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/3t7Gz5zlUNc.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_1199_0"}, {"texts": ["A person whose hand is visible on the right side has pigeon food on his hand and is feeding pigeons."], "durations": null, "exact_frames_per_prompt": [81], "background": "In the background, there is a lake, green grasses, soil, trees, people are speaking, and there is a miscellaneous sound.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/-mVvWNrAjSQ.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_1201_0"}, {"texts": ["A woman wearing a white shirt is tying a bow tie."], "durations": null, "exact_frames_per_prompt": [41], "background": "In the background, there is a grey wall.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/6tLxSP7dt8U.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_1202_0"}, {"texts": ["A woman in a white dress is standing on the left side of the car, covering the pump from a white cloth and placing it in the machine.", "The woman is taking the tab from between her legs and walking towards the car."], "durations": null, "exact_frames_per_prompt": [59, 21], "background": "In the background, there is a white surface, white building, doors of glass, a black pump, black car, black sky, voices of the women and some miscellaneous sounds are audible.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/-o9dVimfAHw.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_1203_0"}, {"texts": ["A man wearing a white t-shirt is standing and playing golf.\n"], "durations": null, "exact_frames_per_prompt": [80], "background": "In the background, there is a green grass surface, a green mat, an iron structure, a grey surface, green trees, parked cars, a blue sky, a golf bag, a golf stick, a pole, and the music playing sound is audible.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/1cGuPiqCsuk.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_1206_0"}, {"texts": ["A person whose only thumb is visible wearing a gray cloth is taking out a black cloth from the orange packet."], "durations": null, "exact_frames_per_prompt": [80], "background": "In the background, music is playing. There is a red surface, an orange packet with white slips pasted on it and a black cloth.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/-2gL5FducRk.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_1209_0"}, {"texts": ["A man is wearing a black suit is sitting and speaking."], "durations": null, "exact_frames_per_prompt": [25], "background": "In the background, a man is speaking. There is a graphically edit screen, a white floor, a gray wall, a white wall with black boundary and a railing.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/11ZPBc9tEb4.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_120_0_ms"}, {"texts": ["A black cat is walking on a white surface."], "durations": null, "exact_frames_per_prompt": [54], "background": "In the background, a man is speaking. There is a graphically edit screen, a white floor, a gray wall, a white wall with black boundary and a railing.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/11ZPBc9tEb4.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_120_1_ms"}, {"texts": ["A person whose only hand is visible is taking out a bowl from an oven with a cloth.", "The person is putting bowl on the countertop."], "durations": null, "exact_frames_per_prompt": [34, 16], "background": "In the background, there is an oven, a countertop, a bowl, a container, cupboards, and some other stuff and people's voices and some miscellaneous sounds are audible.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/1CAA1ciBgKY.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_1211_0"}, {"texts": ["A person whom hand is visible is taking a bowl out of the microwave.", "The person puts it on the kitchen shelf."], "durations": null, "exact_frames_per_prompt": [27, 23], "background": "In the background, people are audible. There is a white cloth, a white microwave, white cupboards, and white bowl, a storage box, a water jug, a kettle, a white wall, and a few miscellaneous kitchen items.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/1CAA1ciBgKY.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_1212_0"}, {"texts": ["A bare-chested baby is sitting on the baby chair and making noise."], "durations": null, "exact_frames_per_prompt": [80], "background": "In the background, the baby and other people are audible and miscellaneous sounds are also audible. There is a white wall, a brown table, brown chairs, a dark colored object on the table and a baby blue and white baby chair.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/-tzRVLORGvY.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_1217_0"}, {"texts": ["A person wearing a shirt is sitting on a club chair and drinking beer.", "The person is showing the bottle."], "durations": null, "exact_frames_per_prompt": [20, 44], "background": "In the background, there is a club chair, a beer bottle, black door, white walls, an aquarium and people speaking, screaming and some music is audible.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/600a4kiWCv8.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_1219_0"}, {"texts": ["A man wearing a printed sweater is sitting, drinking liquid.", "The man wearing a printed sweater is removing cigarette ashes in the glass bowl."], "durations": null, "exact_frames_per_prompt": [37, 42], "background": "In the background, there are glasses, a glass bowl with cigarettes, a table, a red couch, a glass bottle, and the voice of people speaking, and some miscellaneous sounds are audible.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/4haZVWuL98M.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_1221_0"}, {"texts": ["A woman wearing white clothes is plucking the eyebrows of a woman with a plucker."], "durations": null, "exact_frames_per_prompt": [71], "background": "In the background, there is a plucker, and a blue sheet.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/0Z9QAMI6txY.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_1223_0"}, {"texts": ["A woman wrapped in a blue sheet is getting her eyebrows plucked by the woman in white clothes."], "durations": null, "exact_frames_per_prompt": [71], "background": "In the background, there is a plucker, and a blue sheet.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/0Z9QAMI6txY.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_1223_1"}, {"texts": ["A woman in black and white clothing is walking in the left direction while watching the child in an orange cap playing golf while a kid in a white cap is picking something from the grass surface."], "durations": null, "exact_frames_per_prompt": [25], "background": "In the background, there are bushes, canopies, trees, grassy surface, and the golf balls.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/-3dnFPNzuW4.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_1225_2"}, {"texts": ["A baby wearing a white cloth and a white feeding bib on the neck is sitting on the baby hug booster chair while holding a spoon and eating the food."], "durations": null, "exact_frames_per_prompt": [81], "background": "In the background, a woman is speaking; there is a white babyhug booster chair with a green cushion, a steel container on the left side, a pink spoon with an orange food, a white wall; and a white object behind the chair.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/2aPVxlEm5r8.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_1228_1"}, {"texts": ["A person wearing yellow clothes is sitting and holding a white cloth.", "The person is trying to wrap on the hand of a person."], "durations": null, "exact_frames_per_prompt": [16, 64], "background": "In the background, music is playing and people are speaking. There is a white chair and some other objects.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/2NelhDcRxA0.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_1229_0"}, {"texts": ["A man wearing black pants is sitting on a brown sofa, using a drill machine with an apple attached and peeling the apple with a vegetable peeler."], "durations": null, "exact_frames_per_prompt": [61], "background": "In the background, there is the sound of drill machine, a steel pot with apples and apples peel, a brown sofa, and a brown floor.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/2cwC0mVVls0.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_1232_0"}, {"texts": ["A woman with one visible hand, wearing a black cloth, is holding an ice cream and letting the baby eat it.", "The woman is wiping the baby's mouth with a tissue while a person in a black outfit walks away."], "durations": null, "exact_frames_per_prompt": [49, 31], "background": "In the background, there is a gray baby chair, white floor, chairs, voices of the people, sound of crying baby and some miscellaneous sound is audible.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/-X_bcgATtH4.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_1236_0"}, {"texts": ["A baby wearing a white-pink cloth is sitting on the baby's chair while a person whose hand is visible is holding an ice-cream cone and is feeding it to the baby.", "The baby wearing a white-pink cloth is eating an ice-cream then the person wipes the baby's mouth.", "The baby wearing a white-pink cloth starts crying while a person wearing black clothes moves towards the left."], "durations": null, "exact_frames_per_prompt": [31, 24, 25], "background": "In the background, there is a gray baby chair, white floor, chairs, voices of the people, sound of crying baby and some miscellaneous sound is audible.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/-X_bcgATtH4.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_1236_1"}, {"texts": ["A person on the right side wearing black pants is playing golf on a green grass surface."], "durations": null, "exact_frames_per_prompt": [80], "background": "In the background there is a green grass surface, golf balls, green trees, a building, a golf flag-stick, green fencing, a hole, and a breathing sound, and birds voices are audible.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/0_tWYFDPZd8.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_1240_0"}, {"texts": ["A person wearing white-blue lining clothes is sitting, holding a baby in their lap and feeding the food to a baby."], "durations": null, "exact_frames_per_prompt": [81], "background": "In the background, there is a purple spoon, some other stuff, and the voice of people speaking, and laughing is audible.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/45idT3DXxoQ.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_1245_0"}, {"texts": ["A baby wearing pink bottoms is sitting in the lap of a person and getting fed by the person."], "durations": null, "exact_frames_per_prompt": [81], "background": "In the background, there is a purple spoon, some other stuff, and the voice of people speaking, and laughing is audible.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/45idT3DXxoQ.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_1245_1"}, {"texts": ["A man whose only hands are visible is mixing the egg yolk with a fork in the liquid in the fry pan.", "The man is closing the egg storage box."], "durations": null, "exact_frames_per_prompt": [34, 4], "background": "In the background, music is playing, a man is speaking; there is a white electric gas stove, a black fry pan with egg yolk on the gas stove, another black fry pan on the gas stove, egg peels, a brown egg storage box, some stuff on the counter, and a brown surface.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/11J84KX8K3w.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_1247_0"}, {"texts": ["A woman wearing a full-sleeve gray t-shirt and jeans is opening the microwave door with her left hand, and taking out the cup of tea with her right hand from the microwave.", "The woman wearing a full-sleeve gray t-shirt and jeans walks and putting the cup on the countertop."], "durations": null, "exact_frames_per_prompt": [15, 65], "background": "In the background, a woman is speaking; a microwave, curtains hanging, gray walls, cupboards, a red cup of tea, and a white counter-top with stuff.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/2bPBOkqkwGo.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_1249_0"}, {"texts": ["A woman wearing a green shirt is applying a cream on her face and neck."], "durations": null, "exact_frames_per_prompt": [80], "background": "In the background, there are beige colored walls, and off-white ceiling, and a glass object.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/4i3OTta8DZ4.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_1250_0"}, {"texts": ["A person whose hands are visible with black trousers, holding a tube.", "The person inflating a tire."], "durations": null, "exact_frames_per_prompt": [63, 17], "background": "In the background, a man is audible. There is a tire inflating pipe, a tire with a silver rim, and a cemented floor.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/-P32zs455zc.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_1252_0"}, {"texts": ["A girl on the left, standing is making a sandwich while the girl standing on the right side is touching the food.", "The girl is picking up a red bowl."], "durations": null, "exact_frames_per_prompt": [62, 18], "background": "In the background, person one is speaking. There are light brown walls, a window with a window blind, a small plant, a sandwich, a green and white object, a microwave, brown cabinets, a white ceiling and white lights.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/2vfyDImgsNM.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_1254_0"}, {"texts": ["A girl on the right wearing a blue hoodie is standing while another girl on the left side in a black-white printed top is standing and talking.", "The girl on the right wearing a blue hoodie is adjusting the sandwich and the girl on the left picks up a red object."], "durations": null, "exact_frames_per_prompt": [20, 60], "background": "In the background, person one is speaking. There are light brown walls, a window with a window blind, a small plant, a sandwich, a green and white object, a microwave, brown cabinets, a white ceiling and white lights.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/2vfyDImgsNM.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_1254_1"}, {"texts": ["A pandemonium of parrots is sitting on the railing.", "One of the parrots is walking towards the other parrots and then it flies to the other side of the railing."], "durations": null, "exact_frames_per_prompt": [34, 46], "background": "In the background, there is a railing, and trees.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/0rLZNg8EmJ8.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_1258_0_ms"}, {"texts": ["A black and white dog is lying on the floor, dog is looking at the fingers.", "The black and white dog starts searching for something after the person speaks."], "durations": null, "exact_frames_per_prompt": [63, 17], "background": "In the background there is wooden floor and a person is speaking.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/3Qu9OQcbFDU.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_1259_1"}, {"texts": ["A man wearing white-black clothes is standing on a gray surface, holding papers and moving his hand."], "durations": null, "exact_frames_per_prompt": [80], "background": "In the background, people speaking sounds are audible, there are black curtains, gray surface and a table with some glasses.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/5BF1w9DXsWE.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_1260_0"}, {"texts": ["A girl wearing a white dress is sitting on the back of a camel and riding in a right direction and a woman wearing a purple t-shirt is walking while holding the camel's leash, and another camel is also walking behind it."], "durations": null, "exact_frames_per_prompt": [80], "background": "In the background, there is clear blue sky, tower, street lights, trees, buildings, vehicles, grassy surface, and the bouncing house.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/-LHHfaxzj7I.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_1261_0"}, {"texts": ["A woman wearing a blue t-shirt is holding the halter of the camel and walking along with the camel while the girl sits on the camel, and in back other camel follows them."], "durations": null, "exact_frames_per_prompt": [65], "background": "In the background, there is clear blue sky, tower, street lights, trees, buildings, vehicles, grassy surface, and the bouncing house.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/-LHHfaxzj7I.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_1261_1"}, {"texts": ["A camel is walking in a right direction while giving a ride to the girl on its back while a woman wearing a purple t-shirt is walking and holding the camel's leash, and another camel is also walking behind it."], "durations": null, "exact_frames_per_prompt": [80], "background": "In the background, there is clear blue sky, tower, street lights, trees, buildings, vehicles, grassy surface, and the bouncing house.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/-LHHfaxzj7I.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_1261_2"}, {"texts": ["A woman wearing a white top is standing and is mixing noodles and lettuce in a glass bowl with silver pincers.\n"], "durations": null, "exact_frames_per_prompt": [80], "background": "In the background, music is playing and the woman is speaking. There is a glass bowl, silver pincers, a black surface and a black and white wall.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/2u0tVHw5oOk.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_1264_0"}, {"texts": ["A girl wearing a yellow dress is sitting on a brown sofa.", "The girl wearing a yellow dress picks up a white shirt and unfolds it."], "durations": null, "exact_frames_per_prompt": [14, 66], "background": "In the background, there is a brown sofa, cushions, colorful clothes, magazines, a white switch, a scenery, and a man is speaking.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/-xDXYkOG5B0.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_1266_0"}, {"texts": ["A person whose hands are visible is binding a book with a needle and thread."], "durations": null, "exact_frames_per_prompt": [80], "background": "In the background, there is a wooden table, a bookbinding sewing frame with a book and push pins, another book, papers, some other stuff, wooden chairs, a pink wall, a window, and maroon curtains.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/1tIBysElEBE.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_1276_0"}, {"texts": [" A woman, whose only hands are visible, is holding an egg shell.", "The woman is putting the egg shell aside."], "durations": null, "exact_frames_per_prompt": [38, 42], "background": "In the background, there is a cooking stove, a pan, egg yolk, egg shell, oven gloves, a white surface, some other stuff, and a woman's voice. Some miscellaneous sounds and some music are audible.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/2T-O7apS96c.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_1279_0"}, {"texts": ["A person wearing blue clothes is putting the sushi pieces in a plate.", "The person is piecing the sushi roll with a knife."], "durations": null, "exact_frames_per_prompt": [48, 32], "background": "In the background, there is a sushi roll, a knife, white utensil, a chopping board, and a sushi mat.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/-dx9-XDIFh8.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_1283_0"}, {"texts": ["A man whose hand is visible is caressing the chinchilla rodent's head."], "durations": null, "exact_frames_per_prompt": [80], "background": "In the background, a man is speaking. There is a white bowl with food material, a black cage, and brown straws in the cage.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/-0-ukHRelxA.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_1291_0"}, {"texts": ["A brown chinchilla rodent is sitting in the cage and chewing something and its head caressed by the man."], "durations": null, "exact_frames_per_prompt": [80], "background": "In the background, a man is speaking. There is a white bowl with food material, a black cage, and brown straws in the cage.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/-0-ukHRelxA.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_1291_1"}, {"texts": ["A man wearing a white shirt opens a champagne bottle with a knife.", "The man puts it on the table.", "The man walks away."], "durations": null, "exact_frames_per_prompt": [22, 26, 29], "background": "In the background, cheering, miscellaneous sounds, and people are audible. There is a champagne bottle, a knife, a T. V screen, black walls, a light, black ceiling, and a dining area.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/s84sJC48qG4.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_129_0"}, {"texts": ["A man wearing a sky blue shirt and brown pants is standing behind the counter, putting the green bowl on the counter with his right hand.", "The man starts rotating the nut of the mincer machine with his hands."], "durations": null, "exact_frames_per_prompt": [13, 67], "background": "In the background, there is the sound of a machine, a meat mincer machine, a green bowl with mince ground meat, another green bowl on the counter, a mobile phone, a digital sound level meter, a black box, ground meat extracting from the mincer machine, a white wall, and a black granite counter.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/3hJ3DvXucnk.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_1301_0"}, {"texts": ["A person wearing a blue shirt is keeping a green bucket aside.", "The person is unscrewing a metal piece of a meat grinder."], "durations": null, "exact_frames_per_prompt": [14, 66], "background": "In the background, there is a meat grinder machine, green buckets, ground meat, a weighing machine, a sound level meter, a black device, a black table-top, a white wall, and some miscellaneous sounds are audible.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/3hJ3DvXucnk.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_1302_0"}, {"texts": ["A woman wearing a dark blue top and white shorts is standing and playing golf on the green surface."], "durations": null, "exact_frames_per_prompt": [48], "background": "In the background, there is a white building, green trees, green bushes, green surface, a lamp post, blue canopies, a cement block, white table, sky, a golf ball, a green-white pole, voices of the people and some miscellaneous sounds are audible.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/0djBLoTFYrA.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_1305_1"}, {"texts": ["A person wearing pink clothes is pouring food in the baking tray with a spoon."], "durations": null, "exact_frames_per_prompt": [80], "background": "In the background, there is a baking tray, a white surface, a spoon, a brown surface, and the voice of a person speaking, and some miscellaneous sounds are audible.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/-l3Cq12DG3Q.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_1306_0"}, {"texts": ["A woman wearing a pink t-shirt is standing.", "The woman is putting chocolates in a tray with the spoon."], "durations": null, "exact_frames_per_prompt": [19, 61], "background": "In the background, there are chocolates, a tray, black stove, a bowl, a spoon, brown surface, white rack, and a woman's voice is audible.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/-l3Cq12DG3Q.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_1307_0"}, {"texts": ["A baby wearing a white t-shirt is sitting on the red mat, holding a book.", "The baby is moving their hands."], "durations": null, "exact_frames_per_prompt": [54, 26], "background": "In the background, there is a red mat, a wooden table, a bowl, a gray surface, a book, a wall, black wires, some other stuff, and the voice of people speaking is audible.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/0EE-R-9pOY4.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_1310_0"}, {"texts": ["A kid wearing a blue t-shirt is running on the gray surface and is trying to catch the pigeons."], "durations": null, "exact_frames_per_prompt": [81], "background": "In the background, a person is speaking and miscellaneous sounds are audible. There is a gray surface, concrete stairs and two strollers.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/0hKrYZXvsfA.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_1312_0"}, {"texts": ["A man wearing black clothes is standing and holding fishing rod.", "The man wearing black clothes is watching on the wrist."], "durations": null, "exact_frames_per_prompt": [40, 41], "background": "In the background, people are speaking, there is a blue sky, green trees, a black tent, a fishing rod, ice surface and black object.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/2botTVCE9dI.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_1313_0"}, {"texts": ["A man wearing a black t-shirt and white pants is sitting on the back of the camel in the desert."], "durations": null, "exact_frames_per_prompt": [45], "background": "In the background, there is a desert, blue sky, sheds, a truck, some objects, people are speaking and wind blowing voices are audible.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/-6usjfP8hys.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_1318_0"}, {"texts": ["A brown camel is walking in the middle on the sand surface while another two camels are also walking on the sand surface and a group of three people are sitting on the camels."], "durations": null, "exact_frames_per_prompt": [80], "background": "In the background, there is a desert, blue sky, sheds, a truck, some objects, people are speaking and wind blowing voices are audible.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/-6usjfP8hys.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_1318_5"}, {"texts": ["A boy wearing a blue t-shirt and cap is standing in the middle and feeding the bird while other boys are also trying to feed the bird."], "durations": null, "exact_frames_per_prompt": [80], "background": "In the background, there is an iron fencing, wooden stick, trees, poles, soil surface, concrete surface, plants, a rock and people's voices and bird chirping sounds are audible.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/-C5aPmkmjA4.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_1321_0"}, {"texts": ["A bird is sitting on a wooden stick and eating from the first boy hand while the boy standing to the left of first boy also tried to feed bird"], "durations": null, "exact_frames_per_prompt": [80], "background": "In the background, there is an iron fencing, wooden stick, trees, poles, soil surface, concrete surface, plants, a rock and people's voices and bird chirping sounds are audible.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/-C5aPmkmjA4.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_1321_1"}, {"texts": ["Another boy wearing a blue-white t-shirt is standing on the right and holding something in his hand and moving his hand towards the bird.\n while a man wearing blue t-shirt holding boy kid giving something for birds"], "durations": null, "exact_frames_per_prompt": [80], "background": "In the background, there is an iron fencing, wooden stick, trees, poles, soil surface, concrete surface, plants, a rock and people's voices and bird chirping sounds are audible.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/-C5aPmkmjA4.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_1321_2"}, {"texts": ["A boy wearing a purple t-shirt is standing on the left side and holding the kid's hand and a white bird is eating food from the palm of a man."], "durations": null, "exact_frames_per_prompt": [80], "background": "In the background, there is an iron fencing, wooden stick, trees, poles, soil surface, concrete surface, plants, a rock and people's voices and bird chirping sounds are audible.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/-C5aPmkmjA4.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_1321_3"}, {"texts": ["A boy wearing a blue t-shirt and a white cap is standing in the middle on the left side and is feeding the bird while a boy wearing blue strip t-shirt is taking food from right and giving it to the bird and other two person are standing on the left."], "durations": null, "exact_frames_per_prompt": [80], "background": "In the background, people are speaking; birds are chirping; there is a metal fence cage, a wooden stick, green trees, green plants, a cage, rocks, and a brown surface with dry leaves and brown soil.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/-C5aPmkmjA4.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_1322_0"}, {"texts": ["A bird is on the wooden stick and eating from the first boy's hand while a boy in a blue striped t-shirt feeds the bird with his hand, two other boys stand on the left side, looking at the bird."], "durations": null, "exact_frames_per_prompt": [78], "background": "In the background, people are speaking; birds are chirping; there is a metal fence cage, a wooden stick, green trees, green plants, a cage, rocks, and a brown surface with dry leaves and brown soil.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/-C5aPmkmjA4.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_1322_1"}, {"texts": ["A boy wearing a white-blue striped t-shirt and black jeans is standing on the left side and is holding feeding material in his right hand and is moving his hand towards the bird while the others joins him."], "durations": null, "exact_frames_per_prompt": [80], "background": "In the background, people are speaking; birds are chirping; there is a metal fence cage, a wooden stick, green trees, green plants, a cage, rocks, and a brown surface with dry leaves and brown soil.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/-C5aPmkmjA4.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_1322_2"}, {"texts": ["A boy whose upper body is visible wearing a purple t-shirt is standing on the left side behind the child boy, and is holding the child boy's right hand while a boy wearing a cap is standing and feeding a bird with his hands, another boy wearing a multicolored t-shirt is standing, holding bird food in his hand."], "durations": null, "exact_frames_per_prompt": [80], "background": "In the background, people are speaking; birds are chirping; there is a metal fence cage, a wooden stick, green trees, green plants, a cage, rocks, and a brown surface with dry leaves and brown soil.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/-C5aPmkmjA4.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_1322_3"}, {"texts": ["A woman wearing a black dress is doing gift wrapping while instructing how to do gift wrapping."], "durations": null, "exact_frames_per_prompt": [80], "background": "In the background, there are curtains, a dining table, chairs, a Christmas tree, wall, and the gift boxes. There is the sound of a female voice in the background.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/-ff8k7ezmv0.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_1323_0"}, {"texts": ["A girl wearing a yellow t-shirt and pink shorts is sitting on the bed and folding the clothes."], "durations": null, "exact_frames_per_prompt": [80], "background": "In the background, there is a white wall, with black design, a few clothes on the bed and few is hanging on the wall, a pink bed-sheet, and the television and some miscellaneous sounds are audible.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/2fRUyhKgx9k.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_1325_0"}, {"texts": ["A person whose hands are visible is opening a brown cardboard box with a knife."], "durations": null, "exact_frames_per_prompt": [80], "background": "In the background, the person is speaking. There is a white surface, a brown cardboard box with a white paper on it, a silver object and a knife. There is also a logo and a caption written in the video.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/0duJnIG59fQ.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_1330_0"}, {"texts": ["A person whose only hands are visible is cutting a brown cardboard box with a knife.", "The person trying to open the box with his hands."], "durations": null, "exact_frames_per_prompt": [41, 39], "background": "In the background, there is a knife, a white surface, and the voice of a person speaking is audible.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/0duJnIG59fQ.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_1331_0"}, {"texts": ["A person whose hands are only visible is cutting a brown box with a knife.", "The person whose hands are only visible is tearing the brown box with his hands."], "durations": null, "exact_frames_per_prompt": [45, 35], "background": "In the background, the man is speaking, there is a miscellaneous sound, there is a white surface, a metal object, a knife, a brown box with stickers, and a logo on the screen.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/0duJnIG59fQ.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_1332_0"}, {"texts": ["A woman wearing a purple jacket, blue jeans, black gloves, and black shoes is standing on the left side while a brown horse is standing on the right side on the pebble surface.", "The woman is measuring the figure of the horse while touching the horse from his hip to the legs.", "The woman is picking the horse's leg and starts touching from her own leg to her waist."], "durations": null, "exact_frames_per_prompt": [29, 40, 12], "background": "In the background, a woman is speaking. There is a surface with dry grass, soil, and white pebbles.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/3QD1bUNhFh8.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_1333_0"}, {"texts": ["A brown horse is standing on the surface.", "The horse's leg is picked by the woman."], "durations": null, "exact_frames_per_prompt": [28, 53], "background": "In the background, a woman is speaking. There is a surface with dry grass, soil, and white pebbles.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/3QD1bUNhFh8.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_1333_1"}, {"texts": ["A woman wearing a gray top is standing on the left and holding a red plastic cup while a man wearing grey and black t-shirt is standing on the right holding a beer bottle in his hand and he is talking.", "The woman starts drinking from the cup."], "durations": null, "exact_frames_per_prompt": [58, 18], "background": "In the background, there is a white wall, a photo frame, cupboard, red cups, a bottle, a table, some other stuff and the voices of people and some music is audible.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/-ndZE5bxS3A.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_1343_0"}, {"texts": ["A man wearing a gray-black t-shirt is standing, holding a bottle and talking with the first woman.", "The man wearing a gray-black t-shirt is standing, holding a bottle and talking with the second woman."], "durations": null, "exact_frames_per_prompt": [76, 4], "background": "In the background, there is a white wall, a photo frame, cupboard, red cups, a bottle, a table, some other stuff and the voices of people and some music is audible.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/-ndZE5bxS3A.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_1343_1"}, {"texts": ["A woman whose hands are visible is holding the pen in her right hand.", "The woman is writing on white paper."], "durations": null, "exact_frames_per_prompt": [50, 30], "background": "In the background, there is a brown surface, white paper with written words, a pen, and the women speaking sound is audible.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/1XzuqBmJxK4.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_1344_0"}, {"texts": ["A man wearing a blue shirt is standing.", "The man holds the mic."], "durations": null, "exact_frames_per_prompt": [68, 12], "background": "In the background, a person is speaking, music is playing. There is a blue stage, stairs, stage lights, black surface, a brown surface and railings.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/52oamnB8PFc.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_1348_0"}, {"texts": ["A kid wearing a yellow t-shirt is standing and eating noodles through his hand from the plate."], "durations": null, "exact_frames_per_prompt": [80], "background": "In the background, there is a brown chair, a white plate, a spoon, a table, a white plate, a brown object, cloth, and the people speaking and slow music playing sound is audible.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/-yQvLuX6Vfg.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_134_0"}, {"texts": ["A person whose hand is only visible is opening the glass cover of the electric egg cooker."], "durations": null, "exact_frames_per_prompt": [27], "background": "In the background, there is an electric egg cooker, a brown surface, wires, a cup, eggs, and the voice of the electric egg cooker is audible.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/21jxy4pXfYA.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_1357_0"}, {"texts": ["A person whose hands are visible is pouring something from the white cup into a green bowl.", "The person whose hands are visible is mixing it."], "durations": null, "exact_frames_per_prompt": [36, 43], "background": "In the background, a person is audible. There is a green mixing bowl with a white mixture, a white cup, a black pepper bottle, a salt bottle, a white plate with flour, a fork, and a multi-color surface.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/-veWoU_yOjc.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_1358_0"}, {"texts": ["A woman wearing a blue life jacket is standing and giving something to a group of fishes."], "durations": null, "exact_frames_per_prompt": [80], "background": "In the background, there is water, and the voice of people speaking, and some miscellaneous sounds are audible.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/-BT1xG7BvjM.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_1359_0"}, {"texts": ["A woman wearing white clothes is sitting and she is turning book pages and the woman wearing blue top sitting on the right is also turning pages", "The woman is also talking so dies the woman wearing blue top"], "durations": null, "exact_frames_per_prompt": [36, 44], "background": "In the background there is wooden table, book, newspaper, wooden rack, black telephone, boxes, wooden baskets, red horn. People are talking.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/4hlk-7eLH7U.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_1363_0"}, {"texts": ["A woman wearing blue clothes is also sitting on the right.", "The woman is turning newspaper pages.", "The woman is also talking while a woman wearing white top is sitting on left and she is talking and turning pages."], "durations": null, "exact_frames_per_prompt": [13, 20, 47], "background": "In the background there is wooden table, book, newspaper, wooden rack, black telephone, boxes, wooden baskets, red horn. People are talking.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/4hlk-7eLH7U.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_1363_1"}, {"texts": ["A girl wearing a white and pink t-shirt is speaking.", "The girl is eating a chip from a blue plastic packet."], "durations": null, "exact_frames_per_prompt": [56, 24], "background": "In the background there is a brown ceiling fan, white ceiling, a blue plastic packet, a plant, a yellow wall and people's speaking, a chewing and a miscellaneous sound is audible.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/-mz_oMFUZL8.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_1365_0"}, {"texts": ["A man wearing a white shirt is putting dressing in the green beans salad. ", "The man starts mixing the salad with a tong.", "The man puts eggs in the salad with a knife."], "durations": null, "exact_frames_per_prompt": [16, 51, 13], "background": "In the background, there is a miscellaneous sound, the man is speaking, there is a silver countertop, a white chopping board, a steel bowl with green beans salad, a radish, an onion, a white tiled wall, glass bowls, a black bottle with an orange sticker, a knife, a tong, and some other kitchen equipment.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/12fQulARGTE.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_1370_0"}, {"texts": ["A man in a white-black shirt inserts salt into the bowl.", "The man begins marinating."], "durations": null, "exact_frames_per_prompt": [41, 39], "background": "In the background, there is a tile wall, a white bowl, oil bottle, a yellow container, glass bowls, a cup, white plate, stone surface, pasta and vegetables, a man's voice and some miscellaneous voices are audible.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/32SxoDEQ3T4.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_1372_0"}, {"texts": ["A man wearing a white t-shirt is sitting behind the table, putting some salami on the pizza.", "The man is picking up a bottle.", "The man is sprinkling something on it."], "durations": null, "exact_frames_per_prompt": [21, 36, 23], "background": "In the background, there is a table of glass, a container, a white packet, a dark brown refrigerator, white wall, posters, wooden cupboards, an entrance, utensils, voice of the man and a miscellaneous sound is audible.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/0VWdE2fREQw.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_1379_0"}, {"texts": ["A man wearing a white t-shirt is sitting and adding some toppings and cheese on a pizza.\n"], "durations": null, "exact_frames_per_prompt": [80], "background": "In the background there is a yellow wall, a glass table, a pizza, a door, a multi coloured object, a tin container, a wooden cabinet, bulletin boards, some stickers, some kitchen equipment and a man's voice is audible.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/0VWdE2fREQw.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_1380_0"}, {"texts": ["A person wearing a red t-shirt is holding a cat in their arms and rubbing its tummy with their hand."], "durations": null, "exact_frames_per_prompt": [80], "background": "In the background, there is a dark background, the voice of the person, the voice of the cat and a miscellaneous sound is audible.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/1rRNNOo3J-A.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_1384_0"}, {"texts": ["A white cat is being held by a person in their arms and getting rubbed by their hand on the belly of the cat, then the cat starts meowing."], "durations": null, "exact_frames_per_prompt": [80], "background": "In the background, there is a dark background, the voice of the person, the voice of the cat and a miscellaneous sound is audible.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/1rRNNOo3J-A.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_1384_1"}, {"texts": ["A man wearing black vest is putting his hand on the shoulder of the first man and holding a red paper strip in his mouth while third man wearing red and white outfit is standing on the right side with his arms folded and looking at other men.", "The man is standing and taking out the paper strip from his mouth."], "durations": null, "exact_frames_per_prompt": [18, 17], "background": "In the background, people are speaking, there are light green curtains, a blue alphabet letter and a green alphabet letter.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/2bPlceN5pgY.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_1386_1"}, {"texts": ["A man wearing a white-grey t-shirt is holding a white paper strip in his mouth and the other man tries to grab that paper in his mouth"], "durations": null, "exact_frames_per_prompt": [17], "background": "In the background, people are speaking, there are light green curtains, a blue alphabet letter and a green alphabet letter.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/2bPlceN5pgY.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_1386_2"}, {"texts": ["A man wearing a patterned purple shirt is moving in left direction on a grass surface while a woman wearing a black t-shirt is walking behind a man on the green grass surface while looking here and there."], "durations": null, "exact_frames_per_prompt": [80], "background": "In the background there is a slanted grass surface, trees, bushes, a metal container, a yellow glove and a man's voice and some miscellaneous sounds are audible.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/1xQunZuW7Ds.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_1387_0"}, {"texts": ["A woman wearing a black bee protection suit is moving in the left direction on a grass surface while an old man on the left side wearing a grey shirt is moving in the left direction on a grass surface."], "durations": null, "exact_frames_per_prompt": [64], "background": "In the background there is a slanted grass surface, trees, bushes, a metal container, a yellow glove and a man's voice and some miscellaneous sounds are audible.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/1xQunZuW7Ds.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_1387_1"}, {"texts": ["A woman wearing blue jeans whom leg is visible is getting her leg wiped by the man and getting a tattoo on her leg by the man."], "durations": null, "exact_frames_per_prompt": [80], "background": "In the background, people are speaking, there is the sound of the tattoo making machine, there is a black bed, a chair, a tattoo making machine, and white-black objects.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/1VQwWwyq1nA.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_1388_0"}, {"texts": ["A boy wearing sky blue clothes is sitting and washing clothes near the pond."], "durations": null, "exact_frames_per_prompt": [80], "background": "In the background, miscellaneous sounds are audible, there is green water, a wall, clothes and soap.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/0mC_uFdzcIA.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_1403_0"}, {"texts": ["A man wearing a white striped shirt is tying a brown and white tie."], "durations": null, "exact_frames_per_prompt": [80], "background": "In the background, there is a brown and white tie and a brown wall.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/0qYcfEu4heI.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_1406_0"}, {"texts": ["A man wearing a checkered shirt is tying a black neck tie around his neck."], "durations": null, "exact_frames_per_prompt": [80], "background": "In the background, a person is speaking and music is playing. There is a dark gray background, two logos and a caption written in the video.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/0qYcfEu4heI.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_1407_0"}, {"texts": ["A baby wearing white clothes is lying on the blue cloth, starts yawning and cooing."], "durations": null, "exact_frames_per_prompt": [80], "background": "In the background, there is a light cream mat, a table, a dark brown surface, an object, a blue cloth, the voice of a woman and the voice of the baby is audible.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/1-gAdYe4zSw.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_1415_0"}, {"texts": ["A man wearing a navy blue t-shirt is sitting, licking the ice cream and then giving it to the boy."], "durations": null, "exact_frames_per_prompt": [80], "background": "In the background, a song is playing, a person and the boy are laughing. There are trees, a white sky, building and a gray surface.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/0kKfRQWmUHU.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_1418_0"}, {"texts": ["A boy wearing a light blue t-shirt is sitting in the man's lap, licking the ice cream.", "The boy is laughing while the man is licking the boy's ice-cream."], "durations": null, "exact_frames_per_prompt": [51, 29], "background": "In the background, a song is playing, a person and the boy are laughing. There are trees, a white sky, building and a gray surface.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/0kKfRQWmUHU.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_1418_1"}, {"texts": ["A girl wearing a blue t-shirt is sitting and eating noodles.", "The girl is clapping."], "durations": null, "exact_frames_per_prompt": [41, 39], "background": "In the background, there is a wall, a green plant, a sofa, a brown rack, a blue cup, some objects, a fork, purple straw, a white cup, white plate, noodles, a bowl, a yellow spoon, a table, a woman is speaking, a girl is speaking, and clapping sounds are audible.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/-bwMStYyMNI.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_1419_0"}, {"texts": ["A man wearing a chef dress is standing and frying rolls in a pan and flipping them with a tong."], "durations": null, "exact_frames_per_prompt": [80], "background": "In the background, there is a frying pan, rolls, a tong, white bowls, black countertops, cooking stove, designer wall, cupboards filled with glass, drawers, a tray, plates, some other stuff, and the man is speaking and some music is audible.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/-k3VNtZJUaM.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_141_0"}, {"texts": ["A man wearing a white t-shirt is standing and holding a mic.", "The man is pointing finger towards the soil surface while the others react to it."], "durations": null, "exact_frames_per_prompt": [27, 53], "background": "In the background, there are green trees, sky, tent, bottles, burgers, brown table, soil surface, a mic, people are speaking and shouting voices are audible.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/1BRwWRBtGYg.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_1421_0"}, {"texts": ["A boy wearing a green t-shirt is standing, eating a burger and drinking water along with other competitors.", "The boy picks something from the soil surface while the boy in left side in brown t-shirt and the girl turns their head and looks at him.", "The boy starts eating it while the boy in right side takes the bottle from the table to drink."], "durations": null, "exact_frames_per_prompt": [50, 8, 3], "background": "In the background, there are green trees, sky, tent, bottles, burgers, brown table, soil surface, a mic, people are speaking and shouting voices are audible.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/1BRwWRBtGYg.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_1421_2"}, {"texts": ["A boy wearing a white printed t-shirt is standing while a group of kids is drinking and eating, and a group of people is standing on the brown surface.", "The boy wearing a white printed t-shirt is drinking water while a man wearing a white t-shirt bends downwards and points out towards the brown surface.", "The boy wearing a white printed t-shirt puts the bottle on the table again while a boy wearing a green t-shirt bend towards the brown surface and picks up some stuff from the surface and eats it.", "The boy wearing a white printed t-shirt is holding the bottle and eating a burger."], "durations": null, "exact_frames_per_prompt": [16, 15, 39, 10], "background": "In the background, there are green trees, sky, tent, bottles, burgers, brown table, soil surface, a mic, people are speaking and shouting voices are audible.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/1BRwWRBtGYg.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_1421_3"}, {"texts": ["A man in black clothes is holding a mike and he is at first standing near the woman while two men are tightening the chain of the motorbike and a group of people are standing and watching it.", "The man in black clothes is moving here and there."], "durations": null, "exact_frames_per_prompt": [32, 48], "background": "In the background, there is a stage, a bike, a door, a screen, and the video of the man fixing a tire is playing on the screen.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/4PDLT41nRZA.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_1422_0"}, {"texts": ["A man wearing a black t-shirt is standing near the bike and holding the front part of a bike while a man wearing black clothes is holding a mic and talking to a woman, and a group of people are looking at the bike."], "durations": null, "exact_frames_per_prompt": [80], "background": "In the background, there is a stage, a bike, a door, a screen, and the video of the man fixing a tire is playing on the screen.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/4PDLT41nRZA.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_1422_1"}, {"texts": ["A man on the right side is kneeling on the floor, and fixing the rear tire of a bike, and attaching it with the bike chain while a group of people is standing  behind the man and watching him, a few people are standing at the back, another man is standing in front of the bike, another man stands up and walks away, and a man wearing black clothes is standing holding a mic and taking an interview of a woman who is standing and touching the tank of the bike."], "durations": null, "exact_frames_per_prompt": [80], "background": "In the background, there is a stage, a bike, a door, a screen, and the video of the man fixing a tire is playing on the screen.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/4PDLT41nRZA.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_1422_2"}, {"texts": ["A person who is at first sitting near the stage and then stands up while a man wearing black clothes is holding a mic and talking to a woman, and two other men are holding the front and back parts of the bike.", "The person turns around and starts walking while a group of people are looking at the bike."], "durations": null, "exact_frames_per_prompt": [18, 23], "background": "In the background, there is a stage, a bike, a door, a screen, and the video of the man fixing a tire is playing on the screen.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/4PDLT41nRZA.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_1422_4"}, {"texts": ["A woman in a black t-shirt is at first speaking to the man on a mic while a man is sitting and he is fixing the bike tyre and another man is holding the bike's front portion.", "The woman is posing while people are standing on the stage."], "durations": null, "exact_frames_per_prompt": [29, 51], "background": "In the background, there is a stage, a bike, a door, a screen, and the video of the man fixing a tire is playing on the screen.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/4PDLT41nRZA.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_1422_5"}, {"texts": ["A girl wearing a white-red checkered shirt, black jeans, and brown boots is standing while holding the first woman's hand.", "The girl is walking while holding a glass on the brown surface."], "durations": null, "exact_frames_per_prompt": [27, 53], "background": "In the background, people are speaking, there is a white ceiling, a white wall, white metal fence railing, a black structure on the right side, and a brown surface.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/1wRsP9VYTBE.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_1425_3"}, {"texts": ["A light brown goat is standing on the left side and looking at the first woman and the girl and a group of people and goats are moving here and there, and some of them are standing."], "durations": null, "exact_frames_per_prompt": [80], "background": "In the background, people are speaking, there is a white ceiling, a white wall, white metal fence railing, a black structure on the right side, and a brown surface.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/1wRsP9VYTBE.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_1425_4"}, {"texts": ["A woman wearing a dark purple t-shirt, and blue jeans and black shoes is feeding the food to the big deer while standing on the right side while a group of people are doing different activities, some are standing and some are walking.", "The woman starts walking to the right side while a sheep is moving to the right side and group of goats are standing on the ground."], "durations": null, "exact_frames_per_prompt": [38, 25], "background": "In the background, people are speaking, there is a white ceiling, a white wall, white metal fence railing, a black structure on the right side, and a brown surface.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/1wRsP9VYTBE.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_1425_5"}, {"texts": ["A big deer is standing on the left side of the brown surface and eating the food given by the second woman and other are moving here and there.", "The deer starts walking towards the right side."], "durations": null, "exact_frames_per_prompt": [59, 20], "background": "In the background, people are speaking, there is a white ceiling, a white wall, white metal fence railing, a black structure on the right side, and a brown surface.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/1wRsP9VYTBE.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_1425_6"}, {"texts": ["A light brown sheep is walking on the brown surface from left to right while a group of people are moving here and there and some animals are moving here and there on the brown surface."], "durations": null, "exact_frames_per_prompt": [80], "background": "In the background, people are speaking, there is a white ceiling, a white wall, white metal fence railing, a black structure on the right side, and a brown surface.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/1wRsP9VYTBE.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_1425_8"}, {"texts": ["A person wearing black clothes is tying the shoelace."], "durations": null, "exact_frames_per_prompt": [80], "background": "In the background, there is a black shoe, and a grey tiled flooring.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/1f0uzBmXElA.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_1426_0"}, {"texts": ["A woman wearing a purple t-shirt is talking in a sigh language."], "durations": null, "exact_frames_per_prompt": [80], "background": "In the background, there is a striped wall.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/6CbEav6UomA.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_1427_0"}, {"texts": ["A girl wearing a printed t-shirt and black jeans is sitting at the other washing machine while holding clothes, and then putting the clothes into the washing machine."], "durations": null, "exact_frames_per_prompt": [80], "background": "In the background, the song is playing, there are clothes, gray walls, a slab with stuff, gray curtains hanging, a silver pipe, other stuff on the floor, and two white washing machines.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/5fdjeWMVscM.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_1429_0"}, {"texts": ["A man wearing a black jacket and blue jeans is sitting on the horse and is horse riding in the straight direction.", "The man is riding horse in the backward direction in the shed house."], "durations": null, "exact_frames_per_prompt": [33, 47], "background": "In the background, a man is speaking. There is the sound of the bell, a wooden beam ceiling, white wall with wooden logs, two yellow lights, a wooden structure, and the soil surface.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/3joQORBsR8c.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_1431_0"}, {"texts": ["A brown horse is running in the straight direction and then it stops.", "Then the horse is walking in the backward direction while a person is riding on it."], "durations": null, "exact_frames_per_prompt": [35, 45], "background": "In the background, a man is speaking. There is the sound of the bell, a wooden beam ceiling, white wall with wooden logs, two yellow lights, a wooden structure, and the soil surface.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/3joQORBsR8c.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_1431_1"}, {"texts": ["A man wearing a black t-shirt is sitting on the chair, keeping one leg on the drawer while holding a plate and eating a hot dog."], "durations": null, "exact_frames_per_prompt": [80], "background": "In the background, there is a wall, an almirah, a white carry bag, a chair, a white plate, a hot dog, a brown drawer, a video is playing on the television, and the sound of the television is audible.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/4WEPeQYLPfI.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_1433_0"}, {"texts": [" A small boy whose only upper body is visible wearing a blue t-shirt and a white-brown-striped feeding bib, is standing on the left side.", "The boy is eating a cake from his mouth while tilting his neck and speaking."], "durations": null, "exact_frames_per_prompt": [27, 53], "background": "In the background, people are speaking; a person is laughing; there is a yellow wall, a cake on the white table, and a printed poster board on the white table.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/-p_w4ZZpVh4.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_1436_0"}, {"texts": ["A man wearing a white thobe is walking along with the camels."], "durations": null, "exact_frames_per_prompt": [80], "background": "In the background, there is a flag, a pillar, a sign board, a vehicle, and sand.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/-chxCS3E9qI.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_1437_2"}, {"texts": ["A man wearing a black suit is walking and moving his hand as a woman wearing a black t-shirt is speaking while holding a mic."], "durations": null, "exact_frames_per_prompt": [80], "background": "In the background, people are audible. There are chairs, tables, an orange carpet floor, speakers, a black-gray stage, doors, a white poster, a black curtain, orange-yellow walls, a yellow ceiling, and lights.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/1imVrO0b9GM.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_1439_1"}, {"texts": ["A woman wearing black clothes is sitting on the gray surface and opening the box with her hands."], "durations": null, "exact_frames_per_prompt": [80], "background": "In the background, people are speaking, there is a packed box, a bed with a white mattress, white curtains hanging and a gray surface.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/0s2cFiqUl2M.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_144_0"}, {"texts": ["A man wearing a white shirt is standing in the front and tying a brown-grey printed tie."], "durations": null, "exact_frames_per_prompt": [80], "background": "In the background, there are wooden walls, a wooden door, a ceiling, and a white light.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/-tj3gU_fMfU.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_1451_0"}, {"texts": ["A boy wearing a red sweatshirt is unpacking a chair base on a beige surface."], "durations": null, "exact_frames_per_prompt": [81], "background": "In the background there is a beige wall, a chair, a wooden cabinet, a polythene piece, a black device, a floor carpet and a boy's speaking and tearing sound of bubble sheet are audible.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/-USWJfkof9Y.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_1452_0"}, {"texts": ["A girl on the left side wearing green clothes is sitting on a beige surface while another girl wearing a pink-purple top is sitting on the right side and opening a gift.", "The girl wearing green clothes is opening a gift box while a person wearing black clothing is sitting on a sofa."], "durations": null, "exact_frames_per_prompt": [34, 46], "background": "In the background there is a blue cloth, a beige surface, chairs, a brown wall, a wooden structure, and people's voices and some miscellaneous sounds are audible.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/-koPqEecDtY.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_1453_0"}, {"texts": ["A girl on the right side is opening a gift box and sitting on a beige surface.\n while a girl sitting on the left is looking at the girl on the right and removing the wraps of the gift."], "durations": null, "exact_frames_per_prompt": [80], "background": "In the background there is a blue cloth, a beige surface, chairs, a brown wall, a wooden structure, and people's voices and some miscellaneous sounds are audible.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/-koPqEecDtY.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_1453_1"}, {"texts": ["A man wearing a vest is standing and picking an object from a container and putting it in the glasses."], "durations": null, "exact_frames_per_prompt": [80], "background": "In the background, there are bottles, glasses, white walls, a countertop, shelves, a tissue paper, some other stuff, and the voice of a man speaking is audible.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/3JKOc-rtfYA.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_1454_0_ms"}, {"texts": ["A man wearing a white printed t-shirt is holding a rod and dragging it into the sea water to catch the fish while another man wearing a white t-shirt and hat is standing on the left side, holding a fishing rod."], "durations": null, "exact_frames_per_prompt": [46], "background": "In the background, there is white boat, a rod, and the sea water.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/0WNprz_EwiQ.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_1455_0"}, {"texts": ["A man wearing a beige colored hat is pulling the fishing rod in an up-ward direction while a person wearing white and grey outfit is standing in a bending position on a boat."], "durations": null, "exact_frames_per_prompt": [16], "background": "In the background, there is white boat, a rod, and the sea water.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/0WNprz_EwiQ.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_1455_1"}, {"texts": ["A man wearing white blue clothes is standing and wrapping a box with colored paper."], "durations": null, "exact_frames_per_prompt": [80], "background": "In the background, music is playing and miscellaneous sounds are audible. There is a yellow wall, yellow floor, white-gray wall, a cupboard, papers, tapes, some pen, a glass window, white pillars, a box, and some other objects.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/1qi8ZXUH_wY.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_1456_0"}, {"texts": ["A man wearing white black cloth is standing and holding a paper while another old man is wrapping a box with the red and while colored paper", "The man is moving towards the left side and the old attached piece of tape to the box and folding with the red and white paper"], "durations": null, "exact_frames_per_prompt": [19, 61], "background": "In the background, music is playing and miscellaneous sounds are audible. There is a yellow wall, yellow floor, white-gray wall, a cupboard, papers, tapes, some pen, a glass window, white pillars, a box, and some other objects.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/1qi8ZXUH_wY.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_1456_1"}, {"texts": ["A man wearing a navy blue blazer is standing and wrapping a white and blue box with his hands while another man in a black blazer holds the paper and walks away and some people are walking on the right side."], "durations": null, "exact_frames_per_prompt": [80], "background": "In the background there are some tapes and pens, a glass shelf with some objects, a chair, a white counter-top, some boxes, a laptop, a drawer, a brown object, some papers, a white and blue box, a brown surface and a music sound, people's voices and some miscellaneous sounds are audible.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/1qi8ZXUH_wY.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_1457_0"}, {"texts": ["A man wearing a black jacket and black pants is standing on the left side of the first man and moving his hands while the first man wearing a green sweatshirt is standing while holding a golf club and looking at the second man.", "The man is sitting on the left side of the third man and holding the stick while the third man wearing blue pants is standing while holding a gold club."], "durations": null, "exact_frames_per_prompt": [25, 55], "background": "In the background, there is a green surface, green trees, golf balls, sky, the voice of the man and the sound of the music is audible.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/0V8kH1EDmpU.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_1458_1"}, {"texts": ["A man wearing a yellow striped blue t-shirt and blue pants is standing on the right side of the second man and holding a golf stick while the man in black pants is sitting and talking to him."], "durations": null, "exact_frames_per_prompt": [56], "background": "In the background, there is a green surface, green trees, golf balls, sky, the voice of the man and the sound of the music is audible.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/0V8kH1EDmpU.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_1458_2"}, {"texts": ["A person wearing a black cloth is standing and peeling the potato.", "The person is washing the potato in the big glass bowl."], "durations": null, "exact_frames_per_prompt": [38, 42], "background": "In the background, there is a grey surface, a wooden brown table surface, a big bowl, potato, and water.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/2LzWCwYtUzs.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_1459_0"}, {"texts": ["A man wearing black t-shirt is standing.", "The man is showing a white bottle."], "durations": null, "exact_frames_per_prompt": [30, 50], "background": "In the background, the man is speaking and miscellaneous sounds are audible. There is a bar, bottles, a black poster, a white bottle, some miscellaneous objects and white lights.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/1LF4BrYt4wY.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_1464_0"}, {"texts": ["A woman whose only hands are visible is holding a knife and applying buttercream on a cake."], "durations": null, "exact_frames_per_prompt": [80], "background": "In the background, there are cakes, buttercream, a knife, and a reddish surface. There is the sound of a female voice in the background.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/134j3CCEsQM.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_1466_0"}, {"texts": ["A girl wearing a red top is scratching her head while the boy in black pants watching the toy.", "The girl standing and moving on the floor."], "durations": null, "exact_frames_per_prompt": [21, 59], "background": "In the background, people are speaking, there is a wooden floor, a green toy tractor, a gift wrap, a brown box with some stuff, a wooden table, a pink sofa with a cushion, a red-white cloth, a red-white baby high chair, a table lamp, light-brown walls, picture frames, a white counter with a glass surface and some stuff, a grey chair, some toys on the floor, and curtains.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/1elitRm1sK4.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_1467_1"}, {"texts": ["A kid, who is a boy, wearing a dark red t-shirt and spectacles is standing behind the gray countertop and spreading pizza sauce on the pizza base with the ladle."], "durations": null, "exact_frames_per_prompt": [80], "background": "In the background, there is a dark gray refrigerator, metallic racks, metallic table racks, bottle, buckets, some stuff, a black bag, a brown bag, plastics, white floor, white wall, gray trays, dark gray countertop, white paper, pizza base, a container with pizza sauce, a platter with a food, some other stuff, a blue bag and a miscellaneous sound is audible.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/0KAuR2nsIj4.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_146_0"}, {"texts": ["A boy wearing a green-white printed t-shirt is sitting on a bed.", "The boy is opening a newspaper.", "The boy starts tapping on the newspaper."], "durations": null, "exact_frames_per_prompt": [35, 28, 17], "background": "In the background, there is a miscellaneous sound, the boy is speaking, there is a white-pink-red bed with red pillows, white printed clothes, and a newspaper.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/8IZfazDanKI.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_1473_0"}, {"texts": ["A person wearing a green shirt is squeezing the white sauce on the bread and the meat."], "durations": null, "exact_frames_per_prompt": [80], "background": "In the background, there is a kitchen, kitchen cabinets, bread, sauce and the sauce bottle, and the food packet.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/-i2wIUzvEYI.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_1477_0"}, {"texts": ["A boy wearing an orange t-shirt and blue jeans is sitting on the right side, speaking while opening the gift wrapped paper, and smiling while holding a book while a group of people are sitting and watching the boy."], "durations": null, "exact_frames_per_prompt": [80], "background": "In the background, people are speaking, there is the sound of the wrapping papers; green trees, the sky, a house, chairs, a table, bottles and glasses on the table, a wrapping paper, and a concrete surface.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/-kRj53ftJnU.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_1478_1"}, {"texts": ["A baby wearing an orange-striped gray t-shirt and gray shorts is sitting on the woman's lap a baby wearing an orange t-shirt is sitting and opening a book, and group of people are sitting behind the baby and looking at it.", "The baby starts holding a wrapping paper with his hand a woman is sitting and looking at the right side."], "durations": null, "exact_frames_per_prompt": [61, 19], "background": "In the background, people are speaking, there is the sound of the wrapping papers; green trees, the sky, a house, chairs, a table, bottles and glasses on the table, a wrapping paper, and a concrete surface.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/-kRj53ftJnU.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_1478_3"}, {"texts": ["A man wearing a gray shirt is sitting on the dining table and holding his cap."], "durations": null, "exact_frames_per_prompt": [71], "background": "In the background, there is a gray surface, dark blue walls with designs, green walls, a wooden desk, a black object, a white door, wooden tables, chairs, glasses, plates, foods, a photo frame, windows, the voices of the people and some miscellaneous sounds are audible.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/-KpXV80cC8M.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_1479_1"}, {"texts": ["A woman wearing a white top and white trousers with a gray floral design is lying and getting microblading by another woman."], "durations": null, "exact_frames_per_prompt": [80], "background": "In the background, there is a microblading tool, a blue cloth, a bed with white sheet, a white light, a white wall, a white wall with gray pattern, and a brown floor.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/0rbCqeVfTBI.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_1480_0"}, {"texts": ["A woman wearing a white doctor uniform is sitting and microblading the eyebrows of the first woman."], "durations": null, "exact_frames_per_prompt": [56], "background": "In the background, there is a microblading tool, a blue cloth, a bed with white sheet, a white light, a white wall, a white wall with gray pattern, and a brown floor.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/0rbCqeVfTBI.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_1480_1"}, {"texts": ["A woman wearing sky blue clothes is lying on a surface and getting eyebrows brushed by another woman."], "durations": null, "exact_frames_per_prompt": [80], "background": "In the background, there is a brown surface, blue surface, gray object and a gray wall, and a white metal frame.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/0rbCqeVfTBI.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_1481_0"}, {"texts": ["A woman wearing white clothes is sitting and brushing the eyebrows of the first woman."], "durations": null, "exact_frames_per_prompt": [80], "background": "In the background, there is a brown surface, blue surface, gray object and a gray wall, and a white metal frame.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/0rbCqeVfTBI.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_1481_1"}, {"texts": ["A person wearing a black cap is sitting and holding a fish and a fishing rod with his hands."], "durations": null, "exact_frames_per_prompt": [80], "background": "In the background there is a black tent, a fold-able chair, a fishing rod, a hole in the snow surface, polythene bags, a yellow perch fish, water and a person's speaking, a person's laughing and music sound is audible.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/432wdISAiaQ.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_1483_0"}, {"texts": ["A woman on the right side is sitting on a pink and blue seat and talking to the man facing left while holding a fork while a woman in black-white checked cloth is sitting next to the woman, eating something, and a person is standing at the back."], "durations": null, "exact_frames_per_prompt": [50], "background": "In the background there are pink and brown walls, a hanging light, picture frames, glasses with dessert, plates with desert, brown tables, a brown and pink counter, people voices and a music sound is audible.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/07u-CPz93rQ.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_1489_1"}, {"texts": ["A woman wearing spectacles sitting with the first woman is eating some dessert from a plate a man is sitting and speaking."], "durations": null, "exact_frames_per_prompt": [80], "background": "In the background there are pink and brown walls, a hanging light, picture frames, glasses with dessert, plates with desert, brown tables, a brown and pink counter, people voices and a music sound is audible.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/07u-CPz93rQ.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_1489_2"}, {"texts": ["A man wearing a blue-white jacket is sitting and rotating the car jack.", "The man stands up and starts screwing the car wheel."], "durations": null, "exact_frames_per_prompt": [34, 46], "background": "In the background, there is a yellow-black building, a green grass surface, a blue poly bag, a red parked car, a car jack, a stone, a grey surface, and the man speaking sound is audible.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/2xTzdb3xHSc.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_1490_0"}, {"texts": ["A man wearing a blue t-shirt is bending on the left side of the second woman and watching the second man, who is trying to catch a fish and another woman in a black outfit is standing on the left side and watching them."], "durations": null, "exact_frames_per_prompt": [80], "background": "In the background, there is a shed, a black helmet, white boat, blue water, blue sky, fishing rods, the voices of the people and the sound of the music is audible.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/0A6a_oKa8mc.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_1491_2"}, {"texts": ["A man wearing a white shirt is standing on the left side of the first man, holding a fishing rod and trying to catch a fish while a woman wearing a purple t-shirt and another woman wearing black clothes are standing on the white boat."], "durations": null, "exact_frames_per_prompt": [77], "background": "In the background, there is a shed, a black helmet, white boat, blue water, blue sky, fishing rods, the voices of the people and the sound of the music is audible.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/0A6a_oKa8mc.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_1491_3"}, {"texts": ["A woman wearing a sky-blue t-shirt on the left side is standing and pulling other woman's head down.", "The woman apply cake on her face.", "The woman starts eating cake from the plate another woman starts eating cake from the plate."], "durations": null, "exact_frames_per_prompt": [27, 13, 40], "background": "In the background, there is a white countertop, plate, bottle, cake, sink, a gas stove, a chimney, white-green wall, picture frames, wooden shelves, a white refrigerator with some stuff on the top, some white- yellow object, and the woman laughing and also other women speaking sound is audible.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/2j8zidMcXPA.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_1492_0"}, {"texts": ["A woman wearing a black t-shirt on the right side is standing and eating cake along with the first woman."], "durations": null, "exact_frames_per_prompt": [80], "background": "In the background, there is a white countertop, plate, bottle, cake, sink, a gas stove, a chimney, white-green wall, picture frames, wooden shelves, a white refrigerator with some stuff on the top, some white- yellow object, and the woman laughing and also other women speaking sound is audible.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/2j8zidMcXPA.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_1492_1"}, {"texts": ["A man wearing a black t-shirt and blue jeans is riding a horse and waving his cap while a man wearing a cap is standing, holding a camera, and recording the man's activity."], "durations": null, "exact_frames_per_prompt": [75], "background": "In the background, miscellaneous sounds are audible. There are yellow grasses, a sand surface, a brown hill ridge, and a clear blue sky.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/-kd_rdZ9Xaw.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_1496_0"}, {"texts": ["A brown horse is running on sand surface while a man in black T-shirt sits on horse and removes the cap from head and riding."], "durations": null, "exact_frames_per_prompt": [80], "background": "In the background, miscellaneous sounds are audible. There are yellow grasses, a sand surface, a brown hill ridge, and a clear blue sky.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/-kd_rdZ9Xaw.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_1496_1"}, {"texts": ["A man wearing a white jacket is standing and pulling threads from a pink mattress."], "durations": null, "exact_frames_per_prompt": [58], "background": "In the background there is a mattress shop, some houses, a grey road, tree, a white sky, a television, a white wall, some cabinets, mattresses, cylindrical containers, a mirror, some other stuff and some miscellaneous sounds are audible.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/IztcW1J_pUE.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_1497_0"}, {"texts": ["A black and white cat, sitting on a blanket is sleeping.", "The black and white cat is being caressed by a person."], "durations": null, "exact_frames_per_prompt": [27, 17], "background": "In the background, a person is speaking. There is white and brown blanket and a black blanket.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/2aIXpCUMon0.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_1498_0"}, {"texts": ["A person whose only hand is visible is caressing the black and white cat."], "durations": null, "exact_frames_per_prompt": [20], "background": "In the background, a person is speaking. There is white and brown blanket and a black blanket.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/2aIXpCUMon0.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_1498_1"}, {"texts": ["A person whose hand is visible, is coming from the left and starts touching the cat."], "durations": null, "exact_frames_per_prompt": [20], "background": "In the background, the person is speaking, there is a white-gray bed, white black pillow, a white wall and some other objects.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/2aIXpCUMon0.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_1499_0"}, {"texts": ["A black and white cat is lying on the gray-white bed.", "The person starts touching the cat."], "durations": null, "exact_frames_per_prompt": [30, 14], "background": "In the background, the person is speaking, there is a white-gray bed, white black pillow, a white wall and some other objects.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/2aIXpCUMon0.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_1499_1"}, {"texts": ["A man wearing a dark-colored shirt is standing, holding a knife in his left hand and picking the pineapples from the table, and putting them on wooden cardboard."], "durations": null, "exact_frames_per_prompt": [80], "background": "In the background, there are chairs, utensils, wooden shelves, a refrigerator, a cupboard, a table, a brown surface, a wall clock, white walls, a wooden door, some other objects, and the voice of people speaking, and laughing is audible.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/29BV21mCC0M.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_1503_0"}, {"texts": ["A man wearing black shirt is sitting with a spoon in his hand, picking up.", "The man is eating food."], "durations": null, "exact_frames_per_prompt": [58, 22], "background": "In the background, the man is speaking, music is playing. There are plates with food in it, cutlery, a tissue, a red table mat, a gray table, photo frames, and a blue wall with white line.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/1cYd7mmJpZg.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_1504_0"}, {"texts": ["A woman wearing white-black clothes is standing", "The woman wearing white-black clothes is ironing a white cloth with a white iron."], "durations": null, "exact_frames_per_prompt": [30, 49], "background": "In the background, there is a wall, a white iron, and the voice of a woman speaking, and the sound of music is audible.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/1sCZZv7WQuM.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_1508_0"}, {"texts": ["A girl wearing a black top is standing on the left side.", "The girl is ironing a white cloth with an iron on a gray ironing board."], "durations": null, "exact_frames_per_prompt": [31, 49], "background": "In the background, music is playing, a woman is speaking. There is a white gray wall, a window, a brick wall, a gray ironing bed, a white cloth, and a white green iron.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/1sCZZv7WQuM.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_1509_0"}, {"texts": ["A lady wearing a blue t-shirt is attaching a milk sucking machine into the tears of goats."], "durations": null, "exact_frames_per_prompt": [80], "background": "In the background there is a black surface, milk sucking machines, white wall and sound of machines are audible in the background.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/0WDRWysQK2k.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_1510_0"}, {"texts": ["A man wearing a white jacket is sitting on the right side, and holding a fork while a woman whose hand is visible is wearing a brown jacket, holding a fork, and moving her hand.", "The man is eating food from a plate."], "durations": null, "exact_frames_per_prompt": [70, 10], "background": "In the background, people are speaking. There are white plates and bowls with food in it, tissue papers, forks, blue sky, mountains with snow, and a black table.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/0yyXWU6JKtc.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_1515_0"}, {"texts": ["A woman whose hand is visible is holding a fork and picking food from a plate while a person on the right wearing white jacket is eating."], "durations": null, "exact_frames_per_prompt": [80], "background": "In the background, people are speaking. There are white plates and bowls with food in it, tissue papers, forks, blue sky, mountains with snow, and a black table.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/0yyXWU6JKtc.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_1515_1"}, {"texts": ["A woman wearing a pink vest is sitting, holding a tube.", "The woman wearing a pink vest presses it, takes the cream out.", "The woman wearing a pink vest puts the tube away and shows the cream on her hand."], "durations": null, "exact_frames_per_prompt": [12, 47, 21], "background": "In the background, miscellaneous sounds are audible. There is a tube, a peach blanket, a white sheet, a white floor, a black-white door, and a green wall.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/0AqWASz_kWs.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_1518_0"}, {"texts": ["A boy wearing a white t-shirt and white shorts is sitting on the stool and washing clothes while putting clothes in the tub."], "durations": null, "exact_frames_per_prompt": [65], "background": "In the background, people are speaking. There is the sound of the rooster, a small red stool, a black small tub with clothes, a white object on the right side, a big red tub with water, a bicycle behind the boy, and a gray pebble surface.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/2EVwbnZT7OE.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_1519_0"}, {"texts": ["A person whom half body is visible is standing.", "The person is making sushi from a sushi making wooden mold."], "durations": null, "exact_frames_per_prompt": [18, 62], "background": "In the background, there is a wooden sushi mold, a white bowl, sushi, two countertops, cupboards, some other stuff, a red object and people's voices are audible.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/5hpJitgVpAE.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_1520_0"}, {"texts": ["A girl wearing a graphic black t-shirt is sitting on a chair and getting a tattoo on her right arm while a man wearing white gloves is making a tattoo on the left arm of the woman."], "durations": null, "exact_frames_per_prompt": [80], "background": "In the background, there is a tattoo making machine, a black chair, white grill, white floor, voices of the people and the sound of tattoo machine is audible.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/0yi-nkwLEnI.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_1521_0"}, {"texts": ["A man wearing a black t-shirt and white gloves is making a tattoo on the right arm of the girl."], "durations": null, "exact_frames_per_prompt": [80], "background": "In the background, there is a tattoo making machine, a black chair, white grill, white floor, voices of the people and the sound of tattoo machine is audible.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/0yi-nkwLEnI.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_1521_1"}, {"texts": ["A girl wearing black designer vest is sitting on a chair and getting a tattoo on her right arm while a man wearing gloves is making the tattoo on the woman's arm."], "durations": null, "exact_frames_per_prompt": [80], "background": "In the background, there is a tattoo making machine, gloves, a black chair, white grill, white floor and people's voices and tattoo machine sound is audible.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/0yi-nkwLEnI.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_1522_0"}, {"texts": ["A man wearing a black t-shirt is making a tattoo on the girl's left arm."], "durations": null, "exact_frames_per_prompt": [80], "background": "In the background, there is a tattoo making machine, gloves, a black chair, white grill, white floor and people's voices and tattoo machine sound is audible.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/0yi-nkwLEnI.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_1522_1"}, {"texts": ["A boy wearing green t-shirt is sitting and he is eating a doughnut while a boy in a brown patterned t-shirt picks up a donut, sits down, and starts eating it."], "durations": null, "exact_frames_per_prompt": [80], "background": "In the background there is yellow and red wall, windows, doughnut and people are talking.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/2uJyGgx8_24.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_1530_0"}, {"texts": ["A boy wearing white and grey striped t-shirt takes the doughnut.", "The boy wearing white and grey striped t-shirt starts eating while the other boy is also eating"], "durations": null, "exact_frames_per_prompt": [60, 20], "background": "In the background there is yellow and red wall, windows, doughnut and people are talking.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/2uJyGgx8_24.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_1530_1"}, {"texts": ["A man wearing a checkered shirt is holding a book and is speaking."], "durations": null, "exact_frames_per_prompt": [80], "background": "In the background, the man is speaking. There are white walls, windows with white window blinds, two lights, a book and a white ceiling.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/1DTd-OJa1eA.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_1533_0"}, {"texts": ["A man wearing a blue-gray lining shirt is sitting, speaking, and showing a book."], "durations": null, "exact_frames_per_prompt": [80], "background": "In the background, there are windows, a white ceiling, white walls, wall lamps, and the voice of a man speaking is audible.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/1DTd-OJa1eA.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_1534_0"}, {"texts": ["A baby wearing a red shirt is sitting on the baby's chair and eating food while moving her hands.  while a person whose hands are visible is feeding the baby with a yellow spoon."], "durations": null, "exact_frames_per_prompt": [80], "background": "In the background, there is a white wall, black pillows, a baby's chair, grey bed sheet, a brown object, an orange bowl, an orange spoon, stickers on the wall, people are speaking, and some miscellaneous sounds are audible.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/0qcwqmkRLaw.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_1535_0"}, {"texts": ["A woman whose only hands are visible, feeding food to a baby with the spoon."], "durations": null, "exact_frames_per_prompt": [80], "background": "In the background, there is a white wall, black pillows, a baby's chair, grey bed sheet, a brown object, an orange bowl, an orange spoon, stickers on the wall, people are speaking, and some miscellaneous sounds are audible.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/0qcwqmkRLaw.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_1535_1"}, {"texts": ["A woman whose only upper half-body is visible wearing a black t-shirt is sitting on a chair on the left side and touching the girl's hair while group of people are sitting on chairs and doing different activities.", "The woman is pouring the food material on the bread."], "durations": null, "exact_frames_per_prompt": [33, 47], "background": "In the background, people are speaking, there is a glass surface table, white walls with black moldings, a television, white slabs with photo frames and some stuff, a frame on the left wall, bowls on the white tray, a black bowl with red sauce on the table, spatula, a bread on the table and a gray surface.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/0AHW5Xlyi3A.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_1539_0"}, {"texts": ["A woman whose only upper half-body is visible wearing a green top is sitting on a chair on the right side and is spreading the red sauce on the bread while a group of kids and a woman holding them are spreading the red sauce on the bread.", "The woman is pouring the white food material on the bread a group of kids and women are pouring the white food material on the bread."], "durations": null, "exact_frames_per_prompt": [33, 46], "background": "In the background, people are speaking, there is a glass surface table, white walls with black moldings, a television, white slabs with photo frames and some stuff, a frame on the left wall, bowls on the white tray, a black bowl with red sauce on the table, spatula, a bread on the table and a gray surface.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/0AHW5Xlyi3A.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_1539_2"}, {"texts": ["A boy wearing a black t-shirt and red shorts is sitting on a chair on the right side while holding a spatula while two kids are sitting and holding spatula and taking food for a bowl, and a group of women are sitting and watching them.", "The boy wearing a black t-shirt and red shorts is holding a white cream butter."], "durations": null, "exact_frames_per_prompt": [33, 47], "background": "In the background, people are speaking, there is a glass surface table, white walls with black moldings, a television, white slabs with photo frames and some stuff, a frame on the left wall, bowls on the white tray, a black bowl with red sauce on the table, spatula, a bread on the table and a gray surface.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/0AHW5Xlyi3A.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_1539_3"}, {"texts": ["A boy wearing a white t-shirt and dark green shorts is sitting on a chair along with other people.", "The boy is holding a spatula and pouring the spatula into the bowl while the girl and right side woman spread the sauce on the bread.", "The boy is then pouring the white food material on the bread along with other people."], "durations": null, "exact_frames_per_prompt": [20, 14, 46], "background": "In the background, people are speaking, there is a glass surface table, white walls with black moldings, a television, white slabs with photo frames and some stuff, a frame on the left wall, bowls on the white tray, a black bowl with red sauce on the table, spatula, a bread on the table and a gray surface.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/0AHW5Xlyi3A.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_1539_4"}, {"texts": ["A lady wearing a black dress lifts a chicken piece with silver tongs and checks the temperature of the chicken piece with a food thermometer."], "durations": null, "exact_frames_per_prompt": [68], "background": "In the background, there are chicken pieces, silver bowls, a paper napkin, kitchen cabinets, yellow battery, food thermometer, tongs, miscellaneous objects, a lady is speaking, and there is a miscellaneous sound.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/-gRYGmwgGd4.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_1545_0"}, {"texts": ["A person is holding a fried chicken piece with a tong and checking its temperature then a woman wearing a black chef's coat is speaking while facing forward."], "durations": null, "exact_frames_per_prompt": [68], "background": "In the background there is a tongue, a tissue paper, steel bowls, wooden table-top, a food thermometer, fried chicken pieces, wooden cabinets, some other items and a woman's voice is audible.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/-gRYGmwgGd4.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_1546_0"}, {"texts": ["A woman wearing a black chef coat is speaking facing front while a person whose hands are visible is holding a chicken piece and checking its temperature."], "durations": null, "exact_frames_per_prompt": [68], "background": "In the background there is a tongue, a tissue paper, steel bowls, wooden table-top, a food thermometer, fried chicken pieces, wooden cabinets, some other items and a woman's voice is audible.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/-gRYGmwgGd4.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_1546_1"}, {"texts": ["A person wearing black shoes is shearing a grey sheep on a green grass surface."], "durations": null, "exact_frames_per_prompt": [81], "background": "In the background there is a green grass surface, a sheep shearing machine, some wool, a white object, a grey and red object and a sheep's voice, a person's voice and miscellaneous sounds are audible.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/0cyNPIGScbA.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_1547_0"}, {"texts": ["A person of whom only hand is visible is holding a white paper with words on it while a group of people are walking and two people are sitting on chairs, a man wearing a gray t-shirt is standing and opening a can and drinking from it, and a group of vehicles are moving in different directions."], "durations": null, "exact_frames_per_prompt": [81], "background": "In the background, a person is speaking; there is a black sky, a blue color counter with posters, a green umbrella tent, chairs, a table, street light poles, glass walls, buildings, traffic lights, a tree, a concrete road, and a concrete sidewalk.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/8VXOxaT2Us0.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_1559_2"}, {"texts": ["A man wearing a dark gray t-shirt, white pants, and dark blue shoes is standing while holding a bottle in his hands while another man holding a poster and moves down.", "The man opening the bottle cap. and then starts drinking from the bottle while a man in a blue T-shirt watching him", "The man then starts drinking from the bottle."], "durations": null, "exact_frames_per_prompt": [24, 22, 35], "background": "In the background, a person is speaking; there is a black sky, a blue color counter with posters, a green umbrella tent, chairs, a table, street light poles, glass walls, buildings, traffic lights, a tree, a concrete road, and a concrete sidewalk.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/8VXOxaT2Us0.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_1559_3"}, {"texts": ["A man wearing white-black clothes is standing on the sidewalk while a group of vehicles is moving in different directions, in a group of people, two of them are sitting on the chairs, one is walking towards the right, and a woman wearing a blue t-shirt is standing on the right side."], "durations": null, "exact_frames_per_prompt": [81], "background": "In the background, a person is speaking; there is a black sky, a blue color counter with posters, a green umbrella tent, chairs, a table, street light poles, glass walls, buildings, traffic lights, a tree, a concrete road, and a concrete sidewalk.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/8VXOxaT2Us0.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_1559_4"}, {"texts": ["A girl wearing a purple graphic t-shirt and yellow trousers is sitting on the floor, showing a book and speaking."], "durations": null, "exact_frames_per_prompt": [74], "background": "In the background, a girl is audible. There is a book, a brown wooden floor, a stool, black cupboards, miscellaneous items on the cupboard, decorative LED lighting, a mirror, a white ceiling in the reflection, a ceiling light in the reflection, and white walls with a white electric switchboard.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/-jPJvKYKkqo.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_1561_0"}, {"texts": ["A person whom hands and hair are visible is folding his white shirt."], "durations": null, "exact_frames_per_prompt": [80], "background": "In the background, there is a blue bed-sheet, a white wall and some more clothes lying on the bed.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/0pSJ1kAkawU.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_1562_0"}, {"texts": ["A person wearing a white t-shirt is folding clothes."], "durations": null, "exact_frames_per_prompt": [80], "background": "In the background, there is a white shirt, a gray shirt, a black bed sheet, and a white wall.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/0pSJ1kAkawU.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_1563_0"}, {"texts": ["A bald man wearing a black shirt is sitting, talking, and moving his hand."], "durations": null, "exact_frames_per_prompt": [80], "background": "In the background, the music and people are audible. There is a cup, beer bottles, a beer bottle container, tables, chairs, a street light pole, a wall clock, a neon sign, clear glass windows, and a red-green wall.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/2jJBLg_O5Y8.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_1566_0"}, {"texts": ["A man wearing a white-blue t-shirt is sitting and drinking something from the cup."], "durations": null, "exact_frames_per_prompt": [27], "background": "In the background, the music and people are audible. There is a cup, beer bottles, a beer bottle container, tables, chairs, a street light pole, a wall clock, a neon sign, clear glass windows, and a red-green wall.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/2jJBLg_O5Y8.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_1566_1_ms"}, {"texts": ["A woman wearing an orange top is sitting drinking something from the cup and talking while the other person watches her."], "durations": null, "exact_frames_per_prompt": [16], "background": "In the background, the music and people are audible. There is a cup, beer bottles, a beer bottle container, tables, chairs, a street light pole, a wall clock, a neon sign, clear glass windows, and a red-green wall.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/2jJBLg_O5Y8.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_1566_3"}, {"texts": ["A baby wearing orange clothes is sitting and watching the newspaper on a brown table."], "durations": null, "exact_frames_per_prompt": [79], "background": "In the background, the sound of baby speaking is audible and some miscellaneous sounds are audible. There is a white wall, a white door, a brown table, a brown chair, a newspaper, and white surface.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/65cgE8eXyrk.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_1569_0"}, {"texts": ["A girl wearing orange clothes is sitting on a chair and watching the newspaper on a brown table."], "durations": null, "exact_frames_per_prompt": [79], "background": "In the background, there is a white wall, a white door, a brown table, a brown chair, a newspaper, and a white surface, and the girl speaking and miscellaneous sounds are audible.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/65cgE8eXyrk.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_1570_0"}, {"texts": ["A person, whose only hand is visible, wearing a gray cloth is holding a fruit while a person wearing blue clothes is standing in the backside holding a camera and taking pictures."], "durations": null, "exact_frames_per_prompt": [28], "background": "In the background, there is a soil surface, green surface, green trees, fruits, sky and the sound of the music is audible.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/1Y-093zNUYw.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_1572_3"}, {"texts": ["A person whose hand is visible is mixing brown liquid in a white-blue cup with a spoon.", "The person is mixing light-yellow liquid in a white cup with the spoon."], "durations": null, "exact_frames_per_prompt": [14, 17], "background": "In the background, there is a miscellaneous sound, there is a grey counter top, a wooden yarn butler, a white-blue cup with brown liquid, a white cup with light-yellow liquid, a green packet, a white-green lid, and an electric kettle.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/3-wGSNUMoIM.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_1576_0"}, {"texts": ["A person wearing pink striped trousers is at first hitting the golf ball with a golf stick.", "The person picks up another golf ball.", "The person takes position to hit it with a golf stick."], "durations": null, "exact_frames_per_prompt": [33, 29, 18], "background": "In the background, there is a golf ground, golf balls, trees, a golf stick, and the sky.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/-IOJwNb02fA.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_1579_0"}, {"texts": ["A woman wearing white-black clothes is lying on the gray bed and is getting tattooed by a man while a person wearing black clothes is standing."], "durations": null, "exact_frames_per_prompt": [80], "background": "In the background, the sound of tattoo machine and people speaking are audible, there is a white wall, white-black surface, a gray dustbin, a gray table with some bottles and other stuff, a white object and a pole.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/09GcDW_bbg0.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_1580_0"}, {"texts": ["A man wearing white clothes is holding a pen and making a tattoo on the leg of the woman while a person wearing black clothes is standing."], "durations": null, "exact_frames_per_prompt": [80], "background": "In the background, the sound of tattoo machine and people speaking are audible, there is a white wall, white-black surface, a gray dustbin, a gray table with some bottles and other stuff, a white object and a pole.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/09GcDW_bbg0.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_1580_1"}, {"texts": ["A girl wearing a gray t-shirt is sitting and applying makeup on her eyebrow with a brush."], "durations": null, "exact_frames_per_prompt": [80], "background": "In the background, there is a white wall, a sofa, a table, a desk, some other stuff, white floor, home decor item, and a song is playing.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/0dh0SVaws_E.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_1584_0"}, {"texts": ["A person wearing white clothes is standing on the right side of the machine and folding a paper."], "durations": null, "exact_frames_per_prompt": [80], "background": "In the background, there is a gray-white machine, paper, another machine, scissors, wooden table, white wall, voices of the people and some miscellaneous sounds are audible.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/11JpwKsubmc.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_1586_0"}, {"texts": ["A man in an army dress is sitting and looking to the right while speaking while another man wearing grey outfit is sitting on the right side of first man and he is eating something."], "durations": null, "exact_frames_per_prompt": [57], "background": "In the background, there are green plants, a red curtain, red walls, an entrance, some objects, the voices of the men and the sound of music is audible.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/5GNA1wOpfPk.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_1587_1"}, {"texts": ["A man wearing a brown shirt is sitting, and eating sesame cake.", "The man starts talking."], "durations": null, "exact_frames_per_prompt": [49, 31], "background": "In the background, the music and people are audible. There is a sesame cake, green plants, a photo frame, a white wall, a room decoration piece, a red cloth, red walls, and a few miscellaneous things.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/5GNA1wOpfPk.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_1588_1"}, {"texts": ["A woman whose only hands are visible is folding a white envelope from the corner to the center line."], "durations": null, "exact_frames_per_prompt": [64], "background": "In the background, there is a white envelope, wooden surface and a woman's voice and some music is audible.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/1FCzkG5Z1Lg.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_1589_0"}, {"texts": ["A woman whose hands are visible is folding a white paper sheet."], "durations": null, "exact_frames_per_prompt": [64], "background": "In the background, music is playing, a woman is speaking. There is a wooden table and a white paper sheet.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/1FCzkG5Z1Lg.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_1590_0"}, {"texts": ["A girl wearing a grey-blue jacket with red stripes is sitting inside a car.", "The girl starts eating chips."], "durations": null, "exact_frames_per_prompt": [27, 53], "background": "In the background, another girl is speaking, a miscellaneous sound, there is a car with a window, an outside view of trees and a bright sky.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/0J2hIKBCSDg.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_1596_0"}, {"texts": ["A person whose hands are visible is peeling a potato.", "The person puts it on the cutting board.", "The person lifts the knife."], "durations": null, "exact_frames_per_prompt": [40, 8, 30], "background": "In the background, a person is audible. There is a peeler, a potato, a knife, a brown wooden cutting board, and a brown surface.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/2M_mheOleKA.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_1597_0"}, {"texts": ["A woman whose hands are visible is peeling the purple sweet potato with the potato peeler utensil.", "The woman sides the sweet potato peels.", "The woman is picking up the knife with her right hand and holding a sweet potato in her left hand on the table."], "durations": null, "exact_frames_per_prompt": [32, 18, 30], "background": "In the background, a woman is speaking; a purple sweet potato, sweet potato peel, a knife on the table, and a wooden surface table.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/2M_mheOleKA.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_1598_0"}, {"texts": ["A woman wearing a white shirt is sitting on a black chair, holding an opened book and pointing at the book while showing it in front and speaking."], "durations": null, "exact_frames_per_prompt": [80], "background": "In the background, there is a wooden cupboard, a poster, pink wall, white wall, books, a frog toy, some stuff, black chair and the voice of the woman is audible.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/-XlUXlF_8sc.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_1600_0"}, {"texts": ["A person whose only hands are visible is folding the bank note into a triangular shape."], "durations": null, "exact_frames_per_prompt": [80], "background": "In the background, there is a bank note, a colourful book front cover, and the man speaking sound is audible.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/0-y-znu_-vw.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_1602_0"}, {"texts": ["A person whose hand is visible is holding a marker and writing on a white board."], "durations": null, "exact_frames_per_prompt": [80], "background": "In the background, a person is speaking and writing sounds are audible, there is a white board and a marker.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/1atLOEBiXzs.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_1604_0"}, {"texts": ["A baby wearing a red t-shirt is eating while sitting on a red chair while the woman is feeding the baby.", "The baby starts walking on a beige surface."], "durations": null, "exact_frames_per_prompt": [50, 27], "background": "In the background there is a window with blind curtains, a chair, a table, a blue and white wall, a beige surface, some objects and women's voices and some miscellaneous sounds are audible.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/1OOkguyr8zw.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_1605_0"}, {"texts": ["A woman wearing blue pants is sitting on a chair while a baby wearing a red t-shirt is sitting on another chair.", "The woman wearing blue pants is feeding something to a baby."], "durations": null, "exact_frames_per_prompt": [14, 63], "background": "In the background there is a window with blind curtains, a chair, a table, a blue and white wall, a beige surface, some objects and women's voices and some miscellaneous sounds are audible.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/1OOkguyr8zw.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_1605_1"}, {"texts": ["A woman wearing black clothes is lying and laughing and talking."], "durations": null, "exact_frames_per_prompt": [80], "background": "In the background, music is playing and people are speaking, there is black-white surface, a red surface, a bottle and a white object.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/-9Ccqyo9bzw.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_1607_0"}, {"texts": ["A man wrapped in a black sheet is eating a watermelon that is kept on a table with is mouth."], "durations": null, "exact_frames_per_prompt": [80], "background": "In the background, there are slices of watermelon, trees, and a red table.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/2VYtvaXujSE.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_160_0"}, {"texts": ["A woman wearing a yellow t-shirt and brown tights is standing on a green grass field, holding a golf club.", "The woman is moving it back to hit a golf ball."], "durations": null, "exact_frames_per_prompt": [74, 6], "background": "In the background, miscellaneous sounds and people's giggles are audible. There is a golf club, a golf ball, a green grass field, a brown soil surface, and green trees.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/1fp_39s2cvg.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_1611_0"}, {"texts": ["A man wearing a white t-shirt is standing and drinking.", "The man starts vomiting."], "durations": null, "exact_frames_per_prompt": [17, 63], "background": "In the background, there is a white refrigerator with some stuff, a white door, a brown table with containers, a brown wall, and wooden shelves, a kitchen counter-top with some stuff, a white tile floor, and the man speaking, laughing and other miscellaneous sounds are audible.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/70_3beY6Boo.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_1612_0"}, {"texts": ["A man wearing a blue shirt is sitting and tapping on the newspaper while holding a baby while the baby is turning the pages of news paper."], "durations": null, "exact_frames_per_prompt": [42], "background": "In the background, miscellaneous sounds are audible. There are white walls, a brown table, a radio, a newspaper, a window, a curtain and green bushes.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/1bxgd3_srgU.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_1615_0"}, {"texts": ["A woman sitting opposite person one is looking at the baby while the baby is turning the paper page."], "durations": null, "exact_frames_per_prompt": [42], "background": "In the background, miscellaneous sounds are audible. There are white walls, a brown table, a radio, a newspaper, a window, a curtain and green bushes.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/1bxgd3_srgU.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_1615_1"}, {"texts": ["A man wearing white clothes is holding a snake around his neck and he is at first looking in the left direction while speaking, then he looks at the snake."], "durations": null, "exact_frames_per_prompt": [80], "background": "In the background, there is a sky, hill, trees, an asphalt road, grassy surface, and the fence.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/5Rtkyr2T_Ro.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_161_0"}, {"texts": ["A greyish green colored snake is held by a man around his neck."], "durations": null, "exact_frames_per_prompt": [80], "background": "In the background, there is a sky, hill, trees, an asphalt road, grassy surface, and the fence.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/5Rtkyr2T_Ro.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_161_1"}, {"texts": ["A brown-white-black dog tied with a leash is walking on the green grass surface and on the grey road."], "durations": null, "exact_frames_per_prompt": [80], "background": "In the background, there is a miscellaneous sound, there is a green grass surface, wired barriers, green trees, and a grey road.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/08gIDD85azQ.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_1623_0"}, {"texts": ["A fish is being caught while a boy whose head is visible is standing in front.", "The fish is being pulled out of the water."], "durations": null, "exact_frames_per_prompt": [24, 8], "background": "In the background, people are audible. There is a fishing rod and a water body.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/0QrJg_OGVfE.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_1628_3"}, {"texts": ["A woman wearing a camouflage t-shirt is mixing a yellow liquid and speaking."], "durations": null, "exact_frames_per_prompt": [80], "background": "In the background, there is a white wooden shelves, glass containers, a red and yellow colander, black wall with some hand crafts, a white ceiling and the woman speaking and dog barking sound is audible.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/21pDj2CR75M.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_1631_0"}, {"texts": ["A woman wearing a blue t-shirt is standing and holding the bottle with another girl."], "durations": null, "exact_frames_per_prompt": [45], "background": "In the background, there are wooden racks, plastic containers, some other stuff, a bicycle, trees, green grass, white wall, soil surface, concrete surface and a song is audible.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/324KYNOHS6Y.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_1633_1"}, {"texts": ["A girl whose only hands are visible is holding the bottle with the woman."], "durations": null, "exact_frames_per_prompt": [45], "background": "In the background, there are wooden racks, plastic containers, some other stuff, a bicycle, trees, green grass, white wall, soil surface, concrete surface and a song is audible.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/324KYNOHS6Y.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_1633_2_ms"}, {"texts": ["A woman wearing a gray t-shirt is applying makeup on her right eyebrow."], "durations": null, "exact_frames_per_prompt": [80], "background": "In the background, there is a mirror, white wall with flower print, cupboards, racks, boxes, a microphone, a makeup brush, and a woman's voice and some music is audible.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/0YW9slPyVtU.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_1635_0"}, {"texts": ["A man wearing white clothes is juggling with bottles on the stage while a man wearing a white t-shirt is standing in front, holding a mic, a group of people is standing and watching him, and a few people are recording him."], "durations": null, "exact_frames_per_prompt": [80], "background": "In the background, there are lights, bottles, a stage, a canopy, a banner, metal pillars, a tripod, some other objects, and the voice of singing, the sound of music, cheering, and some miscellaneous sounds are audible.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/2rEdUUvNYOs.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_1637_0"}, {"texts": ["A man wearing a black t-shirt is standing on the stage and holding a white object in his hand while another man wearing a white t-shirt is performing juggling with bottles on the stage and moving."], "durations": null, "exact_frames_per_prompt": [80], "background": "In the background, there are lights, bottles, a stage, a canopy, a banner, metal pillars, a tripod, some other objects, and the voice of singing, the sound of music, cheering, and some miscellaneous sounds are audible.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/2rEdUUvNYOs.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_1637_1"}, {"texts": ["A man wearing a cap is speaking, standing, and holding a microphone in his hand while a man in a white t-shirt is performing bartending, a group of people are watching him and some of them are clapping."], "durations": null, "exact_frames_per_prompt": [80], "background": "In the background, there are lights, bottles, a stage, a canopy, a banner, metal pillars, a tripod, some other objects, and the voice of singing, the sound of music, cheering, and some miscellaneous sounds are audible.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/2rEdUUvNYOs.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_1637_3"}, {"texts": ["A man wearing a white t-shirt is standing and recording the juggling performance while a man wearing a hat is standing while holding a microphone in his hand and speaking."], "durations": null, "exact_frames_per_prompt": [80], "background": "In the background, there are lights, bottles, a stage, a canopy, a banner, metal pillars, a tripod, some other objects, and the voice of singing, the sound of music, cheering, and some miscellaneous sounds are audible.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/2rEdUUvNYOs.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_1637_4"}, {"texts": ["A man wearing a black sweatshirt, blue jeans and yellow-black hat is sitting near the table and eating the food."], "durations": null, "exact_frames_per_prompt": [80], "background": "In the background, there is a table of glass, a plate of glass, cream color with printed placemats, a red bottle, red-white carton, a white bowl, white wall, metallic racks, white couch, photo frames, a blue vacuum cleaner machine, brown border, brown surface, black surface, red lights and a miscellaneous sound is audible.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/-X094A_ynWs.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_1638_0"}, {"texts": ["A person whose hands are visible is scratching the packet.", "The person is holding the packet.", "The person is putting their hands on it."], "durations": null, "exact_frames_per_prompt": [27, 14, 39], "background": "In the background, two people are speaking. There is a wooden floor and a packet.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/1byIuPsTD4o.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_1642_0"}, {"texts": ["A person wearing shorts puts a lid on the weber kettle grill.", "The person lifts the lid and starts touching the sausage with a tong."], "durations": null, "exact_frames_per_prompt": [50, 30], "background": "In the background, there is a Weber kettle grill, a gray surface, and some miscellaneous sounds are audible.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/2TUJ-ULecDE.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_1643_0"}, {"texts": ["A woman whose upper half-body is visible wearing a black tank-top is looking left and up while smiling."], "durations": null, "exact_frames_per_prompt": [74], "background": "In the background, music is playing; there are papers with tattoo designs, a stick, a red surface, a gray surface table, pink walls; guitars, a mirror, and frames are hanging on the wall, a black counter with stuff, a big board with yellow words, ceiling lights, and chairs.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/2LL6SXh_Cjk.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_1644_1_ms"}, {"texts": ["A man wearing a black t-shirt and black jeans is standing and looking down while moving his body."], "durations": null, "exact_frames_per_prompt": [47], "background": "In the background, music is playing; there are papers with tattoo designs, a stick, a red surface, a gray surface table, pink walls; guitars, a mirror, and frames are hanging on the wall, a black counter with stuff, a big board with yellow words, ceiling lights, and chairs.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/2LL6SXh_Cjk.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_1644_5"}, {"texts": ["A man wearing a white t-shirt is touching the tree, and plucking an apple from the tree while a girl wearing a black and white dress is standing on a grassy surface and putting apples into a plastic bag.", "The man is giving apple to the girl."], "durations": null, "exact_frames_per_prompt": [63, 17], "background": "In the background, there are trees, green grasses, apples, a carry bag, people are speaking and vehicle's voices are audible.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/-B14kmeX4EI.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_1646_0"}, {"texts": ["A girl is standing, holding a carry bag full of apples while a man in brown shorts is standing and picking apples from the tree and giving them to the girl.", "The girl is taking an apple from the man."], "durations": null, "exact_frames_per_prompt": [64, 16], "background": "In the background, there are trees, green grasses, apples, a carry bag, people are speaking and vehicle's voices are audible.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/-B14kmeX4EI.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_1646_1"}, {"texts": ["A man wearing a black t-shirt is standing.", "The man is pouring a meat piece into the food and then pressing the meat piece."], "durations": null, "exact_frames_per_prompt": [44, 36], "background": "In the background, there is a wall, white cardboard, a knife, a container, some other objects, and the voice of a man speaking, and some miscellaneous sounds are audible.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/3RFfz4X4SBg.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_1647_0"}, {"texts": ["A person wearing a black t-shirt is standing and opening a yellow envelope."], "durations": null, "exact_frames_per_prompt": [47], "background": "In the background there is a brown surface, white counter-top, a paper glass, wooden cabinets, kitchen appliances, a yellow envelope, computer devices, some English letters, numbers and a music playing sound is audible.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/-a3lfKV4tEE.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_1648_0"}, {"texts": ["A baby wearing a yellow cloth is lying on the baby bed.", "The baby starts crying."], "durations": null, "exact_frames_per_prompt": [31, 49], "background": "In the background, people are speaking and the baby is crying, there is a white baby bed, and some other objects.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/067FgZFiyNQ.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_1649_0"}, {"texts": ["A man wearing a white-black t-shirt is sitting on a camel while another man in a white outfit is standing and holding a leash of the camel and then moves towards the right."], "durations": null, "exact_frames_per_prompt": [80], "background": "In the background, people and miscellaneous sounds are audible. There is a sand surface, a wooden barn, shed, poles, chairs, and a yellow-blue sky.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/0yAa3oIB0TI.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_1652_0"}, {"texts": ["A brown camel is standing and moving while a man wearing a wearing a white cap is sitting on the camel back is speaking while showing thumps up and then the another man starts pulling the camel leash and walks toward the right."], "durations": null, "exact_frames_per_prompt": [80], "background": "In the background, people and miscellaneous sounds are audible. There is a sand surface, a wooden barn, shed, poles, chairs, and a yellow-blue sky.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/0yAa3oIB0TI.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_1652_1"}, {"texts": ["A man wearing a white dishdasha is standing, holding the camel leash while a man wearing a black-white t-shirt is sitting on the back of the camel and showing his hand.", "The man is moving."], "durations": null, "exact_frames_per_prompt": [30, 50], "background": "In the background, people and miscellaneous sounds are audible. There is a sand surface, a wooden barn, shed, poles, chairs, and a yellow-blue sky.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/0yAa3oIB0TI.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_1652_2"}, {"texts": ["A girl wearing white clothes is eating something.", "The girl is making faces."], "durations": null, "exact_frames_per_prompt": [25, 55], "background": "In the background, there are papers, maroon shelves, a maroon table, some other stuff, and the voice of people speaking, and some miscellaneous sounds are audible.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/2-slSV1Uakc.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_1653_0"}, {"texts": ["A man wearing a white chef coat is standing near cooked meat", "The man is showing thumbs up."], "durations": null, "exact_frames_per_prompt": [20, 20], "background": "In the background, there brick walls, a table, cooked meat, and the bushes.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/0g-1i2ZSnwU.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_1655_0"}, {"texts": ["A person whose hand is visible is holding a bowl and putting egg yolk in a frying pan."], "durations": null, "exact_frames_per_prompt": [45], "background": "In the background, sizzle sounds are audible, there is black fry pan, a led, brown wall, a gas stove, a spatula, and a newspaper on the brown surface.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/12_f_NEzN9o.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_1656_0"}, {"texts": ["A woman wearing a white-blue checked shirt is standing on the left side of the horse and combing the hair of the horse."], "durations": null, "exact_frames_per_prompt": [58], "background": "In the background, there is a white truck, white building, white surface, sky, yellow ceiling, lights, yellow wall, windows, clothes, soil surface, voice of the woman and some miscellaneous sounds are audible.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/-wa9wu7bCJ0.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_1657_0_ms"}, {"texts": ["A brown horse is standing on the right side of the first woman and getting his hair combed by that woman."], "durations": null, "exact_frames_per_prompt": [58], "background": "In the background, there is a white truck, white building, white surface, sky, yellow ceiling, lights, yellow wall, windows, clothes, soil surface, voice of the woman and some miscellaneous sounds are audible.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/-wa9wu7bCJ0.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_1657_1"}, {"texts": ["A brown horse is standing on the soil surface with another woman who is sitting on his back.\n"], "durations": null, "exact_frames_per_prompt": [22], "background": "In the background, there is a white truck, white building, white surface, sky, yellow ceiling, lights, yellow wall, windows, clothes, soil surface, voice of the woman and some miscellaneous sounds are audible.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/-wa9wu7bCJ0.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_1657_3_ms"}, {"texts": ["A person whose fingers are visible is deep frying the onion ring with the spatula and straining the onion ring from the spatula, then showing the onion ring on the frying steel strainer."], "durations": null, "exact_frames_per_prompt": [51], "background": "In the background, a song is playing, there is a fry pan with oil and onion rings, a white gas stove, and a steel spatula.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/34Aj4geYWS4.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_165_0"}, {"texts": ["A man wearing a light colored cap is sitting on a black horse.", "The man wearing a light colored cap is throwing his cap while the black horse is jumping on the brown surface."], "durations": null, "exact_frames_per_prompt": [34, 9], "background": "In the background, there is a soil surface, a big brown rock, a pile of brown grass, and the voice of a person speaking, the sound of music, and some miscellaneous sounds are audible.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/4V_1792oiT4.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_1660_0"}, {"texts": ["A black horse is jumping while lifting man one on the back."], "durations": null, "exact_frames_per_prompt": [43], "background": "In the background, there is a soil surface, a big brown rock, a pile of brown grass, and the voice of a person speaking, the sound of music, and some miscellaneous sounds are audible.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/4V_1792oiT4.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_1660_2"}, {"texts": ["A man wearing a striped t-shirt is hitting the golf ball with the golf stick.", "The man is watching in the right direction."], "durations": null, "exact_frames_per_prompt": [33, 32], "background": "In the background, there are hills, a wooden deck, a golf stick, a golf ball, and the sky.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/-cIRzJUnZTw.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_1666_0"}, {"texts": ["A girl wearing a shirt and denim jeans is walking, putting a liquid on the horse.", "The girl is wearing a glove while the horse is standing on the left side.", "The girl starts rubbing the horse."], "durations": null, "exact_frames_per_prompt": [29, 37, 12], "background": "In the background, there are wooden walls, a brown surface, a black wire, some other objects, and the voice of a girl speaking, and music is audible.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/0bnT_R9v4kk.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_1668_0"}, {"texts": ["A white gray horse is standing, getting a liquid applied on the back while a girl is putting down something.", "The white gray horse is getting rubbed by a girl on the brown surface."], "durations": null, "exact_frames_per_prompt": [29, 51], "background": "In the background, there are wooden walls, a brown surface, a black wire, some other objects, and the voice of a girl speaking, and music is audible.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/0bnT_R9v4kk.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_1668_1"}, {"texts": ["A white-gray horse is standing while a woman wearing white shirt is squeezing a scrub bottle on the horse body then takes a hand towel put it in her hand and starts scrubbings to the horse.", "The horse is getting cleaned by the woman."], "durations": null, "exact_frames_per_prompt": [67, 13], "background": "In the background a person, music and miscellaneous sounds are audible. There is a shampoo bottle, a white towel, a green pipe, a basket, a scrubbing glove, a brown leash, wooden walls, and a brown floor.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/0bnT_R9v4kk.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_1669_1"}, {"texts": ["A woman wearing a gray top is standing and peeling a potato with a knife while another woman wearing white vest, standing on the right side of first woman is also peeling a potato with a knife and third woman wearing white and blue outfit, standing on the right side of second woman is cutting something using a knife."], "durations": null, "exact_frames_per_prompt": [80], "background": "In the background, people are speaking, music is playing. There are white cabinets, a chimney, an oven, a green wall, a white fridge with stickers and some stuff on it, a sink with potatoes in it, a white floor, a glass jar, a tap, and a counter top with some stuff on it.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/2nj2B4HiQ00.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_166_0"}, {"texts": ["A woman wearing a white vest is standing and peeling a potato with a knife while another woman wearing a gray t-shirt is also peeling a potato with a knife and the third woman in a white t-shirt is standing on the right and doing something."], "durations": null, "exact_frames_per_prompt": [80], "background": "In the background, people are speaking, music is playing. There are white cabinets, a chimney, an oven, a green wall, a white fridge with stickers and some stuff on it, a sink with potatoes in it, a white floor, a glass jar, a tap, and a counter top with some stuff on it.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/2nj2B4HiQ00.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_166_2"}, {"texts": ["A man wearing a suit is sitting in front right seat of a car while watching backward and talking and holding money in his hand.", "The man turns into the front direction."], "durations": null, "exact_frames_per_prompt": [75, 5], "background": "In the background, there is an inside view of a car and the street.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/0qTKgJZTU3U.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_1673_0"}, {"texts": ["A man wearing a suit is sitting in the car and is looking behind while speaking."], "durations": null, "exact_frames_per_prompt": [80], "background": "In the background, the man is speaking and other people are also audible. There is a car with front center console visible and some buildings with yellow lights and a black sky.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/0qTKgJZTU3U.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_1674_0"}, {"texts": ["A man wearing a black jacket is sitting on the grey surface and putting the air pressure gauge on the tyre.", "The man stands up. ", "The man shows the air pressure gauge."], "durations": null, "exact_frames_per_prompt": [60, 15, 5], "background": "In the background, there is a sky, trees, building, a pole, a measuring tool, a black car, parked cars, grey surface, a man is speaking and some miscellaneous sound is audible.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/06dCiDZV8MM.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_1675_0"}, {"texts": ["A girl wearing a green dress is standing in the middle and giving a piece of fruit to the other girl while a girl wearing pink clothes is walking to the right and other persons are standing behind."], "durations": null, "exact_frames_per_prompt": [31], "background": "In the background, there are tables, trays filled with fruits, a black tray, cups, another black tray, chairs, brown wall, notice boards with notice, glass doors, iron structure, and some music is audible.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/1JzvBGskXRU.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_1677_0"}, {"texts": ["A girl wearing a light pink jacket is standing on the right side of the first girl.", "The girl picks the piece of fruit from the first girl's hand ", "The girl eats it."], "durations": null, "exact_frames_per_prompt": [19, 7, 5], "background": "In the background, there are tables, trays filled with fruits, a black tray, cups, another black tray, chairs, brown wall, notice boards with notice, glass doors, iron structure, and some music is audible.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/1JzvBGskXRU.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_1677_1_ms"}, {"texts": ["A girl wearing a pink dress is standing on the left side of the first girl then walking on the right side.", "The girl picks a piece of fruit from the tray.", "The girl eats it."], "durations": null, "exact_frames_per_prompt": [31, 8, 23], "background": "In the background, there are tables, trays filled with fruits, a black tray, cups, another black tray, chairs, brown wall, notice boards with notice, glass doors, iron structure, and some music is audible.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/1JzvBGskXRU.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_1677_2"}, {"texts": ["A man wearing orange t-shirt and jeans is performing flair bartending with a bottle and glass on the stage."], "durations": null, "exact_frames_per_prompt": [80], "background": "In the background, there is a stage, lights, a display device, a steel frame, a table, glass bottles and People screaming sound and a song is audible.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/0uDpU9YlPOI.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_1678_0"}, {"texts": ["A baby wearing white cloth is sitting on a baby chair and eating cake from a plate."], "durations": null, "exact_frames_per_prompt": [80], "background": "In the background, a person is speaking. There are white walls, a white door, gray surface, and a baby chair, a white table and white plate.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/2DMxUUkDgFI.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_1679_0"}, {"texts": ["A person whose hand is visible is holding a watermelon cutter and a watermelon and cutting the watermelon while another person whose hand is visible holds the watermelon."], "durations": null, "exact_frames_per_prompt": [80], "background": "In the background, people are speaking, there is a brown countertop, watermelon, a watermelon cutter, a bowl and a light yellow wall.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/aF95fp665tA.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_1680_0"}, {"texts": ["A person whose hand is visible is holding the watermelon while a person wearing a white t-shirt is cutting a watermelon with a knife."], "durations": null, "exact_frames_per_prompt": [64], "background": "In the background, people are speaking, there is a brown countertop, watermelon, a watermelon cutter, a bowl and a light yellow wall.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/aF95fp665tA.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_1680_1"}, {"texts": ["A man on the right side wearing a black t-shirt and blue jeans is standing and puts his hand on his mouth while another boy wearing a gray t-shirt is sitting on a chair next to him and a boy wearing a black vest is sitting on the extreme left.", "The man starts vomiting on the table while a few people are sitting at the front watching them and the second boy starts walking."], "durations": null, "exact_frames_per_prompt": [23, 40], "background": "In the background, there is a white table, chairs, a whiteboard, brown doors, white wall, a television, gray floor, photo frame, a cupboard and people speaking, laughing and screaming sound is audible.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/0eRNvk76Fhg.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_1682_0"}, {"texts": ["A man wearing a black cloth is standing on the black surface, holding a drink in a glass with whipped cream on top in the left hand.", "The man is about to drink the glass of drink with whipped cream and he starts laughing, while holding an empty jar in his right hand.", "The man starts drinking the drink with whipped cream."], "durations": null, "exact_frames_per_prompt": [29, 23, 28], "background": "In the background, there is a white wall, a window, a sign of no smoking zone, black surface, an empty white glass, the voices of the men and a miscellaneous sound is audible.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/0GIc6gWGuhU.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_1683_0"}, {"texts": ["A man wearing black cloth is standing, holding a glass of drink and a white jar, and laughing.", "The man is drinking."], "durations": null, "exact_frames_per_prompt": [54, 26], "background": "In the background, people are speaking and laughing. There is a white wall, a window, a door, black surface, a glass, carton box, and some other objects.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/0GIc6gWGuhU.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_1684_0"}, {"texts": ["A person whose upper half-body is visible wearing a gray-black t-shirt, is standing behind the counter, holding a pineapple with his left hand and is cutting the pineapple on both sides with a knife."], "durations": null, "exact_frames_per_prompt": [80], "background": "In the background, music is playing, a person is speaking; there is a brown granite counter, a brown chopping board with a pineapple on the counter, brown cupboards, light blue walls, backside counter-top with stuff, gas stove and brown cabinets, and a brown bowl on the front counter-top.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/1Dc2ULdonIc.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_1685_0"}, {"texts": ["A woman wearing a pink top walks.", "The woman sits on the floor and starts feeding the goat."], "durations": null, "exact_frames_per_prompt": [51, 30], "background": "In the background, people are audible. There is a cemented floor, an orange surface, wooden barn, green trees, and a blue sky.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/0SDmcLjxbv8.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_1686_0"}, {"texts": ["A person wearing a gray sweater is standing and shredding white paper sheets in a shredding machine."], "durations": null, "exact_frames_per_prompt": [81], "background": "In the background, a person is speaking, and the sound of shredding machine is audible. There is a white wall, white paper sheets, and a shredding machine.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/4NaqD-MybQw.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_1687_0"}, {"texts": ["A person wearing black clothes is sitting holding a book while a cat is lying on the person's lap."], "durations": null, "exact_frames_per_prompt": [80], "background": "In the background, a miscellaneous sound is audible. There is a wall, a blue blanket, a black object and a book.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/1dGXZKsXWyE.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_1688_0"}, {"texts": ["A black brown cat is sitting on the first person's lap."], "durations": null, "exact_frames_per_prompt": [80], "background": "In the background, a miscellaneous sound is audible. There is a wall, a blue blanket, a black object and a book.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/1dGXZKsXWyE.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_1688_1"}, {"texts": ["A man whose hands are visible, wearing a black cloth, is holding a light-yellow sheet of sponge.", "The man starts spraying water on the sheet of sponge with a white-green spray bottle.", "The man is putting the sheet of sponge on a heat press machine."], "durations": null, "exact_frames_per_prompt": [41, 25, 14], "background": "In the background, the man is speaking, there is a white surface, a heat press machine with white papers, a light-yellow sheet of sponge, a white-green spray bottle, a grey thermal binding machine, and a grey wall.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/23HXVAwRjG0.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_1689_0"}, {"texts": ["A person wearing a blue cloth is sitting and cutting a packet with a knife.", "The person wearing a blue cloth is taking out the parcel from the packet."], "durations": null, "exact_frames_per_prompt": [29, 51], "background": "In the background, there is a wooden table, a knife, a parcel and some miscellaneous sounds are audible.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/-Sc5LrF0pzY.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_1697_0"}, {"texts": ["A man whose half body is visible is wearing a blue shirt and jeans is sitting on his knee and tying a bandage on a horse's leg."], "durations": null, "exact_frames_per_prompt": [80], "background": "In the background, there is a tile floor, a bandage and a person's voice is audible.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/03mk_KF1tEc.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_1698_0"}, {"texts": ["A horse whose legs are visible is standing and being bandaged by the man."], "durations": null, "exact_frames_per_prompt": [80], "background": "In the background, there is a tile floor, a bandage and a person's voice is audible.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/03mk_KF1tEc.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_1698_1"}, {"texts": ["A man in a black printed t-shirt keeps one hand on the wall and a beer bottle in the other, first hitting the beer bottle on the wall.", "The man in a black printed t-shirt opens the bottle cap."], "durations": null, "exact_frames_per_prompt": [37, 43], "background": "In the background, there is a wall, brown door, a beer bottle, a t-shirt, music is playing, people are speaking, and miscellaneous sounds are audible.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/ShngatsD30g.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_1699_0"}, {"texts": ["A girl wearing a pink jacket is standing and giving a carrot to a goat while some animals are standing behind the fence and some people are walking on the right side."], "durations": null, "exact_frames_per_prompt": [80], "background": "In the background, there is green iron fencing, straws, road surface, poles, shadows and people speaking and some miscellaneous sounds are audible.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/-MWEwChPyOA.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_1701_0"}, {"texts": ["A person whom hand is visible is holding the girl's hand then leaves her hand while a group of goats are standing and moving behind the metal bars."], "durations": null, "exact_frames_per_prompt": [18], "background": "In the background, there is green iron fencing, straws, road surface, poles, shadows and people speaking and some miscellaneous sounds are audible.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/-MWEwChPyOA.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_1701_2"}, {"texts": ["A boy wearing a red t-shirt is sitting on a wooden chair, holding the noodles with a fork and eating them."], "durations": null, "exact_frames_per_prompt": [80], "background": "In the background, there is a wooden door, a black-white table, fork, a white bowl, noodles, wooden chair, a miscellaneous sound, the voices of the woman and the boy are audible.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/-skUKi4ItkA.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_1705_0"}, {"texts": ["A girl wearing a grey t-shirt is standing in front of the digital screen."], "durations": null, "exact_frames_per_prompt": [80], "background": "In the background, there is a digital screen, a red sun-glass, a girl is speaking and some miscellaneous sounds are audible.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/-uBdzh95I7E.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_1708_0"}, {"texts": ["A baby wearing white and green printed clothes is picking some socks one by one from the basket."], "durations": null, "exact_frames_per_prompt": [80], "background": "In the background there is a plastic bucket, a rocking horse, a light blue wall, wooden shelf, a brown surface, some clothes, some other stuff and some miscellaneous sounds are audible.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/4zEkAv6Xfgw.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_170_0"}, {"texts": ["A woman wearing a teal t-shirt and grey jeans is sitting on the horse and gripping the horse leash on the green grass surface."], "durations": null, "exact_frames_per_prompt": [80], "background": "In the background, there are trees, a hill, cloudy sky, sheds, silver railing, blue bucket, green grass surface, a man is speaking and a woman's voice is audible.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/-u70G5M13co.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_1711_1"}, {"texts": ["A brown horse is walking on the green grass surface while a girl is sitting on the horse."], "durations": null, "exact_frames_per_prompt": [80], "background": "In the background, there are trees, a hill, cloudy sky, sheds, silver railing, blue bucket, green grass surface, a man is speaking and a woman's voice is audible.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/-u70G5M13co.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_1711_2"}, {"texts": ["A man wearing gray pants is smoking and counting the notes while dancing."], "durations": null, "exact_frames_per_prompt": [80], "background": "In the background, there is a white couch, brown cushion, white cloth, some other clothes, white wall, brown mat, wooden table, a bottle, brown surface and the sound of a song is audible.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/14l1iEa80Vk.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_1712_0"}, {"texts": ["A man wearing light brown pants is smoking and counting the notes with his hands while dancing on the brown surface."], "durations": null, "exact_frames_per_prompt": [80], "background": "In the background, music is playing. There is a light brown sofa with a white mattress, a brown cushion on the sofa, some stuff on the right side, a bottle, a white plate, a cream colored wall, and a brown surface.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/14l1iEa80Vk.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_1713_0"}, {"texts": ["A person whose hands are visible is taking an egg.", "The person starts peeling egg.", "The person breaks the egg in half."], "durations": null, "exact_frames_per_prompt": [20, 40, 5], "background": "In the background, the song is playing, there is the sound of tapping, there is a grey-black counter top, a white tissue paper, an egg, a tong, and a metal lid.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/2BG2HElGz00.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_1714_0"}, {"texts": ["A girl wearing a green t-shirt, black shorts, and white-black sneakers is standing in the front while holding a newspaper, opening the newspaper with her hands and then looking at the newspaper."], "durations": null, "exact_frames_per_prompt": [80], "background": "In the background, people are speaking; there are brown wooden chairs, green-surfaced two brown wooden tables, white walls, a red pillar, a spice bottle stand with white napkins on the front table, newspapers, and a gray tile floor.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/4sVlcU4_Sws.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_1715_3"}, {"texts": ["A woman wearing a white t-shirt is sitting on a bed and tearing paper into pieces.\n while a kid wearing a green t-shirt is sitting on the bed and laughing and a man lying on the bed at the backside is watching the kid."], "durations": null, "exact_frames_per_prompt": [81], "background": "In the background, a woman is speaking and laughing, another woman and a baby are laughing. There are white walls, paper, a red pillow, a black bed, a multi colored bed sheet, and a red blanket.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/2Qy9jLxRHSQ.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_1718_1"}, {"texts": ["A baby wearing a green t-shirt is sitting on a bed, watching the woman and laughing a man wearing a gray t-shirt is watching while lying on the bed."], "durations": null, "exact_frames_per_prompt": [81], "background": "In the background, a woman is speaking and laughing, another woman and a baby are laughing. There are white walls, paper, a red pillow, a black bed, a multi colored bed sheet, and a red blanket.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/2Qy9jLxRHSQ.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_1718_2"}, {"texts": ["A kid is standing on the floor while holding the dog leash, and starts walking while the dog is moving around"], "durations": null, "exact_frames_per_prompt": [80], "background": "In the background, miscellaneous sounds are audible. There is a green carpet floor, a green sofa, clothes, white walls, a white ceiling, a door, windows with blinds, a wall decoration piece, a plant, a planter, a red side table, a few decoration pieces, and a chandelier.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/0cK13Pf2Rx8.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_1730_0"}, {"texts": ["A dog is standing on the floor while the boy pulls the belt of the dog.", "The dog starts walking while the boy start walking by holding dog belt."], "durations": null, "exact_frames_per_prompt": [40, 40], "background": "In the background, miscellaneous sounds are audible. There is a green carpet floor, a green sofa, clothes, white walls, a white ceiling, a door, windows with blinds, a wall decoration piece, a plant, a planter, a red side table, a few decoration pieces, and a chandelier.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/0cK13Pf2Rx8.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_1730_1"}, {"texts": ["A girl wearing a grey top is giving training to a puppy.", "The girl wearing a grey top is running on the soil surface."], "durations": null, "exact_frames_per_prompt": [40, 32], "background": "In the background, music is playing. There is a soil surface, hurdles, gray curtains and gray walls.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/-vztWUIq0y0.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_1733_0"}, {"texts": ["A baby wearing a white onesie is sitting on a white surface and playing with a tissue."], "durations": null, "exact_frames_per_prompt": [80], "background": "In the background, there is a grey wall, a red door, a red object, a white surface, tissues, and the miscellaneous sounds are audible.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/0HomhRSwTWA.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_1736_0"}, {"texts": ["A woman whose only half-body is visible is standing and cutting pineapple peel with a knife on a white chopping board."], "durations": null, "exact_frames_per_prompt": [55], "background": "In the background, there is a white tray filled with pineapple pieces and a strawberry, a white chopping board, a knife, a pineapple, black countertop, brown door, and some music is audible.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/0s04hDiq_mY.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_1738_0"}, {"texts": ["A person whose hand is visible is petting the cat."], "durations": null, "exact_frames_per_prompt": [80], "background": "In the background, there is a white wall, a gray surface, and some miscellaneous sounds are audible.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/0au5gb0jJos.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_1740_0"}, {"texts": ["A cat is laying on the gray surface, yawning, and getting petted by a person."], "durations": null, "exact_frames_per_prompt": [80], "background": "In the background, there is a white wall, a gray surface, and some miscellaneous sounds are audible.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/0au5gb0jJos.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_1740_1"}, {"texts": ["A boy wearing a white-blue t-shirt is leaning forward and eating watermelon.\n while another boy wearing a black t-shirt is on the right side and eating watermelon, a woman is coming from the right side holding a mic, and a group of people are looking at the boys."], "durations": null, "exact_frames_per_prompt": [80], "background": "In the background, there are watermelons, a white surface, and the voice of people speaking is audible.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/-fk5STh-Obo.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_1749_0"}, {"texts": ["A boy wearing a black t-shirt is also leaning forward and eating watermelon while a boy wearing a blue-white t-shirt is eating water melon, a woman is pointing at them while holding a mic and a group of people are standing around them and watching."], "durations": null, "exact_frames_per_prompt": [80], "background": "In the background, there are watermelons, a white surface, and the voice of people speaking is audible.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/-fk5STh-Obo.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_1749_1"}, {"texts": ["A woman is holding a microphone and moving her hands toward the boys while a group of people including children are standing around the boys and watching them."], "durations": null, "exact_frames_per_prompt": [72], "background": "In the background, there are watermelons, a white surface, and the voice of people speaking is audible.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/-fk5STh-Obo.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_1749_2"}, {"texts": ["A man wearing a blue shirt is holding a paper and starts the black paper shredding machine", "The man puts the paper in the paper shredding machine."], "durations": null, "exact_frames_per_prompt": [37, 43], "background": "In the background, there is a wooden table, a black paper shredding machine, an electric board, walls, a card, and the voice of people speaking, and some miscellaneous sounds are audible.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/394uA9Eveag.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_1751_0"}, {"texts": ["A person whose only hands are visible is wearing a blue shirt and demonstrating how to use a paper shredder."], "durations": null, "exact_frames_per_prompt": [80], "background": "In the background there are blue walls, switch boards, brown surface, a card, a paper shredder, papers, a wire and the sound of music playing and the sound of paper shredder is audible.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/394uA9Eveag.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_1752_0"}, {"texts": ["A boy wearing a black t-shirt and black jeans is standing while holding a fry pan with a pancake.", "The boy wearing a black t-shirt and black jeans is tossing the pancake and catching it.", "The boy wearing a black t-shirt and black jeans is keeping the fry pan on the gas stove.", "The boy wearing a black t-shirt and black jeans is speaking while putting his left hand on his waist."], "durations": null, "exact_frames_per_prompt": [19, 18, 13, 15], "background": "In the background, a man and a boy are speaking, there is the sound of laughing, blue chairs, white walls, white cupboards, a brown surface countertop with white drawers, a black gas stove with fry pan on the counter-top, bottles, chopping board and other stuff on the countertop, a circular mirror with a blue border on the countertop, a brown surface table on the right side, and a brown floor.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/-K_u_WQx1Ao.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_1755_0"}, {"texts": ["A woman wearing a black t-shirt and white shorts is standing on the right, moving a black pan while another woman wearing a  green and white top stands in front of her and  starts laughing.", "The woman is flipping something in it while a woman starts laughing and clapping."], "durations": null, "exact_frames_per_prompt": [40, 21], "background": "In the background, women are audible. There is a black pan with a brown thing, a white fridge, fridge magnets, cream cupboards, a toaster, a microwave, a white wall, and a few miscellaneous kitchen items.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/2KWWogyb6zY.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_1760_0"}, {"texts": ["A woman wearing a multi-color vest is standing on the left, moving, and laughing while a girl on the right side in a black t-shirt is flipping pancake then starts celebrating."], "durations": null, "exact_frames_per_prompt": [57], "background": "In the background, women are audible. There is a black pan with a brown thing, a white fridge, fridge magnets, cream cupboards, a toaster, a microwave, a white wall, and a few miscellaneous kitchen items.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/2KWWogyb6zY.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_1760_1"}, {"texts": ["A girl wearing a black t-shirt is standing on the right side while another woman wearing an aqua vest is standing on the right side.", "The girl wearing a black t-shirt is tossing the food in the frying pan and another woman starts clapping in joy."], "durations": null, "exact_frames_per_prompt": [16, 45], "background": "In the background, there are white containers, a white refrigerator with some stickers, a light brown wooden shelves, a microwave oven, a frying pan, a white wall, and the people speaking, clapping and laughing sound is audible", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/2KWWogyb6zY.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_1761_0"}, {"texts": ["A girl wearing a sky-blue top is standing on the left side and laughing while another girl wearing black and white outfit, holding a pan in his hand and she is tossing something using the pan."], "durations": null, "exact_frames_per_prompt": [61], "background": "In the background, there are white containers, a white refrigerator with some stickers, a light brown wooden shelves, a microwave oven, a frying pan, a white wall, and the people speaking, clapping and laughing sound is audible", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/2KWWogyb6zY.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_1761_1"}, {"texts": ["A woman wearing a blue jacket is sitting, holding a donut in her hand. ", "The woman is talking."], "durations": null, "exact_frames_per_prompt": [31, 49], "background": "In the background, there is a white wall, grey flooring, donuts, green pants, women talking, and some miscellaneous sounds are audible.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/1pfKrQ-K6Pg.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_1762_1"}, {"texts": ["A man wearing a black jacket is standing on the left, holding an award and speaking on the microphone."], "durations": null, "exact_frames_per_prompt": [80], "background": "In the background, a woman speaks, there is a stage, a dark background, blue lights, and white lights.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/1592Amp57V4.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_1765_0"}, {"texts": ["A girl wearing a pink frock is standing and keeping a snake in her palm while a women in black pink dress standing holding that girl."], "durations": null, "exact_frames_per_prompt": [74], "background": "In the background, there is a snake, a bottle, green grass surface, people are speaking.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/3AAPiiHuQQ0.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_1767_0"}, {"texts": ["A woman wearing a black top is bending and holding the hand of the girl while holding a bottle in her hand."], "durations": null, "exact_frames_per_prompt": [74], "background": "In the background, there is a snake, a bottle, green grass surface, people are speaking.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/3AAPiiHuQQ0.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_1767_1"}, {"texts": ["A girl wearing a blue top is standing and holding her hand in the front direction while a woman in black top is standing, leaning forward, holding the hands of the girl in pink frock and that girl is holding a snake in her hand, later another kid is touching the snake, and the person stopped the kid then lifts the snake up."], "durations": null, "exact_frames_per_prompt": [74], "background": "In the background, there is a snake, a bottle, green grass surface, people are speaking.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/3AAPiiHuQQ0.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_1767_2"}, {"texts": ["A girl wearing a white top is standing while a girl wearing a pink outfit is standing on a green grass surface, holding a snake, a woman wearing black and white is standing on a green grass surface, holding the girl's hands, and another girl wearing a pink outfit is standing at the back.", "The girl is touching the snake while a person whose hand is visible is pulling the girl's hand."], "durations": null, "exact_frames_per_prompt": [19, 16], "background": "In the background, there is a snake, a bottle, green grass surface, people are speaking.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/3AAPiiHuQQ0.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_1767_3"}, {"texts": ["A girl wearing a printed dress is standing on the green grass surface while group of people are standing on the green grass surface."], "durations": null, "exact_frames_per_prompt": [74], "background": "In the background, there is a snake, a bottle, green grass surface, people are speaking.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/3AAPiiHuQQ0.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_1767_5"}, {"texts": ["A boy wearing a light green t-shirt is sitting on the left side behind the counter top and speaking."], "durations": null, "exact_frames_per_prompt": [80], "background": "In the background, there is a white wall, a printed counter-top, a grey board, and the boy speaking sound is audible.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/-snAHpMtWlc.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_176_0"}, {"texts": ["A boy wearing a dark green t-shirt is sitting in the middle behind the counter-top while in his left the boy talking about the some topic."], "durations": null, "exact_frames_per_prompt": [80], "background": "In the background, there is a white wall, a printed counter-top, a grey board, and the boy speaking sound is audible.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/-snAHpMtWlc.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_176_1"}, {"texts": ["A girl wearing a black printed t-shirt is sitting on the right side behind the counter-top a boy in a light blue t-shirt is speaking on the left side."], "durations": null, "exact_frames_per_prompt": [80], "background": "In the background, there is a white wall, a printed counter-top, a grey board, and the boy speaking sound is audible.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/-snAHpMtWlc.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_176_2"}, {"texts": ["A person whose fingers are visible holding a spoon is moving the spoon and mixing the detox water."], "durations": null, "exact_frames_per_prompt": [80], "background": "In the background, miscellaneous sounds are audible. There is a glass of detox water, a jar of honey, a spoon, a white table, and a white tile floor.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/5FHcdooAqUs.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_1772_0"}, {"texts": ["A person of whom only hands are visible is holding an egg and a white bowl."], "durations": null, "exact_frames_per_prompt": [52], "background": "In the background, people are speaking and laughing. There is fire, an egg, a white bowl and a dark background.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/1nCfCIkbUPA.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_1773_0"}, {"texts": ["A man whose hands are visible is holding a white bowl in left hand and holding an egg in right hand, pointing his right hand to the right and keeping the egg in the bowl."], "durations": null, "exact_frames_per_prompt": [51], "background": "In the background, a man is speaking, there is the sound of laughing, a fire burning in the fire burner stove, a white bowl, an egg, a black object, and a black background.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/1nCfCIkbUPA.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_1774_0"}, {"texts": ["A woman wearing a black t-shirt, black jeans, and black boots is sitting on the horse's back while holding the harness, and riding a horse on the soil surface."], "durations": null, "exact_frames_per_prompt": [80], "background": "In the background, there are green trees, fencing, a white wall, green bushes, metal boundary, and a soil surface.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/-QqZCwEJ7g4.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_1778_0"}, {"texts": ["A brown horse is carrying a woman on its back and walking on the soil surface towards the right side a woman wearing a black headgear riding a horse"], "durations": null, "exact_frames_per_prompt": [80], "background": "In the background, there are green trees, fencing, a white wall, green bushes, metal boundary, and a soil surface.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/-QqZCwEJ7g4.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_1778_1"}, {"texts": ["A person wearing a gray top is standing and peeling an apple with a black apple peeling machine."], "durations": null, "exact_frames_per_prompt": [80], "background": "In the background, the music is audible. There is a black apple peeling machine, a marble kitchen countertop, white cupboards, a silver kitchen sink, apple peels, apples, brown wooden chairs, and a white tile floor.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/0cKRYGwRxp4.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_1779_0"}, {"texts": ["A man wearing a cap is at first standing near the car and watching the fuel dispenser machine.", "The man turns and takes out the nozzle."], "durations": null, "exact_frames_per_prompt": [34, 16], "background": "In the background, there is the fuel dispenser machine, a car, and the wall. A fast forward video is running in the background.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/2zlPP1ChaZA.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_1784_1"}, {"texts": ["A woman wearing a blue dress is dancing while holding a griddle and starts touching the round grill grate line."], "durations": null, "exact_frames_per_prompt": [65], "background": "In the background, there is wooden fencing, shrubs, grass, a brick wall, a kettle charcoal grill, food, and the sound of music is audible.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/0EFxOj0mLF0.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_1791_0"}, {"texts": ["A person wearing a grey t-shirt is standing on the left side and pushing meat into the meat grinder with a stuffer filler stick."], "durations": null, "exact_frames_per_prompt": [80], "background": "In the background, there is the sound of the meat grinder, there is a grey counter top, a white counter top, a black card with written words, a brown floor, a white wall, a brown wall, a black object, a meat grinder with meat, a white stuffer filler stick, and a yellow bucket.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/0109TNvx7RY.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_1795_0"}, {"texts": ["A big girl wearing clothes and a helmet is performing horse show jumping with horse riding."], "durations": null, "exact_frames_per_prompt": [80], "background": "In the background, music is playing. There are green trees, an electric wire, a blue sky, a roadside hill with green grass and green bushes, a pole, a show jumping hurdle, a metal stool, a table covered with a white cloth, wooden fencing, and a gray soil surface.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/-xuVfmYKSsE.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_1803_0"}, {"texts": ["A horse is performing show jumping while a girl is riding on him."], "durations": null, "exact_frames_per_prompt": [80], "background": "In the background, music is playing. There are green trees, an electric wire, a blue sky, a roadside hill with green grass and green bushes, a pole, a show jumping hurdle, a metal stool, a table covered with a white cloth, wooden fencing, and a gray soil surface.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/-xuVfmYKSsE.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_1803_1"}, {"texts": ["A baby wearing a red-blue t-shirt is eating food with the spoon by the woman."], "durations": null, "exact_frames_per_prompt": [80], "background": "In the background, there is a wooden table, red seats, bars, red wall, cream wall, posters, a mirror, light, sound of music, the voices of the people and a miscellaneous sound is audible.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/-ZSLez_Qtc4.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_1806_1"}, {"texts": ["A person wearing red clothes is standing and holding a big knife and cutting a watermelon."], "durations": null, "exact_frames_per_prompt": [80], "background": "In the background, a person is speaking. There is a gray counter-top, a big knife, a watermelon, a blue cloth, and a black surface.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/8Ro1fiH4DrU.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_1812_0"}, {"texts": ["A woman is standing on the left side while holding a golf stick.", "The woman is playing golf while hitting the golf ball with a golf stick.", "The woman is looking at the ball while standing and touching her hair with her left hand."], "durations": null, "exact_frames_per_prompt": [12, 14, 13], "background": "In the background, people are speaking; there is the sound of the air, a golf stick, a golf ball with a golf holder, a sky with clouds, green trees, a white flag, two barriers, a gray surface, a green grass ground, and two green octagonal boards on the gray surface.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/-2KvnLMnrA0.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_1815_0"}, {"texts": ["A man wearing a t-shirt is standing under a car's undercarriage, opening a part of the car with a wrench.", "The man wearing a t-shirt is fitting a new part."], "durations": null, "exact_frames_per_prompt": [62, 18], "background": "In the background, there is a car's undercarriage, cars, gray surface, a black tub and a wrench.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/14fEMSt8ZK4.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_1817_0"}, {"texts": ["A man wearing a grey-black t-shirt is standing, opening the bolt with the wrench.", "The man is taking out the bolt.", "The man is putting it."], "durations": null, "exact_frames_per_prompt": [16, 52, 12], "background": "In the background, there is a wrench, a black bowl, base of the car, a black car, and grey surface.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/14fEMSt8ZK4.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_1818_0"}, {"texts": ["A woman wearing white-yellow clothes and tying a black strap."], "durations": null, "exact_frames_per_prompt": [80], "background": "In the background, the woman is speaking, there is a green wall, white ceiling, a white light, a white wall, and a red-brown wall.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/-RnwS8FlLEE.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_1823_0"}, {"texts": ["A man wearing a black t-shirt is smelling a bottle.", "The man starts pouring some liquid from the bottle in a glass."], "durations": null, "exact_frames_per_prompt": [30, 50], "background": "In the background there is a black mug, a glass bottle, white walls, a transparent glass, some posters, a closed window and a person's voice and some miscellaneous sounds are audible.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/2cvS39tCMtg.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_1824_0"}, {"texts": ["A person whose hands are visible is rubbing a piece of paper a baby wearing white clothes sitting on a chair watching the person hand", "The person is tearing the paper while A baby wearing white clothes starts laugh"], "durations": null, "exact_frames_per_prompt": [67, 11], "background": "In the background, a paper tearing sound and the baby laughing sound is audible, a person is speaking. There are chairs, yellow walls, a brown surface, toys, a bottle, a black object and a multi-colored baby bed.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/2hecxkR6nIQ.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_1829_0"}, {"texts": ["A baby wearing white printed clothes is lying on a baby bed and watching the first person.", "A baby wearing white printed clothes is laughing."], "durations": null, "exact_frames_per_prompt": [54, 26], "background": "In the background, a paper tearing sound and the baby laughing sound is audible, a person is speaking. There are chairs, yellow walls, a brown surface, toys, a bottle, a black object and a multi-colored baby bed.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/2hecxkR6nIQ.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_1829_1"}, {"texts": ["A baby wearing white printed clothes is lying on a baby bed and watching the first person and laughing."], "durations": null, "exact_frames_per_prompt": [80], "background": "In the background, a paper tearing sound and the baby laughing sound is audible, a person is speaking. There are chairs, yellow walls, a brown surface, toys, a bottle, a black object and a multi-colored baby bed.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/2hecxkR6nIQ.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_1830_1"}, {"texts": ["A man wearing a black tracksuit is playing golf.", "The man starts walking."], "durations": null, "exact_frames_per_prompt": [39, 12], "background": "In the background, there is a green grass surface, colourful flags, green trees, a clear sky, white golf balls, and the miscellaneous sound is audible.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/1EmxAHGtgG8.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_1839_0"}, {"texts": ["A woman wearing black clothes is standing and is speaking."], "durations": null, "exact_frames_per_prompt": [69], "background": "In the background, person one is speaking and music is playing. There is a red and green background and a caption is also written in the video.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/-F8lZfpwp0Y.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_1841_0_ms"}, {"texts": ["A man wearing a black shirt on the right is also standing and is doing american sign language."], "durations": null, "exact_frames_per_prompt": [48], "background": "In the background, person one is speaking and music is playing. There is a red and green background and a caption is also written in the video.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/-F8lZfpwp0Y.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_1841_1"}, {"texts": ["A man wearing blue clothes is holding a peeler and drilling machine attach with an apple.", "The man wearing blue clothes takes it in other hand.", "The man wearing blue clothes starts rotating and peeling the apple."], "durations": null, "exact_frames_per_prompt": [22, 28, 30], "background": "In the background, the drill machine sounds and man speaking are audible, there is a green surface, green trees, a metal frame and a brick surface.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/-GvB7aLrCqQ.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_1847_0"}, {"texts": ["A man wearing an aqua green t-shirt, a blue woolen cap, black pants, and black shoes is standing in a golf stance position while holding a golf stick.", "The man is playing golf while hitting the golf ball with a golf stick. ", "The man is standing in a straight position while holding a golf stick."], "durations": null, "exact_frames_per_prompt": [19, 6, 55], "background": "In the background, a man is speaking; birds are chirping; there are green trees, green plants, a golf stick, and a surface with green grass and wet soil.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/0T5E-XoI5Zg.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_1848_0"}, {"texts": ["A person wearing a black jacket with white stripes is standing on the right and tapping on a dough.", "The person starts removing flour from the counter top in the hand.", "The person takes a purple bowl.", "The person starts removing flour from the counter-top to the bowl."], "durations": null, "exact_frames_per_prompt": [36, 20, 10, 13], "background": "In the background, the music is playing, there is a brown counter top, a dough, a white wall, a maroon-white bowl, a brown tiled wall with prints, a white tiled floor, and a purple bowl.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/2QJJFZNpENI.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_1853_0"}, {"texts": ["A baby wearing a multicolored dress is sitting on a table eating food with his hand while a man in a blue sweatshirt is sitting on the left and looking at the baby.", "The baby starts looking at the table."], "durations": null, "exact_frames_per_prompt": [35, 9], "background": "In the background, there is a white board, a white wall, a chair, a table, food, and people are speaking and laughing.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/4-AdR_vw8A8.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_1854_0"}, {"texts": ["A man wearing a blue sweater is sitting on a chair looking at the baby while speaking and smiling while a baby on the right side wearing a multicoloured outfit is sitting on a table and eating food with hand."], "durations": null, "exact_frames_per_prompt": [44], "background": "In the background, there is a white board, a white wall, a chair, a table, food, and people are speaking and laughing.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/4-AdR_vw8A8.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_1854_1"}, {"texts": ["A man wearing a white shirt, black coat and black pants is standing on the left side of the other man and tying a tie around his own neck while a baby in blue t-shirt is moving on brown surface."], "durations": null, "exact_frames_per_prompt": [80], "background": "In the background, there is a light brown wall with white borders, dark brown surface, a window, a wooden door and the voices of the people are audible.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/-RdPsVc6psA.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_1856_0"}, {"texts": ["A man wearing a white shirt, gray sweater, and dark blue jeans is standing on the right side of the first man and tying a tie around his own neck while the baby crawls towards them.", "The man is bending.", "The man is stopping the baby with his hand."], "durations": null, "exact_frames_per_prompt": [45, 8, 27], "background": "In the background, there is a light brown wall with white borders, dark brown surface, a window, a wooden door and the voices of the people are audible.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/-RdPsVc6psA.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_1856_1"}, {"texts": ["A baby wearing a dark blue cloth is crawling towards the both men on the dark brown surface."], "durations": null, "exact_frames_per_prompt": [59], "background": "In the background, there is a light brown wall with white borders, dark brown surface, a window, a wooden door and the voices of the people are audible.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/-RdPsVc6psA.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_1856_2"}, {"texts": ["A man wearing a black coat and white shirt is standing, holding a microphone and trophy and speaking."], "durations": null, "exact_frames_per_prompt": [80], "background": "In the background, there is a screen, trophy, microphone and the man's voice is audible.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/0lg1stFRF4A.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_1860_0"}, {"texts": ["A girl wearing black clothes is standing, touching and cleaning a white horse with a brush."], "durations": null, "exact_frames_per_prompt": [80], "background": "In the background, there is a cloth, a brush, a surface, and the voice of a girl speaking is audible.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/-MldnTjJ-zE.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_1865_0"}, {"texts": ["A boy wearing a red t-shirt is sitting on the floor, holding a jewel case.", "The boy puts the jewel case on the floor and takes a red gift in his hand while a woman wearing black outfit is sitting on the right side and a brown dog is standing on the floor beside the woman."], "durations": null, "exact_frames_per_prompt": [27, 28], "background": "In the background, the boy is speaking, a man is speaking, the woman is speaking, there is a wooden floor, a decorated Christmas tree, gift wraps, a brown sofa, some gifts, a wooden door, white walls, and a light-green wall.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/1ei-0k9VJZQ.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_1866_0"}, {"texts": ["A woman wearing a black jacket is sitting on the wooden floor while the boy in red top checks the presents and the dog behind her sniffs the Christmas tree."], "durations": null, "exact_frames_per_prompt": [36], "background": "In the background, the boy is speaking, a man is speaking, the woman is speaking, there is a wooden floor, a decorated Christmas tree, gift wraps, a brown sofa, some gifts, a wooden door, white walls, and a light-green wall.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/1ei-0k9VJZQ.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_1866_1"}, {"texts": ["A boy wearing a red t-shirt is sitting and riding a camel while the woman holds the reins and takes to the direction."], "durations": null, "exact_frames_per_prompt": [40], "background": "In the background, a kid is yelling and miscellaneous sounds are audible. There is a brown surface, a blue canopy with gray poles and grills, a metal structure, wooden fencing, green trees and a blue sky with white clouds.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/15iug-PHLAk.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_186_0"}, {"texts": ["A woman wearing a black top is holding the camel harness and is walking in front of the camel."], "durations": null, "exact_frames_per_prompt": [16], "background": "In the background, a kid is yelling and miscellaneous sounds are audible. There is a brown surface, a blue canopy with gray poles and grills, a metal structure, wooden fencing, green trees and a blue sky with white clouds.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/15iug-PHLAk.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_186_1_ms"}, {"texts": ["A camel is walking on the brown surface, while making a boy sit on his back while the woman holds the reins"], "durations": null, "exact_frames_per_prompt": [40], "background": "In the background, a kid is yelling and miscellaneous sounds are audible. There is a brown surface, a blue canopy with gray poles and grills, a metal structure, wooden fencing, green trees and a blue sky with white clouds.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/15iug-PHLAk.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_186_2"}, {"texts": ["A girl wearing a green top is eating a yellow crackle."], "durations": null, "exact_frames_per_prompt": [80], "background": "In the background, the sound of eating is audible and music is playing. There is a wall, a white surface, a brown dresser and a green bowl.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/-tR1hivzl_U.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_1870_0"}, {"texts": ["A man wearing a black t-shirt is doing flair bar-tending.\n while the man in ash colour vest is looking at him and  encouraging."], "durations": null, "exact_frames_per_prompt": [80], "background": "In the background, there is a hall, and a signage.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/-RLhbQpFa1s.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_1871_0"}, {"texts": ["A man in grey and white clothing is cheering the man in a black t-shirt by lifting his one hand while a group of people is looking in the direction of the man in a black t-shirt and cheering."], "durations": null, "exact_frames_per_prompt": [80], "background": "In the background, there is a hall, and a signage.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/-RLhbQpFa1s.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_1871_1"}, {"texts": ["A man wearing a black jacket is shaving a white sheep."], "durations": null, "exact_frames_per_prompt": [46], "background": "In the background, miscellaneous sounds are audible. There is a grass field, a barn, and a big rock.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/15o6ZISRJPo.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_1872_3_ms"}, {"texts": ["A white sheep is laying on the floor and being shaved by the third man while a man is standing and giving something to him and a group of sheep in which some sheep standing and some sheep are walking."], "durations": null, "exact_frames_per_prompt": [58], "background": "In the background, miscellaneous sounds are audible. There is a grass field, a barn, and a big rock.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/15o6ZISRJPo.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_1872_4_ms"}, {"texts": ["A person whose hands are visible on the left is scratching the wool while another person on the right side is scratching the wool."], "durations": null, "exact_frames_per_prompt": [21], "background": "In the background, miscellaneous sounds are audible. There is a grass field, a barn, and a big rock.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/15o6ZISRJPo.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_1872_5"}, {"texts": ["Another person whose hands are visible on the right is scratching the wool."], "durations": null, "exact_frames_per_prompt": [21], "background": "In the background, miscellaneous sounds are audible. There is a grass field, a barn, and a big rock.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/15o6ZISRJPo.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_1872_6_ms"}, {"texts": ["A man wearing a blue t-shirt standing in the kitchen is flipping a pancake on a plate."], "durations": null, "exact_frames_per_prompt": [80], "background": "In the background, miscellaneous sounds are audible. There is a white floor, white walls, a black stove with pancakes on it, kitchen cabinets, silver sink, utensils, a brown shelf with objects on it, a rack with objects in it, a green polybag, white ceiling and yellow lights.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/01ZrQnMruig.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_1879_0"}, {"texts": ["A man wearing a black t-shirt is standing and is speaking."], "durations": null, "exact_frames_per_prompt": [18], "background": "In the background, person one is speaking and miscellaneous sounds are audible. There is a green and brown surface, a parked vehicle, building structures, green trees and a blue sky.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/-xxpyScg_vY.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_1888_0"}, {"texts": ["A person whom hands are visible is milking the white cow."], "durations": null, "exact_frames_per_prompt": [62], "background": "In the background, person one is speaking and miscellaneous sounds are audible. There is a green and brown surface, a parked vehicle, building structures, green trees and a blue sky.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/-xxpyScg_vY.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_1888_1"}, {"texts": ["A white cow is standing and is being milked by person two."], "durations": null, "exact_frames_per_prompt": [62], "background": "In the background, person one is speaking and miscellaneous sounds are audible. There is a green and brown surface, a parked vehicle, building structures, green trees and a blue sky.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/-xxpyScg_vY.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_1888_3"}, {"texts": ["A man wearing a sky-blue t-shirt, blue jeans, and a dark blue cap is holding a big horse by a rope and then starts walking towards the right side ahead of the big horse and the small horse follows them"], "durations": null, "exact_frames_per_prompt": [80], "background": "In the background, birds are chirping; there are green trees, shed houses, small green plants, a sky, and a green grass surface.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/0VOxfdWK6mw.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_1890_0"}, {"texts": ["A big brown horse is tied to the rope held by the man and is walking along with the man on the green grass surface while the other horse follows them."], "durations": null, "exact_frames_per_prompt": [80], "background": "In the background, birds are chirping; there are green trees, shed houses, small green plants, a sky, and a green grass surface.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/0VOxfdWK6mw.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_1890_1"}, {"texts": ["A baby brown horse is walking along with the big horse towards the right side on the green grass surface while a man wearing blue outfit is walking on green grass surface towards the right side, holding a rope in his hand that is attached to the big horse."], "durations": null, "exact_frames_per_prompt": [56], "background": "In the background, birds are chirping; there are green trees, shed houses, small green plants, a sky, and a green grass surface.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/0VOxfdWK6mw.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_1890_2"}, {"texts": ["A man wearing a sky-blue t-shirt is walking ahead while holding a horse rope while a baby horse is also walking along with the horse."], "durations": null, "exact_frames_per_prompt": [80], "background": "In the background, there are green trees, house buildings, small green plants, a sky, a green grass surface, and the birds chirping and some other miscellaneous sounds are audible.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/0VOxfdWK6mw.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_1891_0"}, {"texts": ["A brown horse is tied with the rope and walking along with the man on the green grass surface while another small brown horse is also walking along on the green grass surface."], "durations": null, "exact_frames_per_prompt": [80], "background": "In the background, there are green trees, house buildings, small green plants, a sky, a green grass surface, and the birds chirping and some other miscellaneous sounds are audible.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/0VOxfdWK6mw.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_1891_1"}, {"texts": ["A baby brown horse is walking along with the big horse on the green grass surface while the man holding the rope tied with a big horse is walking"], "durations": null, "exact_frames_per_prompt": [80], "background": "In the background, there are green trees, house buildings, small green plants, a sky, a green grass surface, and the birds chirping and some other miscellaneous sounds are audible.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/0VOxfdWK6mw.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_1891_2"}, {"texts": ["A woman wearing a blue hoodie is making a sandwich.", "The woman is opening the mustard sauce bottle cap."], "durations": null, "exact_frames_per_prompt": [45, 35], "background": "In the background, the girl is speaking. There is a white background, a marble counter, a packet of bread, a sandwich and a mustard sauce bottle.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/5xIfmiRGQdw.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_1894_0"}, {"texts": ["A girl wearing a blue hoodie with written words is pressing chicken slices on a slice of bread.", "The girl takes a yellow bottle and opens the cap of the yellow bottle then lifts the bottle to pour."], "durations": null, "exact_frames_per_prompt": [36, 43], "background": "In the background, there is a miscellaneous sound, the girl is speaking, there is a brown counter top, a packet of bread, slices of bread with chicken slices, a yellow bottle, a grey object, and a white background.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/5xIfmiRGQdw.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_1895_0"}, {"texts": ["A baby wearing a white printed romper is lying on the yellow-blue-green printed chair, holding a carrot and trying to eat the carrot."], "durations": null, "exact_frames_per_prompt": [80], "background": "In the background, there is a miscellaneous sound, there is a yellow-blue-green printed chair, a grey floor, and a white wall.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/3kavSc71JM8.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_1896_0"}, {"texts": ["A man wearing a black suit is standing on the right, clapping and watching the first man while two men are standing and shaking hands and a woman is holding a tray while standing then walks away.", "The man is shaking hands with the first man."], "durations": null, "exact_frames_per_prompt": [41, 20], "background": "In the background, people are speaking, there is a grey floor, a blue carpet, chairs, a golden cloth table, bottles, a blue wall with a banner, and garlands decorations.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/1idvXPxy3pk.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_1897_2"}, {"texts": ["A woman wearing a golden saree is standing on the backside, holding a tray of award then the award is picked by the second man.  while a man wearing a red shirt is taking the award and another man wearing black clothes is standing on the right side and clapping.", "The woman walks towards the left while the man wearing a red shirt is shaking hands with the man wearing a white shirt."], "durations": null, "exact_frames_per_prompt": [14, 22], "background": "In the background, people are speaking, there is a grey floor, a blue carpet, chairs, a golden cloth table, bottles, a blue wall with a banner, and garlands decorations.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/1idvXPxy3pk.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_1897_3"}, {"texts": ["A girl wearing a cap is sitting on the left side and eating something while another girl on the right side is moving her hand and looking in the front."], "durations": null, "exact_frames_per_prompt": [80], "background": "In the background, there are counters with bottles and some other stuff, a wall with windows, a packet, and a chair.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/1DANNB6_6RI.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_1898_0"}, {"texts": ["A woman wearing a maroon t-shirt and blue jeans is sitting on the horse, holding the bridle and riding from the right side to the left side on the soil surface."], "durations": null, "exact_frames_per_prompt": [61], "background": "In the background, there are green trees, green grass surface, sheds, black railing, a white pipe, soil surface, poles, a child is speaking, bird's chirruping and wind blowing sounds are audible.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/0A_ivMQ1Cz4.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_1900_0"}, {"texts": ["A white horse is running on the soil surface and a woman is sitting on its back."], "durations": null, "exact_frames_per_prompt": [61], "background": "In the background, there are green trees, green grass surface, sheds, black railing, a white pipe, soil surface, poles, a child is speaking, bird's chirruping and wind blowing sounds are audible.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/0A_ivMQ1Cz4.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_1900_1"}, {"texts": ["A boy wearing a white t-shirt is sitting on the wooden floor and removing the gift wrap from a book.", "The boy wearing a white t-shirt puts the book on the floor."], "durations": null, "exact_frames_per_prompt": [58, 22], "background": "In the background, people are speaking, there is a wooden floor, a white carpet with multi coloured borders, green-yellow printed clothes, cards, a gift wrap, a red ribbon, and a black table with black chairs.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/1Dz1FGJ1huE.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_1912_0"}, {"texts": ["A man wearing a blue t-shirt is standing and putting the fishes in the meat grinder."], "durations": null, "exact_frames_per_prompt": [58], "background": "In the background, there is a white boat, grey deck, poles, white boxes, trees, green grass surface, blue buckets, grinder machine, a car, an orange bottle, a red object, cloudy sky, other objects, a river, and voices of the grinder machine are audible.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/5rvOuIYg3yk.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_1920_0"}, {"texts": ["A person whose hands are visible is holding a peeler and an apple and peeling the apple."], "durations": null, "exact_frames_per_prompt": [80], "background": "In the background, there is a white wall, a white cupboard, a peeler, man apple, a black carry bag, a girl voice, a boy's voice and peeling voices are audible.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/2uzt0eAalss.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_1924_0"}, {"texts": ["A woman wearing a green-white top is standing and making sushi by rolling a bamboo sushi mat on the wooden board."], "durations": null, "exact_frames_per_prompt": [40], "background": "In the background, there is a wooden board, a knife, a countertop with some stuff, and a cloth.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/0PM667FvXQI.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_1925_0"}, {"texts": ["A man wearing a white shirt and black pants is making a bed."], "durations": null, "exact_frames_per_prompt": [80], "background": "In the background, there is a bed, a bedside table, lights, mattress, a white bed-sheet, a bottle, a remote, a white wall, a painting, curtains and television sound and some miscellaneous sounds are audible.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/Hyh3D6U5ezo.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_1930_0"}, {"texts": ["A man wearing a gray t-shirt and black trousers is standing and making a bed."], "durations": null, "exact_frames_per_prompt": [80], "background": "In the background, T. V and miscellaneous sounds are audible. There is a bed with a white bed sheet, a lamp, a water bottle, a wooden wall decorative piece, a painting, a red curtain, white walls, and a black floor.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/Hyh3D6U5ezo.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_1931_0"}, {"texts": ["A woman, whose hand is visible, is cleaning the gas stove with white tissue paper while a boy on the right side is standing."], "durations": null, "exact_frames_per_prompt": [43], "background": "In the background, a woman and a boy are speaking; there is a black surface countertop with a gas stove, a steel saucepan with milk on the gas stove, another saucepan with water on the burning burner gas stove, other two cooking steel pot on the gas stove, a white-black cup, a glass with milk on the countertop, a steel plate and a brown object on the countertop, a yellow wall with white molding, a green wall, a cloth hanging on the gas stove's hanger, a green cloth, and a gray tile surface.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/3qCcdAlYf1A.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_1932_0"}, {"texts": ["A boy whose upper half-body is visible, wearing a blue printed sky blue t-shirt, is standing on the right side of the surface and is speaking and then blowing through his mouth on the gas stove."], "durations": null, "exact_frames_per_prompt": [80], "background": "In the background, a woman and a boy are speaking; there is a black surface countertop with a gas stove, a steel saucepan with milk on the gas stove, another saucepan with water on the burning burner gas stove, other two cooking steel pot on the gas stove, a white-black cup, a glass with milk on the countertop, a steel plate and a brown object on the countertop, a yellow wall with white molding, a green wall, a cloth hanging on the gas stove's hanger, a green cloth, and a gray tile surface.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/3qCcdAlYf1A.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_1932_1"}, {"texts": ["A boy wearing a black t-shirt is sitting and chewing.", "The boy wearing a black t-shirt takes something from the person."], "durations": null, "exact_frames_per_prompt": [30, 50], "background": "In the background, music is playing, a boy and a person's voice is audible. There is a white wall and multicolor printed sofa.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/030KGXn4hy4.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_1935_0"}, {"texts": ["A man wearing a white shirt is standing.", "The man is putting the green rope on the hook.", "The man starts pulling the rope."], "durations": null, "exact_frames_per_prompt": [42, 20, 18], "background": "In the background, there is a dry leaf, trees, a green rope, a black bag, a dark grey surface, a river, and the man speaking sound is audible.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/0fEPjC7XGqE.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_193_0"}, {"texts": ["A woman wearing a purple suit is standing, holding a wok and mixing the vegetables in the wok."], "durations": null, "exact_frames_per_prompt": [80], "background": "In the background, there are vegetables, a wok, a bowl, onions, a black stove, a black spoon, a tray, glass bowls, a pink flower, yellow plate, yellow-white grill, and frying voices are audible.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/6cCRuEzXIl8.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_1945_0"}, {"texts": ["A baby wearing a white orange sweatshirt is sitting on the first person's lap.", "The baby is pulling the newspaper while the first person holds the newspaper back."], "durations": null, "exact_frames_per_prompt": [17, 33], "background": "In the background, people are speaking. There is a brown table, a newspaper, an orange wall and a brown sofa.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/-TpohjiPMSE.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_194_1"}, {"texts": ["A man, whom hands are visible only, is holding a knotted rope, rotating and stretching it."], "durations": null, "exact_frames_per_prompt": [80], "background": "In the background, there is a wooden surface, a white rope and the voice of the man is audible.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/02iWPRcCQ94.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_1952_0"}, {"texts": ["A child wearing a gray hoodie, white jeans, and gray-white woolen cap is standing while plucking the red apple from the tree.", "The child starts walking towards the man."], "durations": null, "exact_frames_per_prompt": [41, 39], "background": "In the background, a woman is speaking and cheering, there are red apple trees, and a green grass surface.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/-4Zkoocadr0.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_1953_0"}, {"texts": ["A man whose only hands and legs are visible wearing a black sweatshirt is sitting on the green grass surface on the left side and is pointing his hand towards the child while the child comes towards the man."], "durations": null, "exact_frames_per_prompt": [25], "background": "In the background, a woman is speaking and cheering, there are red apple trees, and a green grass surface.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/-4Zkoocadr0.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_1953_1"}, {"texts": ["A brown dog is walking on the road and is being held by a white metal chain."], "durations": null, "exact_frames_per_prompt": [80], "background": "In the background, a person is speaking, there is the sound of vehicles, green grass, and a concrete road.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/0dI9nksFKUA.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_1956_0"}, {"texts": ["A man wearing a brown-orange jacket is sitting on a chair, holding a knife in his hand and moving his hands.", "The man turns his head to the left."], "durations": null, "exact_frames_per_prompt": [75, 5], "background": "In the background, there is a miscellaneous sound, there is a snow surface with tree branches, wooden logs, a campfire, a metal container, trees, a rifle, a grey bag, and a chair.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/35Fa6wloUMA.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_1957_0"}, {"texts": ["A person whose hand is visible is showing the food to the blue bird."], "durations": null, "exact_frames_per_prompt": [78], "background": "In the background, there is a tree, bushes, and the grass.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/0lxzjBKsQlg.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_1960_0"}, {"texts": ["A blue jay bird is hopping here and there to snatch away the food from the person's hand while the person is giving the food to the bird."], "durations": null, "exact_frames_per_prompt": [80], "background": "In the background, there is a tree, bushes, and the grass.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/0lxzjBKsQlg.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_1960_1"}, {"texts": ["A person whose hands are visible is holding a glass jar of onions.", "The person is putting it in glass bowl.", "The person is mixing it. "], "durations": null, "exact_frames_per_prompt": [21, 35, 24], "background": "In the background, a person is speaking. There is a black counter top, a glass bowl of vegetables, a white cloth, a white object.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/4CJ_gPNUzF4.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_1961_0"}, {"texts": ["A woman wearing a black tank top is sitting and applying butter with a knife on the slices of bread."], "durations": null, "exact_frames_per_prompt": [80], "background": "In the background, there are containers, a brown basket, a knife, red cardboard, a gray wall, a yellow curtain, a white cloth, a bottle, some other stuff, and the voice of people speaking, and some miscellaneous sounds are audible.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/38GhvJu7jhc.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_1965_0"}, {"texts": ["A woman wearing a black outfit is sitting with a butter knife in her right hand and buttering bread on the table."], "durations": null, "exact_frames_per_prompt": [80], "background": "In the background, there is a butter knife, yellow butter, braids, brown basket, yellow curtains, a red chopping board, a table covered with white cloth, a peanut butter, a bottle, a gray object , purple wall, people are speaking and there is a miscellaneous sound.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/38GhvJu7jhc.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_1966_0"}, {"texts": ["A lady wearing a green-purple saree is sitting with another lady and milking milk from a buffalo in a bucket on a brown soil surface."], "durations": null, "exact_frames_per_prompt": [80], "background": "In the background, there is a sloppy surface with green grass, tree trunks, a brown soil surface, and a blue sky, and the milk milking sound is audible.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/1E-twOFAPfw.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_1967_0"}, {"texts": ["A kid wearing a peach color t-shirt is giggling and sitting on a white baby high chair.", "The kid stops giggling."], "durations": null, "exact_frames_per_prompt": [72, 9], "background": "In the background, people are speaking and laughing, there is a white baby high chair with some food, a black object, and a green wall.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/0S20SCkkeFA.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_1968_0"}, {"texts": ["A woman wearing a grey t-shirt is carrying a baby and feeding by the bottle."], "durations": null, "exact_frames_per_prompt": [80], "background": "In the background, there is a basket with red clothes, a bottle, a pink cloth and a black surface, and the miscellaneous sounds are audible.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/16PHEw_PsSk.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_1973_0"}, {"texts": ["A baby wearing a light pink cloth is fed by the woman."], "durations": null, "exact_frames_per_prompt": [80], "background": "In the background, there is a basket with red clothes, a bottle, a pink cloth and a black surface, and the miscellaneous sounds are audible.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/16PHEw_PsSk.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_1973_1"}, {"texts": ["A person is feeding a baby with a pink spoon."], "durations": null, "exact_frames_per_prompt": [80], "background": "In the background, miscellaneous sounds are audible. There is a brown floor, a blue cloth, a white door with glass, a chair, a bowl, a pink spoon, a baby dining chair and black background.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/47sZcVTD3KY.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_1975_0"}, {"texts": ["A baby wearing white clothes is sitting on a baby dining chair and eating food.\n while a woman sitting and feeding food with a spoon."], "durations": null, "exact_frames_per_prompt": [80], "background": "In the background, miscellaneous sounds are audible. There is a brown floor, a blue cloth, a white door with glass, a chair, a bowl, a pink spoon, a baby dining chair and black background.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/47sZcVTD3KY.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_1975_1"}, {"texts": ["A woman wearing a black hoodie is standing.", "The woman opens a carry bag.", "The woman puts something in it."], "durations": null, "exact_frames_per_prompt": [15, 30, 35], "background": "In the background, a person and miscellaneous sounds are audible. There is a multi-color carry bag, packets, a wooden table, a white ceiling, white walls, white cupboards, a white microwave, a light, and a few miscellaneous items on the kitchen shelf.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/-VG42OzQzHc.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_1976_0"}, {"texts": ["A white cat is sitting on a chair in the back while a woman wearing a black outfit is standing, holding, and opening a carry bag."], "durations": null, "exact_frames_per_prompt": [40], "background": "In the background, a person and miscellaneous sounds are audible. There is a multi-color carry bag, packets, a wooden table, a white ceiling, white walls, white cupboards, a white microwave, a light, and a few miscellaneous items on the kitchen shelf.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/-VG42OzQzHc.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_1976_1"}, {"texts": ["A woman wearing a black jacket is standing. ", "The woman is opening a printed carry bag.", "The woman is putting it on the table.", "The woman is starting to put a black object inside the carry bag."], "durations": null, "exact_frames_per_prompt": [14, 22, 39, 5], "background": "In the background, there is a printed carry bag, a roof, white light, white rack, containers, chimney, cupboard, a white bottle, a table, a box, a chair, brown surface, a woman is speaking and some miscellaneous sounds are audible.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/-VG42OzQzHc.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_1977_0"}, {"texts": ["A man is standing, moving his head and eating a piece of food."], "durations": null, "exact_frames_per_prompt": [78], "background": "In the background, there is a donut, a wooden stick, a rope, white light, grey floor, people are speaking and laughing sounds are audible.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/1QbXSzC_MQY.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_1981_0"}, {"texts": ["A woman wearing red clothes whose only hands are visible is making a doll with a white cloth."], "durations": null, "exact_frames_per_prompt": [80], "background": "In the background, the woman is speaking. There is a dark brown surface and a white cloth.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/0nWBr2nZ9xs.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_1983_0"}, {"texts": ["A man wearing a black-blue t-shirt, black shorts, black shoes, and a black hat is sitting on the horse's back while holding the horse harness and riding a horse on the soil surface."], "durations": null, "exact_frames_per_prompt": [80], "background": "In the background, a person is speaking, there is the sound of walking, the sound of wind, green trees, green bushes, houses, a sky with clouds, vehicles standing, a concrete road; and a gray soil surface.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/5tI9jC_onR0.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_1984_0"}, {"texts": ["A white horse is walking on the soil surface while carrying a man on its back."], "durations": null, "exact_frames_per_prompt": [80], "background": "In the background, a person is speaking, there is the sound of walking, the sound of wind, green trees, green bushes, houses, a sky with clouds, vehicles standing, a concrete road; and a gray soil surface.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/5tI9jC_onR0.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_1984_1"}, {"texts": ["A girl wearing a white purple dress is walking on a brown surface and putting clothes in washing machines."], "durations": null, "exact_frames_per_prompt": [80], "background": "In the background, a woman is speaking, a miscellaneous sound is audible. There are white washing machines with clothes in them, and a brown floor.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/7fRljoi32nU.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_1986_0"}, {"texts": ["A person wearing a black jacket, black gloves, brown jeans, and black shoes is pulling the string with their hands from the hole while standing on the snow surface."], "durations": null, "exact_frames_per_prompt": [72], "background": "In the background, there is the sound of the air, a snow surface, a string and a hole on the snow surface.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/3ZXDdqJr7Jg.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_198_1"}, {"texts": ["A woman wearing black shades is sitting on the green grass surface and is reading a book while a man is lying on the ground and reading a book,\u00a0 and other people are standing in a statue position."], "durations": null, "exact_frames_per_prompt": [80], "background": "In the background, music is playing. There is a green grass surface, books and blue and white balloons.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/-htY-hAxcKM.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_1990_0"}, {"texts": ["A man wearing a cap is lying on the green grass surface and is also reading a book while a group of people are standing around."], "durations": null, "exact_frames_per_prompt": [57], "background": "In the background, music is playing. There is a green grass surface, books and blue and white balloons.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/-htY-hAxcKM.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_1990_1"}, {"texts": ["A person whose only hands are visible is at first holding a cardboard box and points on the receipt attached on the box.", "The person puts the box on the wooden surface.", "The person turns the box.", "The person opens the box."], "durations": null, "exact_frames_per_prompt": [38, 12, 20, 10], "background": "In the background, there is a cardboard box, wooden surface, and other things are kept on the wooden surface.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/0Db7KIPFyPY.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_1996_0"}, {"texts": ["A man whose hands are visible.", "The man is pointing at a carton.", "The man is putting it on the wooden table.", "The man is turning it.", "The man is opening it."], "durations": null, "exact_frames_per_prompt": [20, 22, 20, 9, 9], "background": "In the background, there is a wooden table, a carton, a bottle, some packets, a jar, some stuff, the voice of a man and a miscellaneous sound is audible.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/0Db7KIPFyPY.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_1997_0"}, {"texts": ["A woman wearing a white top and a yellow apron is standing and making noodles while another woman wearing a white top and a yellow apron is standing and packing the food, and a group of people is standing and moving at the back."], "durations": null, "exact_frames_per_prompt": [80], "background": "In the background, there is a green plants, yellow banners, noodles, lights, white containers, a rack of steel, some stuff, a wooden log, steel bowls, windows, posters, a blue-white-green ceiling, some other stuff, and the miscellaneous sounds are audible.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/0_Jn6lRpV-4.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_1998_0"}, {"texts": ["A woman wearing a white t-shirt and a yellow apron is standing on the left side of the first woman and packing the noodles."], "durations": null, "exact_frames_per_prompt": [80], "background": "In the background, there is a green plants, yellow banners, noodles, lights, white containers, a rack of steel, some stuff, a wooden log, steel bowls, windows, posters, a blue-white-green ceiling, some other stuff, and the miscellaneous sounds are audible.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/0_Jn6lRpV-4.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_1998_1"}, {"texts": ["A woman wearing a white t-shirt and yellow apron is standing and making noodles while another woman on the left side wearing a white t-shirt and yellow apron is filling the noodles in a box and put the box in the bag."], "durations": null, "exact_frames_per_prompt": [80], "background": "In the background, there is a yellow banner, noodles, lights, white containers, a wooden log, steel bowls, a window, blue ceiling, and some other stuff, and the miscellaneous sounds are audible.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/0_Jn6lRpV-4.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_1999_0"}, {"texts": ["A woman wearing a white t-shirt, yellow apron is standing on the left side is packing the noodles while another woman on the right side is preparing the noodles and a group of people are moving and working at the back."], "durations": null, "exact_frames_per_prompt": [80], "background": "In the background, there is a yellow banner, noodles, lights, white containers, a wooden log, steel bowls, a window, blue ceiling, and some other stuff, and the miscellaneous sounds are audible.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/0_Jn6lRpV-4.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_1999_1"}, {"texts": ["A person on the right side wearing white clothes is sitting near another person."], "durations": null, "exact_frames_per_prompt": [59], "background": "In the background, there are white walls, a silver tool, and the voice of people speaking, and some miscellaneous sounds are audible.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/0pDZwCFDUwc.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_2006_0"}, {"texts": ["A person on the left side wearing pink gloves is scraping a wall with a silver tool while another person wearing white clothes and a white hat is sitting on the right and holding a white plastic bag in their hands."], "durations": null, "exact_frames_per_prompt": [59], "background": "In the background, there are white walls, a silver tool, and the voice of people speaking, and some miscellaneous sounds are audible.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/0pDZwCFDUwc.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_2006_1"}, {"texts": ["A man wearing brown clothes is tearing the wrapping of the sausages and putting it in an electric cooker."], "durations": null, "exact_frames_per_prompt": [80], "background": "In the background, there is a white wall, an electric cooker, cutlery, and sausages. There is the sound of male voice in the background.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/2FEwrS4jMBo.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_2013_0"}, {"texts": ["A person wearing a green t-shirt and cream pants is standing on the left side of the countertop, opening a container and taking out some slices of meat and putting it on the bread."], "durations": null, "exact_frames_per_prompt": [80], "background": "In the background, there is a packet, white plate, food, gray countertop, gray surface, white cupboard, a container of meat, voice of a person and a miscellaneous sound is audible.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/0ppo_8sZM7U.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_2014_0"}, {"texts": ["A person whose only lower body is visible is making a chicken sandwich."], "durations": null, "exact_frames_per_prompt": [80], "background": "In the background, the person is audible and other miscellaneous sounds are also audible. There is a marble counter-top, a plastic container with chicken slices, a white plate, a sandwich, two packets, two white bottles and a yellow bottle. There are also white kitchen cabinets and a floor.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/0ppo_8sZM7U.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_2015_0"}, {"texts": ["A man wearing a black suit is standing outside.", "The man is standing inside, behind the podium and holding a microphone, and then starts talking to another man."], "durations": null, "exact_frames_per_prompt": [62, 18], "background": "In the background, there is a green grass surface, green trees, an iron pole, a white shed structure, chairs, a window, a brown printed wall, boards with a tripod stand, a table covered with white cloth, and some other stuff, and the man and people speaking sound and slow music playing sound is audible.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/0QNaUcfF-Ak.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_2016_0_ms"}, {"texts": ["A group of people is sitting, and a few are standing at the corner."], "durations": null, "exact_frames_per_prompt": [25], "background": "In the background, there is a green grass surface, green trees, an iron pole, a white shed structure, chairs, a window, a brown printed wall, boards with a tripod stand, a table covered with white cloth, and some other stuff, and the man and people speaking sound and slow music playing sound is audible.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/0QNaUcfF-Ak.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_2016_1_ms"}, {"texts": ["A man wearing a green t-shirt and jeans is taking out clothes from the washing machine and putting them into a basket."], "durations": null, "exact_frames_per_prompt": [80], "background": "In the background, there are washing machines, a white wall, clothes, a basket, black pipes, and the voice of a woman is audible.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/6fI9oGwmYaI.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_2017_0"}, {"texts": ["A girl wearing a white top is sitting and eating something while watching something on the green tablet."], "durations": null, "exact_frames_per_prompt": [80], "background": "In the background, the sound of the tablet is audible. There is a white wall, a bed, a green tablet, a brown door, a pink curtain, a brown object, a dark colored floor and some miscellaneous objects at the back.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/2K0T_s36MwQ.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_201_0"}, {"texts": ["A man wearing a blue check shirt is at first lifting the car with a jack tool machine", "The man loses the wheel nut with a wrench.", "The man removes the nut with his hand."], "durations": null, "exact_frames_per_prompt": [31, 24, 25], "background": "In the background, there is a car, and a jack tool machine.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/4F2J-ESGoxY.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_2027_0"}, {"texts": ["A person is sitting and holding a tattoo pen and making a tattoo on the hand of a person."], "durations": null, "exact_frames_per_prompt": [80], "background": "In the background, music is playing and some miscellaneous sounds are audible, there is a tattoo pen, racks with some other stuff, and a surface.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/2dc8r8yYnjA.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_202_0"}, {"texts": ["A person is sitting and getting a tattoo on his hand by the first person.", "And showing middle finger."], "durations": null, "exact_frames_per_prompt": [18, 62], "background": "In the background, music is playing and some miscellaneous sounds are audible, there is a tattoo pen, racks with some other stuff, and a surface.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/2dc8r8yYnjA.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_202_1"}, {"texts": ["A girl whom upper half-body is visible wearing a yellow-printed black top is sitting on a chair and is cleaning her lips with a napkin and her hand.", "The girl starts speaking while eating the yellow bread."], "durations": null, "exact_frames_per_prompt": [36, 45], "background": "In the background, people are speaking; there is the sound of laughing, a building, road, green leaves, a white pillar and a black chair.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/3seboaPCM6E.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_2033_0"}, {"texts": ["A person whose hands are visible wearing a white shirt is tying a bow tie on a mannequin."], "durations": null, "exact_frames_per_prompt": [72], "background": "In the background, a man is speaking. There are brown cupboard with shirts and bow ties in it.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/6pd92GSVcF8.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_2040_1"}, {"texts": ["A man wearing a black-white striped sweatshirt is sitting on the left side of the third man while putting his hands on the table and looking in front then looking to the right while a man in a black shirt sitting in the middle is talking to them."], "durations": null, "exact_frames_per_prompt": [80], "background": "In the background, there is a wooden table, wooden slab, bottles of beers, light brown board, posters, dark brown wall, glasses, voices of the men and the sound of the music is audible.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/2MfVgeSj0EA.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_2041_0"}, {"texts": ["A man wearing a black shirt is sitting on the right side of the third man while holding a glass of beer and talking while another man wearing a black shirt is sitting and drinking."], "durations": null, "exact_frames_per_prompt": [80], "background": "In the background, there is a wooden table, wooden slab, bottles of beers, light brown board, posters, dark brown wall, glasses, voices of the men and the sound of the music is audible.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/2MfVgeSj0EA.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_2041_1"}, {"texts": ["A man wearing a black shirt is sitting in between the man wearing green-white hoodie and the man wearing black shirt, while holding a glass of beer."], "durations": null, "exact_frames_per_prompt": [80], "background": "In the background, there is a wooden table, wooden slab, bottles of beers, light brown board, posters, dark brown wall, glasses, voices of the men and the sound of the music is audible.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/2MfVgeSj0EA.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_2041_2"}, {"texts": ["A man on the left side wearing a printed hoodie is sitting with other men."], "durations": null, "exact_frames_per_prompt": [80], "background": "In the background there are brown glass bottles, posters, a brown wall, wine glasses, a white object, brown countertop and people's speaking voices are audible.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/2MfVgeSj0EA.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_2042_0"}, {"texts": ["A man in the middle of the first and second man is speaking while holding a glass of beer."], "durations": null, "exact_frames_per_prompt": [80], "background": "In the background there are brown glass bottles, posters, a brown wall, wine glasses, a white object, brown countertop and people's speaking voices are audible.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/2MfVgeSj0EA.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_2042_2"}, {"texts": ["A girl on the left is holding a shot glass in her hand then moves her hands while two other girls are also sitting on the right and holding shot glasses.", "The girl starts eating a piece of lemon while two other girls also start eating a piece of lemon.", "The girl moves her hands."], "durations": null, "exact_frames_per_prompt": [44, 14, 22], "background": "In the background, people are speaking, the music is playing, there is a brown table, blue walls, a picture frame, a white light, a statue, and a brown ceiling.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/63zXr8TfSFE.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_2045_0"}, {"texts": ["A girl wearing a cap on the right is moving her hands while two women are raising a toast with the drinks and enjoying themselves.", "The woman wearing a cap takes a shot glass in her hand while two women start eating something.", "The woman wearing a cap puts it on the table."], "durations": null, "exact_frames_per_prompt": [36, 21, 23], "background": "In the background, people are speaking, the music is playing, there is a brown table, blue walls, a picture frame, a white light, a statue, and a brown ceiling.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/63zXr8TfSFE.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_2045_2"}, {"texts": ["A person wearing a brown bracelet is sitting inside a car, tearing a piece of paper.", "The person making a Christians cross then showing the Christians cross."], "durations": null, "exact_frames_per_prompt": [14, 66], "background": "In the background, the music is playing, there is a car, green trees, electric poles, and a sky.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/-t3DBRyt35s.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_2048_0"}, {"texts": ["A person wearing a grey shirt is driving the car while another person is tearing a paper and making a cross from it."], "durations": null, "exact_frames_per_prompt": [80], "background": "In the background, the music is playing, there is a car, green trees, electric poles, and a sky.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/-t3DBRyt35s.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_2048_1"}, {"texts": ["A boy wearing a black-gray t-shirt and black jeans, is standing on the right side and is making a sandwich while spreading the brown sandwich cream on the white bread with the sandwich spatula and speaking."], "durations": null, "exact_frames_per_prompt": [80], "background": "In the background, people are speaking; there is the sound of television playing; cream-colored walls; a cream-colored cupboard; sandwich cream bottles, a bread packet, a white paper roll, a gray container, and other stuff are on the cream-colored counter-top; and a white-cream granite countertop with drawers, a sandwich spatula, and a black granite surface.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/5DYznLZymFw.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_204_0"}, {"texts": ["A boy wearing a green-black t-shirt is sitting on a white chair and unwrapping a gift.\n"], "durations": null, "exact_frames_per_prompt": [80], "background": "In the background, there is a miscellaneous sound, people are speaking, there is a brown-black counter top, a gift, books, a red lantern, bottles, white chairs, a black laptop, white walls, a black chair, a circular table, a mirror, white table lamps, a wooden cupboard, a microwave, windows, a sofa, and green plants.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/-W1d_vhMasw.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_2050_0"}, {"texts": ["A man whose only hands are visible is showing different types of small spice bottles while holding them in his hands."], "durations": null, "exact_frames_per_prompt": [80], "background": "In the background, a man is speaking; there are small spice bottles on the brown counter, spatulas, other stuff on the brown counter, a chicken breast with spice on the brown counter, white walls, a white counter on the left side, a brown surface counter on the right side, and a machine on the brown counter.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/0vl1EYfOJnI.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_2052_0"}, {"texts": ["A person wearing a brown jacket whose hand is visible is pulling a fishing hook from the ice fishing hole while a person wearing black gloves is sitting on the snow surface and bending towards the ice fishing hole.", "The person puts hand in the ice fishing hole and catches a grey fish from the ice fishing hole while a person wearing black gloves is sitting on the snow surface on its knees and then stands up."], "durations": null, "exact_frames_per_prompt": [61, 19], "background": "In the background, there is a miscellaneous sound, a man is speaking and laughing, there is a snowy surface, an ice fishing hole, a fishing hook, and a black object.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/20VZNYJoI8Q.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_2053_0"}, {"texts": ["A person wearing black shoes is standing on the snow surface while a man wearing olive green clothes is sitting near the ice hole and doing something.", "The person sits on knee and starts removing the snow while a person wearing a olive-green clothes is catching something out from the hole.", "The person stands up while the person wearing olive-green clothes is taking a big fish out from the hole."], "durations": null, "exact_frames_per_prompt": [54, 19, 5], "background": "In the background, there is a miscellaneous sound, a man is speaking and laughing, there is a snowy surface, an ice fishing hole, a fishing hook, and a black object.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/20VZNYJoI8Q.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_2053_1"}, {"texts": ["A woman wearing a red t-shirt and blue jeans is standing and washing the dog while the dog turns his head and puts the tongue out"], "durations": null, "exact_frames_per_prompt": [80], "background": "In the background, a person is audible. There is a dog bathing tub, a black rack, a black storage box, a scrubber, a white bottle, a white container, a green soap bottle, and a white wall.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/-q_pKf-eLyE.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_2055_0"}, {"texts": ["A black dog is standing in a bathing tub and being washed by the woman."], "durations": null, "exact_frames_per_prompt": [80], "background": "In the background, a person is audible. There is a dog bathing tub, a black rack, a black storage box, a scrubber, a white bottle, a white container, a green soap bottle, and a white wall.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/-q_pKf-eLyE.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_2055_1"}, {"texts": ["A person whose hands and legs are visible, wearing a white-black t-shirt and blue jeans is sitting on the black surface, folding the white cloth with their hands and then ghosting the cloth."], "durations": null, "exact_frames_per_prompt": [80], "background": "In the background, birds are chirping; there is a white cloth, a black tile surface, and a reflection of the sky and green trees on the surface.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/0CSuK4stBPw.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_2056_0"}, {"texts": ["A woman wearing a black dress is sitting on a chair, speaking, laughing and moving her right hand."], "durations": null, "exact_frames_per_prompt": [39], "background": "In the background, there is a monitor, black chairs, a floor, a white wall, a table, a brown platform, sofa chairs, and some other stuff, and the voice of people speaking is audible.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/3FUreUTGmFY.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_2058_0"}, {"texts": ["A woman wearing a blue dress is sitting on a black chair."], "durations": null, "exact_frames_per_prompt": [38], "background": "In the background, there is a monitor, black chairs, a floor, a white wall, a table, a brown platform, sofa chairs, and some other stuff, and the voice of people speaking is audible.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/3FUreUTGmFY.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_2058_1"}, {"texts": ["A person wearing a black t-shirt whose only hands are visible is peeling a potato with a black peeler."], "durations": null, "exact_frames_per_prompt": [80], "background": "In the background, there is a white wall, a white door, a black peeler, potatoes, peels, white light, a black-light green blanket, a window, a yellow object, and music is playing.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/5-EI0-8WOqo.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_205_0"}, {"texts": ["A man wearing a black t-shirt is standing", "The man is pulling cork from the bottle with a plier."], "durations": null, "exact_frames_per_prompt": [15, 65], "background": "In the background, there is a table, a bottle, a plier, a chair, a box filled with nails and some other stuff, white wall, and people's voices are audible.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/smVeU71ZpDE.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_206_0"}, {"texts": ["A man wearing a dark green t-shirt is grinding meat in the black meat grinder."], "durations": null, "exact_frames_per_prompt": [57], "background": "In the background, music is playing. There is a black counter-top, plastic containers, kitchen cabinets, utensils, a white paper, meat, black meat grinder, a glass jar, a glass dish, a red wall and a white door.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/02Vl8JcqbAE.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_2077_0"}, {"texts": ["A man wearing a red t-shirt is riding on an elephant while holding a stick a man whose half body visible recording the video"], "durations": null, "exact_frames_per_prompt": [47], "background": "In the background, there are bushes, and a stick.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/-SeJQZQSt0M.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_2078_0"}, {"texts": ["A man is also riding on an elephant while sitting behind the man in a red t-shirt."], "durations": null, "exact_frames_per_prompt": [39], "background": "In the background, there are bushes, and a stick.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/-SeJQZQSt0M.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_2078_1"}, {"texts": ["An elephant is walking in a straight direction with the two men riding on its back."], "durations": null, "exact_frames_per_prompt": [47], "background": "In the background, there are bushes, and a stick.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/-SeJQZQSt0M.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_2078_2"}, {"texts": ["A person wearing black clothes whose left hand is visible is turning the pages of a magazine and a brown cat is sitting on the table under the magazine and then starts moving."], "durations": null, "exact_frames_per_prompt": [80], "background": "In the background, there is a brown surface, a white wall, some other objects, and the voice of a person laughing, the sound of music, and some miscellaneous sounds are audible.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/0qV2fTbgXAQ.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_207_0"}, {"texts": ["A brown cat is lying while a person whose hand is visible is flipping the pages of a magazine.", "The brown cat starts moving under the magazine while the person is moving the magazine over the cat."], "durations": null, "exact_frames_per_prompt": [32, 48], "background": "In the background, there is a brown surface, a white wall, some other objects, and the voice of a person laughing, the sound of music, and some miscellaneous sounds are audible.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/0qV2fTbgXAQ.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_207_1"}, {"texts": ["A man wearing a black jacket is standing and drinking.", "The man wipes his mouth with a tissue while the person wearing a black t-shirt is filming it.", "The man holds the glass while others look at him."], "durations": null, "exact_frames_per_prompt": [25, 33, 22], "background": "In the background, there is a yellow wall, a transparent glass window frame, a yellow-black containers, tables, chairs, a television, a glass, and the people speaking, shouting, and laughing sounds are audible.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/5EeJQ99bVvc.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_2083_0"}, {"texts": ["A man wearing camouflage clothes is walking in the green field holding a gun while there are white and black dog on the ground and running and the white dog fetches the bird  and run towards left"], "durations": null, "exact_frames_per_prompt": [80], "background": "In the background, music is playing. There is a green field, trees, and sky.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/1K4FIaiPL6I.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_2084_0"}, {"texts": ["A man wearing a brown cap is shaking hands with the third dog.", "The man walks."], "durations": null, "exact_frames_per_prompt": [15, 13], "background": "In the background, music is playing. There is a green field, trees, and sky.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/1K4FIaiPL6I.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_2084_2"}, {"texts": ["A black dog is running in the green field while a beige colored dog is moving on the grass surface, and two men are walking on the grass while holding shotguns."], "durations": null, "exact_frames_per_prompt": [28], "background": "In the background, music is playing. There is a green field, trees, and sky.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/1K4FIaiPL6I.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_2084_3"}, {"texts": ["A white dog is running in the green field while the black dog comes towards the white dog and the man in orange vest is walking towards them."], "durations": null, "exact_frames_per_prompt": [27], "background": "In the background, music is playing. There is a green field, trees, and sky.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/1K4FIaiPL6I.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_2084_4"}, {"texts": ["A pigeon is getting picked by the third dog in its mouth."], "durations": null, "exact_frames_per_prompt": [20], "background": "In the background, music is playing. There is a green field, trees, and sky.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/1K4FIaiPL6I.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_2084_6"}, {"texts": ["A woman wearing a white-dark blue dress and some jewellery is holding a spatula and melting the butter in a pan on the stove.", "The woman wearing a white-dark blue dress and some jewellery is breaking the shell of an egg into the pan."], "durations": null, "exact_frames_per_prompt": [48, 31], "background": "In the background, there is a white-grey countertop, a stove, a metallic grey object, a metallic grey pan, butter, a spatula, a blue object, the voice of the woman and some miscellaneous sounds are audible.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/-JRT9aCAVsA.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_2085_0"}, {"texts": ["A woman wearing a black top is defying her eyebrow with a tweezer."], "durations": null, "exact_frames_per_prompt": [80], "background": "In the background, a miscellaneous sound is audible. There is a mirror, a tweezer and a multi color curtain.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/0R0Aklb0X3Q.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_2087_0"}, {"texts": ["A man whose hand is visible pulls the bed sheet.", "The man starts moving his hand."], "durations": null, "exact_frames_per_prompt": [17, 60], "background": "In the background, a person is audible. There is a bed with white bed sheet and a brown wooden floor.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/j08arN8v1jM.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_2088_0"}, {"texts": ["A person with only their hands visible is frying an omelette in a black pan with a spatula."], "durations": null, "exact_frames_per_prompt": [30], "background": "In the background, there is a black pan, a spatula, a white gas stove, refrigerators, an oven, a white wall, a person's speaking voice and some miscellaneous sounds are audible.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/1niNKup9egs.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_2091_0"}, {"texts": ["A woman wearing black clothes is standing and is caressing the black horse on the road."], "durations": null, "exact_frames_per_prompt": [28], "background": "In the background, music is playing. There is a gray road, green grass, green bushes, water and a blue sky with white clouds.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/0Mv1_ZvyBfU.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_2097_0"}, {"texts": ["A woman wearing a white top is walking on the road with her right arm up and is holding a rope."], "durations": null, "exact_frames_per_prompt": [13], "background": "In the background, music is playing. There is a gray road, green grass, green bushes, water and a blue sky with white clouds.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/0Mv1_ZvyBfU.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_2097_1_ms"}, {"texts": ["A white horse is standing on the grey road."], "durations": null, "exact_frames_per_prompt": [80], "background": "In the background, music is playing. There is a gray road, green grass, green bushes, water and a blue sky with white clouds.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/0Mv1_ZvyBfU.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_2097_2_ms"}, {"texts": ["A black horse is also standing on the gray road and is being caressed by person one."], "durations": null, "exact_frames_per_prompt": [27], "background": "In the background, music is playing. There is a gray road, green grass, green bushes, water and a blue sky with white clouds.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/0Mv1_ZvyBfU.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_2097_3"}, {"texts": ["A lady wearing a grey top and pink shorts is riding on a brown camel while a man wearing a white cloth is holding a leash of the camel and walking ahead."], "durations": null, "exact_frames_per_prompt": [80], "background": "In the background there is blue sky, sand slope and rock mountain, red shed, dry grass, and the camel walking sound and crowd speaking sound is audible in the background.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/0TFD222QOOw.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_2098_0"}, {"texts": ["A man wearing white clothes is walking in front and holding the camel rope on which the lady is riding."], "durations": null, "exact_frames_per_prompt": [18], "background": "In the background there is blue sky, sand slope and rock mountain, red shed, dry grass, and the camel walking sound and crowd speaking sound is audible in the background.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/0TFD222QOOw.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_2098_1"}, {"texts": ["A white camel is walking while carrying a lady on his back while a man wearing white outfit is walking along with the camel and he is holding a rope that is attached to the camel."], "durations": null, "exact_frames_per_prompt": [80], "background": "In the background there is blue sky, sand slope and rock mountain, red shed, dry grass, and the camel walking sound and crowd speaking sound is audible in the background.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/0TFD222QOOw.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_2098_2"}, {"texts": ["A child wearing a pink top and red pants is held by a boy on his back while the boy released the girl on the floor", "The child gets down and walks towards the front and falls down on the red-cream carpet while crying."], "durations": null, "exact_frames_per_prompt": [27, 53], "background": "In the background, there is a white wall, white surface, gray couch, black laptop, black charger, red-cream carpet, white cloth, a wooden table, a white pillow, some stuff, a wooden door, a red object, voices of the people and a miscellaneous sound is audible.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/1-9KSslytrk.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_2099_0"}, {"texts": ["A boy wearing a gray t-shirt and blue jeans is holding a child on his back and walking, then putting the child down on the red-cream carpet while a child wearing a pink top is crying and after putting him down by a boy child starts running."], "durations": null, "exact_frames_per_prompt": [34], "background": "In the background, there is a white wall, white surface, gray couch, black laptop, black charger, red-cream carpet, white cloth, a wooden table, a white pillow, some stuff, a wooden door, a red object, voices of the people and a miscellaneous sound is audible.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/1-9KSslytrk.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_2099_2"}, {"texts": ["A kid wearing a pink cloth is sitting on the wooden stool on the left side and watching in right side the girl give pose to photo while eating."], "durations": null, "exact_frames_per_prompt": [80], "background": "In the background there is a wooden chair, table, stool, a colourful plate with food, a wooden horse toy, a red toy, a brown surface, and the people speaking sound is audible.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/3xssCibHYe0.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_2101_0"}, {"texts": ["A kid wearing green cloth is sitting on the wooden chair on the right side and moving his body and holding a red toy while the other kid sits along with her"], "durations": null, "exact_frames_per_prompt": [80], "background": "In the background there is a wooden chair, table, stool, a colourful plate with food, a wooden horse toy, a red toy, a brown surface, and the people speaking sound is audible.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/3xssCibHYe0.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_2101_1"}, {"texts": ["A woman on the left side, wearing a blue jacket and grey pants, is sitting on the chair. She stands and walks towards the right side, near the table a man wearing a red shirt sitting on chair."], "durations": null, "exact_frames_per_prompt": [40], "background": "In the background, there is a campfire, black rods, a green object, a green camp, chairs, a green bag, a table, some boxes, a blue container, iron hooks, wooden objects, green grass surface, music is playing and a man's voice is audible.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/24cKLt2hz6Q.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_2105_0"}, {"texts": ["A man on the right side, wearing a red-black shirt and blue jeans is sitting on the chair.\n while a woman wearing grey pants is sitting then moves towards the right and picks up something."], "durations": null, "exact_frames_per_prompt": [80], "background": "In the background, there is a campfire, black rods, a green object, a green camp, chairs, a green bag, a table, some boxes, a blue container, iron hooks, wooden objects, green grass surface, music is playing and a man's voice is audible.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/24cKLt2hz6Q.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_2105_1"}, {"texts": ["A lady wearing a blue jacket is standing on a brown muddy surface and combing the tail of a brown pony."], "durations": null, "exact_frames_per_prompt": [80], "background": "In the background there is a brown muddy surface, brown wall, a window, some objects on the wall, a lady and man speaking with each other sound is audible in the background.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/0qP7PEmZwQw.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_2107_0"}, {"texts": ["A brown pony is standing on a brown muddy surface and tied with ropes and a lady is combing its tail."], "durations": null, "exact_frames_per_prompt": [80], "background": "In the background there is a brown muddy surface, brown wall, a window, some objects on the wall, a lady and man speaking with each other sound is audible in the background.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/0qP7PEmZwQw.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_2107_1"}, {"texts": ["A woman wearing a black cloth is sitting and drinking from the glass while a group of people are sitting and standing at the back."], "durations": null, "exact_frames_per_prompt": [81], "background": "In the background, there are posters, a table, a glass, LED lights, a television, a woman's shouting and laughing voice, and television's voice are audible.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/6UihMjZSmYY.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_2108_0"}, {"texts": ["A woman wearing a white top is standing and doing hand gestures.\n"], "durations": null, "exact_frames_per_prompt": [28], "background": "In the background, there is a blackboard, a white wall, a dark area, some other stuff, and the voice of a person speaking, and some miscellaneous sounds are audible.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/4EoWG3ltIzw.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_2109_0"}, {"texts": ["A man wearing a black jacket is sitting and eating noodles."], "durations": null, "exact_frames_per_prompt": [55], "background": "In the background, music is audible. There are tables, chairs, a fork, with napkin, beige-brown walls, lights and a T. V screen.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/1Ge5azXYS3k.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_2110_0"}, {"texts": ["A man wearing a black jacket, brown pants and a brown cap is sitting on the back of the horse and riding from left to right."], "durations": null, "exact_frames_per_prompt": [80], "background": "In the background, there is a soil surface, a gray wheelbarrow, some decorative items, a christmas tree, white door, white wall, white ceiling, gray floor, a white object, gray ceiling with metals, windows, a wooden cart, white tables, a blue bucket and the sound of the music is audible.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/-gRuiW2S6kM.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_2115_0"}, {"texts": ["A grayish-white horse is walking from left to right while carrying a man on his back."], "durations": null, "exact_frames_per_prompt": [80], "background": "In the background, there is a soil surface, a gray wheelbarrow, some decorative items, a christmas tree, white door, white wall, white ceiling, gray floor, a white object, gray ceiling with metals, windows, a wooden cart, white tables, a blue bucket and the sound of the music is audible.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/-gRuiW2S6kM.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_2115_1"}, {"texts": ["A man wearing a blue t-shirt is sitting on a horse and riding it on a soil surface."], "durations": null, "exact_frames_per_prompt": [80], "background": "In the background, music is playing. There is a soil surface, white walls, a white object, a garland, a gray wall with windows, a brown cart, a gray folded table, a blue object, a decorated Christmas tree, a carriage, and a metal beam ceiling.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/-gRuiW2S6kM.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_2116_0"}, {"texts": ["A gray horse is walking on a soil surface with a man sitting on it."], "durations": null, "exact_frames_per_prompt": [80], "background": "In the background, music is playing. There is a soil surface, white walls, a white object, a garland, a gray wall with windows, a brown cart, a gray folded table, a blue object, a decorated Christmas tree, a carriage, and a metal beam ceiling.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/-gRuiW2S6kM.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_2116_1"}, {"texts": ["A man wearing a brown cap is riding a horse in the right direction on a brown soil surface."], "durations": null, "exact_frames_per_prompt": [80], "background": "In the background there is a horse shed, a horse cart, a blue bucket, wooden chambers, window, soil surface, a metal frame, a Christmas tree, a white can, a trolley and a music sound is audible.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/-gRuiW2S6kM.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_2117_0"}, {"texts": ["A grey horse taking a man on its back is moving in the right direction on a brown soil surface."], "durations": null, "exact_frames_per_prompt": [80], "background": "In the background there is a horse shed, a horse cart, a blue bucket, wooden chambers, window, soil surface, a metal frame, a Christmas tree, a white can, a trolley and a music sound is audible.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/-gRuiW2S6kM.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_2117_1"}, {"texts": ["A person whose only hand is visible is pouring the white batter into a black utensil."], "durations": null, "exact_frames_per_prompt": [80], "background": "In the background, the person is speaking. There are green walls, white cabinets, a pink bottle, a transparent jug, a white batter, a black utensil, a black rack with multiple objects kept in it, a mug, silver storage jars, packets, boxes and some miscellaneous objects.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/2hXdCr316is.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_2119_0"}, {"texts": ["A person wearing a gray black striped t-shirt is standing on the right side and pushing meat in the meat grinder machine."], "durations": null, "exact_frames_per_prompt": [41], "background": "In the background, music is playing. There is a meat grinder machine with meat, a white tray, a spray bottle, and a silver counter.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/41PGTc3XKEY.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_211_0"}, {"texts": ["A woman whose hand is visible is pouring batter into a black tray with a jug."], "durations": null, "exact_frames_per_prompt": [80], "background": "In the background, there is a green wall, white cabinets, a red fire extinguisher, a black shelf, boxes, a black bag, jugs, and the voice of a girl is audible.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/2hXdCr316is.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_2120_0"}, {"texts": ["A person whose only hand and legs are visible is switching on the paper shredder.", "The person is shedding a white paper."], "durations": null, "exact_frames_per_prompt": [43, 37], "background": "In the background, the sound of the shredder is audible. There is a white paper, a brown surface and a black paper shredder.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/6JeRGeHkpDg.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_2125_0"}, {"texts": ["A person whose hand is visible is holding a piece of paper, touching the shredder machine and turning it on.", "The man is putting the paper inside the shredder machine."], "durations": null, "exact_frames_per_prompt": [52, 28], "background": "In the background, there is a shredder machine, wooden floor, a paper, and machine sound and some miscellaneous sounds are audible.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/6JeRGeHkpDg.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_2126_0"}, {"texts": ["A man wearing a bee veil is standing on the right side and watching the other man on the left side while holding a mike."], "durations": null, "exact_frames_per_prompt": [80], "background": "In the background, there are trees, grass and bushes, beehive boxes, and the beehive smoker.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/8uCkXKBWxfo.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_2128_1"}, {"texts": ["A girl wearing light green clothes is sitting on the white kitchen counter-top and then pushing the stuffer filler stick into the grinder while a woman wearing a maroon top is standing on the right side and cutting meat.", "The girl is looking in the meat grinder machine while the woman wearing a maroon top is putting the meat in the grinder machine."], "durations": null, "exact_frames_per_prompt": [32, 48], "background": "In the background, there is a light brown wall, a fish aquarium, a white kitchen countertop, a window with curtain, a meat grinding machine, cloth, a white bottle, meat, some other stuff, a wooden shelf, and the sound of grinding machine and slow woman speaking sound is audible.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/-RP8pbG-JpI.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_2131_0"}, {"texts": ["A woman wearing a purple top is standing and cutting the meat while a girl is taking out the black object and peeking at the hole of the meat grinder.", "The woman is putting the meat into the meat grinder."], "durations": null, "exact_frames_per_prompt": [35, 44], "background": "In the background, there is a light brown wall, a fish aquarium, a white kitchen countertop, a window with curtain, a meat grinding machine, cloth, a white bottle, meat, some other stuff, a wooden shelf, and the sound of grinding machine and slow woman speaking sound is audible.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/-RP8pbG-JpI.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_2131_1"}, {"texts": ["A person wearing a black cap is shearing a white sheep with a sheep shearing scissor and a sheep is moving while a person standing on the right moves."], "durations": null, "exact_frames_per_prompt": [80], "background": "In the background there is a brown canvas, metal fencing, a shearing scissor, a green grass surface and people's voices and some miscellaneous sounds are audible.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/0NeDA66NvCw.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_2135_0"}, {"texts": ["A white sheep is lying on a brown canvas while the man shearing the wool from sheep."], "durations": null, "exact_frames_per_prompt": [80], "background": "In the background there is a brown canvas, metal fencing, a shearing scissor, a green grass surface and people's voices and some miscellaneous sounds are audible.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/0NeDA66NvCw.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_2135_2"}, {"texts": ["A baby girl wearing a red top is sitting and eating an ice cream from a person."], "durations": null, "exact_frames_per_prompt": [78], "background": "In the background, there is a lamp, a green wall, a white ceiling, a wooden table, some other stuff, and the voice of people speaking is audible.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/0vldOeJCp0E.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_2138_0"}, {"texts": ["A person whose hands are visible is holding an ice cream and feeding a baby girl."], "durations": null, "exact_frames_per_prompt": [78], "background": "In the background, there is a lamp, a green wall, a white ceiling, a wooden table, some other stuff, and the voice of people speaking is audible.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/0vldOeJCp0E.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_2138_1"}, {"texts": ["A man standing on a green grass surface is speaking and swinging the golf stick."], "durations": null, "exact_frames_per_prompt": [80], "background": "In the background, the man is speaking. There is a green grass surface, a brown surface, green plants, a brown wall, metal fencing, white flags, green trees, a white golf ball and a blue sky. There is also a white logo in the video.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/0oxvvCfe8ws.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_2139_0"}, {"texts": ["A man wearing a purple-white t-shirt and gray trousers is teaching how to hit a golf ball shot."], "durations": null, "exact_frames_per_prompt": [80], "background": "In the background, a person is audible. There is a green grass field, a golf ball, a golf club, white flags, green trees, green shrubs, a sand surface, and a clear blue sky.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/0oxvvCfe8ws.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_2140_0"}, {"texts": ["A woman wearing a black top is standing and moving in a fuel station, while speaking on her phone."], "durations": null, "exact_frames_per_prompt": [80], "background": "In the background, people are speaking. There is a car, a fuel hose, a fuel station, white ceiling, lights, sky, a tree, gray surface, a red white building, and a road surface.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/54LV_Bx_O2Y.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_2143_0"}, {"texts": ["A woman wearing a black top is standing and moving in a fuel station while speaking on her phone."], "durations": null, "exact_frames_per_prompt": [80], "background": "In the background, there is a car, a fuel hose, a fuel station, white ceiling, lights, a sky, a tree, a gray surface, a red white building, and a road surface and people speaking sound is audible.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/54LV_Bx_O2Y.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_2144_0"}, {"texts": ["A person wearing a light blue shirt tear the packet", "The person puts the piece of meat on a bread slice."], "durations": null, "exact_frames_per_prompt": [32, 48], "background": "In the background, a person and miscellaneous sounds are audible. There are slices of bread, a piece of meat, a jar, a white door, a brown table, white cupboards, an oven, and beige walls.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/3jADp5mXnAM.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_2146_0"}, {"texts": ["A lady wearing an orange t-shirt is sitting on the white surface, holding a girl, and helping her to pick something from the water with an aquarium fish net while a group of people are moving around them."], "durations": null, "exact_frames_per_prompt": [80], "background": "In the background, there are green plants, grass, water, a wooden barrier, a white surface, aquarium fish nets, people are speaking, and there is a miscellaneous sound.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/2UxokcqnsFk.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_2157_0"}, {"texts": ["A girl wearing white shirt is picking something from the water with an aquarium fish net and is being helped by a lady."], "durations": null, "exact_frames_per_prompt": [80], "background": "In the background, there are green plants, grass, water, a wooden barrier, a white surface, aquarium fish nets, people are speaking, and there is a miscellaneous sound.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/2UxokcqnsFk.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_2157_1"}, {"texts": ["A boy wearing grey clothes is holding a plier and removing the lid of a glass bottle."], "durations": null, "exact_frames_per_prompt": [80], "background": "In the background, there is a glass bottle, a plier, white tied flooring, grey walls, flower pots, and the iron frame.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/QXxzSmC2g5s.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_2159_0"}, {"texts": ["A boy, sitting in a man's lap while raising his hands then a man wearing a black jacket is giving a piece of bread the boy.", "The boy takes a doughnut piece from the man.", "The boy starts eating it."], "durations": null, "exact_frames_per_prompt": [57, 11, 12], "background": "In the background, there is a pink tabletop, pink seating, beige wall panel, a brown paper and people's voices and some miscellaneous sounds are audible.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/-ZEG-ZimwJg.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_2162_0"}, {"texts": ["A man wearing a dark blue jacket is giving a piece of doughnut to the boy while holding him with one hand on his lap."], "durations": null, "exact_frames_per_prompt": [80], "background": "In the background, there is a pink tabletop, pink seating, beige wall panel, a brown paper and people's voices and some miscellaneous sounds are audible.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/-ZEG-ZimwJg.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_2162_1"}, {"texts": ["A person wearing a black shirt and black pants is putting the golf stick near the golf ball.", "The person hitting the golf ball in its left direction on the green carpet."], "durations": null, "exact_frames_per_prompt": [18, 30], "background": "In the background, there is a white shed, a golf stick, golf balls, green grass surface, a green carpet, an off white platform, white surface, a yellow object and the voice of hitting the ball is audible.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/0mze20DX6Mw.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_2163_0"}, {"texts": ["A person whose hand is visible only, is picking up a fish from the fish net."], "durations": null, "exact_frames_per_prompt": [48], "background": "In the background, there is water, wooden dock, wooden fence, white fish net, a sea-green rope, the voice of the man and some miscellaneous sounds are audible.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/-yTIisA8eNI.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_2165_0"}, {"texts": ["A bird is standing far on the wooden dock and a person throws something towards the bird."], "durations": null, "exact_frames_per_prompt": [26], "background": "In the background, there is water, wooden dock, wooden fence, white fish net, a sea-green rope, the voice of the man and some miscellaneous sounds are audible.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/-yTIisA8eNI.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_2165_2"}, {"texts": ["A boy on the left side wearing a printed blue t-shirt is laughing and speaking while a woman is at the back and the boy in the right t-shirt is on the right, they are laughing."], "durations": null, "exact_frames_per_prompt": [80], "background": "In the background there is a white ceiling, a yellow wall, photo frames, and people laughing and a music voice is audible.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/14Sph2O9VtQ.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_2166_0"}, {"texts": ["A boy on the right side wearing a red t-shirt is laughing with the first boy while a woman also started laughing."], "durations": null, "exact_frames_per_prompt": [80], "background": "In the background there is a white ceiling, a yellow wall, photo frames, and people laughing and a music voice is audible.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/14Sph2O9VtQ.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_2166_1"}, {"texts": ["A woman wearing a pink sweater is laughing and sitting behind the boy wearing red t-shirt while the boys are also laughing."], "durations": null, "exact_frames_per_prompt": [80], "background": "In the background there is a white ceiling, a yellow wall, photo frames, and people laughing and a music voice is audible.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/14Sph2O9VtQ.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_2166_2"}, {"texts": ["A person wearing purple clothes is standing and grinding the meat in a grinder and collecting in a white bag.\n"], "durations": null, "exact_frames_per_prompt": [80], "background": "In the background, the sound of the grinder is audible. There is a white wall, a grinder, a white bag, wooden rack, a flower plant, a white counter, a door, and some kitchenwares.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/4ZFGw0DSHlo.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_2170_0"}, {"texts": ["A man, whose upper half-body is visible, wearing a purple t-shirt, is standing on the left side and is holding a white pouch in his left hand with the meat grinder machine and he is pouring the meat into the meat grinder machine", "The man is pushing the meat into the meat grinder machine with the stuffer filler stick while holding the stuffer filler stick in his right hand."], "durations": null, "exact_frames_per_prompt": [47, 33], "background": "In the background, there is the sound of the meat grinder machine, a steel meat grinder machine, a white pouch, a wooden cupboard on the right wall, white walls, a white surface counter, a black surface table, some kitchen utensils on the white counter, a flower bouquet hanging on the right wall, and a white stuffer filler stick.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/4ZFGw0DSHlo.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_2171_0"}, {"texts": ["A man wearing a gray suit is sitting, holding a bottle of wine, moving his other hand, and speaking."], "durations": null, "exact_frames_per_prompt": [32], "background": "In the background, music and a man are audible. There are bottles of wine, clear glass glasses with wine, a table with white table cloth, green trees, buildings, plates, green leaves, a plant, a planter, a pepper-salt crusher, a round cheese block with tomatoes, bamboo baskets, a white container, a clear glass glass with black drink, a scissor, and white napkins.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/1QSLrffPCvc.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_2172_0_ms"}, {"texts": ["A baby wearing a white-pink top is sitting in a baby feeding chair, holding a spoon.", "The baby wearing a white-pink top is moving its hand."], "durations": null, "exact_frames_per_prompt": [34, 46], "background": "In the background, a baby, a person, and miscellaneous sounds are audible. There is a white kitchen counter top, a baby feeding chair, a beige wall, windows, brown wooden chairs, a brown wooden table, a white cloth, a few miscellaneous items on the kitchen shelf, a brown floor, a dried tree, and a white-brown wall outside the window.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/-ZnCh1h0pbo.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_2173_0"}, {"texts": ["A person of whom only hands are visible is mixing the melon in a glass bowl with a fork while a boy wearing a t-shirt is standing and talking."], "durations": null, "exact_frames_per_prompt": [80], "background": "In the background, people are speaking and miscellaneous sounds are audible. There are white walls, white washing machine, a brown table, a dustbin, a white surface, a white door, brown kitchen cabinets, a glass bowl, a fork, chopped melon, utensils and other miscellaneous items.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/-96BKD_JX_0.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_2175_0"}, {"texts": ["A boy wearing a black t-shirt is moving and speaking while a woman is mixing the food in a bowl with the help of the spoon."], "durations": null, "exact_frames_per_prompt": [80], "background": "In the background, people are speaking and miscellaneous sounds are audible. There are white walls, white washing machine, a brown table, a dustbin, a white surface, a white door, brown kitchen cabinets, a glass bowl, a fork, chopped melon, utensils and other miscellaneous items.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/-96BKD_JX_0.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_2175_1"}, {"texts": ["A boy wearing a printed black t-shirt is standing behind the glass bowl while another person is mixing the fruit salad."], "durations": null, "exact_frames_per_prompt": [80], "background": "In the background there is a glass bowl with fruit pieces, a spoon, a dustbin, brown table-top, radiator heater, a blue and grey wall, a washing machine, wooden cabinets, a door, some kitchen utensils and people speaking and a miscellaneous sound is audible.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/-96BKD_JX_0.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_2176_1"}, {"texts": ["A woman wearing a green top and black pants is sitting on the cream chair and doing something on the right side while a person wearing black clothes is rotating the handle of the peeler."], "durations": null, "exact_frames_per_prompt": [80], "background": "In the background, there is a white floor, black countertop, a carton box, wooden cupboards, a cream countertop, stove, some utensils, some machines, some stuff, a microwave, some other stuff, an oven, a white cloth, a cream-black chair, a fruit peeler machine, peels, white-brown wall, bottles, a book, the voice of the man and some miscellaneous sounds are audible.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/-T5t3u6xYK4.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_2185_0"}, {"texts": ["A person wearing a black t-shirt is standing.", "The person is peeling an apple with an apple peeling machine."], "durations": null, "exact_frames_per_prompt": [30, 50], "background": "In the background, a man is audible. There is an apple, apple peels, an apple peeling machine, a black kitchen counter top, a white cutting board, a box, a black stove, and oven, brown wooden drawers, an air fryer, white-brown tile wall, a stool, and a few miscellaneous items.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/-T5t3u6xYK4.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_2186_1"}, {"texts": ["A baby wearing yellow clothes is being held by a person while the baby is trying to touch the cat which is lying on the bed while a person whom hand is visible is caressing the cat."], "durations": null, "exact_frames_per_prompt": [80], "background": "In the background, there are blankets, and off-white wall, a window, and the curtain.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/-iMQ2D3ES7c.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_2191_0"}, {"texts": ["A black cat is lying on the bed while being touched by the baby.", "The cat is being caressed by the person on the right side."], "durations": null, "exact_frames_per_prompt": [50, 30], "background": "In the background, there are blankets, and off-white wall, a window, and the curtain.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/-iMQ2D3ES7c.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_2191_1"}, {"texts": ["A person of whom only hands are visible is lying on the bed near the baby and holding the baby while a black cat is sitting in front of the baby and being petted by another person whose hand is visible."], "durations": null, "exact_frames_per_prompt": [80], "background": "In the background, there are blankets, and off-white wall, a window, and the curtain.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/-iMQ2D3ES7c.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_2191_2"}, {"texts": ["A man wearing a blue t-shirt and blue jeans is riding a horse while the woman holds the umbrella and walks along with the horse."], "durations": null, "exact_frames_per_prompt": [80], "background": "In the background, a man is audible. There is a drum, a rope, a rock wall, a multi-color umbrella, a few miscellaneous things, blue sheds, poles, wires, green trees, green shrubs, yellow-green grasses, a hill ridge, a soil surface, and a clear blue sky.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/-cMKlmFfK5c.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_2195_0"}, {"texts": ["A brown horse is walking forward.  while the man wearing blue t-shirt is riding the horse and the person wearing black t-shirt is holding the umbrella and walking beside the horse", "The horse turns backward.", "The horse starts walking while attached to a drum."], "durations": null, "exact_frames_per_prompt": [20, 19, 41], "background": "In the background, a man is audible. There is a drum, a rope, a rock wall, a multi-color umbrella, a few miscellaneous things, blue sheds, poles, wires, green trees, green shrubs, yellow-green grasses, a hill ridge, a soil surface, and a clear blue sky.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/-cMKlmFfK5c.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_2195_1"}, {"texts": ["A woman wearing a black top and black trousers is walking holding an umbrella while a man wearing blue t-shirt riding horse"], "durations": null, "exact_frames_per_prompt": [75], "background": "In the background, a man is audible. There is a drum, a rope, a rock wall, a multi-color umbrella, a few miscellaneous things, blue sheds, poles, wires, green trees, green shrubs, yellow-green grasses, a hill ridge, a soil surface, and a clear blue sky.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/-cMKlmFfK5c.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_2195_2"}, {"texts": ["A man wearing a checkered shirt is standing on the right side and blow drying a dog with a blow dryer."], "durations": null, "exact_frames_per_prompt": [80], "background": "In the background, the blow dryer sound is audible. There is a white tile wall, white walls, mirrors, a green chair, a towel holder with towel, taps, and a white object.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/2jLbjc1My-Q.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_2197_0"}, {"texts": ["A brown dog is standing on a green chair and getting blow dried by the man."], "durations": null, "exact_frames_per_prompt": [80], "background": "In the background, the blow dryer sound is audible. There is a white tile wall, white walls, mirrors, a green chair, a towel holder with towel, taps, and a white object.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/2jLbjc1My-Q.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_2197_1"}, {"texts": ["A kid wearing a white vest and gray shorts is standing and feeding a carrot to a goat while a man wearing a white t-shirt is giving a carrot to a goat, a group of kids are sitting, standing, and looking at the goats. A group of cars are moving at the back."], "durations": null, "exact_frames_per_prompt": [80], "background": "In the background, kids and people are audible. There are green grasses, carrots, a wood-metal net divider, green trees, a pole, and a road.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/1FpAg9qA5AY.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_2198_0"}, {"texts": ["A man wearing a white t-shirt and brown shorts is standing and feeding a carrot to another goat while a boy wearing a white vest is standing and feeding a carrot to the goat, a girl wearing a multicolored top is sitting in a pram on the right side and watching, a person wearing a blue top is coming from the backside and taking a carry bag from the man, a group of goats are standing behind the fencing, and a grey car is moving from right to left on the road in the backside."], "durations": null, "exact_frames_per_prompt": [80], "background": "In the background, kids and people are audible. There are green grasses, carrots, a wood-metal net divider, green trees, a pole, and a road.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/1FpAg9qA5AY.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_2198_1"}, {"texts": ["A boy wearing a brown t-shirt is riding an elephant and raising his hands while other boy wearing black shirt sitting behind him and other boy wearing blue shirt and gray jeans is walking along them."], "durations": null, "exact_frames_per_prompt": [49], "background": "In the background, the music is playing, there is a soil path, green bushes, green trees, and a sky.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/0eSFmX-NTIU.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_2199_0"}, {"texts": ["A boy wearing a black shirt is riding on the elephant while the man with blue t-shirt is holding the elephant and walking along with it"], "durations": null, "exact_frames_per_prompt": [49], "background": "In the background, the music is playing, there is a soil path, green bushes, green trees, and a sky.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/0eSFmX-NTIU.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_2199_1"}, {"texts": ["A boy wearing a blue t-shirt is walking on the soil path and holding the rope of the elephant while the other two people sit on the elephant's back and enjoy the ride."], "durations": null, "exact_frames_per_prompt": [48], "background": "In the background, the music is playing, there is a soil path, green bushes, green trees, and a sky.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/0eSFmX-NTIU.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_2199_2"}, {"texts": ["A brown elephant is walking on the soil path and carrying people on its back while a man in a blue T-shirt walking along with them."], "durations": null, "exact_frames_per_prompt": [49], "background": "In the background, the music is playing, there is a soil path, green bushes, green trees, and a sky.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/0eSFmX-NTIU.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_2199_3"}, {"texts": ["A woman wearing a purple top and gray jeans is bending over and doing something while a woman wearing a purple-white shirt is sorting clothes.", "The woman is getting up.", "The woman is putting some clothes in front."], "durations": null, "exact_frames_per_prompt": [45, 9, 26], "background": "In the background, there is a white wall, a gray pillar, a black object, a wooden cupboard, poster, switch, a white bucket, a white object, some clothes, a red box, the voice of the woman and a song is playing.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/1bn4BAJa0ws.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_2202_0"}, {"texts": ["A woman wearing a purple-gray top and blue jeans is standing, taking out some clothes from the bucket and throwing them here and there while another woman wearing a purple top is leaning forward and picking up some clothes and throwing it on the front."], "durations": null, "exact_frames_per_prompt": [80], "background": "In the background, there is a white wall, a gray pillar, a black object, a wooden cupboard, poster, switch, a white bucket, a white object, some clothes, a red box, the voice of the woman and a song is playing.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/1bn4BAJa0ws.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_2202_1"}, {"texts": ["A girl wearing a blue check top and shorts is riding on a brown horse on the green grass surface."], "durations": null, "exact_frames_per_prompt": [80], "background": "In the background, there is green grass, trees, wooden fencing, white sky and the voice of a woman is audible.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/-CQ7GWxuEUc.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_2208_0"}, {"texts": ["A brown horse is walking while carrying a girl on his back on a green grass surface."], "durations": null, "exact_frames_per_prompt": [80], "background": "In the background, there is green grass, trees, wooden fencing, white sky and the voice of a woman is audible.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/-CQ7GWxuEUc.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_2208_1"}, {"texts": ["A girl whose upper half-body is visible wearing a dark blue t-shirt is standing on the left side, holding a milk bottle and pouring milk into the food mixer machine from the bottle.", "The girl is keeping the bottle on the countertop.", "The girl is smiling."], "durations": null, "exact_frames_per_prompt": [62, 10, 8], "background": "In the background, a person is speaking, there is the sound of a food mixer machine, a food mixer machine on the counter-top, a white counter-top with white drawers, an electric gas stove, a white wire; a brown tile wall, a glass bottle, a brown box with wooden spatulas, a knife, and other stuff on the counter-top, and a milk bottle.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/4QBzHC4eUEk.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_2209_0"}, {"texts": ["A person wearing a graphic blue t-shirt is folding a black t-shirt on the bed."], "durations": null, "exact_frames_per_prompt": [78], "background": "In the background, there is a white bed, cream surface, brown surface and some miscellaneous sounds are audible.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/0p_xMShcYuY.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_220_0"}, {"texts": ["A baby wearing white clothes is sitting, laughing.", "The baby is eating something."], "durations": null, "exact_frames_per_prompt": [44, 36], "background": "In the background, there are stairs, a white wall, and the voice of a person speaking is audible.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/0AaQpoTDKmI.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_2216_0"}, {"texts": ["A person whose upper half-body is visible, wearing a blue t-shirt and is folding a black t-shirt on the bed with their hands."], "durations": null, "exact_frames_per_prompt": [80], "background": "In the background, there is a miscellaneous sound, a black t-shirt on the bed, a bed covered with a white bed sheet, and a wooden tile floor.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/0p_xMShcYuY.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_221_0"}, {"texts": ["A woman wearing a brown top is folding a yellow napkin on the white table."], "durations": null, "exact_frames_per_prompt": [80], "background": "In the background, there is a blue frame, a yellow napkin, a white table, music is playing and a woman's voice is audible.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/0N9X4WX7x5E.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_2220_0"}, {"texts": ["A person wearing blue jeans is sitting and pressing the black white bird while another person wearing black pants is sitting at the back."], "durations": null, "exact_frames_per_prompt": [80], "background": "In the background, a person is speaking, a baby's voice is audible. There is a white floor, a black surface and a sofa", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/0NxxH7g8O5s.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_2222_0"}, {"texts": ["A black white bird is sitting in the first person's lap while a person wearing white shoes is standing on the white surface."], "durations": null, "exact_frames_per_prompt": [80], "background": "In the background, a person is speaking, a baby's voice is audible. There is a white floor, a black surface and a sofa", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/0NxxH7g8O5s.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_2222_2"}, {"texts": ["A bending person wearing a t-shirt is shearing a brown sheep with a sheep shearing scissor."], "durations": null, "exact_frames_per_prompt": [80], "background": "In the background there is a brown canvas, wooden fence, a red wooden panel wall, a shearing scissor and a yellow grass surface.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/0zFbaHxfruk.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_2223_0"}, {"texts": ["A brown sheep is lying on a brown canvas.\n while a man wearing a white and black t-shirt is shearing the wool from the sheep."], "durations": null, "exact_frames_per_prompt": [80], "background": "In the background there is a brown canvas, wooden fence, a red wooden panel wall, a shearing scissor and a yellow grass surface.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/0zFbaHxfruk.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_2223_2"}, {"texts": ["A child wearing a purple cloth is sitting on the chair, her one hand on the white table, and eating cake with her fingers."], "durations": null, "exact_frames_per_prompt": [66], "background": "In the background, there is a white table, a cake, a chair, brown surface, a wall, some objects, an off white table, red-blue toys, a red ball, people are speaking and some miscellaneous sounds are audible.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/27_oTba7mik.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_2224_0"}, {"texts": ["A man wearing a yellow outfit is sitting.", "The man is drinking from the glass and looking here and there."], "durations": null, "exact_frames_per_prompt": [34, 46], "background": "In the background, there are wooden gate, a brown wall, brown floor, a red mat, wooden stools, pillows, a jar, a kettle, yellow plate, glasses, people are speaking and music is playing.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/1QbngReum84.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_2225_1"}, {"texts": ["A woman wearing a pink-black top is wrapping a bandage on the hand of the person.", "The woman wearing a pink-black top is unwrapping it and the person moves its arm."], "durations": null, "exact_frames_per_prompt": [45, 35], "background": "In the background, a woman is audible. There is a white bandage, a room radiator, a green wall, and a white-gray tile floor.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/0tIzlcEDrFw.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_2227_0"}, {"texts": ["A person wearing a white coat is standing and getting a bandage wrapped by the woman wearing a white coat on the left side.", "The person's bandage is unwrapped by the woman."], "durations": null, "exact_frames_per_prompt": [46, 34], "background": "In the background, a woman is audible. There is a white bandage, a room radiator, a green wall, and a white-gray tile floor.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/0tIzlcEDrFw.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_2227_1"}, {"texts": ["A lady wearing pink clothes is trimming the fur of white bichon frise.", "The lady is wearing a pink dress to white bichon frise."], "durations": null, "exact_frames_per_prompt": [59, 21], "background": "In the background there is grey surface, white curtain, white roof, blue object, lights, table, and a music playing in background.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/0tLEOKc-gto.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_2234_0"}, {"texts": ["A white bichon frise is sitting on a table and the lady is trimming his fur and wearing a pink dress."], "durations": null, "exact_frames_per_prompt": [80], "background": "In the background there is grey surface, white curtain, white roof, blue object, lights, table, and a music playing in background.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/0tLEOKc-gto.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_2234_1"}, {"texts": ["A woman wearing a pink outfit with a glove in her right hand, is standing.", "The woman is opening the packet."], "durations": null, "exact_frames_per_prompt": [49, 31], "background": "In the background, there is a brown marble floor, a glass bowl, a table fan, a paper roll, brown eggs, a black egg holding, a knife, a brown ironwood bull statue, white tissue paper, a plastic container and a woman speaking.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/3rzHClxEVqE.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_2239_0"}, {"texts": ["A person whom hands are visible is unfolding a paper."], "durations": null, "exact_frames_per_prompt": [80], "background": "In the background, there is a pistachio green surface, a paper with designs, the voice of the person and a miscellaneous sound is audible.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/-YnkBKzWZsA.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_223_0"}, {"texts": ["A man whose only hand is visible is adding coconut milk to the vegetables in a pan", "A man whose only hand is visible is mixing them with a spoon."], "durations": null, "exact_frames_per_prompt": [48, 32], "background": "In the background, there is a cooking pan, a milk can, a spoon, a cooking stove, and the voice of a man, and some music is audible.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/1wt9JVR9v1E.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_2241_0"}, {"texts": ["A man wearing a white t-shirt standing on the right is eating food in a food-eating competition."], "durations": null, "exact_frames_per_prompt": [80], "background": "in the background, the music and people are audible. There are trophies, water bottles, food buckets, and a blue poster with graphical writings.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/2-Tq5LvwJIc.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_2242_0"}, {"texts": ["A man wearing a blue-red jacket is standing and holding a fish in his hand while another man wearing a brown-green jacket is sitting while holding a fishing rod."], "durations": null, "exact_frames_per_prompt": [24], "background": "In the background, there is a fishing rod, a white container, a black tent, fan ice surface, a blue object, a sky, and the man speaking sound is audible.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/2EwWdFZmsQA.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_224_0"}, {"texts": ["A man wearing a camouflage jacket is sitting.\n while another man wearing a red-black jacket is picking a fish.", "The man is holding a fishing rod in his hand and the other man throws the fish away."], "durations": null, "exact_frames_per_prompt": [18, 16], "background": "In the background, there is a fishing rod, a white container, a black tent, fan ice surface, a blue object, a sky, and the man speaking sound is audible.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/2EwWdFZmsQA.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_224_1"}, {"texts": ["A man wearing a grey t-shirt and blue jeans is sitting on the brown horse and riding.", "The man wearing a grey t-shirt and blue jeans falls on the soil surface, gets up.", "The man wearing a grey t-shirt and blue jeans walks back towards the fence the brown horse also starts walking ."], "durations": null, "exact_frames_per_prompt": [22, 11, 46], "background": "In the background, there are green trees, soil surface, green grasses, a white-green pole, iron fencing, a man is speaking and laughing sounds are audible.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/06kWYuf1qrU.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_2252_0"}, {"texts": ["A brown horse is walking on the soil surface and starts jumping while carrying a man wearing blue jeans on its back, and another black-white horse is standing on the soil surface on the right.", "The brown horse walks towards the left side while the man stands up and walks towards the front."], "durations": null, "exact_frames_per_prompt": [26, 54], "background": "In the background, there are green trees, soil surface, green grasses, a white-green pole, iron fencing, a man is speaking and laughing sounds are audible.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/06kWYuf1qrU.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_2252_1"}, {"texts": ["A boy wearing a black t-shirt is sitting on a red sofa while a boy wearing red t-shirt is sitting on a red sofa and eating noodles.", "The boy is eating noodles from a white plate with a fork while the boy wearing red t-shirt is drinking a black drinks from the glass."], "durations": null, "exact_frames_per_prompt": [14, 65], "background": "In the background, people are speaking, there is a white table, a red sofa, white plates with noodles, glasses with black liquid, a grey floor, a black wall, a white pillar, and a chair.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/-a4MRN79j9I.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_2254_0"}, {"texts": ["A boy wearing a red-blue t-shirt is sitting on the red sofa and eating noodles while a boy wearing a black t-shirt is sitting on the right side of the sofa and eating noodles, and other people are sitting in the back.", "The boy takes a glass from the table.", "The boy starts drinking."], "durations": null, "exact_frames_per_prompt": [39, 34, 7], "background": "In the background, people are speaking, there is a white table, a red sofa, white plates with noodles, glasses with black liquid, a grey floor, a black wall, a white pillar, and a chair.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/-a4MRN79j9I.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_2254_1"}, {"texts": ["A man wearing a black suit is standing and moving his hand while talking."], "durations": null, "exact_frames_per_prompt": [80], "background": "In the background, there is a big whiteboard with written words, a camera with a tripod stand, a blue cloth, a marker, a brown door, a white wall, a grey surface, and the people speaking sounds are audible.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/52kQfr25NXw.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_2255_0"}, {"texts": ["A man wearing a black cloth on the left side is standing and hugging the second man and keeping his head on the shoulder of the second man."], "durations": null, "exact_frames_per_prompt": [59], "background": "In the background, there is an off white wall, a girl's poster, music is playing, a crying voice, a man is speaking and tapping sounds are audible.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/1EzJU1Ks9NU.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_2258_0"}, {"texts": ["A man wearing a black cloth on the right side is standing and hugging the first man and tapping his back."], "durations": null, "exact_frames_per_prompt": [59], "background": "In the background, there is an off white wall, a girl's poster, music is playing, a crying voice, a man is speaking and tapping sounds are audible.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/1EzJU1Ks9NU.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_2258_1"}, {"texts": ["A man wearing a yellow-orange t-shirt is sitting on a grey chair and speaking on the microphone."], "durations": null, "exact_frames_per_prompt": [34], "background": "In the background, men are speaking, there are white walls, a brown door, grey chairs, a wooden counter, and a logo on the screen.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/5fdoTliqIoM.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_2260_0"}, {"texts": ["A man wearing a grey jacket is sitting behind a wooden counter while another man wearing a black suit is walking towards the first man, holding a paper.", "The man starts looking at a file shown by the other man while a group of people are sitting."], "durations": null, "exact_frames_per_prompt": [13, 31], "background": "In the background, men are speaking, there are white walls, a brown door, grey chairs, a wooden counter, and a logo on the screen.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/5fdoTliqIoM.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_2260_1"}, {"texts": ["A man wearing a grey shirt is sitting on a grey chair while a man wearing a black suit is walking towards him and another man wearing a black suit is sitting besides him."], "durations": null, "exact_frames_per_prompt": [45], "background": "In the background, men are speaking, there are white walls, a brown door, grey chairs, a wooden counter, and a logo on the screen.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/5fdoTliqIoM.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_2260_5"}, {"texts": ["A girl wearing a blue top is sitting and eating food."], "durations": null, "exact_frames_per_prompt": [80], "background": "In the background, there is a white bowl with some food, a water bottle, a brick wall, a fan, a white wall, and some other stuff, and the girl speaking and also a woman speaking and television playing sounds is audible.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/0K82C4cGttw.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_2262_0"}, {"texts": ["A woman in the ocean, wearing a black swimming suit with an oxygen cylinder on her back, holding a bottle and inserting a knife near the neck and a group of people wearing scuba diving gear are swimming in the ocean.", "The woman starts drinking from the bottle."], "durations": null, "exact_frames_per_prompt": [52, 28], "background": "In the background, there is blue water, bottles, a knife, stones, an oxygen cylinder, and a song is playing.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/5fHpBJKDecs.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_2269_0"}, {"texts": ["A man wearing a gray cloth is in the ocean holding a bottle.", "The man is drinking from it."], "durations": null, "exact_frames_per_prompt": [52, 28], "background": "In the background, there is blue water, bottles, a knife, stones, an oxygen cylinder, and a song is playing.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/5fHpBJKDecs.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_2269_1"}, {"texts": ["A person wearing red clothes is sitting and drawing on a white paper."], "durations": null, "exact_frames_per_prompt": [74], "background": "In the background, people are speaking, music is playing, and some miscellaneous sounds are audible. There is a white paper, a pen, a white wall, a printed surface, a black surface and some other objects.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/1N5e1bYo63g.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_226_0"}, {"texts": ["A person wearing a gray vest and blue shorts is jumping around.", "The person cracks and opens an egg.", "The person starts jumping again."], "durations": null, "exact_frames_per_prompt": [33, 36, 11], "background": "In the background, miscellaneous sounds are audible. There is a white bowl, a white egg, a brown surface, a black table, black chairs, a blue dustbin, white walls, blue water bottles, and a beige carpet floor.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/-1QHAHq6DnE.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_2271_0"}, {"texts": ["A person wearing a red t-shirt and blue jeans is sitting behind the group of kids and holding a bottle."], "durations": null, "exact_frames_per_prompt": [80], "background": "In the background, there is a green surface, watermelon and the voices of the people are audible.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/-6ABBKUFtOY.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_2272_1"}, {"texts": ["A boy wearing a black-white-red striped t-shirt is standing on the carpet and counting money."], "durations": null, "exact_frames_per_prompt": [32], "background": "In the background, there is the sound of a baby, there is a maroon-black-white design carpet.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/0ZPwjqS38CA.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_2274_0"}, {"texts": ["A man wearing a yellow t-shirt is sitting on the right side of the person and holding a bunch of notes and arranging them properly."], "durations": null, "exact_frames_per_prompt": [80], "background": "In the background, there is a tiled wall, tables, and notes.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/2jVmiP1x1G8.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_2275_1"}, {"texts": ["A woman wearing a black-white top is holding a packet of popcorn and eating from the packet."], "durations": null, "exact_frames_per_prompt": [80], "background": "In the background, there are white bucks, shoes, make-up items, a popcorn packet, some objects, a woman is speaking and some miscellaneous sounds are audible.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/-RL4cP38DpU.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_2276_0"}, {"texts": ["A girl wearing a light blue jacket is standing and pouring hot water from the kettle onto the green cup."], "durations": null, "exact_frames_per_prompt": [80], "background": "In the background, there is a white kitchen counter-top a green cup, a tea bag, a sink, and a red basket, and the girl speaking sound is audible.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/37t1CfrPzJM.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_2277_0"}, {"texts": ["A woman wearing a green top is standing while making an omelet.", "The woman is holding the handle of the pan and cutting the omelet with the spoon."], "durations": null, "exact_frames_per_prompt": [61, 19], "background": "In the background, there is a black counter-top, a pan, a spoon, omelet, a stove, grey surface, a woman is speaking, sizzling and laughing sounds are audible.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/-IVX-ERJDqE.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_2279_0"}, {"texts": ["A person whose hands are visible is folding a piece of green cloth."], "durations": null, "exact_frames_per_prompt": [79], "background": "In the background, there is a green cloth, and the white surface.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/1gTLnnFf13o.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_2282_0"}, {"texts": ["A person whose hands are visible is folding a green cloth."], "durations": null, "exact_frames_per_prompt": [80], "background": "In the background, there is a green cloth and a white surface.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/1gTLnnFf13o.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_2283_0"}, {"texts": ["A person wearing a brown sweatshirt is standing.", "The person is folding a light green cloth."], "durations": null, "exact_frames_per_prompt": [44, 35], "background": "In the background a person is speaking. There is a white wall with black line, a black table, a green plate and a light green cloth.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/-ibEWRNi1Cc.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_2285_0"}, {"texts": ["A man wearing a gray t-shirt and black pants is standing on the left side, holding a drill machine with his left hand on the counter, and holding a vegetable peeler in his right hand and peeling a green apple attached to the drill machine with a vegetable peeler.", "The man is pulling out the apple from the drill machine."], "durations": null, "exact_frames_per_prompt": [64, 7], "background": "In the background, there is the sound of a drill machine, a white counter with a white sink, gray curtains hanging, a white window border, a drill machine, a glass utensil cover, and other stuff on the counter, a white wall, a steel water tap, bottles on the counter, and a black vegetable peeler.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/1-_nzW98f0M.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_2288_0"}, {"texts": ["A man whose hands are visible is holding a tiny wire.", "The man starts folding the wire."], "durations": null, "exact_frames_per_prompt": [59, 21], "background": "In the background, there is a dark brown surface, a plier, a small object, and a tiny wire, and the man speaking sound is audible.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/0u5R1JMjTeQ.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_228_0"}, {"texts": ["A boy wearing a yellow t-shirt is sitting and holding a transparent plastic bag."], "durations": null, "exact_frames_per_prompt": [12], "background": "In the background, there are plants, trees, apples, a white sky, grass surface, and the sound of music is audible.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/-3vKpQQMYbE.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_2292_0_ms"}, {"texts": ["A boy wearing a light blue t-shirt is standing and holding an apple in his hand."], "durations": null, "exact_frames_per_prompt": [34], "background": "In the background, there are plants, trees, apples, a white sky, grass surface, and the sound of music is audible.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/-3vKpQQMYbE.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_2292_1"}, {"texts": ["A woman wearing a black cloth and a watch is standing behind the ironing board and ironing a white cloth."], "durations": null, "exact_frames_per_prompt": [80], "background": "In the background, there is a brown surface, gray ironing board, white cupboards, a black object, a wooden countertop, a sink, a dispenser, a slab, green trees, a white cloth, and iron machine, voice of a woman and a miscellaneous sound is audible.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/1Kpq4cRd6mQ.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_2293_0"}, {"texts": ["A kid is standing next to the dog and eating ice-cream while other people are sitting on the bench."], "durations": null, "exact_frames_per_prompt": [80], "background": "In the background, people are speaking. There is a gray surface, red and black benches and chairs.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/-a8fUUZyEIA.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_2297_0"}, {"texts": ["A brown dog, next to the kid, is sitting on the gray surface while a kid wearing a brown jacket is touches the brown dog while eating an ice-cream."], "durations": null, "exact_frames_per_prompt": [80], "background": "In the background, people are speaking. There is a gray surface, red and black benches and chairs.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/-a8fUUZyEIA.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_2297_5"}, {"texts": ["A man wearing a grey t-shirt at first pours the whiskey in the glasses while a man wearing a striped shirt is sitting on the right side and looking at the whiky while speaking.", "The man closes the lid of the bottle, and put the bottle on the wooden surface while a man wearing a striped shirt is holding the glass of whisky.", "The man bangs his glass with the other man's."], "durations": null, "exact_frames_per_prompt": [36, 21, 23], "background": "In the background, there are glasses, bottles, and the wooden surface. There is the sound of male voices in the background.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/0NIzLI3uk-A.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_2298_0"}, {"texts": ["A man wearing a striped shirt is at first looking at the bottle of whiskey while another man wearing gray outfit is sitting on the left side of first man and he is first pouring whiskey in the glasses then puts the bottle on the table.", "The man picks up his glass.", "The man bangs it."], "durations": null, "exact_frames_per_prompt": [53, 16, 11], "background": "In the background, there are glasses, bottles, and the wooden surface. There is the sound of male voices in the background.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/0NIzLI3uk-A.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_2298_1"}, {"texts": ["A man wearing a black t-shirt and grey jeans is sitting with one knee on the soil surface, holding a black bottle and filling water in the other bottle.", "The man starts drinking water."], "durations": null, "exact_frames_per_prompt": [41, 38], "background": "In the background, there are green trees, a pond, green grass surface, a black bottle, a transparent bottle, a man is speaking, birds chirping sound, and some miscellaneous sounds are audible.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/26YQBBZEk-k.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_2303_0"}, {"texts": ["A woman wearing black clothes is standing and holding an iron and ironing a cloth.", "The woman is putting an iron."], "durations": null, "exact_frames_per_prompt": [26, 3], "background": "In the background, miscellaneous sounds are audible, there are white walls, an iron with wires, a blue table, a cloth, metal frames, hangers and a gray surface.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/-5GOspMRYUQ.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_2304_0"}, {"texts": ["A woman wearing a black shirt and black pants is standing on the gray surface, steaming the tie with the steam iron.", "The woman is fixing it with a suction function.", "The woman is pulling the tie and putting the white rod into the tie."], "durations": null, "exact_frames_per_prompt": [31, 18, 31], "background": "In the background, there is the sound of the steam iron, a blue ironing board, a steam iron with wires, a steam iron stand, a white wall, and a gray surface.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/-5GOspMRYUQ.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_2305_0"}, {"texts": ["A woman wearing a dark blue t-shirt is opening a mayonnaise jar.", "The woman picks up a butter knife."], "durations": null, "exact_frames_per_prompt": [38, 32], "background": "In the background there are green apple pieces, sausage pieces, a mayonnaise jar, a butter knife, bread slices, a polythene, a plastic packet, a red and grey counter-top, a plastic container with some objects and a woman speaking and some miscellaneous sound is audible.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/0O8gaQW6RBc.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_2308_0"}, {"texts": ["A man whose only upper half-body is visible wearing a white t-shirt is standing and pouring drinks into the glass while moving his body.", "The man is picking bottles, then begins juggling with a bottle."], "durations": null, "exact_frames_per_prompt": [59, 20], "background": "In the background, music is playing, there is the sound of cheering, cameras, a lightning counter, drinking bottles, lights, white boxes, a brown wall with a big screen, lightning posters, and a brown surface.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/2--cyRBwKig.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_230_1"}, {"texts": ["A man wearing a greyish-blue shirt is holding the ends of a red tie.", "The man is tying a tie."], "durations": null, "exact_frames_per_prompt": [23, 57], "background": "In the background, there is an object, and a red tie. There is the sound of a male voice in the background.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/1fQZ7pBjBRU.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_2312_0"}, {"texts": ["\\A woman whose upper body is visible wearing a black tank-top and specs, is sitting and speaking and is black-coating her eyebrows with the eyebrow brush.", "The woman is picking up the eyebrow kit in her hands and showing the eyebrow kit."], "durations": null, "exact_frames_per_prompt": [67, 13], "background": "In the background, a woman is speaking; there is a white ceiling, white walls, a black-white printed design poster wall in the front, and an eyebrow kit with a mirror.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/0JSUpFxrBCY.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_2313_0"}, {"texts": ["A man wearing white clothes is standing and holding someone cheats and talking with another man while a man wearing a striped t-shirt is standing on the right side and talking to the man wearing a red t-shirt and exchanging cheats, a man wearing a white t-shirt moves to the right side, and a group of people are sitting backwards."], "durations": null, "exact_frames_per_prompt": [80], "background": "In the background, people are speaking, there is a gray wall, lights, a podium with mic, mic stand with mic, white table and some stuff.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/0RnnCnghS84.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_2318_0"}, {"texts": ["A man wearing black clothes is standing and holding a mic while a man wearing a white t-shirt is speaking and standing behind the podium. Another man wearing a white t-shirt is standing on the right side along with another man in a red t-shirt and is also talking with the man in a red-black t-shirt who is coming from the left side.", "The man is talking with the first man a group of people are sitting and watching at the stage."], "durations": null, "exact_frames_per_prompt": [14, 66], "background": "In the background, people are speaking, there is a gray wall, lights, a podium with mic, mic stand with mic, white table and some stuff.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/0RnnCnghS84.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_2318_1"}, {"texts": ["A girl wearing a top and jeans is riding a mule in front of another mule and moving in a forward direction."], "durations": null, "exact_frames_per_prompt": [73], "background": "In the background, there is sky, mountains, trees, soil surface, road, small poles, electricity poles and wind blowing sound and voice of the women's are audible.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/21VI-zTTnHc.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_2320_0"}, {"texts": ["A black-brown mule is walking on the road while carrying a girl on his back and another mule is walking behind the first mule."], "durations": null, "exact_frames_per_prompt": [78], "background": "In the background, there is sky, mountains, trees, soil surface, road, small poles, electricity poles and wind blowing sound and voice of the women's are audible.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/21VI-zTTnHc.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_2320_1"}, {"texts": ["A black-brown mule is walking behind the first mule on the road while a girl is riding the mule and turning back."], "durations": null, "exact_frames_per_prompt": [80], "background": "In the background, there is sky, mountains, trees, soil surface, road, small poles, electricity poles and wind blowing sound and voice of the women's are audible.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/21VI-zTTnHc.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_2320_2"}, {"texts": ["A man wearing black-white clothes is sitting and watching the woman while the woman is wearing a pink blazer reading the news in front  of the camera."], "durations": null, "exact_frames_per_prompt": [80], "background": "In the background, the woman is speaking and music is playing, there are brown pillar, black table, some papers, white ceiling, white light, brown surface blue screen and a white object.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/35BMgXpxoZ4.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_2324_0"}, {"texts": ["A woman wearing pink clothes is sitting and speaking while the man in black suit looking at her and  turn his head into the normal position."], "durations": null, "exact_frames_per_prompt": [80], "background": "In the background, the woman is speaking and music is playing, there are brown pillar, black table, some papers, white ceiling, white light, brown surface blue screen and a white object.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/35BMgXpxoZ4.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_2324_1"}, {"texts": ["A woman wearing black coat is sitting and massaging her face."], "durations": null, "exact_frames_per_prompt": [80], "background": "In the background, there is a white wall, books, a table, glass racks with objects, and a woman's voice is audible.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/4OQ0M_3dUYA.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_2328_0"}, {"texts": ["A woman, whose hands are visible only, is holding the thread of a bracelet.", "The woman, whose hands are visible only, is tying a knot."], "durations": null, "exact_frames_per_prompt": [57, 23], "background": "In the background, there is a light brown surface, white surface, a bracelet and the voice of the woman is audible.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/0NN9loNl9PA.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_2331_0"}, {"texts": ["A man whose upper half-body is visible wearing a red t-shirt and a white apron is standing on the left side, picking out the cheese with his hands from the container.", "The man is dropping it on the pizza base."], "durations": null, "exact_frames_per_prompt": [29, 51], "background": "In the background, music is playing, a man is speaking, there is a steel counter with a cover, a steel container with cheese on the counter, a pizza base with red sauce, and a white wall.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/15eB6lgwnzs.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_2335_0"}, {"texts": ["A man wearing a light gray jacket, white shirt and spectacles is standing behind the lectern and speaking on the mic while looking down."], "durations": null, "exact_frames_per_prompt": [80], "background": "In the background, there is a brown lectern, white wall, a black object, a mic, a window, green trees, the voice of the man and some miscellaneous sounds are audible.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/0Es9zrEHL84.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_233_1"}, {"texts": ["A man whose only hands are visible is tying a thread into the fish hook."], "durations": null, "exact_frames_per_prompt": [28], "background": "In the background, there is a black surface, a fish hook, thread, a metal object, music is playing and a man's voice is audible.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/-NTVZAZ0xQA.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_2344_0"}, {"texts": ["A girl wearing a green top is sitting on the backside with her hand on the counter top while a person wearing blue clothes is standing on the right side and cutting a pineapple.", "The girl puts her head on her hand while the person wearing blue clothes takes the slice of pineapple.", "The girl starts looking at the pineapple while the person wearing blue clothes cuts the slice of pineapple."], "durations": null, "exact_frames_per_prompt": [15, 8, 17], "background": "In the background, people are speaking, the music is playing, there is a grey-black counter top, a wooden chopping board with a pineapple, a green poly-bag with pineapple peel, a white wall, a brown cupboard, a grey floor, a knife, a door, a white plate with green flower design, and a black chair.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/31Cv9o3MPng.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_2346_1"}, {"texts": ["A person whose hands are visible is holding a funnel and pouring something into the machine from a bottle.\n"], "durations": null, "exact_frames_per_prompt": [80], "background": "In the background, there is a black bottle, a black funnel, wires, a machine, a white radiator, a white wall, and a white floor.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/-157wqwwtAo.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_2347_0"}, {"texts": ["A man wearing a jacket is standing and speaking on the mic."], "durations": null, "exact_frames_per_prompt": [80], "background": "In the background, person one is speaking and miscellaneous sounds are audible. There is a podium, a black mic, white walls, a window and some miscellaneous objects.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/0Es9zrEHL84.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_234_0"}, {"texts": ["A man wearing a gray suit is sitting in a chair, holding papers, and speaking."], "durations": null, "exact_frames_per_prompt": [81], "background": "In the background, a man is audible. There are chairs, mics, papers, tables, a door, a photo frame, and a brown-white wall.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/4lxvVcMpka0.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_2353_0"}, {"texts": ["A man wearing a white hat is standing behind the podium and sprechstimme while holding a mike."], "durations": null, "exact_frames_per_prompt": [80], "background": "In the background, there is an off-white wall, a pillar, a podium, light fixtures, curtains, table, and the chairs.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/3U82HTyQZZI.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_2356_0"}, {"texts": ["A woman wearing a white shirt is standing on the left side. She at first throws the bed sheet towards the other woman and then starts spreading the bed sheet on the bed."], "durations": null, "exact_frames_per_prompt": [80], "background": "In the background, there is a bed, white bed sheet, a paper, and the floor.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/hzooUrk_3_Q.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_2358_0"}, {"texts": ["A woman wearing a white shirt is standing on the right side and spreading the bed sheet on the bed with the woman on the left side."], "durations": null, "exact_frames_per_prompt": [80], "background": "In the background, there is a bed, white bed sheet, a paper, and the floor.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/hzooUrk_3_Q.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_2358_1"}, {"texts": ["A girl on the right, standing on the stage is spreading the bed sheet on the bed while the other girl on the left is also spreading the bed sheet, and a group of people in which some people are screaming on them and the two people are standing on the backside and speaking on the mic."], "durations": null, "exact_frames_per_prompt": [80], "background": "In the background, people are speaking and yelling and music is playing. There is a multicolored background, a white bed sheet, a bed, a brown surface and a brown table.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/hzooUrk_3_Q.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_2359_0"}, {"texts": ["A girl on the left is also spreading the bed sheet on the bed with person one while a group of people stands behind the bed and some in front of the bed."], "durations": null, "exact_frames_per_prompt": [80], "background": "In the background, people are speaking and yelling and music is playing. There is a multicolored background, a white bed sheet, a bed, a brown surface and a brown table.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/hzooUrk_3_Q.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_2359_1"}, {"texts": ["A girl wearing a pink-gray top and purple shorts is sitting while holding doughnut.", "The girl wearing a pink-gray top and purple shorts is eating the doughnut and keeping the doughnut on the table."], "durations": null, "exact_frames_per_prompt": [23, 57], "background": "In the background, people are speaking; there are red walls, a red surface table, a brown paper bag on the table, a doughnut, a glass door with a poster, buildings are seeing through the glass door, a green grass surface, and a concrete road are also seen through the glass door, and a light brown tile floor.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/38qB3KzurOA.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_2362_0"}, {"texts": ["A girl wearing a red-grey dress is sitting on the bench, holding a donut and eating it."], "durations": null, "exact_frames_per_prompt": [80], "background": "In the background, there is a carry bag, a donut, a red table, a brown bench, tile flooring, red-brown wall, a door, a road, green grass surface, a poster with text, building, music is playing, people are speaking, and some miscellaneous sounds are audible.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/38qB3KzurOA.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_2363_0"}, {"texts": ["A woman wearing a gray-black cloth is standing, holding a bowl.", "The woman is pouring sauce over the cookies."], "durations": null, "exact_frames_per_prompt": [71, 9], "background": "In the background, a woman is speaking. There is a black counter-top, a white tray, a kettle, a knife holder, knives, some glasses, white wall, draws, cookies, a cloth, a bowl, and some other objects.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/-1MXpPymXFU.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_2365_0"}, {"texts": ["A person whose hands are only visible is mixing a black coffee in a glass mug with the help of a spoon."], "durations": null, "exact_frames_per_prompt": [67], "background": "In the background, there is a brown wooden surface, a glass mug filled with black coffee, a spoon, a small steel glass, a dark background, and the music playing sound is audible.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/0INHWJX9G0o.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_2366_0"}, {"texts": ["A kid wearing a t-shirt with written words is sitting on a grey baby high chair, putting hand on the green-blue bottle and drinking water from the green-blue bottle."], "durations": null, "exact_frames_per_prompt": [80], "background": "In the background, a woman is speaking, there is a grey baby high chair, a white-grey bed, a green-blue bottle, and a grey wall.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/162ro8sHvXM.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_236_0"}, {"texts": ["A girl wearing a blue t-shirt is standing.", "The girl wearing a blue t-shirt is playing golf on the grass surface."], "durations": null, "exact_frames_per_prompt": [17, 63], "background": "In the background, there are trees, a blue bucket with some stuff, a golf stick, a grass surface, a white sky, a soil surface, and a transmission tower.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/0b9xWsXdp0Q.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_2372_0"}, {"texts": ["A woman wearing a black blazer is sitting on a grey chair, speaking then moves her hand while a man wearing a black blazer is sitting next to the woman."], "durations": null, "exact_frames_per_prompt": [33], "background": "In the background, women are speaking, there is a wooden table, a brown wall with a blue banner, a grey chair, beehive boxes, a grey surface, wooden fencing, a green grass surface, a standing fan, and green trees with a light pole.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/50HF2b-570o.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_2375_0_ms"}, {"texts": ["A man wearing a black blazer is sitting next to the woman while a woman on the left side wearing a black blazer is sitting and speaking while moving her hand."], "durations": null, "exact_frames_per_prompt": [33], "background": "In the background, women are speaking, there is a wooden table, a brown wall with a blue banner, a grey chair, beehive boxes, a grey surface, wooden fencing, a green grass surface, a standing fan, and green trees with a light pole.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/50HF2b-570o.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_2375_1_ms"}, {"texts": ["A man wearing a green t-shirt is standing in the front and putting a hive frame in the beehive box while a man wearing a grey shirt is standing near the fence and looking at the bee hive."], "durations": null, "exact_frames_per_prompt": [46], "background": "In the background, women are speaking, there is a wooden table, a brown wall with a blue banner, a grey chair, beehive boxes, a grey surface, wooden fencing, a green grass surface, a standing fan, and green trees with a light pole.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/50HF2b-570o.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_2375_2_ms"}, {"texts": ["A man wearing a brown shirt is standing on the backside while another man wearing green and blue outfit, holding an artificial bee hive and he is bending forward and put the bee hive in between other bee hives."], "durations": null, "exact_frames_per_prompt": [38], "background": "In the background, women are speaking, there is a wooden table, a brown wall with a blue banner, a grey chair, beehive boxes, a grey surface, wooden fencing, a green grass surface, a standing fan, and green trees with a light pole.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/50HF2b-570o.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_2375_3_ms"}, {"texts": ["A person wearing green shirt whose head is visible is on the backside."], "durations": null, "exact_frames_per_prompt": [33], "background": "In the background, women are speaking, there is a wooden table, a brown wall with a blue banner, a grey chair, beehive boxes, a grey surface, wooden fencing, a green grass surface, a standing fan, and green trees with a light pole.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/50HF2b-570o.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_2375_5_ms"}, {"texts": ["A person wearing a white t-shirt is holding the red collar belts of the group of dogs and walking in a straight direction along with them."], "durations": null, "exact_frames_per_prompt": [32], "background": "In the background, there is a grassy surface, pavement, and the red collar belts.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/0hSgEonuekA.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_2376_0"}, {"texts": ["A man wearing a black suit is walking towards the mic.", "The man is holding a mic with his left hand and holding a trophy in his right hand.", "The man starts speaking on the mic."], "durations": null, "exact_frames_per_prompt": [13, 4, 63], "background": "In the background, a man is speaking, there is the sound of cheering, the sound of clapping hands, speakers on the stage, a white podium on the right side, trophies, a microphone with a stand, a black surface stage with stairs, and a big three dimensional wall with blue-green lights and white words.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/0_r87O-US2E.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_2378_1"}, {"texts": ["A woman wearing a white-black top and a black blazer is walking while holding a black diary behind the wooden table.", "The woman is sitting and putting that black diary on the wooden table."], "durations": null, "exact_frames_per_prompt": [30, 32], "background": "In the background, there is a wooden table, wooden wall, a white cloth, a white object, wooden door, a mic, a miscellaneous sound, voices of the man and the women are audible.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/53VSF1OS7Mo.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_2379_0"}, {"texts": ["A woman wearing a purple top and leggings is riding a horse in a forward direction on the soil surface."], "durations": null, "exact_frames_per_prompt": [77], "background": "In the background, there are trees, poles, green grass, a pipeline, obstacles, buildings, a hut, soil surface, white sky and clopping sound is audible.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/0d5Y3zRJJkw.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_237_0"}, {"texts": ["A light brown horse is walking on the soil surface with a woman sitting on its back."], "durations": null, "exact_frames_per_prompt": [77], "background": "In the background, there are trees, poles, green grass, a pipeline, obstacles, buildings, a hut, soil surface, white sky and clopping sound is audible.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/0d5Y3zRJJkw.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_237_1"}, {"texts": ["A man wearing a black suit is speaking.", "The man is walking.", "The man is moving his hands towards the glass screen."], "durations": null, "exact_frames_per_prompt": [31, 17, 32], "background": "In the background, there is a glass screen, the blue surface, a ceiling, and the voice of a person speaking is audible.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/0OgsfR4qDMs.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_2382_0"}, {"texts": ["A woman wearing a blue dress is standing and combing a dog's fur.", "The woman puts the sprayer aside after spraying on it.", "The woman is holding the dog's ear."], "durations": null, "exact_frames_per_prompt": [33, 28, 19], "background": "In the background, there is a comb, a sprayer, a grey wall, a green table, and the woman is speaking, and some miscellaneous sounds are audible.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/2H6koOMfDr0.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_2386_0"}, {"texts": ["A black dog is standing on a table and getting combed by the woman."], "durations": null, "exact_frames_per_prompt": [34], "background": "In the background, there is a comb, a sprayer, a grey wall, a green table, and the woman is speaking, and some miscellaneous sounds are audible.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/2H6koOMfDr0.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_2386_1"}, {"texts": ["A person wearing a blue t-shirt is standing, holding and combing a black dog with a comb."], "durations": null, "exact_frames_per_prompt": [31], "background": "In the background, there is a gray wall, a black spray bottle, a surface, and the voice of a person speaking, and some miscellaneous sounds are audible.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/2H6koOMfDr0.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_2387_0"}, {"texts": ["A black dog is standing and getting combed by a person."], "durations": null, "exact_frames_per_prompt": [80], "background": "In the background, there is a gray wall, a black spray bottle, a surface, and the voice of a person speaking, and some miscellaneous sounds are audible.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/2H6koOMfDr0.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_2387_1"}, {"texts": ["A woman is sitting, closing her eyes, and getting her eyebrows welded by a woman."], "durations": null, "exact_frames_per_prompt": [48], "background": "In the background, there is a white cloth, a pencil, a tile surface, a wall, and some miscellaneous sounds are audible.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/-gqrlbkjaaA.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_238_1"}, {"texts": ["A child whose hands and legs are visible is sitting while holding a green book with both hands.", "The child starts turning the pages of a book with his hands."], "durations": null, "exact_frames_per_prompt": [28, 9], "background": "In the background, a child is speaking; there is a green book with printed cover, and a black background.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/-grY2svgF3o.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_2390_0"}, {"texts": ["A man wearing a cap is standing and holding a snake in his left hand and around his neck while a man in a blue t-shirt is helping him, some people are standing and walking near him, and a group of people at the back are moving here and there."], "durations": null, "exact_frames_per_prompt": [80], "background": "In the background, There is a white sky, a net cage, a black pole, a yellow object and a yellow shed.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/2LAs3iohl6E.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_2393_0"}, {"texts": ["A man wearing a blue shirt is standing on the right side and giving the snake in the first man's hand while another man wearing white clothes is coming from the right and taking something.", "The man starts moving.  while a group of people are standing and moving at the back."], "durations": null, "exact_frames_per_prompt": [45, 35], "background": "In the background, There is a white sky, a net cage, a black pole, a yellow object and a yellow shed.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/2LAs3iohl6E.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_2393_1"}, {"texts": ["A man, wearing a white top, is walking behind the first man while a man wearing purple shirt and white cap holding a snake around his neck and hands", "The man stands while a man wearing blue shirt touches the snake and stood aside", "The man puts something in his pocket while the other man in blue t-shirt and black pant is standing and seeing the snake."], "durations": null, "exact_frames_per_prompt": [27, 22, 22], "background": "In the background, There is a white sky, a net cage, a black pole, a yellow object and a yellow shed.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/2LAs3iohl6E.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_2393_2"}, {"texts": ["A snake is laying around the first man's neck while a man wearing a blue shirt is trying to touch the snake and then turning to the left, a man wearing a white kurta is coming on the left, putting something in his pocket, and standing near the man holding a snake,\u00a0 a man wearing a blue shirt is standing right next to the first man, and a group of people are standing in the backside."], "durations": null, "exact_frames_per_prompt": [80], "background": "In the background, There is a white sky, a net cage, a black pole, a yellow object and a yellow shed.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/2LAs3iohl6E.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_2393_4"}, {"texts": ["A man wearing a striped shirt is sitting on a stool, near the boy in a blue t-shirt while the boy was hit by the tail of the cow.", "The man puts his hand in front of the cow.", "The man starts milking the cow.", "The boy comes near him for milking the cow."], "durations": null, "exact_frames_per_prompt": [16, 26, 17, 21], "background": "In the background, there is a cow shed, and a bucket.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/-tjARoxdBJw.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_2396_0"}, {"texts": ["A man whose only upper body is visible wearing a black shirt and a silver watch is holding a black bean sauce pouch in his right hand.", "The man is putting the black bean sauce in the pan and then holding the sauce pouch in his left hand.", "The man is holding a wooden spatula in his right hand and starts moving the wooden spatula in the pan."], "durations": null, "exact_frames_per_prompt": [29, 34, 17], "background": "In the background, a man is speaking, there is the sound of cooking, a black pan, a white gas stove, foods in the pan, a white wall, a white counter, a flat wooden spatula and yellow words on the bottom side.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/58g6Qx7alp0.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_2397_0"}, {"texts": ["A kid wearing a dark blue t-shirt is sitting on a white baby chair. He is playing with a blue spoon.", "The kid is being fed by a person."], "durations": null, "exact_frames_per_prompt": [22, 19], "background": "In the background, miscellaneous sounds are audible. There is a white baby chair, a red wall, a baby bottle, a yellow liquid, a white bowl, two blue spoons, shelves, miscellaneous objects and a brown pan handle.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/-XKwRCwbjvA.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_23_0"}, {"texts": ["A person whose only hand is visible is feeding the baby."], "durations": null, "exact_frames_per_prompt": [54], "background": "In the background, miscellaneous sounds are audible. There is a white baby chair, a red wall, a baby bottle, a yellow liquid, a white bowl, two blue spoons, shelves, miscellaneous objects and a brown pan handle.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/-XKwRCwbjvA.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_23_1"}, {"texts": ["A person whose hands are visible wearing a purple woolen cloth is standing while holding a transparent bag with apples on the left side and another person whose also hand is visible is showing an apple in his hand and then putting it into the bag."], "durations": null, "exact_frames_per_prompt": [80], "background": "In the background, people are speaking; there are apples, a transparent bag, some apple on the green grass surface, and a green grass surface.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/-lvzCHLAEtQ.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_2404_0"}, {"texts": ["A man wearing a white t-shirt first inserts the breaker bar in the studs.", "The man starts rotating the rim down to tighten the studs."], "durations": null, "exact_frames_per_prompt": [33, 46], "background": "In the background, there is a rim, and the breaker bar. There is the sound of male voice in the background.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/3loaf7qu6og.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_2408_0"}, {"texts": ["A man wearing black clothes is participating in a bed setting competition while a woman is also participating in a bed-setting competition, two men are standing at the back and watching the competition."], "durations": null, "exact_frames_per_prompt": [40], "background": "In the background, people are audible and music is playing. There is a carpet floor, two beds with white bedding, light brown walls, white ceiling, chairs, a black table, white clothes and pillows, AC duct and white lights.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/j3kn3UKNep4.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_2416_0"}, {"texts": ["A woman on the right is also participating in a bed setting competition while a man wearing black clothes is spreading the bed sheet on the bed, a man wearing a black-and-white striped t-shirt moves forward towards the woman, and other people are looking in the direction of the man and the woman."], "durations": null, "exact_frames_per_prompt": [80], "background": "In the background, people are audible and music is playing. There is a carpet floor, two beds with white bedding, light brown walls, white ceiling, chairs, a black table, white clothes and pillows, AC duct and white lights.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/j3kn3UKNep4.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_2416_1"}, {"texts": ["A boy wearing a blue t-shirt is standing and putting the cookies on the white paper with a spatula."], "durations": null, "exact_frames_per_prompt": [21], "background": "In the background, there is a blue plate with paper, orange walls, a white ceiling, and some other stuff, and the voice of a boy speaking is audible.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/0acHgv8A6U8.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_2419_0"}, {"texts": ["A baby wearing blue and black clothes is picking a red cloth.", "The baby putting it inside a washing machine."], "durations": null, "exact_frames_per_prompt": [27, 53], "background": "In the background there are laundry bags, a white washing machine, a yellow wall, a brown surface, some clothes and some miscellaneous sounds are audible.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/4esUZKQooEM.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_241_0"}, {"texts": ["A woman whose hands are visible is wearing a white full-sleeve t-shirt is spreading the white cream on the bread.", "The woman is putting the chocolate pieces on the white cream bread.", "The woman is putting the other bread on the chocolate.", "The woman is again putting the cream on the other bread, and then spreading the cream with the knife."], "durations": null, "exact_frames_per_prompt": [13, 10, 4, 53], "background": "In the background, music is playing. There are three glass jars with food materials on the left side, a white plate with chocolate powder, another white plate with brown bread, a transparent bowl with white cream, a knife, other stuff on the white slab on the right side, and a brown wooden surface counter.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/1U1ojuY5nPg.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_2420_0"}, {"texts": ["A woman wearing a pink t-shirt and a white cap is sitting on the back of the elephant and feeding a banana to the elephant."], "durations": null, "exact_frames_per_prompt": [20], "background": "In the background, there is a soil surface, green plants, green trees, sky, voices of the people, sound of a music, sound of a chicken rooster and a miscellaneous sound is audible.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/0ACji4353Go.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_2425_0"}, {"texts": ["A gray elephant is being fed a banana by a woman."], "durations": null, "exact_frames_per_prompt": [20], "background": "In the background, there is a soil surface, green plants, green trees, sky, voices of the people, sound of a music, sound of a chicken rooster and a miscellaneous sound is audible.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/0ACji4353Go.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_2425_1"}, {"texts": ["A girl wearing black pants is sitting on a bench, holding a yellow python around her neck and in her hands."], "durations": null, "exact_frames_per_prompt": [57], "background": "In the background, there is a yellow and black banner, some benches, a plant, surfing boards, wooden panels, a colourful surface and people's voices are audible.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/-JehxiVQQ_w.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_2431_0"}, {"texts": ["A yellow python snake is wrapping around a girl's body."], "durations": null, "exact_frames_per_prompt": [57], "background": "In the background, there is a yellow and black banner, some benches, a plant, surfing boards, wooden panels, a colourful surface and people's voices are audible.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/-JehxiVQQ_w.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_2431_1"}, {"texts": ["A man, whose hand is visible, wearing blue gloves, is touching the engine parts of the vehicle."], "durations": null, "exact_frames_per_prompt": [80], "background": "In the background, there is the engine of a vehicle, tyre, gray road, a green object, the voice of the man and some miscellaneous sounds are audible.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/1gkPZP0iHsU.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_2435_0"}, {"texts": ["A man whose hand is visible, is repairing the engine oil tank."], "durations": null, "exact_frames_per_prompt": [80], "background": "In the background, a man is speaking, there is a chassis of the car, and a green bucket.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/1gkPZP0iHsU.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_2436_0"}, {"texts": ["A man wearing spectacles is showing a metal part of a vehicle."], "durations": null, "exact_frames_per_prompt": [80], "background": "In the background, there is an oil drain pan, a grey surface, metal parts and structures, a man's voice and some birds chirping sound is audible.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/1gkPZP0iHsU.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_2437_1"}, {"texts": ["A man wearing a red t-shirt is standing and cleaning an artificial beehive while a man wearing a white t-shirt is moving left and right and picking something from the beehives."], "durations": null, "exact_frames_per_prompt": [80], "background": "In the background, a song is playing and people are speaking. There is a green grass surface, beehives wooden boxes, and a bee protective helmet.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/5o8kbzRUGX4.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_2444_0"}, {"texts": ["A man wearing a white t-shirt is moving an artificial beehive box with a man wearing a red shirt.", "The man is lifting something from the ground.", "The man leans forward."], "durations": null, "exact_frames_per_prompt": [43, 17, 20], "background": "In the background, a song is playing and people are speaking. There is a green grass surface, beehives wooden boxes, and a bee protective helmet.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/5o8kbzRUGX4.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_2444_1"}, {"texts": ["A man wearing a white shirt and gray trousers is shaking hands with another man.", "The man is walking and receiving an award.  "], "durations": null, "exact_frames_per_prompt": [18, 62], "background": "In the background, clapping, miscellaneous sounds, and people are audible. There is an award, a black-white wall poster, red walls, a carpeted floor, a camera, a table, lights, a red ceiling, and a chandelier.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/0PsBN0n4LLU.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_2445_0"}, {"texts": ["A woman wearing a printed red shirt is sitting inside a car and peeling a red apple with a knife."], "durations": null, "exact_frames_per_prompt": [65], "background": "In the background there is a car door, side view mirror, wind shield, power window switches, ac vent, a seat belt ,a knife, a black container, trees, a red apple and, music and some miscellaneous sound is audible.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/-tRfLelwyNU.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_2447_0"}, {"texts": ["A woman wearing a cherry colored top is standing in the kitchen and doing something while a girl in the red top watches her and makes hand gestures ."], "durations": null, "exact_frames_per_prompt": [39], "background": "In the background, a man is speaking and miscellaneous sounds are audible. There is a dark gray countertop, brown kitchen cabinets, a white oven, a white stove, a beater, a jar, a blue container, a measuring cup, a white wire and some miscellaneous objects.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/2nochCAHa3c.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_2448_0"}, {"texts": ["A girl wearing a red top is looking at the beater while a woman wearing a black t-shirt is standing on the left side and shaking something.", "The girl is eating something while a woman wearing a black t-shirt then walks away."], "durations": null, "exact_frames_per_prompt": [22, 58], "background": "In the background, a man is speaking and miscellaneous sounds are audible. There is a dark gray countertop, brown kitchen cabinets, a white oven, a white stove, a beater, a jar, a blue container, a measuring cup, a white wire and some miscellaneous objects.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/2nochCAHa3c.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_2448_1"}, {"texts": ["A woman wearing a gray t-shirt and black trousers is sitting on the floor and setting up the clothes and a brown cat is moving here and there."], "durations": null, "exact_frames_per_prompt": [80], "background": "In the background, a woman and miscellaneous sounds are audible. There are clothes, a white basket, a black sofa, a white plastic bag, and a beige carpet floor.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/-zjeY-7fXJw.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_2451_0"}, {"texts": ["A brown cat is walking here and there while a girl wearing a grey t-shirt is sitting on the brown surface and arranging the clothes ."], "durations": null, "exact_frames_per_prompt": [80], "background": "In the background, a woman and miscellaneous sounds are audible. There are clothes, a white basket, a black sofa, a white plastic bag, and a beige carpet floor.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/-zjeY-7fXJw.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_2451_1"}, {"texts": ["A man wearing a blue t-shirt is coming from the left side and looking in front while speaking then smiling and going back in the left again."], "durations": null, "exact_frames_per_prompt": [18], "background": "In the background, there is a gas pump, a red building, window, gray stairs, gray surface, stones, a fire exhaust cylinder, a board, green grass, the voices of the people and a miscellaneous sound is audible.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/3-lYrM9FzsA.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_2452_0"}, {"texts": ["A man wearing a blue t-shirt is standing and speaking."], "durations": null, "exact_frames_per_prompt": [18], "background": "In the background, there are windows, fuel dispensers, a gray surface, a board, a wall, stones, and the voice of a man speaking, and some miscellaneous sounds are audible.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/3-lYrM9FzsA.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_2453_0"}, {"texts": ["A man wearing a brown t-shirt is holding a baby and touching a plant."], "durations": null, "exact_frames_per_prompt": [80], "background": "In the background, a man and miscellaneous sounds are audible. There are green plants, yellow grasses, wood pieces, and a tree stem.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/-QXKPOgbPno.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_2455_0"}, {"texts": ["A woman wearing a grey printed t-shirt is standing, holding a bowl and mixing the salad with the spoon while other man wearing blue t-shirt is standing on the right."], "durations": null, "exact_frames_per_prompt": [80], "background": "In the background, there is a wall, brown cupboards, a window, a microwave oven, some objects on the counter-top, bottles, a salad bowl, capsicum in a white plate, some objects on the table, a woman is speaking and some miscellaneous sounds are audible.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/3MkJXn3YsF0.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_2461_0"}, {"texts": ["A man wearing a blue t-shirt is standing and touching the capsicum in the white plate as a woman wearing a grey floral top is mixing sprouts in a bowl with a spoon."], "durations": null, "exact_frames_per_prompt": [80], "background": "In the background, there is a wall, brown cupboards, a window, a microwave oven, some objects on the counter-top, bottles, a salad bowl, capsicum in a white plate, some objects on the table, a woman is speaking and some miscellaneous sounds are audible.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/3MkJXn3YsF0.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_2461_1"}, {"texts": ["A woman whose hand is visible is applying the corn flour to the meat slices"], "durations": null, "exact_frames_per_prompt": [80], "background": "In the background, there is a big glass bowl with yellow liquid, a slice of meat, a white plate, a glass container, a white surface, and the women speaking and slow music playing sound is audible.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/233Vj6YHJps.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_2463_0"}, {"texts": ["A woman wearing a sleeveless top is standing near the counter, holding a drink.", "The woman is speaking."], "durations": null, "exact_frames_per_prompt": [45, 35], "background": "In the background, people are speaking. There are brown chairs, brown tables, a black counter, glasses, papers, books, a computer screen and a dark background. There is also a logo in the video.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/2MXHBy-veBc.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_2468_0"}, {"texts": ["A woman wearing a brown top is standing on the left side holding a glass of drink.\n while a man in front of her is standing and using his phone."], "durations": null, "exact_frames_per_prompt": [41], "background": "In the background, people are speaking. There is a black counter, cards, papers, glasses, a blue object, chairs, tables and a brown wall.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/2MXHBy-veBc.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_2469_0"}, {"texts": ["A man wearing a white gray t-shirt is standing on the right side holding a glass of drink and using his mobile with his left hand while a woman wearing a brown dress is standing on the left side holding a wine glass, and a group of people are sitting."], "durations": null, "exact_frames_per_prompt": [62], "background": "In the background, people are speaking. There is a black counter, cards, papers, glasses, a blue object, chairs, tables and a brown wall.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/2MXHBy-veBc.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_2469_1"}, {"texts": ["A man wearing a grey suit is standing in front of the podium and speaking on the mic while other man in black suit stands beside him."], "durations": null, "exact_frames_per_prompt": [72], "background": "In the background, there are brown tables with papers, files, a book, a mic, chairs, a podium with a mic, papers, a black object, stairs, railings, carpet flooring, barrier rope, a light, glass walls, and the voices of people talking are audible.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/4NEFN0EOp5c.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_2475_1"}, {"texts": ["A woman wearing a light pink t-shirt and gray jeans is standing on the left side and speaking while holding the horse's tail in her hands.", "The woman is then tilting her body to pick up the spray bottle.", "The woman is standing while holding a spray bottle in her right hand and holding the horse's tail with her left hand"], "durations": null, "exact_frames_per_prompt": [30, 19, 31], "background": "In the background, a woman is speaking; there is a poster on the wall, a brown wall, a black object, and a wooden broom.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/10Dr44B7XYs.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_2480_0"}, {"texts": ["A brown horse whose only hip and tail are visible is standing on the right side while the woman explains the procedure."], "durations": null, "exact_frames_per_prompt": [80], "background": "In the background, a woman is speaking; there is a poster on the wall, a brown wall, a black object, and a wooden broom.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/10Dr44B7XYs.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_2480_1"}, {"texts": ["A man wearing a blue t-shirt is sitting and holding the leg of the horse on his thigh on the grey surface."], "durations": null, "exact_frames_per_prompt": [80], "background": "In the background, there is wooden door, grey wall, grey surface, hoof nipper, a red pipe, blue belts, a red stripe, cloth, a man's voice is audible.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/2z2bb1syxjU.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_2481_0"}, {"texts": ["A brown horse is standing and keeping its leg on the man's thigh while a man wearing blue t-shirt is holding a horse leg and speaking."], "durations": null, "exact_frames_per_prompt": [80], "background": "In the background, there is wooden door, grey wall, grey surface, hoof nipper, a red pipe, blue belts, a red stripe, cloth, a man's voice is audible.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/2z2bb1syxjU.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_2481_1"}, {"texts": ["A boy wearing a black jacket and gray jeans is standing and fishing with the fishing rod while rolling the fishing rod handle while the fish with the fishing rod moves its tail", "The boy is speaking while the fish waves its tail"], "durations": null, "exact_frames_per_prompt": [32, 33], "background": "In the background, a boy is speaking. There is the sound of air, dry trees, a blue sky, a hill with green grass, a black fishing rod, a gray water pond, a pile of brown soil, and a gray stone surface.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/0-MHr4N9ooI.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_2488_0"}, {"texts": ["A white fish is hanging with the fishing rod hook.\n and the boy wearing black jacket is holding the fishing rod"], "durations": null, "exact_frames_per_prompt": [81], "background": "In the background, a boy is speaking. There is the sound of air, dry trees, a blue sky, a hill with green grass, a black fishing rod, a gray water pond, a pile of brown soil, and a gray stone surface.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/0-MHr4N9ooI.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_2488_1"}, {"texts": ["A man on the backside wearing a maroon-white striped t-shirt is walking on the backside and a group of people in which some are sitting, some are standing, and one is walking to the left side."], "durations": null, "exact_frames_per_prompt": [14], "background": "In the background, people are speaking, there is a miscellaneous sound, the song is playing, there is a brown floor, chairs, tables with crockery, a sofa, lights, a brown ceiling, a brown pillar, brown walls, a picture frame, a grey-brown barrier, and a dark room with red lights.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/0f66UEeLLeM.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_2489_3_ms"}, {"texts": ["A white sheep is caught by a person and gets rimmed while a herd of sheep is standing on the right side and two people are standing and watching the man."], "durations": null, "exact_frames_per_prompt": [80], "background": "In the background, there is a red car, a brown tractor, white sacks, green trees, soil surface, green surface, wooden fence, sky, voices of the people, voices of the sheeps and some miscellaneous sounds are audible.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/0HOlEUzjESo.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_248_0"}, {"texts": ["A person wearing a gray t-shirt and blue pants is trimming the hairs of sheep while bending then a woman on the right is watching the person and a group of sheep is standing in the cage."], "durations": null, "exact_frames_per_prompt": [80], "background": "In the background, there is a red car, a brown tractor, white sacks, green trees, soil surface, green surface, wooden fence, sky, voices of the people, voices of the sheeps and some miscellaneous sounds are audible.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/0HOlEUzjESo.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_248_1"}, {"texts": ["A black-brown cat is sitting and getting caressed by a person."], "durations": null, "exact_frames_per_prompt": [61], "background": "In the background, there is a white object, a table, gray floor, some other stuff, and cat's voice and some miscellaneous sounds are audible.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/1mBYF0v9A-A.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_2493_0"}, {"texts": ["A person whose half body is visible is caressing the cat with his hand."], "durations": null, "exact_frames_per_prompt": [61], "background": "In the background, there is a white object, a table, gray floor, some other stuff, and cat's voice and some miscellaneous sounds are audible.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/1mBYF0v9A-A.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_2493_1"}, {"texts": ["A baby wearing a white diaper is sitting, holding a book, turning the pages and looking at it."], "durations": null, "exact_frames_per_prompt": [80], "background": "In the background, there is a black cushion, white couch, brown seat, a book, voices of the baby and the women are audible.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/1g11NL3V2-g.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_2497_0"}, {"texts": ["A baby wearing a yellow bib is at first spilling the food from his mouth.", "The person wipes the food from a spoon and insert it into the baby's mouth.", "The baby eats the food."], "durations": null, "exact_frames_per_prompt": [22, 29, 29], "background": "In the background, there is a pink toy, a yellow bib, and the baby bed.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/3W3bGkkFjnY.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_2498_0"}, {"texts": ["A person whose only a hand is visible is holding a spoon and wipes the food that is spilled out of the baby's mouth and then feeds the spilled food to the baby."], "durations": null, "exact_frames_per_prompt": [31], "background": "In the background, there is a pink toy, a yellow bib, and the baby bed.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/3W3bGkkFjnY.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_2498_1"}, {"texts": ["A baby boy wearing a blue t-shirt is tapping a spoon in a bowl.", "The baby is eating from another spoon."], "durations": null, "exact_frames_per_prompt": [26, 28], "background": "In the background there is a feeding bottle, a baby chair, spoons, a bowl, some racks, a pan handle, some containers, a yellow wall, books, a red object and some miscellaneous sounds are audible.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/-XKwRCwbjvA.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_24_0"}, {"texts": ["A person whose hand is visible holding a spoon is feeding a baby bird."], "durations": null, "exact_frames_per_prompt": [80], "background": "In the background, a person and miscellaneous sounds are audible. There is a white spoon, a silver utensil with dried yellow grass, green plants, a building structure, a gray bucket, a white pole, and a white plastic sheet.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/08DWvdZkfAw.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_2500_0"}, {"texts": ["A brown baby bird is eating food from the spoon while a person whose hand is visible is holding the spoon."], "durations": null, "exact_frames_per_prompt": [80], "background": "In the background, a person and miscellaneous sounds are audible. There is a white spoon, a silver utensil with dried yellow grass, green plants, a building structure, a gray bucket, a white pole, and a white plastic sheet.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/08DWvdZkfAw.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_2500_1"}, {"texts": ["A kid wearing blue clothes is standing on the soil surface and feeding a baby goat from a bottle while a white color puppy first comes towards the kid then moves towards the right side and a black and white dog moves towards the boy."], "durations": null, "exact_frames_per_prompt": [80], "background": "In the background, there is a yellow chaff, wooden wall, soil surface, a woman is speaking and laughing, and some miscellaneous voices are audible.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/-h0pXulZaPg.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_2506_0"}, {"texts": ["A brown baby goat is moving around the kid and drinking from the bottle while a boy wearing the blue jacket is holding the bottle and two other dogs comes around."], "durations": null, "exact_frames_per_prompt": [80], "background": "In the background, there is a yellow chaff, wooden wall, soil surface, a woman is speaking and laughing, and some miscellaneous voices are audible.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/-h0pXulZaPg.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_2506_1"}, {"texts": ["A white puppy is coming towards the kid as a brown dog is eating something from kid's hand.", "The white puppy walks away on the right side a black and white dog is also coming toward the kid."], "durations": null, "exact_frames_per_prompt": [13, 10], "background": "In the background, there is a yellow chaff, wooden wall, soil surface, a woman is speaking and laughing, and some miscellaneous voices are audible.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/-h0pXulZaPg.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_2506_2"}, {"texts": ["A man wearing a blue t-shirt is sitting on a brown sofa and holding a kid.", "The man is looking at the kid."], "durations": null, "exact_frames_per_prompt": [19, 61], "background": "In the background, people are speaking, the child is coughing, there is a brown sofa, white walls, an orange object, and a white table lamp.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/-oSKT0KuI3I.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_250_0"}, {"texts": ["A kid wearing a multicolored printed romper is sitting in the front of the man, being held by the man, laughing.", "The kid is lying on the man."], "durations": null, "exact_frames_per_prompt": [55, 25], "background": "In the background, people are speaking, the child is coughing, there is a brown sofa, white walls, an orange object, and a white table lamp.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/-oSKT0KuI3I.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_250_1"}, {"texts": ["A person whose hands are visible is holding a white box and moving.", "The person is shaking the box."], "durations": null, "exact_frames_per_prompt": [64, 16], "background": "In the background, there is a white box, and the iPad. There is the sound of male voice in the background.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/-0gO3FmnKD8.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_2514_0"}, {"texts": ["A person whose only hands are visible is pumping out some red liquid with a pump."], "durations": null, "exact_frames_per_prompt": [13], "background": "In the background, a person is speaking and the sound of the pump is audible. There is a pump, a gray background, a white container, a yellow container and a car engine.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/1eHzetBUA1A.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_2518_0_ms"}, {"texts": ["A person on the right is holding the pump pipe into a yellow container while a person wearing black clothes is on the left side and holds the opening of the yellow container and pump pipe."], "durations": null, "exact_frames_per_prompt": [66], "background": "In the background, a person is speaking and the sound of the pump is audible. There is a pump, a gray background, a white container, a yellow container and a car engine.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/1eHzetBUA1A.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_2518_2_ms"}, {"texts": ["A girl wearing a pink dress is standing, hitting the red spoon.", "The girl wearing a pink dress is putting her hand in the bowl."], "durations": null, "exact_frames_per_prompt": [47, 33], "background": "In the background, there is a white wall, a refrigerator, cooking tools, gas stove, a tray, drawer, a red spoon, metal tray, dough, a table, bowls, metal objects, music is playing, and girl's voices are audible.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/1XtdtLcNCt8.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_2521_0"}, {"texts": ["A man wearing a blue denim jacket is standing and cooking and stirring food with the help of a wooden spatula.", "The man serve into another white bowl."], "durations": null, "exact_frames_per_prompt": [61, 18], "background": "In the background, there are brown wooden shelves, a black gas stove, a microwave oven, glass bottles, a window, a grey tile wall, bowls, a sink, a water tap, a grey counter-top, a knife with a stand, and man speaking and slow music playing sound is audible.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/0M8-82fq7oY.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_2524_0"}, {"texts": ["A person wearing a red t-shirt is standing and making sushi using a bamboo mat on a black counter-top."], "durations": null, "exact_frames_per_prompt": [80], "background": "In the background there are brown cabinets, knives, a black bowl, a white bowl with spoon, a bamboo mat with sushi stuff, a black pan with rice and wooden spoon, a white chopping board, a black bowl with vegetables, a black counter-top, a printed cloth, a blue wall and a person's voice and some music sound is audible.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/2Fc1ehZsbiw.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_2526_0"}, {"texts": ["A woman on the left side wearing a black top is sitting on a bed while a woman wearing a blue top is sitting on the right side on a bed.", "The woman is drinking from the cup while a woman wearing a blue top is drinking from the cup."], "durations": null, "exact_frames_per_prompt": [12, 42], "background": "In the background, there is a white cupboard, red curtain, white wall, white roof, a bed, white bed sheet, boxes, cups, and women's speaking and some music is audible.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/2qxd5BDn0t8.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_2532_0"}, {"texts": ["A woman on the right side wearing a blue top is sitting while other woman is looking at her", "The woman is drinking from the cup while the other woman is drinking", "The woman is moving her hand in the front."], "durations": null, "exact_frames_per_prompt": [18, 28, 8], "background": "In the background, there is a white cupboard, red curtain, white wall, white roof, a bed, white bed sheet, boxes, cups, and women's speaking and some music is audible.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/2qxd5BDn0t8.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_2532_1"}, {"texts": ["A man wearing a light-grey t-shirt is sitting on a wooden chair, holding a glass and looking to the right side while some people are sitting, laughing, and talking."], "durations": null, "exact_frames_per_prompt": [80], "background": "In the background, people are speaking, a woman is laughing, there are chairs, a grey table with glasses, a black bottle, a white-black object, wooden fencing, a shed, green plants, and a sky.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/3WL0y2LA-q4.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_2533_0"}, {"texts": ["A man wearing a dark-grey t-shirt is sitting next to the first man, holding a glass then raises his hand while a woman wearing a black top is sitting on the left and is smiling, another man is sitting on the left of the woman."], "durations": null, "exact_frames_per_prompt": [80], "background": "In the background, people are speaking, a woman is laughing, there are chairs, a grey table with glasses, a black bottle, a white-black object, wooden fencing, a shed, green plants, and a sky.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/3WL0y2LA-q4.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_2533_1"}, {"texts": ["A woman wearing black top is riding a brown horse in the right direction while the other person moves forward."], "durations": null, "exact_frames_per_prompt": [80], "background": "In the background there is a grass and soil surface, wooden fencing, trees, grey sky and a miscellaneous sound is audible.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/1B_d3HxMGMs.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_2534_1"}, {"texts": ["A brown horse, taking a woman on its back moving in the right direction on a grass surface while a man wearing a cap is walking on the grass surface towards the back."], "durations": null, "exact_frames_per_prompt": [80], "background": "In the background there is a grass and soil surface, wooden fencing, trees, grey sky and a miscellaneous sound is audible.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/1B_d3HxMGMs.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_2534_2"}, {"texts": ["A girl on the right lying on the floor is holding a phone and is speaking and laughing while a girl is lying on the floor on the left side talking to the other girl and smiling."], "durations": null, "exact_frames_per_prompt": [80], "background": "In the background, there is an RGB filter effect and both the girls are speaking.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/79CsWzgZI58.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_2535_0"}, {"texts": ["A woman on the left also laying on the floor is looking at the phone and is speaking while another girl looked into her phone and laughed."], "durations": null, "exact_frames_per_prompt": [80], "background": "In the background, there is an RGB filter effect and both the girls are speaking.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/79CsWzgZI58.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_2535_1"}, {"texts": ["A man wearing a red jacket is standing while another man wearing a grey jacket is standing on the right side.", "The man starts the meat grinding machine with the help of a electric drill machine while the other man standing on the right is putting his hands over the meat grinding machine."], "durations": null, "exact_frames_per_prompt": [44, 36], "background": "In the background, there is a brown wooden table, a green cloth, a hanging flesh, a white light, a wooden ceiling, a meat grinding machine, a red meat, a poly bag, a brown wall, a white plastic bucket, and some stuff, and the electric drill machine and the man speaking sound are audible.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/67OWWENrUFg.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_2536_0"}, {"texts": ["A man wearing a gray jacket is standing.\n while another man wearing a red jacket is also standing and holding a silver colored object.", "The man starts pressing the meat into the grinder and also smoking a cigarette while the man wearing a red jacket starts moving the silver colored object."], "durations": null, "exact_frames_per_prompt": [40, 40], "background": "In the background, there is a brown wooden table, a green cloth, a hanging flesh, a white light, a wooden ceiling, a meat grinding machine, a red meat, a poly bag, a brown wall, a white plastic bucket, and some stuff, and the electric drill machine and the man speaking sound are audible.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/67OWWENrUFg.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_2536_1"}, {"texts": ["A person whose hand is visible wearing blue cloth is holding a grinding machine and grinding the chicken."], "durations": null, "exact_frames_per_prompt": [80], "background": "In the background, a person is speaking, there is a white grinding machine, fruits and vegetables, brown cupboards, white walls, and gray counter-top.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/0BxSSwxuqWg.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_2540_0"}, {"texts": ["A person whose hand is visible is tying a knot in the rope."], "durations": null, "exact_frames_per_prompt": [80], "background": "In the background, music is playing, there is a white wall, and a rope.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/-Y__PsM58V8.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_2541_0"}, {"texts": ["A girl wearing a pink dress is sitting and unwrapping a gift."], "durations": null, "exact_frames_per_prompt": [81], "background": "In the background, there is a decorated Christmas tree, gifts, a designer wall, white floor, a woman's and a girl's voices, and some music is audible.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/-ygjTvEdGJ0.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_2548_0"}, {"texts": ["A man is wearing white cloth is holding a golf stick.", "The man is hitting a golf ball with his golf stick."], "durations": null, "exact_frames_per_prompt": [54, 26], "background": "In the background, man is speaking and miscellaneous sounds are audible, there is a green surface, green trees, a house, mountains, a golf bag, golf stick, golf ball, and white sky.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/-YBrw6cDVEE.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_2549_0"}, {"texts": ["A man wearing a black suit is standing and moving his hands in sign language."], "durations": null, "exact_frames_per_prompt": [80], "background": "In the background, there is a grey tile floor, a brown table, a tripod stand with a camera, a window, a white structure, a white wall, a book, a lectern, a black safety grill, and the people speaking sound are audible.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/70szp6HioeU.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_2550_0"}, {"texts": ["A man wearing a black t-shirt is standing behind the first man.", "The man is moving his hand while speaking in sign language."], "durations": null, "exact_frames_per_prompt": [71, 8], "background": "In the background, there is a grey tile floor, a brown table, a tripod stand with a camera, a window, a white structure, a white wall, a book, a lectern, a black safety grill, and the people speaking sound are audible.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/70szp6HioeU.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_2550_1"}, {"texts": ["A person whose hands are visible is tearing the paper.", "The person is showing the pieces of paper in his hand"], "durations": null, "exact_frames_per_prompt": [53, 26], "background": "In the background, a miscellaneous sound is audible, a person is speaking. There is a white wall and a black surface.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/2HBKBssNyqM.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_2554_0"}, {"texts": ["A man wearing a white shirt is standing, he ties a blue bow tie, shows it, and starts talking."], "durations": null, "exact_frames_per_prompt": [63], "background": "In the background, a man is audible. There is a black curtain, a white textured ceiling, and a white wall.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/1HJI_Issf0w.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_2557_0"}, {"texts": ["A lady wearing a purple dress is standing and presenting the weather forecasting through a digital screen.\n"], "durations": null, "exact_frames_per_prompt": [65], "background": "In the background, there is a digital screen with some text, and showing a weather forecasting report, and a lady speaking voice is audible.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/-g25DqskpqU.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_2558_0"}, {"texts": ["A kid wearing a green t-shirt is feeding milk to a baby goat from a milk bottle while a person wearing a black top is assisting the kid."], "durations": null, "exact_frames_per_prompt": [80], "background": "In the background, people are speaking. There is a brown surface, brown straw, a brown pole and a milk bottle.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/0ETrTlZQ2uo.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_2559_0"}, {"texts": ["A person wearing blue jeans is also feeding milk to the baby goat with the kid."], "durations": null, "exact_frames_per_prompt": [80], "background": "In the background, people are speaking. There is a brown surface, brown straw, a brown pole and a milk bottle.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/0ETrTlZQ2uo.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_2559_1"}, {"texts": ["A baby goat is being fed milk by the kid and the person."], "durations": null, "exact_frames_per_prompt": [80], "background": "In the background, people are speaking. There is a brown surface, brown straw, a brown pole and a milk bottle.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/0ETrTlZQ2uo.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_2559_2"}, {"texts": ["A man wearing a grey t-shirt is speaking while moaning.", "The man is clapping his hands."], "durations": null, "exact_frames_per_prompt": [37, 28], "background": "In the background, there is a golf ground, trees, and a sky.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/0O3S2OqHDKE.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_255_0"}, {"texts": ["A man wearing blue clothes is wrapping the gauze on the legs of a person."], "durations": null, "exact_frames_per_prompt": [80], "background": "In the background, a person is speaking, there is a gray floor, a black surface, a black object, a green plant and a brown surface.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/0LpGDkiyvaQ.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_2561_0"}, {"texts": ["A man wearing a blue t-shirt is standing and leaning forward and applying and rubbing a medical tape on another man leg."], "durations": null, "exact_frames_per_prompt": [80], "background": "In the background, there is a grey surface, a block object, a white medical tape, a blue medical tape, a green object, and the man speaking sound is audible.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/0LpGDkiyvaQ.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_2562_0"}, {"texts": ["A person whose thumb is visible is holding a steel bowl.\n"], "durations": null, "exact_frames_per_prompt": [80], "background": "In the background, there is the sound of whisking, the music is playing, there is a steel bowl with eggs and a whisker.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/-NRNRutEk40.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_2569_0"}, {"texts": ["A boy wearing a black t-shirt and dark gray pants is standing and holding a golf stick.", "The boy is hitting a golf shot in a golf artificial game."], "durations": null, "exact_frames_per_prompt": [24, 56], "background": "In the background, there is a green surface, golf balls, golf artificial game and music is audible.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/1d7QuWpicJg.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_2570_0"}, {"texts": ["A man wearing a black full-sleeve t-shirt, brown pants, and white shoes is standing while holding a golf stick.", "The man is hitting the golf ball with a golf stick. ", "The man again hitting the ball with a golf stick on the backside, and then standing."], "durations": null, "exact_frames_per_prompt": [24, 33, 23], "background": "In the background, music is playing, there are white walls with a projector screen reflection; a golf stick; and a green surface.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/1d7QuWpicJg.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_2571_0"}, {"texts": ["A man wearing an olive green t-shirt is standing behind the table.", "The man is flipping the box.", "The man is opening the box."], "durations": null, "exact_frames_per_prompt": [27, 31, 22], "background": "In the background, there is a scissor, a metal scale, a brush, a box, a black table with some stuff, a white plastic buckets, a red object, and other stuff, a green surface with some stuff, a white bowl, a yellow-grey object, a brown surface and the man speaking sound is audible.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/1RU9q4lx3go.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_2572_0"}, {"texts": ["A man whose legs and hands are visible, wearing black shoes and a black watch, is standing on the left side, holding a wooden cover and opening the wooden box."], "durations": null, "exact_frames_per_prompt": [14], "background": "In the background, a man is speaking; there is the sound of pumping, wooden boxes, green leaves, a cloth bag, and a soil surface.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/8TeayjSoz_E.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_2573_0"}, {"texts": ["A boy wearing a printed white t-shirt and green shorts is standing on the left side on the green grass surface and is holding a brown snake in his hands."], "durations": null, "exact_frames_per_prompt": [80], "background": "In the background, music is playing and people are speaking. There is a green grass surface.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/50BOKCH-Tz8.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_2579_0"}, {"texts": ["A brown snake is held in the boy's hand."], "durations": null, "exact_frames_per_prompt": [80], "background": "In the background, music is playing and people are speaking. There is a green grass surface.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/50BOKCH-Tz8.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_2579_4"}, {"texts": ["A dark green snake is visible while the boy is holding it."], "durations": null, "exact_frames_per_prompt": [80], "background": "In the background, music is playing and people are speaking. There is a green grass surface.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/50BOKCH-Tz8.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_2579_5"}, {"texts": ["A person whom hand is visible, holding books and opening it."], "durations": null, "exact_frames_per_prompt": [80], "background": "In the background, a person is speaking, there is a pink-white bed sheet, books, and some other objects.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/0gRu8EC_L9o.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_2582_0"}, {"texts": ["A girl wearing a black t-shirt is sitting on a bed and opening a bottle of champagne."], "durations": null, "exact_frames_per_prompt": [80], "background": "In the background, the man is speaking, the girl is speaking and laughing, there is a red-black bed, a pillow, a bag, a white wall, a grey curtain, and black objects.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/Q_ErmJZysmA.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_2583_0"}, {"texts": ["A woman wearing a black coat is standing just behind the podium."], "durations": null, "exact_frames_per_prompt": [80], "background": "In the background, there is a red banner, a big projector screen, a brown ceiling with light, a brown-white wall, chairs, wall white light, a table with some stuff, a brown surface, and the woman speaking and people speaking sound are audible.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/1Bt4ADs-kG4.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_2586_0"}, {"texts": ["A man wearing a black t-shirt is holding the leash of the camel and walking in front of it while a brown camel is moving towards the front side and a kid wearing white outfit is sitting on top of it and a woman wearing blue outfit and a cap is also sitting on the camel behind the kid."], "durations": null, "exact_frames_per_prompt": [34], "background": "In the background, there are green trees, hills, blue sky, poles, a shed, cars, grey surface, green grass surface, people are speaking, music is playing, and wind's blowing sounds are audible.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/-wHLbRfZPWM.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_258_0"}, {"texts": ["A girl wearing a white t-shirt is sitting on the back of the camel while a man is walking on the grass surface along with the camels in a right direction."], "durations": null, "exact_frames_per_prompt": [80], "background": "In the background, there are green trees, hills, blue sky, poles, a shed, cars, grey surface, green grass surface, people are speaking, music is playing, and wind's blowing sounds are audible.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/-wHLbRfZPWM.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_258_1"}, {"texts": ["A woman wearing a blue shirt is sitting on the back of the camel behind the girl while a man wearing black sweatshirt is walking and pulling the rope of the camel."], "durations": null, "exact_frames_per_prompt": [80], "background": "In the background, there are green trees, hills, blue sky, poles, a shed, cars, grey surface, green grass surface, people are speaking, music is playing, and wind's blowing sounds are audible.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/-wHLbRfZPWM.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_258_2"}, {"texts": ["A brown camel is walking from right to left on the green grass surface while other person directing the brown camel"], "durations": null, "exact_frames_per_prompt": [80], "background": "In the background, there are green trees, hills, blue sky, poles, a shed, cars, grey surface, green grass surface, people are speaking, music is playing, and wind's blowing sounds are audible.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/-wHLbRfZPWM.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_258_4"}, {"texts": ["A woman wearing a green top is standing and peeing the papaya with a knife."], "durations": null, "exact_frames_per_prompt": [81], "background": "In the background, there is a papaya, a green bowl with peel, a knife, bottles, a box, some objects, a white table, plates, peeling voices, and some miscellaneous voices are audible.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/4Hecs1_f5yc.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_2591_0"}, {"texts": ["A boy is standing in the river while another boy is walking out of the river.", "The boy is pulling an object from the river while another boy moves to the side.", "The boy stands near the shore on the left side and holding an object while another boy is also holding an object then lifts is leg."], "durations": null, "exact_frames_per_prompt": [14, 23, 43], "background": "In the background, there is a river, shore, rocks, cliffs and some miscellaneous sounds are audible.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/0zu0S6ttK3w.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_2594_0"}, {"texts": ["A boy is walking near the shore while the other boy is in the water.", "The boy is standing on the right side and holding an object with other boy."], "durations": null, "exact_frames_per_prompt": [37, 42], "background": "In the background, there is a river, shore, rocks, cliffs and some miscellaneous sounds are audible.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/0zu0S6ttK3w.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_2594_1"}, {"texts": ["A man wearing a black t-shirt is standing, holding a slice of the watermelon.", "While eating the watermelon, he breaks the slice into two parts."], "durations": null, "exact_frames_per_prompt": [41, 39], "background": "In the background, there is a pond, plants, grasses, soil surface, a white object, a pole, wires, and music is playing.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/-fkVMf-qLEI.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_2595_0_ms"}, {"texts": ["A white bird is flying above the pond."], "durations": null, "exact_frames_per_prompt": [51], "background": "In the background, there is a pond, plants, grasses, soil surface, a white object, a pole, wires, and music is playing.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/-fkVMf-qLEI.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_2595_1"}, {"texts": ["A woman on the left is handing over the black snake to the man on the right while two person are standing and watching the snake."], "durations": null, "exact_frames_per_prompt": [32], "background": "In the background, people are speaking. There is a green grass surface, some wooden pieces and green trees.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/-n0PW0qMNpY.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_2596_0"}, {"texts": ["A man on the right is taking the snake from the hands of person one while a woman wearing red t-shirt is coming from the left with a girl wearing a green top and looking towards the jungle."], "durations": null, "exact_frames_per_prompt": [32], "background": "In the background, people are speaking. There is a green grass surface, some wooden pieces and green trees.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/-n0PW0qMNpY.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_2596_1"}, {"texts": ["A black snake is being held by person one and is being handed over to person two while two girls are moving forward and watching the snake."], "durations": null, "exact_frames_per_prompt": [32], "background": "In the background, people are speaking. There is a green grass surface, some wooden pieces and green trees.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/-n0PW0qMNpY.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_2596_4"}, {"texts": ["A man wearing a green t-shirt with a print is standing on the right and taking the snake from the woman while in opposite of them the girls are watching."], "durations": null, "exact_frames_per_prompt": [32], "background": "In the background, people are speaking, there is a green grass surface, a christian cross, wooden panels, green shrubs, and green trees.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/-n0PW0qMNpY.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_2597_1"}, {"texts": ["A black snake is held by the woman and given in the arms of the man while two people approach them"], "durations": null, "exact_frames_per_prompt": [32], "background": "In the background, people are speaking, there is a green grass surface, a christian cross, wooden panels, green shrubs, and green trees.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/-n0PW0qMNpY.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_2597_4"}, {"texts": ["A girl wearing a white top is lying while facing the wall.", "The girl turns around turns around, holding a cushion and then lying straight on the bed."], "durations": null, "exact_frames_per_prompt": [34, 46], "background": "In the background, there are cushions, a bed, a white-blue lining bed sheet, a white wall, and the sound of laughing, and some miscellaneous sounds are audible.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/1A191XqZSQw.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_2599_0"}, {"texts": ["A woman wearing a black-white dress is sitting on the back of the mule and riding in the stable while the mule stops riding and turns back."], "durations": null, "exact_frames_per_prompt": [80], "background": "In the background, there is a soil surface, wooden fence, green trees, a blue barrel, a pile of straws, green surface, some green-dried bushes, blue sky, clouds, the sound of air breezing, the voice of the woman and a miscellaneous sound is audible.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/4hwQbVmw4jo.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_259_0"}, {"texts": ["A brown mule is walking in the stable while carrying a woman on his back."], "durations": null, "exact_frames_per_prompt": [80], "background": "In the background, there is a soil surface, wooden fence, green trees, a blue barrel, a pile of straws, green surface, some green-dried bushes, blue sky, clouds, the sound of air breezing, the voice of the woman and a miscellaneous sound is audible.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/4hwQbVmw4jo.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_259_1"}, {"texts": ["A girl wearing a peach colour t-shirt is in the bending position, holding the back leg of the horse and scraping the hoof of the horse with a hoof knife.", "The girl stands up and starts moving her hand while the horse is standing at the back."], "durations": null, "exact_frames_per_prompt": [37, 24], "background": "In the background, the girl is speaking, there is a soil surface, green-dry trees, a wooden stable, a rope, a blue slide, wooden structures, and a sky.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/34-UQmVjQ-Q.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_25_0"}, {"texts": ["A brown horse is standing on the soil surface and getting its back leg scraped by the girl."], "durations": null, "exact_frames_per_prompt": [61], "background": "In the background, the girl is speaking, there is a soil surface, green-dry trees, a wooden stable, a rope, a blue slide, wooden structures, and a sky.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/34-UQmVjQ-Q.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_25_1"}, {"texts": ["A man wearing a multi-color shirt is sitting, watching, and touching the back tire of a car."], "durations": null, "exact_frames_per_prompt": [45], "background": "In the background, there is a car, a gray surface, a red object, and the voice of a man speaking, and the sound of music is audible.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/-8Smxrww8a0.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_2604_0"}, {"texts": ["A woman whose fingers are visible is holding a carrot and moving the carrot towards the right side a kid is coming from the right side and smiling and taking the carrot."], "durations": null, "exact_frames_per_prompt": [58], "background": "In the background, the woman is speaking, there is a black-white floor with a white border, an electronic drum set, a television, a television unit, windows, brown walls, lights, some toys on the white border, and a wooden table.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/0qFPNlKijPs.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_2606_0"}, {"texts": ["A kid wearing a grey t-shirt is standing on the right side.", "The kid takes the carrot.", "The kid starts eating the carrot."], "durations": null, "exact_frames_per_prompt": [27, 29, 7], "background": "In the background, the woman is speaking, there is a black-white floor with a white border, an electronic drum set, a television, a television unit, windows, brown walls, lights, some toys on the white border, and a wooden table.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/0qFPNlKijPs.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_2606_1"}, {"texts": ["A girl in green clothing is standing on a muddy surface and folds her hands while speaking.", "The girl takes the food from the woman's hand and then drops it on the floor while a black colour goat ate the food"], "durations": null, "exact_frames_per_prompt": [39, 41], "background": "In the background, there is a muddy floor.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/-oCrtpjHIDs.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_2607_0"}, {"texts": ["A woman whose only a hand is visible is giving the food to the girl and a black goat eats the fallen food from a grey surface."], "durations": null, "exact_frames_per_prompt": [36], "background": "In the background, there is a muddy floor.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/-oCrtpjHIDs.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_2607_1"}, {"texts": ["A black goat is looking in the right side while the man beside it gives an object to the girl", "The black goat is eating the food on the floor and the girl gets scared and moves backward"], "durations": null, "exact_frames_per_prompt": [22, 19], "background": "In the background, there is a muddy floor.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/-oCrtpjHIDs.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_2607_3"}, {"texts": ["A woman wearing a green shirt is squeezing something into the bowl.", "The woman wearing a green shirt is mixing the food in the bowl.", "The woman wearing a green shirt is serving the dish into a green leaf."], "durations": null, "exact_frames_per_prompt": [34, 22, 24], "background": "In the background, the woman is speaking and music is playing. There is a white wall, silver utensils, shelves, a slicer, red spoons, silver counters, a silver bowl, tomatoes, some green vegetables, a brown table, a brown bowl, a brown dish, a plate and a green item.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/1M1yw4BBliE.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_2608_0"}, {"texts": ["A woman squeezing a lemon inside a bowl on some stuff.", "The woman starts mixing the stuff.", "The woman is putting that stuff on a cabbage leaf."], "durations": null, "exact_frames_per_prompt": [31, 21, 28], "background": "In the background there is a white tile wall, some kitchen utensils, a wooden chopping board, a blue plate, some bowls, metal counter-top, vegetables, a pink object, a woman's voice and some music sound is audible.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/1M1yw4BBliE.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_2609_0"}, {"texts": ["A dark brown elephant is walking on the gray surface with a group of people \nwho are sitting on its back."], "durations": null, "exact_frames_per_prompt": [80], "background": "In the background, there are canopies, lights, stairs, black sky, gray surface, flags, voices of the people and a miscellaneous sound is audible.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/-FmjJcrLs14.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_260_2"}, {"texts": ["A girl wearing a white top is standing, brushing her eyebrows with an eyebrow brush.", "The girl doing hand gestures."], "durations": null, "exact_frames_per_prompt": [34, 22], "background": "In the background, there is a white door, a yellow wall, electric switches, and the voice of a girl speaking is audible.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/-lzh38dpuQU.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_2610_0"}, {"texts": ["A man wearing a white shirt is standing and tying a pattern black tie."], "durations": null, "exact_frames_per_prompt": [80], "background": "In the background, there is a wooden shelf, a white wall, a black pattern tie, papers, gray floor, and the man is speaking and some miscellaneous sounds are audible.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/1ZTCs_44yz0.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_2611_0"}, {"texts": ["A boy wearing a brown-blue hoodie is standing on the grey surface and playing a doughnut on a string game as Another boy wearing a grey sweater is eating donut which tied on a string and then it falls down, then he bends for picks it up.", "The boy starts tossing the doughnut with his thigh."], "durations": null, "exact_frames_per_prompt": [72, 8], "background": "In the background, people are speaking and laughing, there is a grey surface, green grass surfaces, a brick house with a window, strings with donuts, a green dustbin, wooden fencing, and a dark background.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/35e4rrJkn4c.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_2615_0"}, {"texts": ["A boy wearing a grey t-shirt is standing on the grey surface and playing a doughnut on a string game while a boy wearing a grey-black t-shirt is eating doughnuts on a string .", "The boy starts walking on the grey surface while the doughnut falls in the boy's hand who is wearing grey-black t-shirt and he kicks the doughnut from his left knee."], "durations": null, "exact_frames_per_prompt": [56, 24], "background": "In the background, people are speaking and laughing, there is a grey surface, green grass surfaces, a brick house with a window, strings with donuts, a green dustbin, wooden fencing, and a dark background.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/35e4rrJkn4c.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_2615_1"}, {"texts": ["A man wearing a blue shirt is caressing the udder of the cow, then standing with one knee on the straw surface and holding the pipes of the milking machine."], "durations": null, "exact_frames_per_prompt": [80], "background": "In the background, the man is speaking, there is a straw surface with a white border, a bundle of straw, a milking machine with pipes, a metal rod, wired barrier, a grey tub, maroon pillars, and an orange pumpkin.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/-2LiVPczCr8.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_2616_0"}, {"texts": ["A black-white calf is standing behind the cow while a man wearing a blue shirt is sitting on his one knee and holding a pipe that is connected to the cow, and the man is speaking."], "durations": null, "exact_frames_per_prompt": [49], "background": "In the background, the man is speaking, there is a straw surface with a white border, a bundle of straw, a milking machine with pipes, a metal rod, wired barrier, a grey tub, maroon pillars, and an orange pumpkin.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/-2LiVPczCr8.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_2616_2"}, {"texts": ["A elephant is walking behind the baby elephant on a soil surface while a big elephant is walking in front of the baby elephant and carrying a group of people."], "durations": null, "exact_frames_per_prompt": [80], "background": "In the background, miscellaneous sounds are audible, There are trees, green grass surface, blue sky, white clouds and a soil surface.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/-_sZfHi_G7c.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_261_1"}, {"texts": ["A baby elephant is walking behind the first elephant on a soil surface while the three person is sitting on the back of the first elephant."], "durations": null, "exact_frames_per_prompt": [80], "background": "In the background, miscellaneous sounds are audible, There are trees, green grass surface, blue sky, white clouds and a soil surface.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/-_sZfHi_G7c.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_261_2"}, {"texts": ["A baby wearing a cream colored t-shirt is being held by a person and is laughing."], "durations": null, "exact_frames_per_prompt": [80], "background": "In the background, the baby is laughing and another person is also audible. There is a white wall, a blue bedding and a white object.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/1dDzG1Z0tUI.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_2620_0"}, {"texts": ["A baby wearing a white t-shirt is sitting, held by a person and moving his head and laughing."], "durations": null, "exact_frames_per_prompt": [80], "background": "In the background, there is a white wall, a cushion, a blue bed sheet, a wall clock and the babies laughing and a woman's voice is audible.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/1dDzG1Z0tUI.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_2621_0"}, {"texts": ["A boy wearing blue clothes is sitting on a red chair and eating food."], "durations": null, "exact_frames_per_prompt": [80], "background": "In the background there is a white wall, grey wall, grey dustbin, black surface, white refrigerator, mickey mouse print table, black tray, and a person laughing and speaking sound is audible in the background.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/2W0luVW5qek.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_2625_0"}, {"texts": ["A person whom hand is visible is pointing towards the food that is being cooked over the grill."], "durations": null, "exact_frames_per_prompt": [56], "background": "In the background, the person is speaking. There is a black grill, food, burned wood and gray surface.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/-CTvtfED6Ak.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_2627_0"}, {"texts": ["A boy wearing red clothes is standing and holding a cup and a bottle and pouring some powder in the cup."], "durations": null, "exact_frames_per_prompt": [80], "background": "In the background, boy is speaking, there is black counter-top, white wall, a cloth, a metal object, some bottles, a aluminum box and some other objects.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/196rNdL2Kqc.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_2631_0"}, {"texts": ["A man standing on the right side is holding a glass bottle in his hand while the other person ties the knot on the bottle"], "durations": null, "exact_frames_per_prompt": [80], "background": "In the background, there are kitchen cabinets, decorative items kept on the cabinet and the white fridge, an off-white ceiling, and off-white wall, a glass bottle, and the floor..", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/8GayAm3WYNM.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_2634_0"}, {"texts": ["A man on the left side is picking up a thread.", "The man is trying it on a glass bottle, held by the other man on the right side."], "durations": null, "exact_frames_per_prompt": [12, 35], "background": "In the background, there are kitchen cabinets, decorative items kept on the cabinet and the white fridge, an off-white ceiling, and off-white wall, a glass bottle, and the floor..", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/8GayAm3WYNM.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_2634_1"}, {"texts": ["A man wearing a patterned black shirt is standing and pouring a liquid inside a transparent glass."], "durations": null, "exact_frames_per_prompt": [80], "background": "In the background there are bottles, multi colored straws, a black container, brown bottle rack, a metal shaker glass, a brown countertop, black bar mats, a metal spring and a man's voice is audible.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/2ZH0n5EAJVY.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_2640_0"}, {"texts": ["A man wearing a white shirt is adjusting the tie knot.\n"], "durations": null, "exact_frames_per_prompt": [80], "background": "In the background, there is a white wall, a door, a light brown object, a blue tie, and a man speaking and slow music playing sound is audible.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/-a6SxoCScCg.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_2641_0"}, {"texts": ["A man wearing a black sweatshirt is at first sitting in a restaurant and holding a food and shouts and eats the food while a group of people sit around the tables and eating foods.", "The man is standing in a shopping complex and looking the girls and shouts while another girls shocked and look at the man."], "durations": null, "exact_frames_per_prompt": [65, 15], "background": "In the background, there is a shopping complex, racks, things are kept on the racks, a restaurant, glass windows, tables, chairs, and the trees.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/1Ih4HSzwGK4.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_2644_0"}, {"texts": ["A group of people is sitting in a restaurant and looking to the man shouting while a man wearing black outfits scream loudly and start eating something"], "durations": null, "exact_frames_per_prompt": [64], "background": "In the background, there is a shopping complex, racks, things are kept on the racks, a restaurant, glass windows, tables, chairs, and the trees.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/1Ih4HSzwGK4.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_2644_1_ms"}, {"texts": ["A group of people are looking here and there in the shopping complex and then they looks at the man shouting."], "durations": null, "exact_frames_per_prompt": [15], "background": "In the background, there is a shopping complex, racks, things are kept on the racks, a restaurant, glass windows, tables, chairs, and the trees.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/1Ih4HSzwGK4.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_2644_2_ms"}, {"texts": ["A man wearing a black hoodie and black trousers is sitting, he screams, and eats something.", "The man wearing a black hoodie and black trousers is standing in a store, holding something, and starts screaming."], "durations": null, "exact_frames_per_prompt": [64, 16], "background": "In the background, a man's scream and miscellaneous sounds are audible. There are tables, chairs, green trees, green shrubs, building structures, a gray floor, a ketchup bottle, a white sky, a white floor, store shelves, stuffed toys, a shopping cart, and a few miscellaneous items.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/1Ih4HSzwGK4.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_2645_0"}, {"texts": ["A man wearing a blue dress is standing and milking the udder of the cow and then smiling while looking in front."], "durations": null, "exact_frames_per_prompt": [80], "background": "In the background, there is a gray ramp, some metallic rods, metallic ceiling, white wall, gray surface, wires, the voices of the people and a miscellaneous sound is audible.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/-5d8Q_ca7bM.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_2646_0"}, {"texts": ["A whitish cow is standing on the gray ramp and letting the first man milking from her udders."], "durations": null, "exact_frames_per_prompt": [57], "background": "In the background, there is a gray ramp, some metallic rods, metallic ceiling, white wall, gray surface, wires, the voices of the people and a miscellaneous sound is audible.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/-5d8Q_ca7bM.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_2646_1"}, {"texts": ["A man wearing a blue hoodie is touching the udder of a cow.", "The man moves back while a man in white t-shirt started laughing."], "durations": null, "exact_frames_per_prompt": [60, 20], "background": "In the background, people are speaking and laughing, there is a grey shed with metal pipes, milking machines, a white building, and a grey floor.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/-5d8Q_ca7bM.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_2647_0"}, {"texts": ["A brown cow is standing on a grey surface, moving its leg and getting its udder touched by the first man while the man in the blue starts milking the cow"], "durations": null, "exact_frames_per_prompt": [58], "background": "In the background, people are speaking and laughing, there is a grey shed with metal pipes, milking machines, a white building, and a grey floor.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/-5d8Q_ca7bM.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_2647_2"}, {"texts": ["A man wearing a black t-shirt is eating an orange carrot with his hand."], "durations": null, "exact_frames_per_prompt": [80], "background": "In the background there is a white door, a carrot, a white wall, a white and black object and a person's chewing and speaking sound is audible.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/1t6qClHwXq0.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_2648_0"}, {"texts": ["A woman wearing a blue denim jacket is wearing a pearl chain.", "The woman wearing a blue denim jacket is making a knot, while sitting on a white sofa."], "durations": null, "exact_frames_per_prompt": [54, 26], "background": "In the background there is a white sofa chair, a white wall, a rug, a glass side table, a pearl chain, a red and white glass, a grey surface and a grey panel.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/1G-stgsuk0w.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_2649_0"}, {"texts": ["A person whom hand is visible holding food for the fishes."], "durations": null, "exact_frames_per_prompt": [42], "background": "In the background, miscellaneous sounds are audible. There is blue water and a sea coral reef.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/-jJvz1Ul23o.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_2650_1"}, {"texts": ["A white black cat is holded by a woman.", "The white black cat is caressing by her."], "durations": null, "exact_frames_per_prompt": [53, 28], "background": "In the background, people are speaking, there is a white-brown wall, a brown cupboard, a brown door, a table, chairs, brown surface and some other objects.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/3-P1OtsVgfs.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_2651_1"}, {"texts": ["A man wearing a light grey checked shirt is standing and doing fishing with the help of a fishing rod while a person wearing a blue shirt is standing on the grass surface."], "durations": null, "exact_frames_per_prompt": [80], "background": "In the background, there is a pond, green trees, buildings, sky, green grass, a fishing rod, and people speaking sound is audible.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/1IX1Sl456TI.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_2652_0"}, {"texts": ["A fish is fished by a man with a fishing rod and then she starts moving and a person wearing a white shirt is standing while recording a video on a green grass surface."], "durations": null, "exact_frames_per_prompt": [54], "background": "In the background, there is a pond, green trees, buildings, sky, green grass, a fishing rod, and people speaking sound is audible.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/1IX1Sl456TI.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_2652_2"}, {"texts": ["A man wearing a green t-shirt is at first feeding the white dog.", "The man is showing the food to the other dog.", "The man lifted the dog.", "The man put it on the other stool and a dog with white fur jumps to the other stool."], "durations": null, "exact_frames_per_prompt": [26, 15, 11, 28], "background": "In the background, there are grass, white stools, and floor.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/1U5necmvquE.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_2657_0"}, {"texts": ["A white dog is standing on the stool and being fed by the man in green t-shirt.", "The dog jumps on the other stool the man in green t-shirt is feeding and looking towards the table."], "durations": null, "exact_frames_per_prompt": [54, 15], "background": "In the background, there are grass, white stools, and floor.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/1U5necmvquE.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_2657_1"}, {"texts": ["A black and brown colored dog is sitting on the stool, it is at first watching the man feeding the white dog.", "The black and brown dog is lifted by the man and keeps it on the other stool."], "durations": null, "exact_frames_per_prompt": [38, 42], "background": "In the background, there are grass, white stools, and floor.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/1U5necmvquE.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_2657_2"}, {"texts": ["A man on the left side is holding fishes in his hands and swimming in the water while the other person is swimming aside."], "durations": null, "exact_frames_per_prompt": [81], "background": "In the background, there is reef, water, and some miscellaneous sounds are audible.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/0fF_tX7Nl9s.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_2661_1"}, {"texts": ["A girl wearing pink clothes is scuba diving in the water."], "durations": null, "exact_frames_per_prompt": [44], "background": "In the background, there is reef, water, and some miscellaneous sounds are audible.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/0fF_tX7Nl9s.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_2661_2"}, {"texts": ["A man wearing a greyish-blue shirt is standing on the road and speaking."], "durations": null, "exact_frames_per_prompt": [39], "background": "In the background, there are roads, a jeep, muddy surface, and the sky. There is the sound of a male voice in the background.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/094prW-Sesc.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_2665_0"}, {"texts": ["A woman wearing a grey top is holding a metal object.", "The woman wearing a grey top puts it on a tire."], "durations": null, "exact_frames_per_prompt": [50, 30], "background": "In the background, there are tires and metal object. There is the sound of a female voice in the background.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/0Ge_8vGZHmw.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_2667_0"}, {"texts": ["A woman wearing a grey t-shirt is standing and holding the steel tool in her hands", "The woman touches the tire surface."], "durations": null, "exact_frames_per_prompt": [48, 32], "background": "In the background, there is a big black tire, and a steel tool and the woman speaking sound is audible.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/0Ge_8vGZHmw.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_2668_0"}, {"texts": ["A man wearing a gray shirt is wiping the face of a horse."], "durations": null, "exact_frames_per_prompt": [80], "background": "In the background, a man and miscellaneous sounds are audible. There is a horse leash, a aqua cloth, a gray shed roof, a metal net, and a wooden wall.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/2luWYpn3wqI.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_266_0"}, {"texts": ["A gray-white horse is standing and getting its face wiped by a man."], "durations": null, "exact_frames_per_prompt": [80], "background": "In the background, a man and miscellaneous sounds are audible. There is a horse leash, a aqua cloth, a gray shed roof, a metal net, and a wooden wall.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/2luWYpn3wqI.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_266_1"}, {"texts": ["A man wearing a black tracksuit is walking on the grey road surface along with the dog.", "The man wearing a black tracksuit starts dancing."], "durations": null, "exact_frames_per_prompt": [15, 65], "background": "In the background, there is a parked white car, buildings, a light pole, a road light, a gray road surface with a white stripe, and dark background, and the song playing sound is audible.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/-9L5fpps3-M.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_2673_0"}, {"texts": ["A dog on the left side is tied with a leash and is walking ahead with the first man he started dancing and walking forward."], "durations": null, "exact_frames_per_prompt": [80], "background": "In the background, there is a parked white car, buildings, a light pole, a road light, a gray road surface with a white stripe, and dark background, and the song playing sound is audible.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/-9L5fpps3-M.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_2673_2"}, {"texts": ["A boy wearing a black printed t-shirt is holding a fry pan and tossing the food in the fry pan.", "The boy puts the fry pan on the stove."], "durations": null, "exact_frames_per_prompt": [44, 9], "background": "In the background, there are artificial flowers and a vase, a fry pan, stove, a platform, and the shelves.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/0S5GUT-AuxI.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_2675_0"}, {"texts": ["A person whose hands are visible is cooking a lima bean and steering with the help of a wooden spoon."], "durations": null, "exact_frames_per_prompt": [59], "background": "In the background, there is a big steel pan, and a wooden spoon, and the woman and also the man speaking sound are audible.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/4RS0g6Exnog.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_2679_0"}, {"texts": ["A man wearing gray cloth is standing and holding a horse and cleaning his eyes with a cloth."], "durations": null, "exact_frames_per_prompt": [80], "background": "In the background, the man is speaking, there is a gray wall, a net and a cloth.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/2luWYpn3wqI.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_267_0"}, {"texts": ["A black-white horse is standing and held by a man and is cleaned by the man."], "durations": null, "exact_frames_per_prompt": [80], "background": "In the background, the man is speaking, there is a gray wall, a net and a cloth.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/2luWYpn3wqI.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_267_1"}, {"texts": ["A woman wearing a black top is sitting and is speaking in a restaurant while one man is standing beside her and serving the food and the other is behind her holding a wine."], "durations": null, "exact_frames_per_prompt": [80], "background": "In the background, people are speaking and laughing. There is a restaurant, gray walls, a bar counter with some glasses and pitchers kept on it, tables and chairs, a white table, glasses of water, utensils and a white plate with pasta in it.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/-cfCZm3Qp6E.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_2683_0"}, {"texts": ["A man on the left wearing white clothes is serving food to the woman."], "durations": null, "exact_frames_per_prompt": [80], "background": "In the background, people are speaking and laughing. There is a restaurant, gray walls, a bar counter with some glasses and pitchers kept on it, tables and chairs, a white table, glasses of water, utensils and a white plate with pasta in it.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/-cfCZm3Qp6E.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_2683_1"}, {"texts": ["A woman wearing a brown jacket is sitting and pointing with her hand while the man wearing white dress is serving her food"], "durations": null, "exact_frames_per_prompt": [47], "background": "In the background there is a false ceiling, a brown pillar, a chair, some glasses, a flower, a brown wall, cutlery, a bowl with yellow dish, other dishes, a white table top and some people's voices are audible.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/-cfCZm3Qp6E.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_2684_0"}, {"texts": ["A girl wearing maroon cloth is sitting with a girl wearing grey jacket is wiping het mouth and is chewing food in the left", "The girl is eating food."], "durations": null, "exact_frames_per_prompt": [20, 60], "background": "In the background, people are speaking, there is white wall, white surface, refrigerators, black tables, black chairs, tissue roll, and doors.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/2Pchp3jrJs8.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_2685_0"}, {"texts": ["A girl wearing gray cloth is sitting and eating food beside the girl wearing red shirt and eating donut."], "durations": null, "exact_frames_per_prompt": [55], "background": "In the background, people are speaking, there is white wall, white surface, refrigerators, black tables, black chairs, tissue roll, and doors.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/2Pchp3jrJs8.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_2685_1"}, {"texts": ["A girl wearing a pink dress is sitting on the black chair on the right side of another girl, eating.", "The girl puts the food on the table.", "The girl stand up on the white floor."], "durations": null, "exact_frames_per_prompt": [49, 25, 6], "background": "In the background, there are black chairs, black tables, a water bottle, freezers, white floor, grey wall, a girl is speaking, and people are speaking.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/2Pchp3jrJs8.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_2686_0"}, {"texts": ["A woman whose only upper half-body is visible wearing a black tank-top with a dark gray blazer is sitting on the left side and is speaking while smiling."], "durations": null, "exact_frames_per_prompt": [30], "background": "In the background, music is playing, a woman is speaking; there is a three dimensional wall, a black surface counter, a television with news timings, a sky with gray clouds, hills, green trees, light poles, buildings, cars standing, a concrete road, and a gray water sea.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/25Khw8juaEM.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_2688_0_ms"}, {"texts": ["A group of boats are moving in the sea."], "durations": null, "exact_frames_per_prompt": [45], "background": "In the background, music is playing, a woman is speaking; there is a three dimensional wall, a black surface counter, a television with news timings, a sky with gray clouds, hills, green trees, light poles, buildings, cars standing, a concrete road, and a gray water sea.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/25Khw8juaEM.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_2688_2_ms"}, {"texts": [" A man wearing a black t-shirt is standing, taking out mac pro from the brown box.", "The man is opening the seal of the mac pro box."], "durations": null, "exact_frames_per_prompt": [38, 42], "background": "In the background, there is a white wall, desktop screens, a mac pro box, a brown box, a keyboard, a tablet, a laptop, a green can, a white desk, a man is speaking and music is playing.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/-AlzbNiAsPI.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_2693_0"}, {"texts": ["A man whose hand is only visible, holding a black marker, starts writing on white paper.", "The man whose hand is only visible then starts pointing at words written on the paper."], "durations": null, "exact_frames_per_prompt": [56, 24], "background": "In the background, there is a black market, white paper with written words and a man is speaking.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/3eoZJybaoT4.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_2698_0"}, {"texts": ["A man wearing a black hoodie is sitting and eating noodles."], "durations": null, "exact_frames_per_prompt": [45], "background": "In the background, there is a water jug, a glass of water, a white plate, a fork, noodles, a black table, white cupboards, a window with a white curtain, a white wall, and a few miscellaneous things in the back.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/0AisbPBEq-c.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_2699_0"}, {"texts": ["A man wearing brown pants is training a white and brown dog and the dog is following the man's command."], "durations": null, "exact_frames_per_prompt": [80], "background": "In the background, the man is speaking and a dog is barking. There is a cream floor, white walls and brown doors. There is also a white ceiling and white lights.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/474-RxD5H4U.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_269_0"}, {"texts": ["A white and brown dog is being trained by the man wearing brown pants."], "durations": null, "exact_frames_per_prompt": [80], "background": "In the background, the man is speaking and a dog is barking. There is a cream floor, white walls and brown doors. There is also a white ceiling and white lights.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/474-RxD5H4U.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_269_1"}, {"texts": ["A girl with the blond hair is lip-syncing while doing hand gestures while other girl wearing green top is doing the hand gestures."], "durations": null, "exact_frames_per_prompt": [81], "background": "In the background, there is a poster, a wooden table, and the yellow wall. A piece of music is playing in the background.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/6AAKbAWvfcs.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_2700_0"}, {"texts": ["A girl on the left side is doing lip-sync while doing the hand gestures while a  girl on the right side wearing a white-green printed top is doing lip-sync while doing the hand gestures."], "durations": null, "exact_frames_per_prompt": [81], "background": "In the background, there is a poster, a wooden table, and the yellow wall. A piece of music is playing in the background.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/6AAKbAWvfcs.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_2700_1"}, {"texts": ["A girl on the right side wearing a gray-green t-shirt is sitting and doing hand gestures while the other girl does the same hand gestures."], "durations": null, "exact_frames_per_prompt": [81], "background": "In the background, there is a white wall, a poster, some other stuff, and the sound of music is audible.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/6AAKbAWvfcs.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_2701_0"}, {"texts": ["A girl on the left side wearing a green t-shirt is also sitting and doing hand gestures as another girl on the right side wearing a green-grey t-shirt is also sitting and doing hand gestures."], "durations": null, "exact_frames_per_prompt": [81], "background": "In the background, there is a white wall, a poster, some other stuff, and the sound of music is audible.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/6AAKbAWvfcs.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_2701_1"}, {"texts": ["A man wearing a black printed t-shirt is holding a glass mug filled with a beverage and looking the mug.", "The man starts drinking."], "durations": null, "exact_frames_per_prompt": [61, 19], "background": "In the background, there is a glass mug, cans, white window blind curtain, and a wall.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/4acmBeZq2Fk.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_2707_0"}, {"texts": ["A man on the left, wearing a black suit is sitting and is speaking on the mic while the others are listening to him"], "durations": null, "exact_frames_per_prompt": [80], "background": "In the background, person one is speaking and person two is also audible. There is a blue table, a brown table, white papers, two laptops, black mica white wall with three blue squares on it, black chairs, a glass of water and a silver pole with a blue belt. There are also two logos in the video.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/5d2RlEKburo.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_2708_0"}, {"texts": ["A woman wearing an orange coat is sitting opposite to person one and she is about to speak when a man wearing a gray necktie and sitting on the left side finished speaking."], "durations": null, "exact_frames_per_prompt": [80], "background": "In the background, person one is speaking and person two is also audible. There is a blue table, a brown table, white papers, two laptops, black mica white wall with three blue squares on it, black chairs, a glass of water and a silver pole with a blue belt. There are also two logos in the video.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/5d2RlEKburo.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_2708_1"}, {"texts": ["A man wearing a grey t-shirt is standing on the left side as a boy wearing a red t-shirt is standing and looking at the potato.", "The man wearing a grey t-shirt putting a potato on the vegetable peeler machine.", "The man wearing a grey t-shirt steps back and the boy starts rotating the peeling machine."], "durations": null, "exact_frames_per_prompt": [14, 39, 27], "background": "In the background, the man is speaking, a woman is speaking, the boy is laughing, there are black counter tops with cabinets and drawers, a chopping board, potatoes, a tong, tissue papers, some bottles on the countertop, a sink with a tap, a microwave, wooden cupboards, a window, a light-yellow wall, and white lights.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/5PrVKZ5Di6Y.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_270_0"}, {"texts": ["A boy wearing a red t-shirt is standing on the right side, scratching his nose as a man wearing a grey t-shirt is standing and putting the potato into the vegetable peeler machine and then steps back.", "The boy starts rotating the vegetable peeler machine."], "durations": null, "exact_frames_per_prompt": [60, 20], "background": "In the background, the man is speaking, a woman is speaking, the boy is laughing, there are black counter tops with cabinets and drawers, a chopping board, potatoes, a tong, tissue papers, some bottles on the countertop, a sink with a tap, a microwave, wooden cupboards, a window, a light-yellow wall, and white lights.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/5PrVKZ5Di6Y.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_270_1"}, {"texts": ["A man is hanging a pot on a metal stand over a burning fire.", "The main is pulling a pan with a metal stick on the stand."], "durations": null, "exact_frames_per_prompt": [64, 16], "background": "In the background there is a metal stand, black pan and pots, some rocks, a football, hills, trees, a blue sky, vehicles and a person's voice and metal clunking sound is audible.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/295yJgCrLTA.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_2711_0"}, {"texts": ["A man wearing a black sleeveless jacket is in the front and eating French fry.", "The man wearing a black sleeveless jacket puts his hand in a paper packet as another man wearing a blue lining t-shirt is standing while keeping a paper packet.", "The man wearing a black sleeveless jacket takes a french fry and another man eating it."], "durations": null, "exact_frames_per_prompt": [21, 15, 12], "background": "In the background, people are speaking, there is a grey surface, grey barriers, standing trucks, standing cars, a shed, a hill, white walls, green trees, a green grass surface, and a blue sky.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/1Qz2zo--1lE.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_2714_0"}, {"texts": ["A man wearing a white shirt is standing and tying a red bow-tie."], "durations": null, "exact_frames_per_prompt": [80], "background": "In the background, there is a white curtain, a red bow-tie, and a man's voice is audible.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/9D8lgyiM9qk.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_2719_0"}, {"texts": ["A girl wearing a graphic black t-shirt is standing and eating the food and covering her mouth with her hand while a woman wearing purple t-shirt dancing and saying something"], "durations": null, "exact_frames_per_prompt": [80], "background": "In the background, there is a building, a white boundary, gray surface, green surface, green trees, the voices of the people and the sound of the music is audible.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/1Gq5O6AebNU.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_271_0"}, {"texts": ["A girl wearing a white-red striped t-shirt is standing on the right side of the third girl and eating the food while people are standing beside and behind them doing different things such as eating, recording and cheering for them."], "durations": null, "exact_frames_per_prompt": [21], "background": "In the background, there is a building, a white boundary, gray surface, green surface, green trees, the voices of the people and the sound of the music is audible.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/1Gq5O6AebNU.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_271_2"}, {"texts": ["A person whose hand is visible is rubbing the head and neck of a cat."], "durations": null, "exact_frames_per_prompt": [80], "background": "In the background, there are buildings, trees, street lights, the night sky, and the miscellaneous sounds are audible.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/11fx7kNit2M.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_2724_0"}, {"texts": ["A black and white cat is getting its head and neck rubbed by a person."], "durations": null, "exact_frames_per_prompt": [80], "background": "In the background, there are buildings, trees, street lights, the night sky, and the miscellaneous sounds are audible.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/11fx7kNit2M.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_2724_1"}, {"texts": ["A person wearing a white blue t-shirt is sitting and tapping on the bottom of the bottle as a man in a grey hoodie is bending and looking at the front and then stands up while clapping.", "The person is putting the bottle aside and then pointing him toward the back."], "durations": null, "exact_frames_per_prompt": [29, 44], "background": "In the background, people are screaming, speaking and clapping. There are houses, green grass surface, wooden boards, white brick wall, a grill, some objects and a tree.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/svXGPnnheC0.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_2726_0"}, {"texts": ["A man wearing a blue-green dress and black shoes is walking on the snow surface.", "The man wearing a blue-green dress and black shoes is sitting.", "The man wearing a blue-green dress and black shoes is opening the container."], "durations": null, "exact_frames_per_prompt": [25, 36, 8], "background": "In the background, there is a snow surface, dried bushes, dried trees, green trees, blue sky, water, some written words in the below, voices of the people and some miscellaneous sounds are audible.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/3GZqnw9MDJk.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_2727_0"}, {"texts": ["A brown elephant is walking in the jungle while carrying people on its back."], "durations": null, "exact_frames_per_prompt": [80], "background": "In the background, people are speaking; there are green trees, a hill, the sky, and a green grass surface.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/0p82v6cSzOk.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_2728_2"}, {"texts": ["A girl wearing a white top is sitting on a black chair while a man in white shirt holds a tattoo machine in hand.", "The girl is getting a tattoo on her shoulder while a man in white shirt  drawing a tattoo."], "durations": null, "exact_frames_per_prompt": [26, 48], "background": "In the background there is white and red surface, brown wall, lights, grey roof, white cupboard, some objects on white cupboard, fire extinguisher, white frame glass window, two girls are speaking with each other and tattoo machine sound is audible in the background.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/25JUEuCpFtg.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_2729_0"}, {"texts": ["A man wearing a white t-shirt is standing while the woman is sitting in the chair.", "The man is making a tattoo on a girl's shoulder."], "durations": null, "exact_frames_per_prompt": [32, 42], "background": "In the background there is white and red surface, brown wall, lights, grey roof, white cupboard, some objects on white cupboard, fire extinguisher, white frame glass window, two girls are speaking with each other and tattoo machine sound is audible in the background.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/25JUEuCpFtg.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_2729_1"}, {"texts": ["A woman whose upper body is visible wearing a black tank top is sitting on a chair and is getting tattoo on her right hand by the second woman."], "durations": null, "exact_frames_per_prompt": [80], "background": "In the background, people are speaking, there is the sound of a tattoo machine, chairs, a white light, a white wall and a gray surface.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/2z4Yrwj6TsY.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_272_0"}, {"texts": ["A second woman whose upper body is visible wearing a black top is making a tattoo with the tattoo machine on the first woman's right hand."], "durations": null, "exact_frames_per_prompt": [73], "background": "In the background, people are speaking, there is the sound of a tattoo machine, chairs, a white light, a white wall and a gray surface.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/2z4Yrwj6TsY.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_272_1"}, {"texts": ["A man wearing a green-white check shirt and blue jeans is sitting on a dark blue seat and holding a cup of liquid while another person is holding his phone and recording and the others are sitting around him.", "The man wearing a green-white check shirt starts drinking and a person points at something on the table."], "durations": null, "exact_frames_per_prompt": [27, 53], "background": "In the background, there is a yellow wall, wooden border, white pillar, wooden chairs, wooden table, a black cloth, red-black surface, white plates, some wrappers, a glass of red liquid, a green bottle and the voices of the people are audible.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/0Q__osPqNtE.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_2730_0"}, {"texts": ["A person wearing a gray t-shirt and blue jeans is sitting on the right side of the man and looking at him the man wearing green and white shirt drinks a shot"], "durations": null, "exact_frames_per_prompt": [80], "background": "In the background, there is a yellow wall, wooden border, white pillar, wooden chairs, wooden table, a black cloth, red-black surface, white plates, some wrappers, a glass of red liquid, a green bottle and the voices of the people are audible.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/0Q__osPqNtE.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_2730_1"}, {"texts": ["A man wearing black clothes is standing and putting food on a plate.", "The man is putting sauce in it then a man and a woman walks to the side while a woman is sitting on a chair and talking."], "durations": null, "exact_frames_per_prompt": [45, 19], "background": "In the background, music is playing, people are speaking. There are tables, chairs, gray walls, photo frames, glasses, cups, plates, white tablecloths, cutlery and food.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/1mTzDYOIsvw.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_2737_0"}, {"texts": ["A woman wearing black top is sitting and speaking while another woman checked top is involved in that conversation"], "durations": null, "exact_frames_per_prompt": [16], "background": "In the background, music is playing, people are speaking. There are tables, chairs, gray walls, photo frames, glasses, cups, plates, white tablecloths, cutlery and food.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/1mTzDYOIsvw.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_2737_1"}, {"texts": ["A woman wearing a black design dress is sitting and speaking with other girl"], "durations": null, "exact_frames_per_prompt": [18], "background": "In the background, music is playing, people are speaking. There are tables, chairs, gray walls, photo frames, glasses, cups, plates, white tablecloths, cutlery and food.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/1mTzDYOIsvw.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_2737_2"}, {"texts": ["A man wearing a black suit is serving food on a plate."], "durations": null, "exact_frames_per_prompt": [63], "background": "In the background, there is food, tables, a bar, chairs, carpet flooring, grey walls, a white ceiling, white table clothes, big and small vases of flowers, an indoor plant, windows, a brown door, photo frames, glass doors, lights, a wooden trolley, cutlery, dishes and the voices of people talking is audible.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/1mTzDYOIsvw.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_2738_0"}, {"texts": ["A woman wearing black clothes sitting on the left side is talking to another woman."], "durations": null, "exact_frames_per_prompt": [23], "background": "In the background, there is food, tables, a bar, chairs, carpet flooring, grey walls, a white ceiling, white table clothes, big and small vases of flowers, an indoor plant, windows, a brown door, photo frames, glass doors, lights, a wooden trolley, cutlery, dishes and the voices of people talking is audible.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/1mTzDYOIsvw.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_2738_3"}, {"texts": ["A woman wearing a printed top sitting on the right side is talking to the first woman while a woman wearing black top sitting on the left side looking the opposite woman"], "durations": null, "exact_frames_per_prompt": [17], "background": "In the background, there is food, tables, a bar, chairs, carpet flooring, grey walls, a white ceiling, white table clothes, big and small vases of flowers, an indoor plant, windows, a brown door, photo frames, glass doors, lights, a wooden trolley, cutlery, dishes and the voices of people talking is audible.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/1mTzDYOIsvw.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_2738_4"}, {"texts": ["A boy wearing a black t-shirt is sitting.", "The boy is eating a burger.", "The boy is eating fries."], "durations": null, "exact_frames_per_prompt": [37, 41, 3], "background": "In the background, the sound of chewing is audible. There is a brown partition wall, a white table, a white box, a burger, fries and a soft drink can.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/00MUo_0F9-Q.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_2740_0"}, {"texts": ["A person whose only a hand is visible, is at first scratching the gap of a tire with his finger and then scratching it with a knife."], "durations": null, "exact_frames_per_prompt": [43], "background": "In the background, there is a scooter, muddy surface, and the grassy surface. There is the sound of human voices in the background.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/3qral2vBh8o.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_2741_1"}, {"texts": ["A man wearing a grey-white-black t-shirt is eating cake.", "The man wearing a grey-white-black t-shirt starts licking his fingers."], "durations": null, "exact_frames_per_prompt": [55, 25], "background": "In the background, there is the sound of television, the man is speaking, a woman is speaking, there is a white wall, a brick wall, a wooden table, light-brown curtains, a white-brown chairs wooden door, and a window.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/68YgZyPHM3E.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_274_0"}, {"texts": ["A man wearing a graphic gray t-shirt and pants is sitting while holding some coins, taking out one coin.", "The man starts counting the coins on the table."], "durations": null, "exact_frames_per_prompt": [28, 52], "background": "In the background, there is a wooden table, a black cellphone, coins, a white container, a window, the voices of the people and a miscellaneous sound is audible.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/2VKn-EBqwe0.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_2759_0"}, {"texts": ["A boy wearing a red t-shirt is eating a white food item from a hanging thread."], "durations": null, "exact_frames_per_prompt": [80], "background": "In the background there is a yellow chair, a grey surface, hanging food items and people's voices are audible.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/37GGeCScCxQ.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_275_0"}, {"texts": ["A person whose hand is visible is holding a thread with a tied food item while a boy wearing a red top is eating the food on the end of the rope and others are standing few feet away from the boy."], "durations": null, "exact_frames_per_prompt": [42], "background": "In the background there is a yellow chair, a grey surface, hanging food items and people's voices are audible.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/37GGeCScCxQ.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_275_1"}, {"texts": ["A baby is sitting on a blue and white baby eating chair and eating food from a yellow bowl."], "durations": null, "exact_frames_per_prompt": [80], "background": "In the background there is white wall, a blue and white baby eating chair, and a lady is speaking in the background.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/18TVCQD2etE.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_2760_0"}, {"texts": ["A person whose hand is visible is putting a spoon in the baby's mouth."], "durations": null, "exact_frames_per_prompt": [69], "background": "In the background, a person is speaking, the baby's voice is audible. There is a spoon, a baby cot, a window, a white floor, a white wall and a black object", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/2FM5UIAx2o4.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_2777_0"}, {"texts": ["A baby is lying on a baby bed, smiling and licking a spoon while a person with only his hand is visible is holding the spoon and feeding the baby."], "durations": null, "exact_frames_per_prompt": [80], "background": "In the background, a person is speaking, the baby's voice is audible. There is a spoon, a baby cot, a window, a white floor, a white wall and a black object", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/2FM5UIAx2o4.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_2777_1"}, {"texts": ["A child wearing a green pyjama is tying knot of the lace in pyjama.\n"], "durations": null, "exact_frames_per_prompt": [58], "background": "There is a dark background.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/-pq8M-SI_Tk.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_2779_0"}, {"texts": ["A man wearing a gray t-shirt, gray jeans and a black apron, is bending over and cutting the nails of the brown-white dog.", "The man is holding it on the green table while the white dog looking at him"], "durations": null, "exact_frames_per_prompt": [21, 58], "background": "In the background, there is a green table, white cages, a violet basket and the sound of music is audible.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/1_qRIJdVykk.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_277_0"}, {"texts": ["A brown white dog is standing on the green table and getting his nails cut by the man while a white dog is looking behind the glass cabin.", "The brown white dog is sitting on the green table while the man wearing a grey t-shirt is holding the brown dog and a white dog behind the glass cabin is moving."], "durations": null, "exact_frames_per_prompt": [30, 50], "background": "In the background, there is a green table, white cages, a violet basket and the sound of music is audible.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/1_qRIJdVykk.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_277_1"}, {"texts": ["A white dog is standing in the cage and looking here and there while a man is holding a white and brown colored dog and grooming it on a table."], "durations": null, "exact_frames_per_prompt": [54], "background": "In the background, there is a green table, white cages, a violet basket and the sound of music is audible.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/1_qRIJdVykk.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_277_2"}, {"texts": ["A man wearing black cloth is standing.", "The man is holding a kettle.", "The man is pouring some liquids in glass."], "durations": null, "exact_frames_per_prompt": [16, 16, 48], "background": "In the background, music is playing and miscellaneous sounds are audible, there is a wooden wall, a door, lights, a plant, brown surface, a window, bowls, a kettle, and some other objects.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/4MwQS1Mp9sA.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_2783_0"}, {"texts": ["A woman whose hands are visible is holding a maroon-black drilling machine with attached potato and holding a green vegetable peeler towards the potato.", "The woman presses the drilling machine and starts peeling the potato with vegetable peeler."], "durations": null, "exact_frames_per_prompt": [29, 52], "background": "In the background, a man is speaking, there is the sound of the drilling machine, a wooden chopping board, a grey-brown counter top, vegetables, a white bowl with some stuff, a maroon-black drilling machine, potato peel, apples, and a green vegetable peeler.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/5GJRsNPu-Ck.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_2784_0"}, {"texts": ["A woman wearing gray clothes is sitting on a black chair and is speaking on the mic while looking at the papers."], "durations": null, "exact_frames_per_prompt": [23], "background": "In the background, a woman is speaking; there are black chairs, a podium with microphones, a brown wall, and an advertisement on the bottom.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/---QUuC4vJs.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_278_3"}, {"texts": ["A man wearing a blue t-shirt is standing on the left and eating and putting his hand on the mouth then starts drinking from a white cup.\n while a group of people is standing and watching him, and a woman beside the man is standing while eating."], "durations": null, "exact_frames_per_prompt": [80], "background": "In the background, people are speaking, the music is playing, there is a gray floor, a green audience sitting area, a brown wall, white lights, red-blue balloons, and a gray ceiling.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/2DqelQSPOVA.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_2793_0"}, {"texts": ["A woman wearing a gray top is standing on the back side, holding the food, eating the food, and putting her hand on her mouth while a man wearing blue t-shirt standing left side holding a cup and drinking something"], "durations": null, "exact_frames_per_prompt": [80], "background": "In the background, people are speaking, the music is playing, there is a gray floor, a green audience sitting area, a brown wall, white lights, red-blue balloons, and a gray ceiling.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/2DqelQSPOVA.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_2793_2"}, {"texts": ["A man wearing a blue t-shirt is standing on the left and eating and putting his hand on the mouth while others are watching them.", "The man wearing a blue t-shirt starts drinking from a white cup."], "durations": null, "exact_frames_per_prompt": [19, 61], "background": "In the background, people are speaking, the music is playing, there is a grey floor, a green audience sitting area, a brown wall, white lights, red-blue balloons, and a green ceiling.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/2DqelQSPOVA.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_2794_0"}, {"texts": ["A woman wearing pink patterned clothes is standing and touching her eyes and speaking."], "durations": null, "exact_frames_per_prompt": [80], "background": "In the background, the woman speaking and music is playing, there is a white wall, white ceilings, a vent, a poster and a black object.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/3UR5u_w2q2c.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_2795_0"}, {"texts": ["A woman whose upper body is visible, wearing a printed pink cloth, is showing her face while touching her face with her right hand fingers and speaking while moving her hands."], "durations": null, "exact_frames_per_prompt": [80], "background": "In the background, music is playing and a woman is speaking. There is a white ceiling with a light fixture, white walls, a scenery frame on the left wall, a black object on the right side behind the woman, and another black object on the left side.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/3UR5u_w2q2c.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_2796_0"}, {"texts": ["A person wearing a pink jacket and black pants is holding a dog by the leash and starts walking while The dog sits down."], "durations": null, "exact_frames_per_prompt": [80], "background": "In the background, there is a yellow door, red door frame, white walls, tables, chairs, a black cupboard, a black object, a black bag, grey ground, a black jacket, a wooden furniture, a cardboard box, yellow objects, and the voices of people talking is audible.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/-qC7E3t6smU.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_2798_0"}, {"texts": ["A brown dog is walking around the room while a woman in a pink jacket holding the dog belt and walking forward with a dog.", "The dog sits down while the woman is scratching the dog's head."], "durations": null, "exact_frames_per_prompt": [69, 11], "background": "In the background, there is a yellow door, red door frame, white walls, tables, chairs, a black cupboard, a black object, a black bag, grey ground, a black jacket, a wooden furniture, a cardboard box, yellow objects, and the voices of people talking is audible.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/-qC7E3t6smU.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_2798_1"}, {"texts": ["A woman wearing a white shirt is sitting on the black sofa and is tearing papers while looking at the baby."], "durations": null, "exact_frames_per_prompt": [80], "background": "In the background, the sound of tearing the paper is audible and the baby is laughing. There is a black sofa chair, a black cushion, a green plant and pieces of paper.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/2qEOHHrhafY.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_279_0"}, {"texts": ["A baby wearing a purple t-shirt is picking up a piece of paper.  while a woman in white top is plying with baby.", "The baby is laughing."], "durations": null, "exact_frames_per_prompt": [66, 14], "background": "In the background, the sound of tearing the paper is audible and the baby is laughing. There is a black sofa chair, a black cushion, a green plant and pieces of paper.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/2qEOHHrhafY.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_279_1"}, {"texts": ["A boy wearing a white shirt is sitting and tying a maroon tie on his shirt collar.\n while a woman wearing specs sitting on the left side showing her finger towards the man"], "durations": null, "exact_frames_per_prompt": [80], "background": "In the background there is a pink cloth, white benches, white table and people's voices are audible.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/0CkajhOMX1w.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_2803_0"}, {"texts": ["A woman wearing a dark gray-black vest is holding a makeup brush and brushing around her face while looking to the left side."], "durations": null, "exact_frames_per_prompt": [80], "background": "In the background, there is a cream wall, a wooden cloth hanger, a red cloth, a brown cloth, a black cloth, white doors, the voice of the woman and a miscellaneous sound is audible.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/-xM5R1O8tos.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_2806_0"}, {"texts": ["A woman wearing gray-black cloth is standing and holding a brush and spreading cream on her face and speaking."], "durations": null, "exact_frames_per_prompt": [80], "background": "In the background, woman is speaking, there is a white wall, white curtains, black bottles, red-blue and white cloth and a brush.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/-xM5R1O8tos.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_2807_0"}, {"texts": ["A man wearing a black t-shirt and black jeans is sitting on a chair while holding a beer glass mug, putting the glass on the table.", "The man is pointing his right hand in the front.", "The man looks in front. ", "The man is tilting his body towards the glass mug and is drinking beer with his mouth from the glass mug."], "durations": null, "exact_frames_per_prompt": [14, 36, 20, 10], "background": "In the background, people are speaking, there are chairs with stuff, a white ceiling with white light fixtures, walls, a table, and a glass mug with beer on the table, and a brown surface.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/5faGPNmdEcE.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_2808_3"}, {"texts": ["A person whose hands are visible is cutting the meat with a knife, the video of which is streaming side by side."], "durations": null, "exact_frames_per_prompt": [62], "background": "In the background, there is a black countertop, knives, chopping boards, meat, and the white object. There is the sound of human voice and a piece of music in the background.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/1J9GghXhWuY.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_2821_1_ms"}, {"texts": ["A person whose only hands are visible is holding a piece of paper in his hands and moving it."], "durations": null, "exact_frames_per_prompt": [80], "background": "In the background, there is a black counter-top, a chopping board, knives, a white chopping board, and people are speaking and some music is playing.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/1J9GghXhWuY.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_2822_0_ms"}, {"texts": ["A person is making a cut into the meat with a knife."], "durations": null, "exact_frames_per_prompt": [62], "background": "In the background, there is a black counter-top, a chopping board, knives, a white chopping board, and people are speaking and some music is playing.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/1J9GghXhWuY.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_2822_1"}, {"texts": ["A boy wearing a multicolored printed t-shirt is standing in the front, holding a green packet of tissue paper.", "The boy takes the tissue paper.", "The boy puts it into a white cup."], "durations": null, "exact_frames_per_prompt": [27, 38, 15], "background": "In the background, the boy is speaking, there is a miscellaneous sound, there is a grey countertop with stove, kettles, a jar, a brown container, some other stuff, a serving spoon, a white cup, white walls, a white ceiling, a blue house, green trees, and a sky.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/1IxOss60EVU.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_2824_0"}, {"texts": ["A man is standing.", "The man is holding a surgical tape.", "The man is giving instructions how to apply surgical tape."], "durations": null, "exact_frames_per_prompt": [13, 23, 44], "background": "In the background, there are bags, a white wall, a colorful cloth, a paper, an id card, boxes, some other stuff, and the voice of people speaking is audible.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/2bKPE0ZeUU4.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_2828_0"}, {"texts": ["A man whose upper half-body is visible wearing a lavender blue shirt and specs is sitting, making a funny face while putting the necktie on the table.", "The man wears a necktie while putting the necktie on his neck. ", "The man starts tying the necktie."], "durations": null, "exact_frames_per_prompt": [36, 29, 15], "background": "In the background, music is playing, there is a necktie, a green table, and a background with a blue sky, clouds, hills, sun, and a brown-green grass surface.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/0OOUOM8KRB4.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_2830_0"}, {"texts": ["A woman wearing a maroon t-shirt is making a bed."], "durations": null, "exact_frames_per_prompt": [45], "background": "In the background, music is playing. There are white walls, a photo frame, a brown bed, white pillows, and a mattress with a white bed sheet.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/j4t29DTNLQc.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_2833_0_ms"}, {"texts": ["A woman wearing a brown t-shirt is making a bed."], "durations": null, "exact_frames_per_prompt": [44], "background": "In the background, the music is audible. There is a wooden bed, a mattress, a white bed sheet, white pillows, white walls, and a photo frame.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/j4t29DTNLQc.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_2834_0"}, {"texts": ["A man wearing white clothes is sitting on the floor and pouring egg mixture into a pan and spreading it. ", "The man is making scrambled eggs."], "durations": null, "exact_frames_per_prompt": [22, 58], "background": "In the background, a person is speaking and miscellaneous sounds are audible. There is a white wall, white floor, black bed with multiple objects on it, a bed sheet, a table with blue table cloth, a gray almirah with objects in it, a black pan, a knife, packets of bread, a water bottle, a bowl, a fork, a white wire, a black wire, a white plate and a mobile phone.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/2jDDp4ymnUA.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_2843_0"}, {"texts": ["A person whose hands are visible is wrapping a box with wrapping paper and a tape."], "durations": null, "exact_frames_per_prompt": [64], "background": "In the background, music is playing. There is a tape, a brown box, a wrapping paper and a white surface.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/-oCRYzuBews.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_284_0"}, {"texts": ["A girl wearing a graphic blue top and graphic black pants is sitting and eating food.", "The girl moved her hand forward with the food.", "The girl is taking it back."], "durations": null, "exact_frames_per_prompt": [26, 27, 27], "background": "In the background, there are white cages, green pipes, black objects, yellow walls, some white objects, a glass, the voice of the girl and a miscellaneous sound is audible.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/-LVbgohI4HA.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_2853_0"}, {"texts": ["A person whose only hands is visible is turning a folded red napkin on the grey surface to the opposite side.", "The person is pointing to the napkin with his finger."], "durations": null, "exact_frames_per_prompt": [47, 33], "background": "In the background, there is a gray surface, a red napkin, the voice of the person and a miscellaneous sound is audible.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/1n3fYR2Jf8E.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_2855_0"}, {"texts": ["A girl wearing an off-white top is sitting on the bed and looking here and there while the other person sits along with her"], "durations": null, "exact_frames_per_prompt": [35], "background": "In the background, people are speaking. There are white walls, a white curtain, window blinds, a bed, pillows, cushions, stuffed toys, a wooden object, a floor, a black and silver object and a dark background.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/0jFBHhjvbYs.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_2863_0"}, {"texts": ["A girl whose back is visible is also sitting on the bed and is doing something while a girl is sitting in front of her and smiling."], "durations": null, "exact_frames_per_prompt": [34], "background": "In the background, people are speaking. There are white walls, a white curtain, window blinds, a bed, pillows, cushions, stuffed toys, a wooden object, a floor, a black and silver object and a dark background.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/0jFBHhjvbYs.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_2863_1"}, {"texts": ["A girl wearing a gray jacket is sitting and opening the letters."], "durations": null, "exact_frames_per_prompt": [37], "background": "In the background, people are speaking. There are white walls, a white curtain, window blinds, a bed, pillows, cushions, stuffed toys, a wooden object, a floor, a black and silver object and a dark background.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/0jFBHhjvbYs.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_2863_2"}, {"texts": ["A girl wearing a light purple t-shirt and light purple leggings is sitting on the floor and smiling while looking to the left and the right."], "durations": null, "exact_frames_per_prompt": [35], "background": "In the background, people are speaking; there are white walls, white curtains hanging, window blinds, cushions, a brown bed-sheet, toys, a brown wardrobe with stuff on the left side, a brown bench, clothes on the bench, other stuff on the floor, and a gray floor.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/0jFBHhjvbYs.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_2864_0"}, {"texts": ["A woman wearing a gray top and black jeans is sitting on the floor on the right side and is picking up the paper object from the floor with her hands.", "The woman is opening the paper object."], "durations": null, "exact_frames_per_prompt": [21, 14], "background": "In the background, people are speaking; there are white walls, white curtains hanging, window blinds, cushions, a brown bed-sheet, toys, a brown wardrobe with stuff on the left side, a brown bench, clothes on the bench, other stuff on the floor, and a gray floor.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/0jFBHhjvbYs.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_2864_2"}, {"texts": ["A person wearing black clothes is sitting near the car and holding a tool and touching the wheel."], "durations": null, "exact_frames_per_prompt": [70], "background": "In the background, a person is speaking and miscellaneous sounds are audible, there is a gray surface, a black car, a stepney, and a tool.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/4oyV_yblhEo.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_2872_0"}, {"texts": ["A baby wearing blue-white dress is crying."], "durations": null, "exact_frames_per_prompt": [80], "background": "In the background, there is a white wall, a green object, and baby's voice is audible.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/-Dk0FFQ7hiU.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_287_0"}, {"texts": ["A kid wearing a black color cap is sitting on the gray surface and throwing fish food in the water."], "durations": null, "exact_frames_per_prompt": [50], "background": "In the background, miscellaneous sounds are audible. There is water, gray surface, small plants, green leaves, dry leaves, a white bowl and fish food.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/07JFxr9qwVc.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_2880_0"}, {"texts": ["A standing baby is throwing brown stuff from a bowl he is holding in his hands.", "The boy is bending towards the pond."], "durations": null, "exact_frames_per_prompt": [43, 7], "background": "In the background, there is a pond, a bowl, plants, a brown surface and a chicken sound, and some miscellaneous voices are audible.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/07JFxr9qwVc.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_2881_0"}, {"texts": ["A boy wearing a black t-shirt is standing and has taken a brown snake around his neck."], "durations": null, "exact_frames_per_prompt": [80], "background": "In the background, people are speaking. There is a brown floor, brown and white walls, a white door, curtains, a stool, white rods and a camouflage cloth.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/53sNjG-EWmE.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_2884_0"}, {"texts": ["A brown snake lying around the neck of the boy is hissing towards the boy's face."], "durations": null, "exact_frames_per_prompt": [80], "background": "In the background, people are speaking. There is a brown floor, brown and white walls, a white door, curtains, a stool, white rods and a camouflage cloth.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/53sNjG-EWmE.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_2884_1"}, {"texts": ["A girl wearing a polka-dot dress is plucking a red fruit from the green tree."], "durations": null, "exact_frames_per_prompt": [80], "background": "In the background, people are audible and the girl is also speaking. There are green trees, green bushes, a red fruit, a brown branch and a green and brown surface.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/0kNcTxm63JQ.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_289_0"}, {"texts": ["A baby girl wearing a white-black frock standing in a green grass field is plucking fruit from the tree.", "The baby girl is walking and showing the fruit."], "durations": null, "exact_frames_per_prompt": [52, 27], "background": "In the background, miscellaneous sounds and people are audible. There are green grasses, green plants, and a red fruit.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/0kNcTxm63JQ.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_290_0"}, {"texts": ["A man wearing a gray t-shirt and blue jeans is standing and spreading food for the pigeons on the ground a woman wearing a purple dress is also feeding the pigeons."], "durations": null, "exact_frames_per_prompt": [80], "background": "In the background, people and miscellaneous sounds are audible. There is a brown soil surface, a yellow-red wall, a house, green trees, and a blue sky.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/-7BVb-zigbA.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_291_0"}, {"texts": ["A girl wearing a purple frock is standing in the front holding food in her hand."], "durations": null, "exact_frames_per_prompt": [68], "background": "In the background, people and miscellaneous sounds are audible. There is a brown soil surface, a yellow-red wall, a house, green trees, and a blue sky.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/-7BVb-zigbA.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_291_1"}, {"texts": ["A man wearing a white dress and a watch is standing and cutting a slice of a pineapple with a knife.", "The man putting a cookie cutter in the core of the slice and pressing it."], "durations": null, "exact_frames_per_prompt": [56, 24], "background": "In the background, there is a white chop board, knife, a bowl of fruits, wooden countertop, a cookie cutter, a logo, the voice of the man and a miscellaneous sound is audible.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/-tdr5TVLqXs.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_293_0"}, {"texts": ["A girl wearing a dark pink top is sitting on the gray surface and feeding the pigeons."], "durations": null, "exact_frames_per_prompt": [81], "background": "In the background, both the girls are speaking and the sound of the pigeons is also audible. There is a gray surface, green grass surface and green bushes.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/-1VyqbLf1Pg.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_296_0"}, {"texts": ["A girl wearing a light pink jacket is sitting in the front on the road while the other girl is feeding the pigeons", "The girl is feeding the group of pigeons in her open palm."], "durations": null, "exact_frames_per_prompt": [69, 12], "background": "In the background, there is green grass, plants, a road, and the girls are speaking, and some miscellaneous sounds are audible.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/-1VyqbLf1Pg.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_297_1"}, {"texts": ["A man wearing t-shirt is playing golf on the golf course."], "durations": null, "exact_frames_per_prompt": [80], "background": "In the background, there is a golf course, bushes, a tree, a hill, and a sky.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/0eVZWU4H3Ao.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_298_0"}, {"texts": ["A boy wearing a sky-blue t-shirt is sitting and holding a hen the boy licking and eating something."], "durations": null, "exact_frames_per_prompt": [80], "background": "In the background, there is a plant, a black stand, a black surface, a white wall, and the voice of a boy speaking is audible.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/-irl68ywUzI.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_299_0"}, {"texts": ["A boy wearing a green shirt is sitting a boy wearing a sky-blue t-shirt is sitting and holding a hen.", "The boy licking and eating something."], "durations": null, "exact_frames_per_prompt": [21, 59], "background": "In the background, there is a plant, a black stand, a black surface, a white wall, and the voice of a boy speaking is audible.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/-irl68ywUzI.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_299_1"}, {"texts": ["A hen is sitting in the lap of a boy a boy with a green shirt is biting some straw"], "durations": null, "exact_frames_per_prompt": [80], "background": "In the background, there is a plant, a black stand, a black surface, a white wall, and the voice of a boy speaking is audible.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/-irl68ywUzI.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_299_2"}, {"texts": ["A woman wearing a white sweatshirt and black pants is standing on the right side of the countertop, holding a hand mixer and mixing a black paste in a bowl a girl wearing a lavender color sweatshirt is standing on the chair,along with the woman, holding a mixer and mixing a black paste in a bowl."], "durations": null, "exact_frames_per_prompt": [80], "background": "In the background, there is a white countertop, wooden cupboards, white wall, a mixer, wooden chair, tap, dispenser, bottle, switches, a carton, utensils, a hand mixer, a bowl, voices of the people and a miscellaneous sound is audible.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/2X40l56krhc.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_308_0"}, {"texts": ["A girl wearing a lavender color sweatshirt is standing on the chair, beside the woman, holding a mixer and mixing a black paste in a bowl a woman wearing white clothes is standing and mixing a black paste with a hand mixer while holding a girls hand."], "durations": null, "exact_frames_per_prompt": [80], "background": "In the background, there is a white countertop, wooden cupboards, white wall, a mixer, wooden chair, tap, dispenser, bottle, switches, a carton, utensils, a hand mixer, a bowl, voices of the people and a miscellaneous sound is audible.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/2X40l56krhc.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_308_1"}, {"texts": ["A woman wearing white clothes is standing and mixing a black paste with a hand mixer while holding a girl's hand a girl wearing a lavender color sweatshirt is standing on the chair, holding a mixer and mixing a black paste in a bowl along with the woman."], "durations": null, "exact_frames_per_prompt": [80], "background": "In the background, there are cabinets, a hand mixer, a bowl, a chair, a counter top with some stuff, a white wall, and the voice of a person speaking, and some miscellaneous sounds are audible.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/2X40l56krhc.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_309_0"}, {"texts": ["A girl wearing a purple hoodie is standing on the chair and mixing a black paste with a hand mixer while holding a woman's hand."], "durations": null, "exact_frames_per_prompt": [80], "background": "In the background, there are cabinets, a hand mixer, a bowl, a chair, a counter top with some stuff, a white wall, and the voice of a person speaking, and some miscellaneous sounds are audible.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/2X40l56krhc.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_309_1"}, {"texts": ["A kid wearing an orange t-shirt and black-white-red-orange pants is standing and eating a fruit."], "durations": null, "exact_frames_per_prompt": [80], "background": "In the background, there is a white wall, cream surface, a wooden object with metallic wires, a glass door, voices of the people and music is audible.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/3aYXMxjjqc4.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_30_0"}, {"texts": ["A person whose hands are visible is holding a pen.", "And starts writing on the paper."], "durations": null, "exact_frames_per_prompt": [60, 20], "background": "In the background people are audible. There is a pen, white paper, and a brown wooden surface.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/2YRdhTjKgR0.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_311_0"}, {"texts": ["A girl wearing a black jacket is standing on the grey floor, caressing and touching the corn snake a person wearing white printed shirt is holding the snake in his hands"], "durations": null, "exact_frames_per_prompt": [80], "background": "In the background, people are speaking, there is a grey floor, wooden benches, and chairs.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/5L0rNDJoGQY.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_312_1"}, {"texts": ["A person wearing a blue jacket is standing on the left side, touching the snake a person wearing white printed shirt is holding the snake in his hands"], "durations": null, "exact_frames_per_prompt": [80], "background": "In the background, there is a brown floor, brown tables, chairs, people are speaking, and there is a miscellaneous sound.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/5L0rNDJoGQY.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_313_1"}, {"texts": ["A man wearing a black jacket and dark brown pants is standing on the concrete surface and holding a car's tire with his hands on the concrete surface while tilting his body and is speaking while touching the tire with his right hand."], "durations": null, "exact_frames_per_prompt": [80], "background": "In the background, a man is speaking, there are cars and a truck standing, a white wall, a sky, a metal structure, another metal tower, a black tire, and a concrete surface.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/2Qkt9PX0jXU.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_314_0"}, {"texts": ["A child wearing an orange bin is sitting on an orange-green bed, holding a brown object, laughing.", "The child wearing an orange bin is falling on the bed."], "durations": null, "exact_frames_per_prompt": [47, 3], "background": "In the background, the woman is laughing, the child is laughing, there is an orange-green bed, a pink-white gift wrap, a brown card with written words, a white-blue cloth, and black curtains.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/0PUt5XSDGwA.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_317_0"}, {"texts": ["A woman wearing a gray top is sitting, holding a glass of beer while the other man sipping glass of beer.", "The woman scratches her chin while the other man speaks something.", "The woman raises a toast while the other man shows a glass of beer."], "durations": null, "exact_frames_per_prompt": [41, 30, 9], "background": "In the background, a woman and a man are speaking. There is a white wall, a brown window, white curtains, beer cans, a beer bottle, a laptop, a mouse, a bottle opener, beer glasses, a mobile, and a brown table.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/-Y9w-IDDK5Q.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_319_0"}, {"texts": ["A man wearing a gray t-shirt is sitting, holding a glass of beer, and drinking the man wearing a black t-shirt is talking while drinking the beer", "The man raising a toast a woman wearing a grey t-shirt is lifting the glass of beer"], "durations": null, "exact_frames_per_prompt": [72, 8], "background": "In the background, a woman and a man are speaking. There is a white wall, a brown window, white curtains, beer cans, a beer bottle, a laptop, a mouse, a bottle opener, beer glasses, a mobile, and a brown table.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/-Y9w-IDDK5Q.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_319_1"}, {"texts": ["A woman wearing a gray cloth is sitting, and holding a glass of beer the man wearing a black t-shirt is talking while drinking the beer", "The woman wearing a gray cloth is raising a toast a man wearing a black t-shirt is moving the beer glass"], "durations": null, "exact_frames_per_prompt": [70, 10], "background": "In the background, a woman and a man are speaking, there is a white wall, a brown window, white curtains, beer can, beer bottle, a laptop, a mouse, a bottle opener, beer glasses, a mobile, and brown table and chairs.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/-Y9w-IDDK5Q.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_320_0"}, {"texts": ["A woman wearing a white t-shirt is standing and picking up a brown pot.", "The woman is applying some liquid on the food with a brush."], "durations": null, "exact_frames_per_prompt": [44, 36], "background": "In the background, there is a wall, a plate with a food item, a brown pot, a brush, a white object, a countertop, people are speaking and shouting voices are audible.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/4N3YV8tuZKE.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_321_0"}, {"texts": ["A man wearing a grey-black t-shirt is standing on the backside, holding a poly-bag of water and showing the poly-bag to the group of people."], "durations": null, "exact_frames_per_prompt": [80], "background": "In the background, people are speaking, there are grey walls, and grey ceiling with white lights and a poster on the wall.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/4B6D7gpYYBo.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_333_0"}, {"texts": ["A brown fish, facing right side is swimming inside a transparent aquarium while a man standing behind the aquarium and watching fish"], "durations": null, "exact_frames_per_prompt": [25], "background": "In the background there is transparent aquarium, some rocks, green and brown leaves, a yellow wall, some objects and a music sound is audible.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/0wzBFCegdZI.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_334_1_ms"}, {"texts": ["Another small brown fish on the right side is swimming inside a transparent aquarium with the second fish."], "durations": null, "exact_frames_per_prompt": [43], "background": "In the background there is transparent aquarium, some rocks, green and brown leaves, a yellow wall, some objects and a music sound is audible.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/0wzBFCegdZI.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_334_3_ms"}, {"texts": ["A woman wearing a colourful cloth is getting up while holding the table.", "The woman raise her hands in the air and starts speaking."], "durations": null, "exact_frames_per_prompt": [31, 35], "background": "In the background there is a blue wall, grey surface, chairs, side stools, and the man laughing, a woman speaking, clapping and the music playing sound is audible.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/1TZJqNJ4YtE.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_335_1"}, {"texts": ["A person wearing a white shirt is tying a yellow bow around his neck."], "durations": null, "exact_frames_per_prompt": [49], "background": "There is white background, a bow, a drawing of bow and the sound of music is audible.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/0l5AJVEcrb0.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_336_0"}, {"texts": ["A man wearing a black t-shirt is standing and peeling an apple attached to a drill machine.\n a girl wearing a blue t-shirt is keenly looking at the task performed by the man"], "durations": null, "exact_frames_per_prompt": [80], "background": "In the background, people and miscellaneous sounds are audible. There is an apple, a drill machine, a peeler, a white kitchen sink, a black surface, white walls with pattern, knives, white cupboards, a black stove, an oven, a fridge, a photo frame, an arch, and a few miscellaneous things.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/2wqW177zwaA.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_338_0"}, {"texts": ["A girl wearing a blue t-shirt is standing and watching the apple peeling process a man wearing a black t-shirt is standing and peeling an apple attached to a drill machine."], "durations": null, "exact_frames_per_prompt": [80], "background": "In the background, people and miscellaneous sounds are audible. There is an apple, a drill machine, a peeler, a white kitchen sink, a black surface, white walls with pattern, knives, white cupboards, a black stove, an oven, a fridge, a photo frame, an arch, and a few miscellaneous things.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/2wqW177zwaA.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_338_1"}, {"texts": ["A man wearing a graphic dark gray t-shirt and dark blue jeans is holding a pan and flipping the food.", "The man drops the utensil and starts laughing."], "durations": null, "exact_frames_per_prompt": [43, 20], "background": "In the background, there are wooden cupboards, plate, stove, black counter top, switch, green wall, some stuff, some miscellaneous sound and the voice of the man is audible.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/0Or3nOLTTwc.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_339_0"}, {"texts": ["A man wearing a black suit is sitting group of people are looking at the man", "The man stands up and picks up his belongings a man with black suit comes and takes some papers from the table"], "durations": null, "exact_frames_per_prompt": [27, 49], "background": "In the background, people are speaking. There is a brown counter table, brown chairs, another brown table, white booklets, a black object, mics, printed walls and a brown door.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/29_pCX3EY14.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_33_0"}, {"texts": ["A man also wearing a suit comes from the left and stands idle a man wearing a black suit gets up from the chair and moves"], "durations": null, "exact_frames_per_prompt": [43], "background": "In the background, people are speaking. There is a brown counter table, brown chairs, another brown table, white booklets, a black object, mics, printed walls and a brown door.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/29_pCX3EY14.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_33_1"}, {"texts": ["A man wearing a dark grey printed t-shirt is standing and flipping the food in a frying pan."], "durations": null, "exact_frames_per_prompt": [63], "background": "In the background, there is a brown wooden shelf, a grey kitchen countertop with some stuff, a white plate, a white object, a white switchboard, a light green wall, and the man laughing and some miscellaneous sounds is audible.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/0Or3nOLTTwc.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_340_0"}, {"texts": ["A person whose hands are visible is cutting the pieces of food with a knife."], "durations": null, "exact_frames_per_prompt": [37], "background": "In the background, there is a knife, a brown surface, a tray, and some other stuff, and some miscellaneous sound is audible.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/0bN6zUGOehM.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_342_0"}, {"texts": ["A person wearing a white t-shirt is tying a tie."], "durations": null, "exact_frames_per_prompt": [80], "background": "In the background, music is playing. There is a white wall, photo frames and a wooden wall.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/15KFWFkp0MA.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_343_0"}, {"texts": ["A man wearing white clothes is making a tie."], "durations": null, "exact_frames_per_prompt": [80], "background": "In the background, music is playing, there is a brown wall, white wall and photo frames.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/15KFWFkp0MA.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_344_0"}, {"texts": ["A woman wearing red clothes is standing and speaking."], "durations": null, "exact_frames_per_prompt": [31], "background": "In the background, both the women are speaking and music is playing. There is a black table, a brown wall, blue background, a black television, white papers, white walls, multiple monitors, keyboards, mouses and a graphically edited multi-colored background with a caption at the bottom.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/-WdB8oVQHOk.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_346_0_ms"}, {"texts": ["A woman wearing white clothes is also standing and reading the weather forecast."], "durations": null, "exact_frames_per_prompt": [30], "background": "In the background, both the women are speaking and music is playing. There is a black table, a brown wall, blue background, a black television, white papers, white walls, multiple monitors, keyboards, mouses and a graphically edited multi-colored background with a caption at the bottom.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/-WdB8oVQHOk.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_346_1"}, {"texts": ["A man wearing a gray dress is standing, holding a skimmer with one hand and a cauldron from the other hand, sauteing the food from the skimmer on the stove."], "durations": null, "exact_frames_per_prompt": [80], "background": "In the background, there is a black-red cauldron, a black skimmer, black wall, some white bowls with spices, and white countertop, a black stove, a blue flame, a logo, the voice of the man and some miscellaneous sounds are audible.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/0PmEiQ386CY.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_351_0"}, {"texts": ["A man wearing grey pants is sitting on the carpet, first folding the cloth.", "The man is holding his leg.", "The man is starting to hit his leg against his chest."], "durations": null, "exact_frames_per_prompt": [31, 25, 24], "background": "In the background, there is a white door, a wall, white clothes, a black wire, a brown table, shoes, red clothes, a bed, carpets, brown surface, music is playing, people are speaking, and hitting sounds are audible.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/1VlTKG9_h68.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_352_0"}, {"texts": ["A man wearing a jacket is filling fuel in the tank of his motor cycle."], "durations": null, "exact_frames_per_prompt": [80], "background": "In the background, there is a fueling station, a bike, and the sky.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/1UHsw4nNMLA.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_353_0"}, {"texts": ["A woman wearing a white and black printed cloth is speaking, and lying on the soil surface under a vehicle."], "durations": null, "exact_frames_per_prompt": [19], "background": "In the background there is a yellow and black oil drain pan, vehicles, plants, grass and soil surface and a woman's speaking voice, a musical sound and a miscellaneous sound is audible.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/2mmx18Cf5xk.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_356_1"}, {"texts": ["A woman wearing a black jacket is standing and holding a child in her hands near a red fruit tree."], "durations": null, "exact_frames_per_prompt": [80], "background": "In the background, there are green grasses, soil surface, a blue sky, red fruits, green trees, some miscellaneous sounds, and a man's and a woman's voices are audible.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/-M1W0Zj52T8.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_358_0"}, {"texts": ["A child wearing blue clothes is sitting in the lap of a woman and touching the red fruit."], "durations": null, "exact_frames_per_prompt": [80], "background": "In the background, there are green grasses, soil surface, a blue sky, red fruits, green trees, some miscellaneous sounds, and a man's and a woman's voices are audible.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/-M1W0Zj52T8.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_358_1"}, {"texts": ["A man wearing a gray shirt and black trousers is sitting, moving his hand, and telling weather forecast."], "durations": null, "exact_frames_per_prompt": [80], "background": "In the background, a man is audible. There are T. V screens, a black shelf, a T. V remote, and a blue wall.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/-XlLzuqoXag.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_360_0"}, {"texts": ["A man wearing an orange vest and blue jeans is standing while tilting his body on the left side and is shearing the sheep with the sheep clipper a man wearing black t-shirt is trimming the sheep and man with blue pant is walking around"], "durations": null, "exact_frames_per_prompt": [43], "background": "In the background, people are speaking; there is a black ceiling with light fixtures, big posters hanging on the backside, a machine and a counter with a white poster and sheared wool.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/0vjCk03or88.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_362_2"}, {"texts": ["A kid wearing a printed t-shirt is standing and holding a newspaper."], "durations": null, "exact_frames_per_prompt": [81], "background": "In the background, there is a white wall, medicine boxes, books, a white table, newspaper, a brown table, a guitar is playing, a woman's voice and a kid's voice are audible.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/-CFiVdQyv4E.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_366_0"}, {"texts": ["A man wearing a white cloth is sitting, folding a newspaper and beside him is a baby holding a newspaper while looking at what the man is holding.", "The man is putting it on the brown table and the baby hands the newspaper to the man but later changes his mind, and refuses to give the newspaper."], "durations": null, "exact_frames_per_prompt": [44, 37], "background": "In the background, there is a white wall, medicine boxes, books, a white table, newspaper, a brown table, a guitar is playing, a woman's voice and a kid's voice are audible.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/-CFiVdQyv4E.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_366_1"}, {"texts": ["A man wearing a white-brown striped shirt, black shorts, and black slippers is standing by the car and then standing with folding his hands and is talking to the different men."], "durations": null, "exact_frames_per_prompt": [80], "background": "In the background, people are speaking, there is the sound of vehicles, car standing, dispenser machines with fuel dispenser nozzle, two gray containers on the left side, electric poles with black electrical wires, buildings, metal fence boundary, green bushes, green trees, a sky, a green grass surface, and a concrete gray surface.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/3yzVVN43Z5o.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_36_0"}, {"texts": ["A man wearing a white t-shirt and brown shorts is standing while putting his hands on his waist and is talking to the first man."], "durations": null, "exact_frames_per_prompt": [46], "background": "In the background, people are speaking, there is the sound of vehicles, car standing, dispenser machines with fuel dispenser nozzle, two gray containers on the left side, electric poles with black electrical wires, buildings, metal fence boundary, green bushes, green trees, a sky, a green grass surface, and a concrete gray surface.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/3yzVVN43Z5o.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_36_3_ms"}, {"texts": ["A person whom hands are visible is applying a spread on a piece of bread with a knife.\n"], "durations": null, "exact_frames_per_prompt": [80], "background": "In the background there is a brown surface, a knife and bread.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/2vvxK-Yaxgg.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_373_0"}, {"texts": ["A boy wearing black t-shirt is taking out clothes from the washing machine."], "durations": null, "exact_frames_per_prompt": [80], "background": "In the background, there is a washing machine, clothes, tile floor and a boy voice is audible.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/7712i5IsLTQ.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_374_0"}, {"texts": ["A boy wearing a white t-shirt takes the animal food from the man.", "The boy wearing a white t-shirt is feeding the white animal behind the cage."], "durations": null, "exact_frames_per_prompt": [37, 26], "background": "In the background, people are speaking, the boy is audible and the man is yelling. There is a brown surface, a wooden fencing with grill caging.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/0VZAgBAgdP8.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_379_1"}, {"texts": ["A person whose hands are visible is lying on the floor under the vehicle and fastening the nut using his hand.", "The person is fastening the nut using a wrench."], "durations": null, "exact_frames_per_prompt": [44, 36], "background": "In the background, there is a vehicle, tire, a chair, and the floor.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/1ExsR4wfJwU.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_380_0"}, {"texts": ["A person whose hands are visible is holding the bunch of notes in one hand and counting it one by one from the other hand, and putting it on the floor."], "durations": null, "exact_frames_per_prompt": [80], "background": "In the background, there is a bunch of notes, a shoe, floor, and the white wall.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/-WVXLhiSMg0.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_382_0"}, {"texts": ["A person whom hands are visible is counting banknotes and throwing the notes on the floor."], "durations": null, "exact_frames_per_prompt": [80], "background": "In the background, miscellaneous sounds are audible. There is a white wall, banknotes, gray floor, a white object, a shoe and a white paper.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/-WVXLhiSMg0.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_383_0"}, {"texts": ["A woman wearing a white top is at first standing and then she grabs a piece of food a man wearing a yellow top is standing on the left side", "The woman starts eating."], "durations": null, "exact_frames_per_prompt": [41, 24], "background": "In the background, there is a beige colored wall, a white table, food, and the wooden flooring.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/4bIa7q5fAZA.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_384_1"}, {"texts": ["A man wearing a yellow top is standing on the left side of the woman in a white clothes a woman wearing a white top is at first standing and then she grabs a piece of food."], "durations": null, "exact_frames_per_prompt": [63], "background": "In the background, there is a beige colored wall, a white table, food, and the wooden flooring.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/4bIa7q5fAZA.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_384_3"}, {"texts": ["A boy wearing a light gray t-shirt is sitting and folding a white paper sheet."], "durations": null, "exact_frames_per_prompt": [80], "background": "In the background, miscellaneous sounds are audible. There is a red surface and a white paper sheet.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/02JpMEfAtMU.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_388_0"}, {"texts": ["A man wearing a white t-shirt and white gloves is making a tattoo on the right arm of the other man with a tattoo machine a man without a shirt is sitting on a black chair and getting a tattoo"], "durations": null, "exact_frames_per_prompt": [80], "background": "In the background, there is a black chair, a pink cloth, a tattoo making machine, a gray countertop, some stuff, brown surface, the sound of a song and the sound of the tattoo machine is audible.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/1_bxueZ_tKE.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_389_0"}, {"texts": ["A man without shirt is sitting on a black chair and getting a tattoo on his right arm by the man wearing white t-shirt."], "durations": null, "exact_frames_per_prompt": [80], "background": "In the background, there is a black chair, a pink cloth, a tattoo making machine, a gray countertop, some stuff, brown surface, the sound of a song and the sound of the tattoo machine is audible.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/1_bxueZ_tKE.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_389_1"}, {"texts": ["A person wearing gray clothes is holding a white paper toy in their hand."], "durations": null, "exact_frames_per_prompt": [59], "background": "In the background, there are clothes, clothes hangers, a white ceiling, some other stuff, and the voice of people speaking and some miscellaneous sounds are audible.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/0XN9eDzCz1U.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_391_0"}, {"texts": ["A girl wearing a green-blue top and a dark gray jacket is standing and peeling a potato.", "The girl drops it into the sink.", "The girl starts peeling again."], "durations": null, "exact_frames_per_prompt": [33, 13, 21], "background": "In the background, there are wooden cupboards, a microwave, brown surface, gray counter top, brown counter top, a sink, tap, some utensils, a red object, a chimney, brown wall, voices of the people, sound of the music and some miscellaneous sounds are audible.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/55lulACxaew.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_397_0"}, {"texts": ["A woman wearing a gray jacket, standing in a kitchen is peeling a potato with a peeler."], "durations": null, "exact_frames_per_prompt": [67], "background": "In the background, a person is singing, a person is laughing, and miscellaneous sounds are audible. There are wooden cupboards, a microwave, a kitchen shelf, a door, an orange wall, and a few miscellaneous items.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/55lulACxaew.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_398_0"}, {"texts": ["A boy sitting on a grass and soil surface is milking the first cow and collecting its milk into a bottle."], "durations": null, "exact_frames_per_prompt": [80], "background": "In the background there is a grass and soil surface, plastic bottles, a plant and people's voices and some miscellaneous sounds are audible.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/-gQJhQsWFXA.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_399_2"}, {"texts": ["A small white calf is sucking milk from the first cow's breast while the boy is collecting the milk into a bottle"], "durations": null, "exact_frames_per_prompt": [80], "background": "In the background there is a grass and soil surface, plastic bottles, a plant and people's voices and some miscellaneous sounds are audible.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/-gQJhQsWFXA.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_399_3"}, {"texts": ["A man wearing a gray t-shirt and black jeans is sitting on a chair on the left side, and is spreading the red sauce on the pizza base with a spoon while holding the spoon in his right hand."], "durations": null, "exact_frames_per_prompt": [80], "background": "In the background, men are speaking; there is the sound of babbling, black chairs, a glass door, a light green wall, a glass table; and a pizza base with red sauce on the table.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/-xq2AXf-rKE.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_3_0"}, {"texts": ["A child wearing a maroon t-shirt and maroon shorts is sitting on a chair while putting his hands on the table on the right side and babbling while looking at the pizza base while the other person is preparing the pizza base"], "durations": null, "exact_frames_per_prompt": [80], "background": "In the background, men are speaking; there is the sound of babbling, black chairs, a glass door, a light green wall, a glass table; and a pizza base with red sauce on the table.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/-xq2AXf-rKE.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_3_1"}, {"texts": ["A person whose hands are visible is cutting round rings from a flatten dough with a cookie cutter.", "The man putting them into a tray."], "durations": null, "exact_frames_per_prompt": [33, 30], "background": "In the background, there is a white surface, a tray, a paper, cookie cutters, rolling pin, frosting tubes, knife and some music is playing.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/1mxNs9n5C-o.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_400_0"}, {"texts": ["A girl wearing a black jacket is standing and moving her hand in sign language."], "durations": null, "exact_frames_per_prompt": [80], "background": "In the background, the song is playing, there are wooden walls, a soil surface, green plants, green trees, a grey object, a brown pot, a totem, and a sky.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/7jrjnRyQLaE.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_404_0"}, {"texts": ["A person whose only a hand is visible is holding a glass bowl and spreading the red sauce over the food with a spoon."], "durations": null, "exact_frames_per_prompt": [80], "background": "In the background, there is a black tray on which food is kept, a spoon , and a wooden surface.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/-zN1O9ZwjWg.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_406_0"}, {"texts": ["A person wearing white glove is tattooing another person's sole a person wearing a printed black t-shirt is getting a tattoo on his sole."], "durations": null, "exact_frames_per_prompt": [80], "background": "In the background there is a black lamp, a mattress, a tattoo machine, a blue and white patterned surface and people's speaking and a music sound is audible.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/1k_q9azW5ZI.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_40_0"}, {"texts": ["A person wearing a printed black t-shirt is getting a tattoo on his sole.\n a person on the right side is holding the second person's feet with both hands while a person wearing white glove is tattooing."], "durations": null, "exact_frames_per_prompt": [80], "background": "In the background there is a black lamp, a mattress, a tattoo machine, a blue and white patterned surface and people's speaking and a music sound is audible.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/1k_q9azW5ZI.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_40_1"}, {"texts": ["A person on the right side is holding the second person's feet with both hands a person wearing a printed black t-shirt is getting a tattoo on his sole."], "durations": null, "exact_frames_per_prompt": [80], "background": "In the background there is a black lamp, a mattress, a tattoo machine, a blue and white patterned surface and people's speaking and a music sound is audible.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/1k_q9azW5ZI.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_40_2"}, {"texts": ["A boy wearing a white t-shirt with written words is on the left side, putting food in his mouth.\n", "The boy starts drinking with a straw from a white-red cup. ", "The boy puts his hand towards his mouth."], "durations": null, "exact_frames_per_prompt": [45, 16, 19], "background": "In the background, a boy is speaking and laughing, there are wooden tables, wooden chairs, a white wall, windows, a standing vehicle, green trees, a grey floor, and lights.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/1SbmvZwbV4w.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_414_0"}, {"texts": ["A man wearing a colourful t-shirt is holding a dog leash and running along with the dog on the grey surface."], "durations": null, "exact_frames_per_prompt": [80], "background": "In the background, there is a green grass surface, trees, a grey surface, house buildings, parked car, and some other structures, and the slow music playing sound is audible.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/-d_4wMddlK8.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_416_0"}, {"texts": ["A black-white dog tied with a leash and is running along with the man on the grey surface."], "durations": null, "exact_frames_per_prompt": [80], "background": "In the background, there is a green grass surface, trees, a grey surface, house buildings, parked car, and some other structures, and the slow music playing sound is audible.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/-d_4wMddlK8.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_416_1"}, {"texts": ["A man wearing a shirt is sitting behind the table.", "The man counting the notes then speaking while looking away", "The man is counting again."], "durations": null, "exact_frames_per_prompt": [18, 52, 10], "background": "In the background, there is a wall, hanging lamps, a photo frame, table, plate, glass, boundaries, plant, some stuff, some other stuff, notes, the voices of the people, the sound of the music and a miscellaneous sound is audible.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/2M_SxI_s3r4.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_41_0"}, {"texts": ["A man wearing a blue shirt is standing and getting his bow tied by another man."], "durations": null, "exact_frames_per_prompt": [80], "background": "In the background, a man is speaking. There is a gray road, green surface, green trees, green bushes, buildings and parked vehicles.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/5yPwHUy_kNQ.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_420_0"}, {"texts": ["A man wearing a check shirt whose hands are visible is tying the bow of person one."], "durations": null, "exact_frames_per_prompt": [80], "background": "In the background, a man is speaking. There is a gray road, green surface, green trees, green bushes, buildings and parked vehicles.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/5yPwHUy_kNQ.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_420_1"}, {"texts": ["A man whose hands are visible is tying the bow-tie of another man."], "durations": null, "exact_frames_per_prompt": [80], "background": "In the background, there are trees, vehicles, road surface, grass surface, bushes, and the voice of a man speaking is audible.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/5yPwHUy_kNQ.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_421_0"}, {"texts": ["A man wearing a striped shirt is standing and getting bow-tie tied by a man."], "durations": null, "exact_frames_per_prompt": [80], "background": "In the background, there are trees, vehicles, road surface, grass surface, bushes, and the voice of a man speaking is audible.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/5yPwHUy_kNQ.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_421_1"}, {"texts": ["A woman wearing a white-black design top is sitting on the left and holding a small white bowl with a spoon a woman wearing a floral top is eating something with a spoon.", "The woman is pointing her finger toward a bottle a woman wearing a floral top moves behind", "The woman takes the bottle in her hand and starts looking at the bottle a woman wearing a floral top talks to her"], "durations": null, "exact_frames_per_prompt": [22, 19, 39], "background": "In the background, women are speaking, the music is playing, there is a brown table with a blue design, white cups, white walls, a window, a brown curtain, a picture on the wall, a bottle, small white bowls, wooden fences, green trees, and a bright sky.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/0-mcCvalb6Q.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_422_0"}, {"texts": ["A woman wearing a black top is sitting on the right side and taking out a spoon from her mouth while the women sitting on the left side is saying something", "The woman opens her blindfolded.", "The woman scratches her nose the other women picks up the bottle"], "durations": null, "exact_frames_per_prompt": [18, 8, 54], "background": "In the background, women are speaking, the music is playing, there is a brown table with a blue design, white cups, white walls, a window, a brown curtain, a picture on the wall, a bottle, small white bowls, wooden fences, green trees, and a bright sky.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/0-mcCvalb6Q.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_422_1"}, {"texts": ["A baby in pink clothing is at first looking behind while holding some food.", "The baby in pink clothing then puts the food in his mouth while looking here and there."], "durations": null, "exact_frames_per_prompt": [23, 57], "background": "In the background, there are tables, chairs, and the wall.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/22Y6OZHPPV0.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_425_0"}, {"texts": ["A baby wearing a pink dress is sitting.\n", "The baby is eating food."], "durations": null, "exact_frames_per_prompt": [29, 51], "background": "In the background, there are chairs, tables, a purple cloth, lights, maroon wall, white wall, menu cards and people speaking and a song is audible.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/22Y6OZHPPV0.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_426_0"}, {"texts": ["A woman is talking and applying cream on her face."], "durations": null, "exact_frames_per_prompt": [80], "background": "In the background, a woman is audible. There is a white tile wall, a white door, and a white ceiling.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/2qSRaHtfVfA.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_427_0"}, {"texts": ["A man is sitting on the chair and picking paper money from the table."], "durations": null, "exact_frames_per_prompt": [80], "background": "In the background, there are plants, tables, chairs, a glass, a plate, paper money, lights, wall scenery, and a man is speaking, and there is a miscellaneous sound.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/2M_SxI_s3r4.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_42_0"}, {"texts": ["A man wearing a blue suit is standing on the stage and speaking on the mic."], "durations": null, "exact_frames_per_prompt": [80], "background": "In the background, there is a stage, walls and the ceiling, a poster, and the mike.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/-toLMKPWM8k.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_430_0"}, {"texts": ["A person whose hand is visible only, is lifting some meat and putting it\ninto the meat mincer machine."], "durations": null, "exact_frames_per_prompt": [80], "background": "In the background, there is a white bowl, meat mincer machine, black wires, minced meat, red potatoes, a blue cloth, a white container, gray wall, gray metallic countertop, sound of a running mincer machine and a miscellaneous sound is audible.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/6nuOApf8XM8.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_433_0"}, {"texts": ["A person wearing a black t-shirt is opening the artificial flower leaves."], "durations": null, "exact_frames_per_prompt": [80], "background": "In the background, music is audible. There is a white bench, an artificial flower like structure, and green grasses.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/-S4jWwhMpwY.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_435_0"}, {"texts": ["A person whose hand is visible is holding a stick and a tong and setting the kebab on the stick."], "durations": null, "exact_frames_per_prompt": [68], "background": "In the background, people are speaking and music is playing, there is a kebab stick with kabobs, a tong, green trees, a bottle and white sky.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/-3usVvvMGSY.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_436_0"}, {"texts": ["A man wearing black cloth is standing near the woman a man with a blue t-shirt is moving backward from barbecue tray"], "durations": null, "exact_frames_per_prompt": [50], "background": "In the background, music is playing. There is a green surface, blue sky, green trees, metal grill, net, black pole, parked car, buildings, bottle, roasted grill, and some other objects.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/0FYtENQd_DU.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_445_1_ms"}, {"texts": ["A man wearing a brown cloth is standing and laughing a man wearing blue clothes is standing on the left side."], "durations": null, "exact_frames_per_prompt": [42], "background": "In the background, music is playing. There is a green surface, blue sky, green trees, metal grill, net, black pole, parked car, buildings, bottle, roasted grill, and some other objects.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/0FYtENQd_DU.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_445_2_ms"}, {"texts": ["A man wearing blue clothes is standing near the second man."], "durations": null, "exact_frames_per_prompt": [42], "background": "In the background, music is playing. There is a green surface, blue sky, green trees, metal grill, net, black pole, parked car, buildings, bottle, roasted grill, and some other objects.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/0FYtENQd_DU.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_445_3_ms"}, {"texts": ["A girl wearing a white-black printed top standing on the left side is holding a glass bottle on the white table and is trying to open it with the help of scissors a girl wearing a blue dress bends down and picks the glasses from the floor."], "durations": null, "exact_frames_per_prompt": [80], "background": "In the background, there is a white window rolling shutter, a bottle, a scissor, a white printed wall, a white table, a red parked car, a black container, a grey-blue container, red disposal glass, other stuff, and the girls speaking and laughing sound is audible.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/rno1OddKJfw.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_44_0"}, {"texts": ["A girl is wearing a blue top standing while another girl is leaning on the counter top while opening a bottle", "The girl is laying down and picking up a red disposable glass.", "The girl is placing it on the table."], "durations": null, "exact_frames_per_prompt": [52, 17, 11], "background": "In the background, there is a white window rolling shutter, a bottle, a scissor, a white printed wall, a white table, a red parked car, a black container, a grey-blue container, red disposal glass, other stuff, and the girls speaking and laughing sound is audible.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/rno1OddKJfw.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_44_1"}, {"texts": ["A baby wearing a red t-shirt is laughing, sitting on the baby's high chair, and watching the dog a woman is throwing up something and the dog tries to catch it and the woman is laughing."], "durations": null, "exact_frames_per_prompt": [80], "background": "In the background, there is a couch, a baby's high chair, clothes, a brown wall, and the voice of people speaking, and laughing is audible.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/1wQX9m-gEi4.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_453_0"}, {"texts": ["A woman wearing a red t-shirt is sitting, breaking a slice of food and giving it to a dog a baby wearing a red t-shirt is laughing by looking at the dog."], "durations": null, "exact_frames_per_prompt": [80], "background": "In the background, there is a couch, a baby's high chair, clothes, a brown wall, and the voice of people speaking, and laughing is audible.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/1wQX9m-gEi4.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_453_1"}, {"texts": ["A baby wearing red cloth is sitting on a baby chair and laughing while a woman is feeding her and a brown dog"], "durations": null, "exact_frames_per_prompt": [80], "background": "In the background, people are speaking and baby laughing sounds are audible, there is a baby chair, white table, white plates, black object, brown wall and some other objects.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/1wQX9m-gEi4.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_454_0"}, {"texts": ["A brown dog is sitting and jumping a baby wearing a red t-shirt is laughing by looking at the dog."], "durations": null, "exact_frames_per_prompt": [80], "background": "In the background, people are speaking and baby laughing sounds are audible, there is a baby chair, white table, white plates, black object, brown wall and some other objects.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/1wQX9m-gEi4.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_454_2"}, {"texts": ["A baby lying in the baby stroller is being fed by a person."], "durations": null, "exact_frames_per_prompt": [80], "background": "In the background, the person and the baby are audible. There is a multi-colored baby stroller, a white tray, a white wall and a green and blue spoon.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/3sSxqG80obE.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_45_0"}, {"texts": ["A person whose hand is visible is feeding the baby with a green spoon."], "durations": null, "exact_frames_per_prompt": [80], "background": "In the background, the person and the baby are audible. There is a multi-colored baby stroller, a white tray, a white wall and a green and blue spoon.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/3sSxqG80obE.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_45_1"}, {"texts": ["A girl whose half body is visible wearing a gray t-shirt is eating a burger."], "durations": null, "exact_frames_per_prompt": [81], "background": "In the background, there is a green wall, black object, a burger and chewing sound is audible.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/3GD4LE_FTDs.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_460_0"}, {"texts": ["A girl wearing a gray t-shirt is sitting and eating a burger and then taking a bite of it."], "durations": null, "exact_frames_per_prompt": [81], "background": "In the background, miscellaneous sounds are audible. There is a burger and a green wall.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/3GD4LE_FTDs.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_461_0"}, {"texts": ["A girl wearing gray clothes is holding some food in her hand and eating."], "durations": null, "exact_frames_per_prompt": [81], "background": "In the background, miscellaneous sounds are audible, there are white walls, food and a black object.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/3GD4LE_FTDs.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_462_0"}, {"texts": ["A man wearing a light-yellow t-shirt is sitting and floating on the left side while A man wearing a brown t-shirt is standing on the backside, holding a white glove and speaking on the microphone.while a man wearing a blue t-shirt is sitting and floating on the right side and the man with the black t-shirt is floating up and down"], "durations": null, "exact_frames_per_prompt": [80], "background": "In the background, there is an interior of the space station, machines with wires, monitors, a grey wall, a grey ceiling, and blue metal bars.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/0hiu7IaIWds.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_463_1"}, {"texts": ["A man wearing a brown t-shirt is standing on the backside, holding a white glove and speaking on the microphone a man wearing a light-yellow t-shirt is sitting and floating on the left side and man wearing a blue t-shirt is sitting and floating on the right side and the man black t-shirt is floating ups and down"], "durations": null, "exact_frames_per_prompt": [80], "background": "In the background, there is an interior of the space station, machines with wires, monitors, a grey wall, a grey ceiling, and blue metal bars.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/0hiu7IaIWds.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_463_2"}, {"texts": ["A woman wearing a black t-shirt is hanging upside down on a metal bar while a man wearing a brown t-shirt is speaking and other people are smiling"], "durations": null, "exact_frames_per_prompt": [80], "background": "In the background, there is an interior of the space station, machines with wires, monitors, a grey wall, a grey ceiling, and blue metal bars.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/0hiu7IaIWds.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_463_3"}, {"texts": ["A man wearing a yellow shirt is standing on a boat holding a fishing net stick while a man with a light blue shirt is fishing"], "durations": null, "exact_frames_per_prompt": [80], "background": "In the background, miscellaneous sounds are audible, people are speaking. There is blue sky, white clouds, sea, a white boat, fishing net stick and a fishing rod.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/-B9gY3HccMk.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_465_0"}, {"texts": ["A man wearing a white shirt stands with a fishing rod and is fishing while a man in a yellow shirt is holding a fishing net and standing beside"], "durations": null, "exact_frames_per_prompt": [80], "background": "In the background, miscellaneous sounds are audible, people are speaking. There is blue sky, white clouds, sea, a white boat, fishing net stick and a fishing rod.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/-B9gY3HccMk.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_465_1"}, {"texts": ["A girl sitting on a baby chair is rubbing her lips.", "The girl starts eating noodles with a pink spoon from a bowl."], "durations": null, "exact_frames_per_prompt": [26, 54], "background": "In the background there is a baby chair, a pink plastic glass, a bowl, a spoon, cabinets, bananas, a window, kitchen utensils and appliances, a door, a yellow wall, , trees, wooden railing and a person's speaking and a girl's babbling sound is audible.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/-4CxOUPL6o4.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_468_0"}, {"texts": ["A girl is sitting on a baby's chair while holding a pink spoon in her hand.", "The girl is eating noodles from a bowl."], "durations": null, "exact_frames_per_prompt": [31, 49], "background": "In the background, there is a tree, a white door, bananas, shelves, objects on the counter, baby's chair, a pink spoon, bowls, noodles, a white table, a man's voice, a girl's voice, and some miscellaneous voices are audible.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/-4CxOUPL6o4.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_469_0"}, {"texts": ["A man wearing a red t-shirt picks up the golf ball tee and walks towards the right of the green field while the other people standing on the green surface moved backward.", "The man is standing on the green field and is about to hit the golf ball with a golf stick while the man in white shirt keeps some black object in his pocket"], "durations": null, "exact_frames_per_prompt": [59, 21], "background": "In the background, the sound of the wind is audible. There is a green grass surface, a walking path, a rope, a golf ball, a golf stick, green bushes and a standing area with metal railing.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/0eFgAUfHsEk.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_470_0"}, {"texts": ["A man wearing a white cap is standing while the man wearing a red t-shirt bends down and taking something from the ground", "The man wearing a white cap is moving on the green field and is looking at person one while the man wearing a red t-shirt starts walking towards the right side"], "durations": null, "exact_frames_per_prompt": [23, 57], "background": "In the background, the sound of the wind is audible. There is a green grass surface, a walking path, a rope, a golf ball, a golf stick, green bushes and a standing area with metal railing.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/0eFgAUfHsEk.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_470_1"}, {"texts": ["A man whose upper body is visible wearing a blue t-shirt, black goggles, and a helmet is looking in the front and is horse riding on the brown hill path."], "durations": null, "exact_frames_per_prompt": [80], "background": "In the background, there is the sound of water flowing, green trees, rocks, a river with water waving, and a brown hill path.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/2OKtc3Gl-As.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_472_0"}, {"texts": ["A brown horse whose ears are visible is walking on the brown hill path."], "durations": null, "exact_frames_per_prompt": [44], "background": "In the background, there is the sound of water flowing, green trees, rocks, a river with water waving, and a brown hill path.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/2OKtc3Gl-As.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_472_1"}, {"texts": ["A person whose hands are visible is holding a metal plate. ", "The person puts the metal plate away.", "The person lifts the wires."], "durations": null, "exact_frames_per_prompt": [40, 9, 31], "background": "In the background, a man is audible. There is a metal plate, a set of wires, a white box, and a black background.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/-i7P7G7ChAc.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_473_0"}, {"texts": ["A person whose hands are visible is holding some electrical equipment and showing it."], "durations": null, "exact_frames_per_prompt": [80], "background": "In the background, person is speaking, there is a white box, black wall, electric equipment and wooden table.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/-i7P7G7ChAc.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_474_0"}, {"texts": ["A man wearing black clothes is standing on the left side and tying the bow tie second man standing beside the black shirt is also tying the bow tie"], "durations": null, "exact_frames_per_prompt": [80], "background": "In the background, there is a white-red building, a yellow car, a black car, a tree, a red-green light, and the people speaking, laughing sounds are audible.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/8GVXmKtJp9U.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_475_0"}, {"texts": ["A man wearing a black shirt standing on the right side is moving his hands while the other man who's wearing a neck band is laughing and saying something"], "durations": null, "exact_frames_per_prompt": [80], "background": "In the background, there is a white-red building, a yellow car, a black car, a tree, a red-green light, and the people speaking, laughing sounds are audible.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/8GVXmKtJp9U.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_475_1"}, {"texts": ["A man wearing a checkered shirt is sitting.", "The man picks up a newspaper."], "durations": null, "exact_frames_per_prompt": [40, 40], "background": "In the background, a man is speaking. There is a gray door, brown window blind, a white wall and a white t-shirt in a hanger.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/7mEi5joUf0w.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_491_0"}, {"texts": ["A group of people is standing and holding red glasses."], "durations": null, "exact_frames_per_prompt": [35], "background": "In the background, there are glasses, a wooden object, a headphone, walls, a couch, a wooden door, some other stuff, and the voice of people speaking, and some miscellaneous sounds are audible.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/2G1zKFTA8m4.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_498_1_ms"}, {"texts": ["A person whose hands are visible is shaking a sushi roll in a shake mold machine."], "durations": null, "exact_frames_per_prompt": [80], "background": "In the background, there is a sushi roll mold machine, a table, a blue designer cloth, a red cup, yellow object, brown tile floor, a chair, egg tray, white tray, some other stuff and people's voices and shaking sound is audible.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/3Or9260xH8A.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_49_0"}, {"texts": ["A girl wearing white printed top is sitting, speaking.", "The girl starts licking her hand."], "durations": null, "exact_frames_per_prompt": [48, 27], "background": "In the background, there is a table, a plate filled with food, a zipper, trees, rocks, waste, soil surface and people speaking and the girl's voice is audible.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/1_OOsNcx5tQ.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_4_0"}, {"texts": ["A girl wearing white clothes is sitting on a brown chair and is peeling an apple using a green peeler while a girl wearing a pink t-shirt is standing and helps person one in peeling the apple."], "durations": null, "exact_frames_per_prompt": [80], "background": "In the background, the girls are speaking and miscellaneous sounds are audible. There is a gray table, a green peeler, an apple, a brown packet, an orange wall, white fridge, kitchen cabinets, an oven, wall decor, a white paper, apple peel, a brown chair and a glass bowl.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/3I8zuOPeod4.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_500_0"}, {"texts": ["A girl wearing pink clothes is standing wile girl wearing white t-shirt is sitting on a brown chair and is peeling an apple using a green peeler.", "The girl helps person one in peeling the apple."], "durations": null, "exact_frames_per_prompt": [62, 18], "background": "In the background, the girls are speaking and miscellaneous sounds are audible. There is a gray table, a green peeler, an apple, a brown packet, an orange wall, white fridge, kitchen cabinets, an oven, wall decor, a white paper, apple peel, a brown chair and a glass bowl.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/3I8zuOPeod4.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_500_1"}, {"texts": ["A man wearing a denim shirt is holding the peeler while the kids are rotating the peeler"], "durations": null, "exact_frames_per_prompt": [80], "background": "In the background, the girls are speaking and miscellaneous sounds are audible. There is a gray table, a green peeler, an apple, a brown packet, an orange wall, white fridge, kitchen cabinets, an oven, wall decor, a white paper, apple peel, a brown chair and a glass bowl.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/3I8zuOPeod4.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_500_2"}, {"texts": ["A man is wearing black clothes is sitting and holding a baby in his lap while the woman in red t-shirt takes toys from bag and shows to the baby"], "durations": null, "exact_frames_per_prompt": [80], "background": "In the background, people are speaking, there are red white walls, gray surface, a lamp, a yellow wall, a black stairs, a light yellow sofa, a box, toys and some other objects.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/-coRoBr2_cI.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_503_0"}, {"texts": ["A woman wearing red clothes is taking a toy from a box baby trying to eat paper"], "durations": null, "exact_frames_per_prompt": [80], "background": "In the background, people are speaking, there are red white walls, gray surface, a lamp, a yellow wall, a black stairs, a light yellow sofa, a box, toys and some other objects.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/-coRoBr2_cI.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_503_2"}, {"texts": ["A baby boy wearing a designer t-shirt is standing on a chair and cutting a watermelon slice into small pieces with a knife on the table.", "The baby boy wearing a designer t-shirt starts screaming."], "durations": null, "exact_frames_per_prompt": [55, 26], "background": "In the background, there is a brown table, watermelon pieces, a chair, a plate, a white wall, a brown door, a desk, another chair, some other stuff, and the baby's voice is audible.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/Ae08WvfTcF0.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_505_0"}, {"texts": ["A person whose hands are only visible is making a knot from a green plastic thread."], "durations": null, "exact_frames_per_prompt": [80], "background": "In the background there is a white paper, a grey surface, green plastic thread and a person's speaking and some music sounds are audible.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/0d7WFfVA7TA.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_507_0"}, {"texts": ["A man wearing a black t-shirt is standing on a black carpet and juggling with glass bottles while the other man has a white cloth on the shoulder picking up the bottles."], "durations": null, "exact_frames_per_prompt": [80], "background": "In the background there is a black carpet, grey surface, banners, a stall, some objects on the stall, two men are shouting and music playing in the background.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/4hQSTJYwHno.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_511_0"}, {"texts": ["A man on the left side wearing a black t-shirt is sitting on the grey surface while the man in a black t-shirt is playing with the bottles.", "The man moves forward to pick up the glass bottle as the man in black t-shirt takes the glass from the table"], "durations": null, "exact_frames_per_prompt": [41, 39], "background": "In the background there is a black carpet, grey surface, banners, a stall, some objects on the stall, two men are shouting and music playing in the background.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/4hQSTJYwHno.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_511_1"}, {"texts": ["A man wearing a black t-shirt and blue jeans is standing, starts flair bartending a man sitting down enjoy that movement", "The man drops a glass another man trying to move forward", "The man lifts another glass other man searching for something on the floor", "The man again starts flair bartending other man move the table right a bit"], "durations": null, "exact_frames_per_prompt": [30, 20, 13, 17], "background": "In the background, the music, clapping sound, and people are audible. There are posters, a bar counter, glasses, bottles, green light in the back, a white floor, and a black carpet floor.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/4hQSTJYwHno.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_512_0"}, {"texts": ["A man wearing a black t-shirt and light blue jeans is sitting.  while a man wearing black t-shirt and blue jeans is playing with the bottles", "The man is moving on the ground on the left side then the man picked up a bottle from the table and rotated it."], "durations": null, "exact_frames_per_prompt": [42, 38], "background": "In the background, the music, clapping sound, and people are audible. There are posters, a bar counter, glasses, bottles, green light in the back, a white floor, and a black carpet floor.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/4hQSTJYwHno.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_512_1"}, {"texts": ["A woman standing in the kitchen is mixing corn, capsicum and onion in a glass bowl."], "durations": null, "exact_frames_per_prompt": [80], "background": "In the background, the woman is speaking. There is a brown counter, brown walls, posters, an oven, white counter, silver tap, silver sink, kitchen cabinets, white drawers, a black stove, a glass bowl, white bowls, a notepad, a white box, mixed vegetables and there is also a logo in the video.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/2lRD0VCqdfE.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_516_0"}, {"texts": ["A girl wearing a white top is standing on her knees and holding a long piece of paper towards the paper shredder.", "The girl moves her hands."], "durations": null, "exact_frames_per_prompt": [65, 15], "background": "In the background, girls are speaking, there is a wooden floor, papers, a wooden stool with a printer, grey-black speakers, a wooden table with shelf and some stuff, a black chair, a grey-silver paper shredder, a yellow-white box, a white sack, and a grey wall.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/6UMI9fX2Iew.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_519_0"}, {"texts": ["A man wearing an off white shirt is standing, first inserting a hive.", "The man is lifting the hive from a hive box while holding a metal piece in his hand."], "durations": null, "exact_frames_per_prompt": [62, 19], "background": "In the background, there is a hive box, green plants, a metal piece, soil surface, a container, men are speaking, and some miscellaneous sounds are audible.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/59J7V_YRBG8.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_525_0"}, {"texts": ["A woman wearing a white-black top and a black blazer is sitting on a black chair and speaking while looking in front."], "durations": null, "exact_frames_per_prompt": [34], "background": "In the background, there are screens, a fence of glass, a dark gray border, blue lights, light gray border, cream surface, a smartphone, some written words in white, the voice of the woman and the sound of the music is audible.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/-Ikki5WXc20.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_526_0_ms"}, {"texts": ["A person whose hands are visible is using a smartphone."], "durations": null, "exact_frames_per_prompt": [34], "background": "In the background, there are screens, a fence of glass, a dark gray border, blue lights, light gray border, cream surface, a smartphone, some written words in white, the voice of the woman and the sound of the music is audible.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/-Ikki5WXc20.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_526_1_ms"}, {"texts": ["A baby wearing white clothes is sitting on the man's lap and drinking from the transparent glass while holding the glass."], "durations": null, "exact_frames_per_prompt": [80], "background": "In the background, there is a green sofa and the glass.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/2L2yGXV8LLo.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_527_0"}, {"texts": ["A man wearing a white shirt is holding the baby on his lap and making him drink from the glass."], "durations": null, "exact_frames_per_prompt": [80], "background": "In the background, there is a green sofa and the glass.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/2L2yGXV8LLo.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_527_1"}, {"texts": ["A man wearing white cloth is sitting and holding a glass and a baby in his lap and giving a glass to drink water."], "durations": null, "exact_frames_per_prompt": [80], "background": "In the background, people are speaking, there is a gray sofa and a glass.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/2L2yGXV8LLo.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_528_0"}, {"texts": ["A baby wearing white cloth is sitting in the lap of the man and drinking water."], "durations": null, "exact_frames_per_prompt": [80], "background": "In the background, people are speaking, there is a gray sofa and a glass.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/2L2yGXV8LLo.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_528_1"}, {"texts": ["A man wearing white red costume is standing and conducting an orchestra."], "durations": null, "exact_frames_per_prompt": [80], "background": "In the background, there are chairs, tables, musical instruments, musical notes, white ceiling, lights, gray-brown wall, a window, a photo frame and music is audible.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/2NFjzbUfR2k.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_534_0"}, {"texts": ["A man wearing a white shirt is standing on a gray floor while the man in blue t-shirt is trimming sheep's wool", "The man wearing a white shirt is watching the second person."], "durations": null, "exact_frames_per_prompt": [70, 10], "background": "In the background, sheep's voices are audible, trimming sounds are audible. There are black walls, windows, barriers, gray bag with sheep hair, a blue motor, gray surface, a bottle dispenser and wires.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/0WT9sICw61A.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_535_0"}, {"texts": ["A man wearing a blue t-shirt is trimming a sheep's hair with a trimmer."], "durations": null, "exact_frames_per_prompt": [80], "background": "In the background, sheep's voices are audible, trimming sounds are audible. There are black walls, windows, barriers, gray bag with sheep hair, a blue motor, gray surface, a bottle dispenser and wires.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/0WT9sICw61A.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_535_1"}, {"texts": ["A white sheep is lying on the gray floor and getting hair trimmed by the second person and the man in white shirt is monitoring the task performed by the first person"], "durations": null, "exact_frames_per_prompt": [80], "background": "In the background, sheep's voices are audible, trimming sounds are audible. There are black walls, windows, barriers, gray bag with sheep hair, a blue motor, gray surface, a bottle dispenser and wires.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/0WT9sICw61A.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_535_3"}, {"texts": ["A man wearing a blue t-shirt is standing, leaning forward, and shaving a sheep."], "durations": null, "exact_frames_per_prompt": [80], "background": "In the background, trimmer and sheep baa are audible. There is a trimmer, a red carpet, a white bag of wool, railings, a white surface, windows, a wooden divider, a white bottle, and a black shed.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/0WT9sICw61A.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_536_0"}, {"texts": ["A white sheep lying on the floor is being shaved by the first man."], "durations": null, "exact_frames_per_prompt": [80], "background": "In the background, trimmer and sheep baa are audible. There is a trimmer, a red carpet, a white bag of wool, railings, a white surface, windows, a wooden divider, a white bottle, and a black shed.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/0WT9sICw61A.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_536_1"}, {"texts": ["A woman wearing a white t-shirt is peeling the peel of the brown-yellow fruit with her hands."], "durations": null, "exact_frames_per_prompt": [80], "background": "In the background, there is a brown-yellow fruit, a fan, a white-brown ceiling, a white wall, a woman's voice, and the sound of television is audible.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/3GdY3KgWmmE.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_539_0"}, {"texts": ["A boy wearing a white t-shirt is sitting on the bed.", "A boy wearing a white t-shirt is reading and flipping the book pages."], "durations": null, "exact_frames_per_prompt": [19, 61], "background": "In the background there is a brown wooden bed, a book, a white blanket, a white pillow, a grey wall, and the boy speaking sound is audible.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/1Ct5JVPONwY.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_542_0"}, {"texts": ["A man wearing black clothes comes and opens the tyre nozzle cap.", "The man wearing black clothes inserts a silver pressure gauge in the nozzle."], "durations": null, "exact_frames_per_prompt": [64, 16], "background": "In the background, the man is speaking. There is a gray floor, a blue car and a silver pressure gauge.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/2g3ViPY2xlg.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_543_0"}, {"texts": ["A woman wearing a black t-shirt standing on the right side is speaking and caressing the dog while the other women is standing on the left side and touching the dogs hair"], "durations": null, "exact_frames_per_prompt": [80], "background": "In the background people are speaking. There is a white wall, a window, a mirror, a brown table with some stuff, white containers, a spray bottle and a logo.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/-SygWyDFFXQ.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_544_0"}, {"texts": ["A woman wearing a black t-shirt standing on the left side is caressing the dog another women in right side talking about the dog"], "durations": null, "exact_frames_per_prompt": [71], "background": "In the background people are speaking. There is a white wall, a window, a mirror, a brown table with some stuff, white containers, a spray bottle and a logo.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/-SygWyDFFXQ.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_544_1"}, {"texts": ["A white dog is standing and moving its head.\n as the women in black t-shirt are playing with its fur"], "durations": null, "exact_frames_per_prompt": [67], "background": "In the background people are speaking. There is a white wall, a window, a mirror, a brown table with some stuff, white containers, a spray bottle and a logo.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/-SygWyDFFXQ.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_544_3"}, {"texts": ["A man wearing a shirt is drinking something from white glass."], "durations": null, "exact_frames_per_prompt": [80], "background": "In the background, there are lights, a white glass, and the voice of people speaking, laughing, and some miscellaneous sounds are audible.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/04NyBK_C_h0.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_545_0"}, {"texts": ["A man wearing a sky blue shirt drinks something from the white cup."], "durations": null, "exact_frames_per_prompt": [17], "background": "In the background, people are audible. There is a white cup, lamps, and black wall.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/04NyBK_C_h0.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_546_0"}, {"texts": ["A man wearing a black dress is ice fishing on the snow surface with an ice fishing gear."], "durations": null, "exact_frames_per_prompt": [80], "background": "In the background, there is a snow surface, green trees, mountains, water, blue sky, the voices of the men and some miscellaneous sounds are audible.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/35bIxGrNzok.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_549_0"}, {"texts": ["A person wearing black cloth is standing on a white surface and holding a fishing rope and taking out from the water."], "durations": null, "exact_frames_per_prompt": [80], "background": "In the background, people are speaking, there is a white surface, blue sky and black mountain.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/35bIxGrNzok.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_550_0"}, {"texts": ["A woman wearing a tan cloth is sitting on the left side while holding a glass of beer while a man wearing a black cloth is sitting in the middle raising his one hand and a man wearing a black shirt is sitting holding a glass of beer and stroking it with the woman's glass.", "The woman is striking with the glass of second man on the right side.", "The woman starts drinking while the third person raise an hand of second person as soon as he completely drink the glass of beer."], "durations": null, "exact_frames_per_prompt": [15, 10, 55], "background": "In the background, there is a green plant, electric lights, black grill, wooden platform, glass jar, a flower bouquet, a table, beer glasses, people are speaking, and shouting voices are audible.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/3dDOwh5hj1I.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_552_0"}, {"texts": ["A man wearing a black cloth is sitting in the middle, raising his one hand while a woman wearing a tan cloth is sitting on the left side holding a glass of beer and a man wearing a black shirt is sitting, holding a glass of beer and stroking it with the woman's glass.", "The man is holding the hand of the second man and raising it while the woman puts down her glass."], "durations": null, "exact_frames_per_prompt": [55, 25], "background": "In the background, there is a green plant, electric lights, black grill, wooden platform, glass jar, a flower bouquet, a table, beer glasses, people are speaking, and shouting voices are audible.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/3dDOwh5hj1I.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_552_1"}, {"texts": ["A man wearing a black shirt is sitting, holding a glass of beer and stroking it with the woman's glass the man wearing white and black T-shirt is encouraging to drink", "The man wearing a black shirt starts to drink it the woman starts drinking.", "The man in a black t-shirt raised the hand of the second man the woman finishes the drink and put the glass on the table"], "durations": null, "exact_frames_per_prompt": [22, 34, 24], "background": "In the background, there is a green plant, electric lights, black grill, wooden platform, glass jar, a flower bouquet, a table, beer glasses, people are speaking, and shouting voices are audible.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/3dDOwh5hj1I.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_552_2"}, {"texts": ["A woman wearing dark blue t-shirt is standing behind the counter-top and chopping the pineapple on the wooden chopping board with the help of knife."], "durations": null, "exact_frames_per_prompt": [80], "background": "In the background, there is a pineapple, a wooden chopping board, a knife, a brown counter-top, a yellow wall with a blue design and written words, and a woman speaking sound is audible.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/1X_LV5o_krk.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_556_0"}, {"texts": ["A person whose hand is only visible, is holding a spoon and feeding the food to the baby."], "durations": null, "exact_frames_per_prompt": [80], "background": "In the background, there is a white floor, brown surface, blue baby cot, a cream object, the voice of the person and a miscellaneous sound is audible.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/2hJJDGKmJc0.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_558_0"}, {"texts": ["A baby wearing a white diaper is lying on the blue baby cot.", "The baby wearing a white diaper fed by the person."], "durations": null, "exact_frames_per_prompt": [48, 32], "background": "In the background, there is a white floor, brown surface, blue baby cot, a cream object, the voice of the person and a miscellaneous sound is audible.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/2hJJDGKmJc0.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_558_1"}, {"texts": ["A baby wearing a diaper is lying on a printed baby bed and licking his hands and getting fed by a person with a spoon."], "durations": null, "exact_frames_per_prompt": [80], "background": "In the background, there is a printed baby bed, a spoon, white tile floor, brown surface and baby's licking and the person's voice is audible.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/2hJJDGKmJc0.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_559_0"}, {"texts": ["A woman wearing a black sleeveless jacket is sitting in the front and moving her hand."], "durations": null, "exact_frames_per_prompt": [40], "background": "In the background, people are speaking, the music is playing, there is a light-yellow wall with a green design, a brown board with papers and photographs, a window, a food counter, a table with baskets and some stuff, a picture frame, a black board with written words, a light-yellow wall, a green wall, a light-yellow ceiling, lights, a shelf, hanging baskets, and a clock.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/-Pc62CZZMlI.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_55_0_ms"}, {"texts": ["A group of six people, five people are standing around the food counter and one person is taking something from the shelf then starts walking to the left."], "durations": null, "exact_frames_per_prompt": [40], "background": "In the background, people are speaking, the music is playing, there is a light-yellow wall with a green design, a brown board with papers and photographs, a window, a food counter, a table with baskets and some stuff, a picture frame, a black board with written words, a light-yellow wall, a green wall, a light-yellow ceiling, lights, a shelf, hanging baskets, and a clock.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/-Pc62CZZMlI.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_55_1_ms"}, {"texts": ["A kid wearing a yellow t-shirt is standing and rotating the handle of a rotary peeler machine and peeling the potato while the baby in a multi color t-shirt is watching and smiling"], "durations": null, "exact_frames_per_prompt": [45], "background": "In the background, there is a cupboard, a rack filled with containers, white wall, a peeler machine, a wooden box, a counter-top, a peeled potato, some other stuff, and the kids screaming and a woman laughing sound and some miscellaneous sounds are audible.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/0T3YtsWWEBM.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_565_0"}, {"texts": ["A boy wearing a yellow t-shirt is standing and operating a peeling machine baby boy standing beside the first boy shouting with excitement"], "durations": null, "exact_frames_per_prompt": [45], "background": "In the background, there are bottles, a rack, a wooden object, red peeling machine, fruit, a wall, a table, peel, box, red object, a boy is shouting, a woman's laughing and peeling machine sounds are audible.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/0T3YtsWWEBM.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_566_0"}, {"texts": ["A boy wearing a lining t-shirt is standing on the right side of the first boy."], "durations": null, "exact_frames_per_prompt": [45], "background": "In the background, there are bottles, a rack, a wooden object, red peeling machine, fruit, a wall, a table, peel, box, red object, a boy is shouting, a woman's laughing and peeling machine sounds are audible.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/0T3YtsWWEBM.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_566_1"}, {"texts": ["A person whose hand is visible only, wearing a white cloth, is holding the wheel rim and then repeating it."], "durations": null, "exact_frames_per_prompt": [76], "background": "In the background, there is a red-black motorbike, green wall, gray surface, voice of the person and a miscellaneous sound is audible.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/1n1TpFPG7TA.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_56_0"}, {"texts": ["A baby wearing a blue t-shirt is moving their hands and lying on the black surface girl wearing a orange top trying to stop the baby's crying"], "durations": null, "exact_frames_per_prompt": [80], "background": "In the background, there is a book, gray walls, a black surface, a pump cap bottle, some other objects, and the voice of a girl speaking, and crying sound is audible.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/0QQEp8jAnFI.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_570_0"}, {"texts": ["A girl wearing an orange top is standing while a infant wearing blue colour t -shirt is crying", "The girl is moving her hand."], "durations": null, "exact_frames_per_prompt": [62, 18], "background": "In the background, there is a book, gray walls, a black surface, a pump cap bottle, some other objects, and the voice of a girl speaking, and crying sound is audible.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/0QQEp8jAnFI.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_570_1"}, {"texts": ["A man whose only half body is visible is wearing a gray t-shirt standing and taking out green tea from the paper bag with a spoon then putting it into a jar.", "The man puts the paper bag aside on the table.", "The man is placing his hands on the table."], "durations": null, "exact_frames_per_prompt": [34, 13, 33], "background": "In the background, there is a brown table, a paper bag, a jar, a kettle and the man is speaking.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/525tFmwP6Ic.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_571_0"}, {"texts": ["A person whose only hands are visible is folding a yellow paper with their hands."], "durations": null, "exact_frames_per_prompt": [80], "background": "In the background, there is a yellow paper, a white surface, and the sound of music is audible.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/-orvHtyWiEU.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_573_0"}, {"texts": ["A man is wearing white cloth sitting on a chair and eating bread."], "durations": null, "exact_frames_per_prompt": [61], "background": "In the background, music is playing, there is a wooden floor, gray wall, white window, wooden stair handle, white table, white chairs, plates, bottles, glasses, brown surface, green plants, and some other objects.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/3jKFKV7kQ6E.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_574_0"}, {"texts": ["A person wearing blue jeans and black gloves is holding a shearing machine and doing sheep shearing."], "durations": null, "exact_frames_per_prompt": [42], "background": "In the background, there are posters and banners, wooden platform, iron frames, and the wool.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/-BagREykXFM.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_575_3_ms"}, {"texts": ["Another person wearing blue trousers is holding a sheep between his legs."], "durations": null, "exact_frames_per_prompt": [42], "background": "In the background, there are posters and banners, wooden platform, iron frames, and the wool.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/-BagREykXFM.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_575_5_ms"}, {"texts": ["A sheep is lying on the wooden surface and being sheared by the person wearing black gloves."], "durations": null, "exact_frames_per_prompt": [41], "background": "In the background, there are posters and banners, wooden platform, iron frames, and the wool.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/-BagREykXFM.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_575_6"}, {"texts": ["A person whose hand is visible is holding a bike tyre.", "The person whose hand is visible is rotating the bike tyre."], "durations": null, "exact_frames_per_prompt": [67, 9], "background": "In the background, a person is speaking. There is a bike, a white wall, a gray surface.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/1n1TpFPG7TA.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_57_0"}, {"texts": ["A man is standing and holding a juicer and grinding meat in it."], "durations": null, "exact_frames_per_prompt": [80], "background": "In the background, people are speaking and some miscellaneous sounds are audible. There is a juicer machine, a white bowl with meat, and brown surface.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/6SU1P-ehJYU.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_582_0"}, {"texts": ["A big boy whose head is visible wearing a black jacket and specs, is showing the sweets in the spoon while holding a spoon in his right hand.", "And then starts eating the sweets."], "durations": null, "exact_frames_per_prompt": [35, 21], "background": "In the background, a boy is speaking, there is the sound of a spoon, a black door curtain hanging, white walls, a white ceiling, a white surface with sweets, and sweets, and a steel spoon.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/1pd9-tf17NE.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_585_0"}, {"texts": ["A man wearing a green t-shirt and gray jeans is walking while pointing his hands towards the second man the two person wearing a black t-shirts are speaking in a mic", "The man is speaking while a man wearing a red jacket went forward and turned around"], "durations": null, "exact_frames_per_prompt": [37, 28], "background": "In the background, people are speaking, there is the sound of cheering and clapping, a white ceiling with brown moldings and light fixtures, light brown walls, a brown door, a brown counter, a steel jug, trophies and other stuff on the counter, and a mic with a black wire.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/-2jqFdD__ag.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_588_0"}, {"texts": ["A man wearing a black cloth, is standing and speaking on the mic while holding a mic.", "The man starts clapping hands."], "durations": null, "exact_frames_per_prompt": [35, 31], "background": "In the background, people are speaking, there is the sound of cheering and clapping, a white ceiling with brown moldings and light fixtures, light brown walls, a brown door, a brown counter, a steel jug, trophies and other stuff on the counter, and a mic with a black wire.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/-2jqFdD__ag.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_588_2"}, {"texts": ["A person whose hand is visible is plucking a green fruit from a tree."], "durations": null, "exact_frames_per_prompt": [49], "background": "In the background, there are trees, building structure, a gray surface, lights with poles, the sky, a red train, gray stairs, bushes, railings, a tree with fruit, and the voice of a man speaking, and the sound of music is audible.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/-tajNaB6B7Y.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_590_1_ms"}, {"texts": ["A person on the left side whose hand is only visible is stroking his hand on a brown sea lion."], "durations": null, "exact_frames_per_prompt": [80], "background": "In the background there is a metal boat cleat, a blue deck, a sea, wrist watch, a grey pole and people's voices are audible.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/1bsRsgz_3zk.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_592_0"}, {"texts": ["A person on the right side whose hand is only visible is stroking his hand on a brown sea lion."], "durations": null, "exact_frames_per_prompt": [58], "background": "In the background there is a metal boat cleat, a blue deck, a sea, wrist watch, a grey pole and people's voices are audible.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/1bsRsgz_3zk.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_592_1"}, {"texts": ["A brown sea lion is lying on a blue deck while a person on the right side whose hand is only visible is stroking his hand on a brown sea lion."], "durations": null, "exact_frames_per_prompt": [80], "background": "In the background there is a metal boat cleat, a blue deck, a sea, wrist watch, a grey pole and people's voices are audible.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/1bsRsgz_3zk.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_592_2"}, {"texts": ["A person whose hand is visible is pouring food into the yellow liquid.", "The person is then picking food with a fork."], "durations": null, "exact_frames_per_prompt": [62, 18], "background": "In the background, there is a fork, a white plate, and the voice of a person speaking, and music is audible.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/3MOfpmr7L8M.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_593_0"}, {"texts": ["A girl whose only hands are visible is breaking an egg and putting egg yolk into a mixing jar."], "durations": null, "exact_frames_per_prompt": [49], "background": "In the background, there is a mixing jar filled with flour, a blender, egg tray, eggs, a countertop, a red plastic bag, white-black floor and girls voices are audible.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/1u1rHQL28Ao.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_595_0"}, {"texts": ["A woman wearing a white coat and a yellow dress is standing putting a pink folded cloth on the table.", "The woman is moving her hands, and showing the cloth basket."], "durations": null, "exact_frames_per_prompt": [37, 23], "background": "In the background, the music is playing. There is a basket full of folded clothes, a green table, and a green wall.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/19i7oyiKVG0.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_59_0"}, {"texts": ["A kid wearing a red t-shirt and blue jeans is sitting and holding a snake a person's whose hand is only visibling is trying to touch the snake", "The snake is then getting snatched by a person."], "durations": null, "exact_frames_per_prompt": [44, 33], "background": "In the background, there is a graphic carpet, chairs, gray surface, voices of the people and a miscellaneous sound is audible.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/5mGZ_3kHcd0.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_602_0"}, {"texts": ["A kid wearing a white shirt and cream pants is sitting on the right side of the first kid and touching the snake a kid in red t-shirt holding the snake"], "durations": null, "exact_frames_per_prompt": [63], "background": "In the background, there is a graphic carpet, chairs, gray surface, voices of the people and a miscellaneous sound is audible.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/5mGZ_3kHcd0.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_602_1"}, {"texts": ["A person wearing blue jeans is standing on the left side of the second kid while a boy wearing a red t-shirt holding a yellow-black snake", "The person starts snatching the snake from the first kid."], "durations": null, "exact_frames_per_prompt": [50, 27], "background": "In the background, there is a graphic carpet, chairs, gray surface, voices of the people and a miscellaneous sound is audible.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/5mGZ_3kHcd0.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_602_3"}, {"texts": ["A yellow-black snake is getting held by a kid and the kid sitting beside the first kid is touching the snake while a person wearing blue jeans takes off the snake from the first kid."], "durations": null, "exact_frames_per_prompt": [65], "background": "In the background, there is a graphic carpet, chairs, gray surface, voices of the people and a miscellaneous sound is audible.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/5mGZ_3kHcd0.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_602_4"}, {"texts": ["A woman whose hands are visible is mixing her food with a grey spoon.", "The woman picking up a glass bowl filled with black wooden beads."], "durations": null, "exact_frames_per_prompt": [48, 32], "background": "In the background, there are glass bowls, a grey spoon, white floor, a woman is speaking, music is playing, and there is a miscellaneous sound.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/1q6GwJ7ZxIs.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_604_0"}, {"texts": ["A man wearing a black t-shirt is standing on the right side of the stove.", "The man is inserting eggs into a wok on the white stove."], "durations": null, "exact_frames_per_prompt": [19, 61], "background": "In the background, there is a white wall, a gas stove, a metal wok, eggs, a glass, a wooden tray, a blue cup, a black bottle, white cupboards, and brown object, a man is speaking, and cooking sounds are audible.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/2_mqSmwUUik.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_606_0"}, {"texts": ["A girl wearing a pink top is looking and pointing her hand towards the fishes."], "durations": null, "exact_frames_per_prompt": [69], "background": "In the background, a woman is speaking and the girl is also speaking. There are gray rocks, water and small green plants.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/0bpbDvTs4is.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_607_1"}, {"texts": ["A man wearing beige color trousers is standing, speaking, holding a golf club stick and teaching golf."], "durations": null, "exact_frames_per_prompt": [80], "background": "In the background, there are trees, the sky, a green surface, and the voice of people speaking, and some miscellaneous sounds are audible.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/0zTnlcsbP-M.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_609_0"}, {"texts": ["A woman wearing a brown top and blue jeans is standing and feeding the birds."], "durations": null, "exact_frames_per_prompt": [80], "background": "In the background, birds chirping, people, and miscellaneous sounds are audible. There is a yellow fire water hydrant, a brown wooden floor, green-yellow grasses, white railings, a gray box, poles, and a blue sky with clouds.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/-0EXMXaj77A.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_60_0"}, {"texts": ["A woman wearing a sweater and trousers is standing on the left and holding things while a man is laughing"], "durations": null, "exact_frames_per_prompt": [19], "background": "In the background, people are audible. There are houses, windows, stairs, metal railings, bags, chairs, and a brown wall.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/1yNF2AahQEg.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_610_0_ms"}, {"texts": ["A man wearing a suit is standing on the right and smiling while the woman is holding some packages in both the hands."], "durations": null, "exact_frames_per_prompt": [18], "background": "In the background, people are audible. There are houses, windows, stairs, metal railings, bags, chairs, and a brown wall.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/1yNF2AahQEg.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_610_1_ms"}, {"texts": ["A man wearing a black suit is sitting in a chair.", "The man raises his hand.\n", "The man puts it down."], "durations": null, "exact_frames_per_prompt": [28, 5, 27], "background": "In the background, people are audible. There are houses, windows, stairs, metal railings, bags, chairs, and a brown wall.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/1yNF2AahQEg.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_610_2_ms"}, {"texts": ["A person whose legs are visible is lying on a white bed and getting a tattoo on the left foot."], "durations": null, "exact_frames_per_prompt": [80], "background": "In the background, the sound of the tattoo machine is audible. There is a white wall, a white bed, a tattoo machine, gray wall, white curtains, a white object and mirrors.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/19Cl27Wpf4I.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_611_0"}, {"texts": ["A man wearing a black t-shirt is standing and making a tattoo on the first person's left foot with a tattoo machine."], "durations": null, "exact_frames_per_prompt": [80], "background": "In the background, the sound of the tattoo machine is audible. There is a white wall, a white bed, a tattoo machine, gray wall, white curtains, a white object and mirrors.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/19Cl27Wpf4I.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_611_1"}, {"texts": ["A man wearing a black shirt is sitting and making a tattoo on the leg of a woman with a tattoo machine and wiping with the tissue paper."], "durations": null, "exact_frames_per_prompt": [80], "background": "In the background, there is a white wall, a mirror, a basin, relaxing chair, tattoo machine, a bed, and the voice of the tattoo machine is audible.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/19Cl27Wpf4I.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_612_0"}, {"texts": ["A person whose hand is visible is writing on a wall."], "durations": null, "exact_frames_per_prompt": [80], "background": "In the background, there is a white wall with words written on it, grey pavement, grey road, a block object and the voice of a person laughing and talking is audible.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/1fNnSAka2Ec.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_616_0"}, {"texts": ["A girl wearing a white pink dress is standing holding a white cup and feeding a green parakeet."], "durations": null, "exact_frames_per_prompt": [80], "background": "In the background, people are speaking, birds chirping sounds are audible. There are trees, a black net cage, white sky, stones, a black container, a yellow board and a gray surface.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/0nYkK11fDZk.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_618_0"}, {"texts": ["A green parakeet is eating from a white cup while girl wearing pink dotted frock feeding the bird"], "durations": null, "exact_frames_per_prompt": [80], "background": "In the background, people are speaking, birds chirping sounds are audible. There are trees, a black net cage, white sky, stones, a black container, a yellow board and a gray surface.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/0nYkK11fDZk.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_618_1"}, {"texts": ["A woman wearing green-blue clothes is standing and holding cups and feeding the birds."], "durations": null, "exact_frames_per_prompt": [80], "background": "In the background, people are speaking and birds are chirping and miscellaneous sounds are audible, there is a black surface, a water pipe, green surface, pillars, poles, barriers and white sky.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/-0EXMXaj77A.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_61_0"}, {"texts": ["A girl on the right side is sitting in front of a yellow cabinet facing left with the other two girls"], "durations": null, "exact_frames_per_prompt": [80], "background": "In the background there is a wooden cabinet, a wooden shelf, a sink with a tap, a tissue box, a curtain, a metal frame, bottles, stickers, a blue wall, file folders, books, plastic baskets, some other objects and people's voices are audible.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/1MvYfgPOAzI.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_622_0"}, {"texts": ["A girl on the left side wearing a blue top and spectacles is sitting in front of a yellow cabinet while a girl is reading and sitting in between the first and the second girl."], "durations": null, "exact_frames_per_prompt": [80], "background": "In the background there is a wooden cabinet, a wooden shelf, a sink with a tap, a tissue box, a curtain, a metal frame, bottles, stickers, a blue wall, file folders, books, plastic baskets, some other objects and people's voices are audible.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/1MvYfgPOAzI.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_622_1"}, {"texts": ["A girl is reading and sitting in between the first and the second girl while the two girls were watching the middle girl's actions."], "durations": null, "exact_frames_per_prompt": [80], "background": "In the background there is a wooden cabinet, a wooden shelf, a sink with a tap, a tissue box, a curtain, a metal frame, bottles, stickers, a blue wall, file folders, books, plastic baskets, some other objects and people's voices are audible.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/1MvYfgPOAzI.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_622_2"}, {"texts": ["A woman whose only eye and head is visible is brushing her eyebrow with a makeup brush."], "durations": null, "exact_frames_per_prompt": [80], "background": "In the background there is a makeup brush, and the music playing sound is audible.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/-nLtSqMcmvo.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_624_0"}, {"texts": ["A girl wearing a blue white top is sitting and holding an ice cream cone in her left hand."], "durations": null, "exact_frames_per_prompt": [80], "background": "In the background, people are speaking. There is a white object, an ice cream cone, a shop, a white pillar, a brown floor, a brown counter and a gray table.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/07KR3jta1Uo.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_627_0"}, {"texts": ["A woman wearing a blue shirt is standing on white floor.", "The woman picks up her food from a silver tray.", "The woman puts it into a black pan."], "durations": null, "exact_frames_per_prompt": [12, 34, 34], "background": "In the background, there is a silver tray, black pan, stove, brown cabinets, a glass bottle, a fork, a silver kettle, a brown gate, grey slab, a cloth, a chimney, a woman is speaking and music is playing.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/2mzLvIuhIUE.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_628_0"}, {"texts": ["A man wearing blue cloth is sitting and speaking and moving his hand."], "durations": null, "exact_frames_per_prompt": [80], "background": "In the background, a man is speaking, there are white walls, white door and white ceiling.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/6GQQo-Zw2hw.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_631_0"}, {"texts": ["A man wearing a dark blue vest is standing and tying a boxing hand wrap round his hand."], "durations": null, "exact_frames_per_prompt": [80], "background": "In the background, the man is speaking. There is a blue boxing ring, white surface, white ropes, white walls, a blue and a red punching bag and a white hand wrap. There is also a logo in the video.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/-oww2bPbZIE.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_632_0"}, {"texts": ["A man wearing a black suit is walking on the right side, holding a remote and moving his hands."], "durations": null, "exact_frames_per_prompt": [81], "background": "In the background, the man is speaking, there is a blue-white-green weather display with written words.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/05MI7UlnY4A.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_635_0"}, {"texts": ["A man wearing a black t-shirt is sitting and speaking a women in black glasses is moving her hand and saying something"], "durations": null, "exact_frames_per_prompt": [58], "background": "In the background, people are speaking and shouting and the sound of glass breaking is also audible. There is a marble surface, a white paper, a broken glass, a brown object, a white wall, windows, a silver tap, small plants, an off-white wall, kitchen cabinets, green trees and a white sky.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/1zNdXTAHOzE.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_637_0"}, {"texts": ["A woman wearing specs is also speaking."], "durations": null, "exact_frames_per_prompt": [45], "background": "In the background, people are speaking and shouting and the sound of glass breaking is also audible. There is a marble surface, a white paper, a broken glass, a brown object, a white wall, windows, a silver tap, small plants, an off-white wall, kitchen cabinets, green trees and a white sky.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/1zNdXTAHOzE.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_637_1"}, {"texts": ["A man wearing a black shirt is putting butter into the pan.", "The man is tossing the pan, while the food is being cooked."], "durations": null, "exact_frames_per_prompt": [44, 36], "background": "In the background, the man is speaking and a miscellaneous sound is audible. There is a red wall, a hanger with kitchen tools hangs on it, a black stove, a black pan, a white tray, white bowls, a chopping board, white plates, utensils, a silver beater, a glass jar with red chili, a brown counter, miscellaneous objects, a silver fridge, and oven and two books. There is also a logo in the video.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/1HYTGM88W3o.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_639_0"}, {"texts": ["A woman wearing a blue vest is sitting, talking, chewing something, and eating chips."], "durations": null, "exact_frames_per_prompt": [80], "background": "In the background, a woman and miscellaneous sounds are audible. There is a chip, white walls, a red curtain, a room decorative piece, a red-black vase, brown stairs, a beige sofa, a black backpack, a white ceiling, a white smoke sensor, and a few miscellaneous things.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/2eWnvXJWT_4.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_63_0"}, {"texts": ["A man wearing a black shirt is opening a butter packet.", "The man is putting butter in the pan.", "The man is tossing the pan."], "durations": null, "exact_frames_per_prompt": [23, 22, 35], "background": "In the background, the man is speaking, a sizzling sound is audible. There is a red wall, a white wall, an oven, a fridge, a black counter top, gas stoves, a pan, white bowls with some stuff in it , a brown counter with some stuff, drawers, a mixing machine, utensils, a chopping board, a glass jar and posters.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/1HYTGM88W3o.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_640_0"}, {"texts": ["A woman whose upper body is visible wearing a brown woolen sweater is standing and is visible in the mirror and is holding a mobile phone and recording the boy on a mobile phone camera."], "durations": null, "exact_frames_per_prompt": [61], "background": "In the background, there is a black table fan, a black mobile phone, a blue-pink printed white cloth-covered object, a wooden brown door, white walls, a wooden tile floor, and other stuff are shown in the mirror, a face cream, and a wooden wall wardrobe with the mirror.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/4u_u3VPnGDM.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_642_0"}, {"texts": ["A boy whose upper body is visible, wearing a purple jacket, is applying the face cream to his cheek with his finger and smiling while a woman is recording in the mobile phone, which is reflecting in a mirror"], "durations": null, "exact_frames_per_prompt": [80], "background": "In the background, there is a black table fan, a black mobile phone, a blue-pink printed white cloth-covered object, a wooden brown door, white walls, a wooden tile floor, and other stuff are shown in the mirror, a face cream, and a wooden wall wardrobe with the mirror.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/4u_u3VPnGDM.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_642_1"}, {"texts": ["A woman wearing a white top is standing.", "The woman is trying to flip the food in the black pan."], "durations": null, "exact_frames_per_prompt": [35, 23], "background": "In the background, the woman is speaking and laughing and another person is also audible. There is a gray floor, brown kitchen cabinets, a black chair, a black pan, utensils, a window, a brown door, a fruit basket, white walls and a white cloth.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/-LxaYFnEu2U.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_646_0"}, {"texts": ["A girl wearing a white top is standing on a tile floor while holding a pan with some stuff in it.", "The girl is tossing the stuff in the pan and then starts laughing."], "durations": null, "exact_frames_per_prompt": [33, 25], "background": "In the background, miscellaneous sounds are audible. There are wooden cabinets, and countertop with some stuff on it, a window, a white wall, a wooden door, a black chair, a white cloth, a pan and a gray tile floor.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/-LxaYFnEu2U.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_647_0"}, {"texts": ["A man wearing a navy blue jacket and brown pants is standing and speaking while holding a coin.", "The man is sitting on the right side of the car.", "The man is placing the coin in between the tire tread."], "durations": null, "exact_frames_per_prompt": [21, 35, 24], "background": "In the background, there is a white wall, red border, glass window, gray surface, black car, white car, white-black written words in metals and the voice of the man is audible.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/30FW-Um5sSk.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_654_0"}, {"texts": ["A woman wearing a graphic black sweatshirt is dancing while lip-syncing."], "durations": null, "exact_frames_per_prompt": [81], "background": "In the background, there is a white door, brown wall and the sound of music is audible.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/2L4l0WoTntQ.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_655_0"}, {"texts": ["A girl wearing a blue cloth is walking and touching the goat inside the mesh wire cage while a small black goat is running away from the girl."], "durations": null, "exact_frames_per_prompt": [80], "background": "In the background, there is a wooden fencing with some stuff, mesh wire cage, a sky, green grass surface, a blue shed, and the people speaking sound is audible.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/0_4zOwgabn4.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_656_0"}, {"texts": ["A brown baby goat is standing while a baby wearing a blue t-shirt is playing with the goats.", "The brown baby goat starts walking on the green grass surface inside the mesh wire cage while the baby falls down"], "durations": null, "exact_frames_per_prompt": [27, 53], "background": "In the background, there is a wooden fencing with some stuff, mesh wire cage, a sky, green grass surface, a blue shed, and the people speaking sound is audible.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/0_4zOwgabn4.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_656_1"}, {"texts": ["A man wearing blue clothes is pulling the thread out of the hole on the snowy surface."], "durations": null, "exact_frames_per_prompt": [80], "background": "In the background, there is snowy surface, and the hole.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/2eeQoqp7YWw.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_657_0"}, {"texts": ["A man is sitting on the right side of another man.", "The man is eating a burger man beside first man is also eating burger and talking something with the first man"], "durations": null, "exact_frames_per_prompt": [15, 65], "background": "In the background, there is a white wall, a counter, red wall, green leaves, tile floor, burgers, lights, and voices of the people and some miscellaneous sounds are audible.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/-1z6wTJmBIU.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_662_0"}, {"texts": ["A man wearing a black shirt is sitting on the left side of the first man while the man wearing blue-shirt is eating the burger", "The man is wiping his mouth with a tissue paper.", "The man is holding a burger in his hand."], "durations": null, "exact_frames_per_prompt": [27, 28, 23], "background": "In the background, there is a white wall, a counter, red wall, green leaves, tile floor, burgers, lights, and voices of the people and some miscellaneous sounds are audible.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/-1z6wTJmBIU.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_662_1"}, {"texts": ["A man wearing a blue shirt is adjusting his bow tie."], "durations": null, "exact_frames_per_prompt": [41], "background": "In the background, music is playing, the man is speaking. There is a brown wall.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/8cDQ_8nLMPU.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_670_0"}, {"texts": ["A person whose hand is visible, is putting a paper in a shredding machine and shredding it."], "durations": null, "exact_frames_per_prompt": [42], "background": "In the background, shredding machine sounds are audible, there are white curtains, white surface, a shredding machine and a white wire.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/5F01Ci5yMLQ.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_671_0"}, {"texts": ["A person whose only a hand is visible is holding a brush and brushing the back and the tail of a horse."], "durations": null, "exact_frames_per_prompt": [80], "background": "In the background, there is soil surface, grey and black walls, brown bears, and the voice of a person speaking is audible.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/2P3foaIntm8.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_675_0"}, {"texts": ["A white horse is getting its back and tail brushed by a person."], "durations": null, "exact_frames_per_prompt": [80], "background": "In the background, there is soil surface, grey and black walls, brown bears, and the voice of a person speaking is audible.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/2P3foaIntm8.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_675_1"}, {"texts": ["A man wearing a white t-shirt and black pants is standing on the gray surface and holding a black snake."], "durations": null, "exact_frames_per_prompt": [80], "background": "In the background, there is a dark brown metallic door, dark brown metallic window, white wall, gray surface, a wiper, a broom, a red-yellow sac, a sea-green object, another sea-green object, the voices of the people and a miscellaneous sound is audible.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/3x-E0J1yjX4.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_680_0"}, {"texts": ["A black snake is getting caught by a man."], "durations": null, "exact_frames_per_prompt": [80], "background": "In the background, there is a dark brown metallic door, dark brown metallic window, white wall, gray surface, a wiper, a broom, a red-yellow sac, a sea-green object, another sea-green object, the voices of the people and a miscellaneous sound is audible.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/3x-E0J1yjX4.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_680_1"}, {"texts": ["A man wearing a white t-shirt is standing on the gray surface and holding a snake in his hands."], "durations": null, "exact_frames_per_prompt": [80], "background": "In the background, there are windows, a wooden door, white walls, and gray surface, some other stuff, and the voice of people speaking is audible.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/3x-E0J1yjX4.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_681_0"}, {"texts": ["A snake is being held by a man."], "durations": null, "exact_frames_per_prompt": [80], "background": "In the background, there are windows, a wooden door, white walls, and gray surface, some other stuff, and the voice of people speaking is audible.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/3x-E0J1yjX4.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_681_1"}, {"texts": ["A boy whose upper half-body is visible, wearing a black vest is sitting on a chair on the left side and eating the brown food while a girl whose upper half-body is visible, wearing a blue top is standing on the right side, putting something in her mouth and starts walking towards the right."], "durations": null, "exact_frames_per_prompt": [60], "background": "In the background, a girl is speaking; there is the sound of chewing, white walls, a counter with brown drawers, a white gas stove and some stuff, brown cupboards, a glass window, gray chairs, a brown wardrobe behind the boy on the left side, a photo frame on the wall, wires, and a brown floor.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/2F4jKXQDFtk.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_684_0"}, {"texts": ["A girl whose upper half-body is visible, wearing a blue top is standing on the right side, putting something in her mouth and eating while a boy whose upper half-body is visible wearing a black vest is sitting on a chair on the left side and eating the brown food.", "The girl starts walking towards the right."], "durations": null, "exact_frames_per_prompt": [39, 8], "background": "In the background, a girl is speaking; there is the sound of chewing, white walls, a counter with brown drawers, a white gas stove and some stuff, brown cupboards, a glass window, gray chairs, a brown wardrobe behind the boy on the left side, a photo frame on the wall, wires, and a brown floor.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/2F4jKXQDFtk.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_684_1"}, {"texts": ["A man wearing a blue t-shirt, black shorts, and a cap is climbing on an apple tree.", "The man is plucking the green apples with his hands."], "durations": null, "exact_frames_per_prompt": [26, 19], "background": "In the background, music is playing. There are green apple trees, green apples in the poly bags and a brown tub, another brown tub with apples, the sky, transparent poly bags, and a green grass surface.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/-nvFKzOKsxs.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_687_1"}, {"texts": ["A woman wearing a royal blue dress and a black blazer is standing on the right side of the screen and forecasting the weather report on the screen.", "The woman is turning and going backward while speaking."], "durations": null, "exact_frames_per_prompt": [21, 15], "background": "In the background, there is a big screen, a weather report, a logo and the voice of the woman is audible.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/0DpD7CHbhrw.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_688_0"}, {"texts": ["A woman wearing a blue dress is standing, holding something in her hand", "A woman wearing a blue dress starts walking toward the right."], "durations": null, "exact_frames_per_prompt": [26, 9], "background": "In the background, there is a screen showing the weather report forecast, and the voice of a woman speaking is audible.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/0DpD7CHbhrw.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_689_0"}, {"texts": ["A girl wearing a red t-shirt is standing and holding a peeled potato, then holding a grey bucket and speaking while the man on right with red jacket and green t-shirt is holding a drilling machine"], "durations": null, "exact_frames_per_prompt": [80], "background": "In the background, there is a bucket, hand drill attached with brush, potatoes, water, cupboards, cooking stove, white countertop, white tile wall, some other stuff, tile floor and the girl's and boy's voices and some music is audible.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/0VtQ4hcBhpU.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_68_0"}, {"texts": ["A man wearing a black t-shirt is playing golf on a\n golf course and is giving a high-five."], "durations": null, "exact_frames_per_prompt": [41], "background": "In the background, the man is audible and another person is laughing. There are black boundaries, a green field, a floor, miscellaneous objects, a golf stick, green trees and a blue sky.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/0MgG4dV56Ik.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_695_0"}, {"texts": ["A baby wearing a purple hair band is eating a colorful cake while sitting on a printed brown baby chair."], "durations": null, "exact_frames_per_prompt": [80], "background": "In the background there are chairs with tray, kitchen utensils, poster frames, a cake, kitchen appliances, wooden cabinets, grey walls, brown counter-top and people's speaking and laughing voices are audible.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/4mnnAWp42cg.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_704_0"}, {"texts": ["A person on the left side wearing a blue t-shirt is holding the cake in her hand in front of the first baby while the baby is eating the cake."], "durations": null, "exact_frames_per_prompt": [80], "background": "In the background there are chairs with tray, kitchen utensils, poster frames, a cake, kitchen appliances, wooden cabinets, grey walls, brown counter-top and people's speaking and laughing voices are audible.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/4mnnAWp42cg.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_704_3"}, {"texts": ["A child wearing a blue hoodie is sitting on the floor.", "The child is tearing gift wrap from a box."], "durations": null, "exact_frames_per_prompt": [21, 59], "background": "In the background, the woman is speaking, a man is speaking, the child is speaking, there is a light-brown floor, a light-brown wall with a white border, gifts, bookshelves, some boxes, gift wraps, and a brown-grey table.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/1exarhq0Tlg.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_705_0"}, {"texts": ["A man wearing a pink t-shirt is standing and is eating food and also a man wearing blue t-shirt on the right side eating food"], "durations": null, "exact_frames_per_prompt": [80], "background": "In the background, people are audible. There is a brown floor, brown walls, sofas, a black chair, a white poster, a red table, glass, cutlery, tissue papers, brandings and white lights.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/2S5DMUMvkyw.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_706_0"}, {"texts": ["A man wearing a pink t-shirt is standing and eating a hot dog, in a competition with the first man."], "durations": null, "exact_frames_per_prompt": [80], "background": "In the background there are brown panels, some posters, chairs, a brown surface, a white board, a plant, papers and a bottle, a red table-top and people laughing, crying and speaking sounds are audible.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/2S5DMUMvkyw.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_707_1"}, {"texts": ["A man wearing a black shirt, black pants, and black shoes is sitting on the camel's back while holding a rope with his left hand and a man with a blue shirt is walking along with the camel"], "durations": null, "exact_frames_per_prompt": [46], "background": "In the background, people are speaking; there is a blue sky, a big pyramid, retaining walls, rocks, and a soil surface.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/19DdD2JaU-Q.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_708_0"}, {"texts": ["A camel is standing on the soil surface while carrying the first man on his back while a man wearing black jacket sitting on the camel and saying something"], "durations": null, "exact_frames_per_prompt": [40], "background": "In the background, people are speaking; there is a blue sky, a big pyramid, retaining walls, rocks, and a soil surface.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/19DdD2JaU-Q.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_708_2"}, {"texts": ["A big boy wearing a gray t-shirt and dark blue shorts is sitting while the girls wearing black t-shirts sitting on the floor and trying to eat doughnuts.", "The big boy wearing a gray t-shirt and dark blue shorts is standing while holding donuts in his right hand while all the others are trying to catch the doughnuts with their mouth.", "The big boy wearing a gray t-shirt and dark blue shorts walks on the gay surface while a girl wearing black and brown t-shirt catches a doughnut and eats"], "durations": null, "exact_frames_per_prompt": [14, 18, 48], "background": "In the background, people are speaking; there is the sound of screaming, donuts are hanging with a thin rope, a black surface table with stuff on the left side, a box, a black wardrobe, and other stuff on the wardrobe are on the right side, white walls with a white pillar, a mobile phone, a switch board on the wall, a black wire plug; and a concrete gray surface.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/3S4RfsZcmGM.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_709_2"}, {"texts": ["A horse is walking from left to right behind the person."], "durations": null, "exact_frames_per_prompt": [32], "background": "In the background, there is a metal fence with wooden logs, a blue sky with clouds, a yellow-red shine in the sky, and a grass surface.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/0e2OdCAOwe8.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_711_1"}, {"texts": ["A person whose only a hand is visible is turning the page of a book."], "durations": null, "exact_frames_per_prompt": [80], "background": "In the background, there is a wooden surface, and a book. There is the sound of a male voice in the background.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/1BTY5leiQFY.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_722_0"}, {"texts": ["A person whose hand is visible is writing on a white paper with a yellow pencil."], "durations": null, "exact_frames_per_prompt": [80], "background": "In the background, miscellaneous sounds are audible. There is a white paper, a brown surface and a yellow pencil.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/2bGbK9LY3Jw.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_724_0"}, {"texts": ["A baby wearing a graphic black dress is sitting on the gray surface and eating food."], "durations": null, "exact_frames_per_prompt": [72], "background": "In the background, there are white tables, gray chairs, white ceiling, light, a wooden log, some stuff, doors of glasses, gray surface, food, a black slipper, the voices of the people and the sound of music is audible.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/0qLW6rdj_Fk.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_726_0"}, {"texts": ["A man wearing a green sweatshirt is sitting and eating food."], "durations": null, "exact_frames_per_prompt": [80], "background": "In the background, there is a brown sitting chair, a grey wall, a glass, a white ceiling, pillars, a brown object, a straw, and the people speaking, baby crying, woman laughing sounds are audible.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/1gS5Udgciy0.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_727_0"}, {"texts": ["A woman wearing a bracelet is getting a tattoo on her wrist and a woman wearing a red t-shirt is holding the bracelet hand"], "durations": null, "exact_frames_per_prompt": [80], "background": "In the background there is a chair, a yellow wall, a tattoo machine, a black object, some wires, a beige surface, people speaking and a tattoo machine sound is audible.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/-36nz7JP2Ac.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_729_0"}, {"texts": ["A person wearing black gloves is tattooing a woman's wrist."], "durations": null, "exact_frames_per_prompt": [76], "background": "In the background there is a chair, a yellow wall, a tattoo machine, a black object, some wires, a beige surface, people speaking and a tattoo machine sound is audible.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/-36nz7JP2Ac.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_729_1"}, {"texts": ["A man wearing a gray-black t-shirt and a red-white apron is standing in the kitchen and giving instructions to the kid."], "durations": null, "exact_frames_per_prompt": [51], "background": "In the background, a man and a kid are audible. There is a wooden rolling pin, white dough, a white kitchen counter, brown surface, white cupboards, and a few miscellaneous kitchen items.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/-GKnuhPmiwM.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_72_0"}, {"texts": ["A kid wearing a black t-shirt and a red-white apron is standing near the kitchen counter, holding a wooden rolling pin and rolling the dough and a man is guiding the kid holding a wooden rolling pin and rolling the dough"], "durations": null, "exact_frames_per_prompt": [51], "background": "In the background, a man and a kid are audible. There is a wooden rolling pin, white dough, a white kitchen counter, brown surface, white cupboards, and a few miscellaneous kitchen items.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/-GKnuhPmiwM.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_72_1"}, {"texts": ["A man wearing black clothes is standing and holding a golf stick.", "The man is hitting a ball with his golf stick.", "The man is arranging the ball platform."], "durations": null, "exact_frames_per_prompt": [14, 44, 22], "background": "In the background, music is playing, there is a green surface, a golf ball platform and a golf stick.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/06rZT_6Rc5Q.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_731_0"}, {"texts": ["A person whose legs are visible is standing and playing golf.", "The person is using a line putting mirror."], "durations": null, "exact_frames_per_prompt": [56, 24], "background": "In the background, there is a golf line putting mirror, golf stick, a green grass surface, a golf ball, and the music playing sound is audible.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/06rZT_6Rc5Q.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_732_0"}, {"texts": ["A man wearing cap whose reflection is visible in the golf line putting mirror is putting white object into the small mirror holes."], "durations": null, "exact_frames_per_prompt": [22], "background": "In the background, there is a golf line putting mirror, golf stick, a green grass surface, a golf ball, and the music playing sound is audible.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/06rZT_6Rc5Q.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_732_1"}, {"texts": ["A woman is talking and showing tweezers."], "durations": null, "exact_frames_per_prompt": [80], "background": "In the background, a woman is audible. There is a tweezer and a white wall.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/1-2iEw6vVB0.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_733_0"}, {"texts": ["A woman whose head is visible is holding a plucker and showing it."], "durations": null, "exact_frames_per_prompt": [80], "background": "In the background, a woman speaking is audible, there are white walls and a plucker.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/1-2iEw6vVB0.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_734_0"}, {"texts": ["A woman wearing a black top is sitting on the first camel's back and a woman wearing a purple top is siting beside the another camel", "The woman hugged by the other woman and the woman wearing pink and black t-shirt waving the hand"], "durations": null, "exact_frames_per_prompt": [69, 11], "background": "In the background, there is a sand surface, blue sky, a tent house, poles, and a white building.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/-W9-MeRbLeA.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_738_2"}, {"texts": ["A woman wearing a black top is coming from the left side and then hugs the first woman."], "durations": null, "exact_frames_per_prompt": [14], "background": "In the background, there is a sand surface, blue sky, a tent house, poles, and a white building.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/-W9-MeRbLeA.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_738_3"}, {"texts": ["A man wearing a blue jacket is sitting in the car, holding a taco in his hand and mixing sauce on the taco."], "durations": null, "exact_frames_per_prompt": [80], "background": "In the background, there are trees, sky, a taco, a green packet, a building, cars, black seat, car roof, steering, car door, rear mirror, a man's voice is audible.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/0-x8UaESVnY.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_739_0"}, {"texts": ["A man wearing a black t-shirt is sitting on the right side and speaking."], "durations": null, "exact_frames_per_prompt": [67], "background": "In the background, there is a railing, a window, a grey ceiling, a grey surface, a light brown wall, a white wall, a brown wall, and a brown ceiling, a brick wall with some advertisement, and the man speaking sound is audible.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/2X9FfApT3Eg.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_740_0"}, {"texts": ["A man wearing a dark blue t-shirt is sitting on the left side speaking.", "The man starts drinking. "], "durations": null, "exact_frames_per_prompt": [30, 37], "background": "In the background, there is a railing, a window, a grey ceiling, a grey surface, a light brown wall, a white wall, a brown wall, and a brown ceiling, a brick wall with some advertisement, and the man speaking sound is audible.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/2X9FfApT3Eg.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_740_1_ms"}, {"texts": ["A brown horse is walking and carrying a boy upon him while a woman in a brown jacket holds the horse's leash and turns on the sand surface."], "durations": null, "exact_frames_per_prompt": [40], "background": "In the background, wind blowing sounds and a boy and woman speaking sounds are audible, there is a green surface, soil surface, yellow surface, wooden barriers, blue sky with white clouds and some other objects.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/07e-mCJlvDw.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_748_0"}, {"texts": ["A boy wearing green clothes is riding the horse while a woman wearing a black sweater is holding the leash and moving on the ground."], "durations": null, "exact_frames_per_prompt": [40], "background": "In the background, wind blowing sounds and a boy and woman speaking sounds are audible, there is a green surface, soil surface, yellow surface, wooden barriers, blue sky with white clouds and some other objects.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/07e-mCJlvDw.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_748_1"}, {"texts": ["A woman wearing brown clothes is standing in the center and holding a rope and watching the horse while a boy wearing a green jacket is sitting on the brown horse and looking at the woman."], "durations": null, "exact_frames_per_prompt": [40], "background": "In the background, wind blowing sounds and a boy and woman speaking sounds are audible, there is a green surface, soil surface, yellow surface, wooden barriers, blue sky with white clouds and some other objects.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/07e-mCJlvDw.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_748_2"}, {"texts": ["A woman wearing a dark brown jacket, dark blue jeans, and goggles is holding the horse by a rope with her left hand and holding a stick in her right hand and a boy in a green jacket is riding the horse."], "durations": null, "exact_frames_per_prompt": [40], "background": "In the background, birds are chirping; there is the sound of the air, green trees, a blue sky with clouds, wooden fencing, a soil surface, a pile of soil covered with a sheet, and a green grass surface.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/07e-mCJlvDw.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_749_0"}, {"texts": ["A boy wearing a black jacket, black jeans, a black helmet, and boots is sitting on the horse's back while holding the horse's harness and riding a horse on the soil surface while a woman wearing a brown jacket is holding the horse's harness and moving on the soil surface."], "durations": null, "exact_frames_per_prompt": [40], "background": "In the background, birds are chirping; there is the sound of the air, green trees, a blue sky with clouds, wooden fencing, a soil surface, a pile of soil covered with a sheet, and a green grass surface.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/07e-mCJlvDw.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_749_1"}, {"texts": ["A brown horse is walking on the soil surface while carrying a boy on its back while a woman wearing a brown jacket and blue jeans is standing on the soil surface and holding a red rope in her left hand and a horsewhip in her right hand."], "durations": null, "exact_frames_per_prompt": [40], "background": "In the background, birds are chirping; there is the sound of the air, green trees, a blue sky with clouds, wooden fencing, a soil surface, a pile of soil covered with a sheet, and a green grass surface.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/07e-mCJlvDw.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_749_2"}, {"texts": ["A yellow python is held by a group of people."], "durations": null, "exact_frames_per_prompt": [80], "background": "In the background, there are chairs, a white tile floor, a stick, a yellow python, and the people speaking and laughing sounds are audible.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/-e_t-PH4AOM.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_750_3"}, {"texts": ["A man wearing a black shirt is standing and knotting his tie while two men are knotting the tie, a woman wearing a black y-shirt is standing and filming with a camera, and a woman wearing a printed top is looking at the men."], "durations": null, "exact_frames_per_prompt": [80], "background": "In the background, people are audible. There are mirrors, pink walls, windows, a green plant, a camera, white ceiling, a ceiling fan with a bulb, shelves, books, red flowers, a flower pot, storage boxes, a computer screen, a brown wooden table, and papers with few miscellaneous things.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/1c9Vb0MI4OI.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_75_2"}, {"texts": ["A woman wearing a black top is standing and holding a camera while a man in a black shirt is tying a tie, the other man helps the third man to tie a tie and the other woman wearing spectacles is standing with the men and watching them."], "durations": null, "exact_frames_per_prompt": [46], "background": "In the background, people are audible. There are mirrors, pink walls, windows, a green plant, a camera, white ceiling, a ceiling fan with a bulb, shelves, books, red flowers, a flower pot, storage boxes, a computer screen, a brown wooden table, and papers with few miscellaneous things.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/1c9Vb0MI4OI.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_75_5"}, {"texts": ["A man wearing an orange t-shirt is standing in a bending position, holding a trimmer and trimming the sheep's hair."], "durations": null, "exact_frames_per_prompt": [80], "background": "In the background, music is playing and a trimmer sound is audible. There is a soil surface, a metal barrier, green plants, green grass surface, a parked car, sheep hair, a brown road and a trimmer.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/-QIB4pgBNuA.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_767_0"}, {"texts": ["A sheep is lying on a soil surface and getting its hair trimmed by the man."], "durations": null, "exact_frames_per_prompt": [80], "background": "In the background, music is playing and a trimmer sound is audible. There is a soil surface, a metal barrier, green plants, green grass surface, a parked car, sheep hair, a brown road and a trimmer.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/-QIB4pgBNuA.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_767_1"}, {"texts": ["A man wearing orange clothes is holding a trimmer and trimming the hairs of a sheep."], "durations": null, "exact_frames_per_prompt": [80], "background": "In the background, music is playing and trimmer sounds are audible, there is a soil surface, a metal barrier, green plants, a parked car, sheep hairs, a brown road and a blue object.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/-QIB4pgBNuA.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_768_0"}, {"texts": ["A sheep is getting hair trimmed by the man."], "durations": null, "exact_frames_per_prompt": [80], "background": "In the background, music is playing and trimmer sounds are audible, there is a soil surface, a metal barrier, green plants, a parked car, sheep hairs, a brown road and a blue object.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/-QIB4pgBNuA.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_768_1"}, {"texts": ["A person whose hand is visible is writing on the white board with a pen."], "durations": null, "exact_frames_per_prompt": [50], "background": "In the background, there is a white board, a pen, and the sound of music is audible.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/1W2-thF1RX8.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_777_0"}, {"texts": ["A girl wearing a blue t-shirt is sitting, speaking, and doing hand gestures on the chair."], "durations": null, "exact_frames_per_prompt": [50], "background": "In the background, there is a blue curtain, a poster, a white wall, a chair, black wires, and the voice of a girl speaking is audible.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/8FN0X4e4FOg.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_778_0_ms"}, {"texts": ["A girl wearing a blue t-shirt is sitting, speaking, and doing hand gestures on the chair."], "durations": null, "exact_frames_per_prompt": [49], "background": "In the background, there is a blue curtain, a poster, a white wall, a chair, black and white wires, and a girl speaking is audible.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/8FN0X4e4FOg.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_779_0_ms"}, {"texts": ["A man wearing a blue t-shirt is standing on a green mat, holding a golf club.", "The man wearing a blue t-shirt is playing a golf shot."], "durations": null, "exact_frames_per_prompt": [24, 56], "background": "In the background, miscellaneous sounds are audible, people are speaking. There is a green grass field, trees, a white surface, a golf club, a golf ball, a net basket, black poles with nets, white sky, and a green mat.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/09_k1eXA-U8.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_77_0"}, {"texts": ["A person whose only hands are visible is holding a bowl, picks an object from a bowl.", "The person puts it on a plate."], "durations": null, "exact_frames_per_prompt": [40, 40], "background": "In the background, there is a glass bowl, a plate, a ring, a small glass bowl, a surface, some food, the voice of a person speaking, and music is audible.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/2d1qX_z7nps.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_780_0"}, {"texts": ["A man wearing a greenish-white shirt is holding the lid of the barbeque grill and speaking while holding a tong.", "The man closes the lid of the barbeque grill."], "durations": null, "exact_frames_per_prompt": [53, 8], "background": "In the background, there are trees, grass and bushes, plants and flowers, a barbeque grill, a tong and the food. There is the sound of male voice in the background.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/0OpeXEmne1o.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_781_0"}, {"texts": ["A boy is holding a fork and picking butter from a bowl and pouring over a bread and spreading it."], "durations": null, "exact_frames_per_prompt": [80], "background": "In the background, people are speaking, there is a white wall, and electric board, a toaster machine, wires, a white object, bowl, a basin, and a brown counter-top.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/5-hila5PdW0.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_782_0"}, {"texts": ["A person wearing a white apron is standing and making sushi using a bamboo mat on a white chopping board."], "durations": null, "exact_frames_per_prompt": [80], "background": "In the background there is a knife, a white chopping board, brown counter-top, some leaves, shredded carrots, a bamboo mat, boiled rice, metal trays, some bottles, a brown table, some objects and people speaking voices and some miscellaneous sounds are audible.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/2k3n5OqKlrA.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_784_0"}, {"texts": ["A man wearing a black cloth is sitting on the floor and moving his hand towards the yellow stuff and looking at the boy while a boy is standing on the right side and reading a paper loudly."], "durations": null, "exact_frames_per_prompt": [80], "background": "In the background, the boy is speaking, a woman is laughing, there is a black tiled floor, green walls, windows with some objects, some yellow stuff, and a metal container.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/0PKC1pzBCFA.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_785_0"}, {"texts": ["A boy is standing in the front and reading a newspaper while a man wearing black clothes is sitting and looking in the direction of the boy."], "durations": null, "exact_frames_per_prompt": [80], "background": "In the background, the boy is speaking, a woman is laughing, there is a black tiled floor, green walls, windows with some objects, some yellow stuff, and a metal container.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/0PKC1pzBCFA.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_785_1"}, {"texts": ["A man whose hand and head is visible, is holding a pair of tongs and grilling some shrimps and meat on the barbeque."], "durations": null, "exact_frames_per_prompt": [80], "background": "In the background, there is a barbeque, a silver tray, shrimps, meats, a cauldron with food, wooden barrier, green trees, sky, a red stick, a red bowl, black slab, the voice of the man and a miscellaneous sound is audible.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/0y8Ni1jXW-Y.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_78_0"}, {"texts": ["A man wearing a striped t-shirt is standing near the table and peeling the potato while a group of people including kids are standing, sitting, and moving and a stopwatch on a tablet is running."], "durations": null, "exact_frames_per_prompt": [80], "background": "In the background, there is a table, glass bottles, potatoes, walls, and the speakers.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/4obTIAbhTeE.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_795_0"}, {"texts": ["A boy wearing a printed t-shirt is looking on the iPad in which a timer is running while a group of people in which some people are sitting, a man is standing and peeling a potato and another man is standing and drinking and two people are standing behind him."], "durations": null, "exact_frames_per_prompt": [80], "background": "In the background, there is a table, glass bottles, potatoes, walls, and the speakers.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/4obTIAbhTeE.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_795_2"}, {"texts": ["A boy wearing a yellow t-shirt is sitting on the bed, and trying to fold the cloth."], "durations": null, "exact_frames_per_prompt": [72], "background": "In the background, there is a brown bed, a printed bed-sheet, a white pillow, a pink pillow, a light brown wall, clothes, and the woman speaking, a girl speaking sound is audible.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/-qlJmXRti-g.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_798_1"}, {"texts": ["A man whose only hand is visible is holding a tong and turning over the food on the grill machine."], "durations": null, "exact_frames_per_prompt": [80], "background": "In the background, there is a cloudy sky, plants, a black grill machine, foods, a tong, a red stick, a brown wooden wall, a man's voice and sizzling sounds are audible.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/0y8Ni1jXW-Y.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_79_0"}, {"texts": ["A man wearing a black t-shirt and red shorts is sitting and opening a tape."], "durations": null, "exact_frames_per_prompt": [80], "background": "In the background, a person and miscellaneous sounds are audible. There is white tape, a white bandage, a cream wall, a white surface, and garbage.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/-R5bm4G2SpQ.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_7_0"}, {"texts": ["A man wearing green clothes is sitting while the other man in green top is holding a glass.", "The man is holding a glass.", "The man is cheering up.", "The man is drinking."], "durations": null, "exact_frames_per_prompt": [21, 32, 18, 9], "background": "In the background, people speaking are audible, there are yellow-green walls, photo frames, two glass, a black table, a black-brown sofa, a yellow paper and some bottles.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/2RS9CrgQftk.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_801_0"}, {"texts": ["A man wearing black clothes is sitting and holding a glass while another man wearing a green shirt is sitting on a brown sofa while speaking and then holding a glass.", "The man wearing black clothes is cheering up and then drinking the man wearing a green shirt is cheering up and then drinking."], "durations": null, "exact_frames_per_prompt": [56, 24], "background": "In the background, people speaking are audible, there are yellow-green walls, photo frames, two glass, a black table, a black-brown sofa, a yellow paper and some bottles.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/2RS9CrgQftk.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_801_1"}, {"texts": ["A baby girl wearing a pink dress is sitting on a couch, holding a pink cloth and playing with it."], "durations": null, "exact_frames_per_prompt": [80], "background": "In the background, there is a brown couch, clothes, a cushion, a wooden table, some other stuff, a grey wall, and a woman's and baby's voices are audible.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/0mUIY4K-98c.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_805_0"}, {"texts": ["A woman wearing a maroon dress is sitting on a wooden floor with a cat on her lap and her hand in the front.", "The woman then starts caressing the cat."], "durations": null, "exact_frames_per_prompt": [29, 51], "background": "In the background, the cat is purring, there is a wooden floor, a black table with some stuff, wires, black object, a white wall, wooden shelf, and a brown rug.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/0qhFXtmwx9A.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_806_0"}, {"texts": ["A white-black cat is lying in the lap of the woman.", "The cat is getting caressed by the woman."], "durations": null, "exact_frames_per_prompt": [29, 51], "background": "In the background, the cat is purring, there is a wooden floor, a black table with some stuff, wires, black object, a white wall, wooden shelf, and a brown rug.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/0qhFXtmwx9A.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_806_1"}, {"texts": ["A man wearing a white t-shirt is standing, places a glass on a white napkin.", "The man places a long clear glass on the table, and lifts the first glass again."], "durations": null, "exact_frames_per_prompt": [40, 40], "background": "In the background, the music and people are audible. There are glasses filled with drinks, white napkins, empty glasses, decorative lights, a table, a bar counter, a white wall, and a few miscellaneous things.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/0jNFcr6wvFk.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_809_0"}, {"texts": ["A person whose hand is visible is caressing a dog."], "durations": null, "exact_frames_per_prompt": [62], "background": "In the background, the person is speaking and miscellaneous sounds are audible. There is a gray floor, a brown couch and a black object.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/-iGWHpZ2TV4.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_811_0"}, {"texts": ["A dog lying on the brown couch is being caressed by the person."], "durations": null, "exact_frames_per_prompt": [80], "background": "In the background, the person is speaking and miscellaneous sounds are audible. There is a gray floor, a brown couch and a black object.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/-iGWHpZ2TV4.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_811_1"}, {"texts": ["A man wearing a brown-black t-shirt is standing on a soil golf course, moving his hand.", "The man starts playing golf."], "durations": null, "exact_frames_per_prompt": [43, 37], "background": "In the background, the man is speaking, there is a soil golf course, green trees, hill ridge, and a blue sky.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/1BmPuA4_AGk.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_813_0"}, {"texts": ["A boy wearing an orange t-shirt is standing in front of the goats and feeding them the food."], "durations": null, "exact_frames_per_prompt": [80], "background": "In the background, there is a green surface, soil surface, gray road, a fence, the voices of the people and a miscellaneous sound is audible.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/0T_zBZCfq74.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_816_0"}, {"texts": ["A dark brown-white goat is standing on the left side of the boy and eating food by the boy while a white goat on the right side hits the brown-white goat.", "The goat walks away while other white-grey goat comes from the left and starts eating the food by the boy and white goat walks towards the left."], "durations": null, "exact_frames_per_prompt": [21, 59], "background": "In the background, there is a green surface, soil surface, gray road, a fence, the voices of the people and a miscellaneous sound is audible.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/0T_zBZCfq74.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_816_1"}, {"texts": ["A light brown-white goat is standing on the right side of the first goat and eating the food by the boy while another brown-white goat is coming from behind, towards the boy, and eating the food."], "durations": null, "exact_frames_per_prompt": [80], "background": "In the background, there is a green surface, soil surface, gray road, a fence, the voices of the people and a miscellaneous sound is audible.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/0T_zBZCfq74.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_816_2"}, {"texts": ["A boy wearing orange clothes is standing near the net and holding a cup and feeding the goats."], "durations": null, "exact_frames_per_prompt": [80], "background": "In the background, people are speaking, there is a green surface, black surface and a net.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/0T_zBZCfq74.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_817_0"}, {"texts": ["A white brown goat is standing and eating from the boy while another white goat is standing on the right.", "The goat is going to the left."], "durations": null, "exact_frames_per_prompt": [18, 7], "background": "In the background, people are speaking, there is a green surface, black surface and a net.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/0T_zBZCfq74.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_817_1"}, {"texts": ["A man wearing transparent gloves is putting a silver wheel trim on the wheel.", "The man pointing his finger on the brake fluid container.", "The man starts opening the top of that container."], "durations": null, "exact_frames_per_prompt": [45, 26, 9], "background": "In the background, the man is speaking, the music is playing, there is a grey surface, a fan, a black car, a metal frame, a white wall, and a brown wall.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/1j7qDoi1LP0.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_82_0"}, {"texts": ["A man wearing a dark-green t-shirt is standing and flipping a pancake in a pan with a spatula."], "durations": null, "exact_frames_per_prompt": [30], "background": "In the background, the song is playing, the men are speaking, there is a black stove with a grey counter top, a microwave, a cupboard, windows with window blinds, white walls, posters, blue sofas, bottles, a black table, a maroon cloth, a black chairs white ceiling, a black toaster, a basket and a cup.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/1eCO28FZTXs.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_830_0"}, {"texts": ["A girl wearing a white shirt is tying a black and red neck tie."], "durations": null, "exact_frames_per_prompt": [80], "background": "In the background, the girl is speaking and music is playing. There are white walls, windows, white ceiling, a gray slab, utensils and other miscellaneous objects at the back.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/-GnF9V3-nuw.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_831_0"}, {"texts": ["A girl wearing a white shirt is standing and adjusting a blue-red tie."], "durations": null, "exact_frames_per_prompt": [80], "background": "In the background, there is a white wall, a window, a white ceiling, a table with some stuff, a yellow object, and some other stuff, a red-blue tie, and the girl speaking and slow music playing sounds are audible.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/-GnF9V3-nuw.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_832_0"}, {"texts": ["A woman wearing a pink top is standing and helping the girl while a boy wearing a brown t-shirt is standing, holding a knife, and spreading the sauce with it, and a man wearing a beige t-shirt is standing on the right side and putting sauce in the food."], "durations": null, "exact_frames_per_prompt": [55], "background": "In the background, T. V and people are audible. There is a kitchen countertop, a tap, a white table, a flower, a flower pot, gray walls, a T. V, a chair, a clear glass sliding door, ketchup bottles, and a few miscellaneous things.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/1JK-yDbqK_M.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_834_0"}, {"texts": ["A girl wearing a white frock is standing and making a sandwich.\n while a woman wearing a red t-shirt is guiding her, a boy in a brown t-shirt is also making a sandwich, and a man in gray t-shirt is standing on the right and pouring the sauce."], "durations": null, "exact_frames_per_prompt": [55], "background": "In the background, T. V and people are audible. There is a kitchen countertop, a tap, a white table, a flower, a flower pot, gray walls, a T. V, a chair, a clear glass sliding door, ketchup bottles, and a few miscellaneous things.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/1JK-yDbqK_M.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_834_1"}, {"texts": ["A man wearing a grey t-shirt is standing and helping the boy.\n while a woman wearing a red top is standing on the left and assisting the girl."], "durations": null, "exact_frames_per_prompt": [55], "background": "In the background, T. V and people are audible. There is a kitchen countertop, a tap, a white table, a flower, a flower pot, gray walls, a T. V, a chair, a clear glass sliding door, ketchup bottles, and a few miscellaneous things.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/1JK-yDbqK_M.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_834_3"}, {"texts": ["A man wearing a blue tracksuit is standing and doing a sheep shearing on the green grass surface with the help of a sheep shearing machine while group of people are standing on the backside."], "durations": null, "exact_frames_per_prompt": [24], "background": "In the background, there is a green grass surface, a black wire, a shearing machine, a white object, and the song playing sound is audible.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/-ZKVV01VwLM.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_836_0"}, {"texts": ["A white sheep is getting sheared by the man while a group of people are standing at the back and few are moving."], "durations": null, "exact_frames_per_prompt": [24], "background": "In the background, there is a green grass surface, a black wire, a shearing machine, a white object, and the song playing sound is audible.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/-ZKVV01VwLM.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_836_1"}, {"texts": ["A woman wearing a pink top is flipping an omelet in a black pan."], "durations": null, "exact_frames_per_prompt": [20], "background": "In the background, the people are audible and the sound of the pan is also audible. There are white walls, a window, brown kitchen cabinets, a black stove, a counter top, two black pans, a chopping board, a white cloth, plastic bottles, miscellaneous objects, a silver sink and a white washing machine.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/1z9-Z2gpWEI.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_840_0"}, {"texts": ["A man wearing a black t-shirt and pants is standing in a golf swing position.", "The man is hitting a golf ball with a golf stick.", "The man is leaning forward and picking up the golf tee from the ground."], "durations": null, "exact_frames_per_prompt": [25, 40, 16], "background": "In the background, there are trees, green grass, soil surface, blue sky, a golf ball, a golf stick, and wind blowing sound, ball hitting sound and people's voices are audible.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/-RvbvRbhpgk.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_841_0"}, {"texts": ["A woman wearing floral printed top is standing, removing the lid of the mixing jar.", "The woman wearing floral printed top starts putting stuff from the mixing jar into a red bowl.", "The woman wearing floral printed top then shows the bowl."], "durations": null, "exact_frames_per_prompt": [19, 45, 16], "background": "In the background, the woman is speaking, there is a grey counter top with some bottles of items, a red bowl, cupboards, drawers, a metal dustbin, a coffee maker, a toaster, a wooden door, white walls, a vase with flowers, a grey refrigerator, and a white tiled floor.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/-36efvC2K54.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_845_0"}, {"texts": ["A person whose only hands are visible is holding a pan handle in his left hand, and holding a steel spatula in his right hand and frying the green vegetables while stirring the vegetables in the pan with a steel spatula."], "durations": null, "exact_frames_per_prompt": [80], "background": "In the background, there is the sound of frying the vegetables, the sound of the steel spatula, brown plates with food, a white bowl with a brown sauce, a steel spatula, a black pan, a boiled chow mein in the steel pot utensil, a steel pot utensil on the brown chopping board, an electric stove and a brown counter.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/08MbEs0I1Y4.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_850_0"}, {"texts": ["A brown dog is sitting on the white surface and is looking at the white-brown dog and the person wearing grey clothes."], "durations": null, "exact_frames_per_prompt": [80], "background": "In the background, people are speaking, there is the sound of a dog barking, a red counter with stuff; a white stool, glass door with red border, a big steel rack with stuffs; a black counter with a white mattress, a black toy on the white mattress, and other stuffs, and a light gray tile floor.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/3Q7L3gWx7QA.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_857_4"}, {"texts": ["A boy wearing a black t-shirt is sitting in the baby trend sit chair and eating food."], "durations": null, "exact_frames_per_prompt": [80], "background": "In the background, there are wooden cabinets, a counter top, a baby trend sit chair, a red glass, and some other objects.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/-tTDXLTRZwQ.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_860_0"}, {"texts": ["A person wearing black clothes is fixing the tyre."], "durations": null, "exact_frames_per_prompt": [64], "background": "In the background, some miscellaneous sounds are audible, there is a gray floor, a black car and a white object.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/2ztRm_qsKxo.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_862_0"}, {"texts": ["A woman wearing a white t-shirt and blue jeans is standing and starts walking.", "The woman is tilting her body to pick up the bed-sheet."], "durations": null, "exact_frames_per_prompt": [34, 46], "background": "In the background, a woman is speaking; there are white walls, a white ceiling with white moldings and a light fixture, a colorful bed sheet, and a brown metal double decker cot bed with white bed sheet.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/i8rtS5z9iJU.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_864_0"}, {"texts": ["A man whose hand is only visible is taking out a baking tray from the oven. ", "The man is taking out some chicken juice through the spoon and pour it on the chicken.", "The man is adding some coriander."], "durations": null, "exact_frames_per_prompt": [22, 40, 18], "background": "In the background, there is a light brown table mat, a white cloth, a spoon, a bowl filled with cooked chicken, a brown wooden table surface, and a microwave oven, a baking tray, and the man speaking sound is audible.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/0rJ0lbQDl3k.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_865_0"}, {"texts": ["A man on the right is standing and is speaking while a man on the left side in a blue jacket is standing on the grass while holding a golf stick.", "The man on the right then sits on the ground and is looking at the golf ball while speaking while the man on the left side bends towards the grass surface, he hits the golf ball to the left."], "durations": null, "exact_frames_per_prompt": [39, 41], "background": "In the background, person one is speaking and miscellaneous sounds are audible. There is a green grass surface, gray road, two golf sticks, two caddies, fountain, green trees and mountains.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/1A7LqsE_knI.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_868_0"}, {"texts": ["A man on the left is standing still while another man wearing blue and grey outfit, holding a golf stick in his right hand, is standing on the right side of first man and he is moving his left hand and saying something.", "The man is looking at the golf ball.", "The man hits the golf ball."], "durations": null, "exact_frames_per_prompt": [35, 41, 4], "background": "In the background, person one is speaking and miscellaneous sounds are audible. There is a green grass surface, gray road, two golf sticks, two caddies, fountain, green trees and mountains.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/1A7LqsE_knI.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_868_1"}, {"texts": ["A man wearing blue cloth is standing and holding golf stick while a man wearing blue cloth is standing on the left side on a green grass surface, holding a golf stick.", "The man wearing blue cloth then sits down and talking about the golf while a man wearing blue cloth is playing a golf shot."], "durations": null, "exact_frames_per_prompt": [38, 42], "background": "In the background, a man is speaking. There is a green surface, green trees, mountains, a black road, packed cars, and some other object.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/1A7LqsE_knI.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_869_0"}, {"texts": ["A man wearing blue-black cloth is standing and holding golf stick while another man wearing grey pats is first standing and speaking then sits down while holding the golf stick.", "The man is hitting a ball with the golf stick."], "durations": null, "exact_frames_per_prompt": [77, 3], "background": "In the background, a man is speaking. There is a green surface, green trees, mountains, a black road, packed cars, and some other object.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/1A7LqsE_knI.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_869_1"}, {"texts": ["A person whose hands are visible, is holding fork and picking food from the bowl."], "durations": null, "exact_frames_per_prompt": [42], "background": "In the background, a person is speaking, there is a black counter-top, a black bowl with food, a cloth and gray surface.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/0n7pCDumJXs.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_86_0"}, {"texts": ["A girl wearing a maroon dress is sitting and picking up a glass of shot from the table while a group of people are sitting near the woman and a few are standing at backside and one of them wipe the woman face with a white cloth."], "durations": null, "exact_frames_per_prompt": [44], "background": "In the background, there is a white ceiling, brown wooden shelves, and a white light, and the song playing sound is audible.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/5NP93lDKXK0.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_872_0_ms"}, {"texts": ["A boy wearing a white t-shirt is sitting in the middle and looking at the girl while a person whose hand is visible is wiping the face of the woman with a white cloth, another girl wearing a pink jacket is sitting on the right side, and another boy wearing a red sweatshirt is moving at the back while holding a green bottle."], "durations": null, "exact_frames_per_prompt": [44], "background": "In the background, there is a white ceiling, brown wooden shelves, and a white light, and the song playing sound is audible.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/5NP93lDKXK0.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_872_1_ms"}, {"texts": ["A girl wearing a red cloth whose hands are visible is wiping the neck of the first girl while a boy in a white t-shirt and a girl in a pink jacket is sitting at the back, and a person in red-white clothes is walking at the back holding a green bottle."], "durations": null, "exact_frames_per_prompt": [13], "background": "In the background, there is a white ceiling, brown wooden shelves, and a white light, and the song playing sound is audible.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/5NP93lDKXK0.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_872_2"}, {"texts": ["A girl wearing a pink jacket is sitting on the right side of the boy while another girl wearing a maroon top is sitting on the left and drinking shots, a person whose hand is visible is cleaning the girl's face with a towel, and another boy wearing a red sweatshirt is moving at the back while holding a green bottle."], "durations": null, "exact_frames_per_prompt": [44], "background": "In the background, there is a white ceiling, brown wooden shelves, and a white light, and the song playing sound is audible.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/5NP93lDKXK0.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_872_3_ms"}, {"texts": ["A boy wearing a white-black striped t-shirt is standing and drinking while a group of people are standing and drinking in the backside.", "The boy starts eating something from the girl hand while a group of people looks at the man."], "durations": null, "exact_frames_per_prompt": [19, 17], "background": "In the background, there is a white ceiling, brown wooden shelves, and a white light, and the song playing sound is audible.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/5NP93lDKXK0.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_872_5"}, {"texts": ["A group of people is standing and drinking."], "durations": null, "exact_frames_per_prompt": [35], "background": "In the background, there is a white ceiling, brown wooden shelves, and a white light, and the song playing sound is audible.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/5NP93lDKXK0.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_872_6_ms"}, {"texts": ["An elephant is walking in the middle of the first and the second elephant on the beach."], "durations": null, "exact_frames_per_prompt": [80], "background": "In the background, people are speaking, there is the sound of water waving, green trees, a blue sky, big umbrellas, big rocks, a gray water sea with water waving, and a sandy beach.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/-TItfMkZpXo.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_875_5"}, {"texts": ["A man wearing a light-blue shirt is fixing a blue bow with black spots.", "The man then turns the collar of the shirt down."], "durations": null, "exact_frames_per_prompt": [58, 22], "background": "In the background, the man is speaking, there is a black wall with a yellow world map.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/29tH0AWxd84.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_876_0"}, {"texts": ["A woman wearing a black coat is standing in front of a lectern and speaking."], "durations": null, "exact_frames_per_prompt": [53], "background": "In the background, there is a lectern, a mike, mike holding stand, book shelf, books, white wall, shadows, wooden racks and a woman's voice is audible.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/5JxdV8u3H9U.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_877_0_ms"}, {"texts": ["A woman wearing a black coat is standing and delivering a speech."], "durations": null, "exact_frames_per_prompt": [53], "background": "In the background, a woman is audible. There is a mic, a bookshelf, books, a white wall, and a table.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/5JxdV8u3H9U.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_878_0_ms"}, {"texts": ["A person whose only hands are visible is wearing white gloves and is rotating an apple with a black knife."], "durations": null, "exact_frames_per_prompt": [80], "background": "In the background, the sound of rotating the apple is audible. There are red apples, a knife and a brown carry bag.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/2hiPMnxCtDw.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_884_0"}, {"texts": ["A man wearing a dark blue full-sleeve t-shirt and blue-printed sky blue shorts, is standing while holding a chainsaw machine, and he starts to turn on the switch of the chainsaw machine.", "The man touches cap of the bottle by the chainsaw guide bar to opens the cap of the bottle.", "The man keeps the chainsaw machine on the surface and tilts his body to pick up the bottle."], "durations": null, "exact_frames_per_prompt": [33, 37, 10], "background": "In the background, a person is speaking, there is the sound of the chainsaw machine, a chainsaw machine, green trees, a blue sky with white clouds, wooden logs on the surface, a green bottle, a wooden structure, a brown wooden sheet on the surface, and green and dry brown grasses, and a surface with small stones.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/sG4BmLx8DUQ.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_888_0"}, {"texts": ["A woman wearing a black cloth is standing and spreading the rice on the half nori sheet."], "durations": null, "exact_frames_per_prompt": [80], "background": "In the background, there is a white table, an orange plate with some stuff, a packet, a brown chair, a white wall, rice, a half nori sheet, an orange surface, and the woman speaking sound and also other woman speaking sound is audible.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/1DqDmAXzMhQ.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_890_0"}, {"texts": ["A baby on the left side is sitting on a blue chair wearing white clothes."], "durations": null, "exact_frames_per_prompt": [80], "background": "In the background there is a beige wall, a white door, blind window curtains, baby chairs, white bed sheet, a black object, some stuff, a pillow and baby laughing sounds are audible.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/0Bzn7lSvTmA.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_891_0"}, {"texts": ["A man wearing a red t-shirt and a black cap is standing on the cream colored surface, and is putting the red food material on the bread.", "The man is pouring the brown food material on the bread."], "durations": null, "exact_frames_per_prompt": [32, 48], "background": "In the background, music is playing, a man is speaking; there is the sound of throwing objects; a black counter; a steel utensil pot with red food material, a white bowl with brown food materials are on the black counter, a steel counter with stuff, a glass container on the steel counter, gray walls; a brown door and a brown window, another countertop in the room, a glass door; and a cream colored granite floor.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/5X07hibp4MI.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_896_0"}, {"texts": ["A person wearing black-red clothes is sitting and holding a surgical tape and peeling it."], "durations": null, "exact_frames_per_prompt": [80], "background": "In the background, the person is speaking and some miscellaneous sounds are audible, there is a brown table, a white bottle, a surgical tape and some other objects.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/-R5bm4G2SpQ.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_8_0"}, {"texts": ["A man is standing and holding the leash of the dog.", "The man is feeding the dog food."], "durations": null, "exact_frames_per_prompt": [59, 21], "background": "In the background, there is an off white door, a wall, grey surface, a dog leash, some objects, people are speaking and some miscellaneous voices are audible.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/2lB6dcyys34.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_901_0"}, {"texts": ["A brown dog is walking on the grey floor and eating food while a person is standing and holding the leash, and the other person is feeding it."], "durations": null, "exact_frames_per_prompt": [80], "background": "In the background, there is an off white door, a wall, grey surface, a dog leash, some objects, people are speaking and some miscellaneous voices are audible.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/2lB6dcyys34.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_901_1"}, {"texts": ["A girl wearing a blue top is standing and mixing a dough in a bowl while a woman wearing a purple top is standing and mixing a dough in her hands and then she puts a dough in a bowl."], "durations": null, "exact_frames_per_prompt": [80], "background": "In the background, girls are speaking. There are brown cabinets, a light, white ceiling, a yellow wall, counter top with some stuff on it, a window, a glass bowl, a blue bowl and a spatula.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/0Y4zTYoHg4Y.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_906_0"}, {"texts": ["A girl wearing a purple top is standing and mixing a dough with hands while another girl wearing blue outfit is standing on the left side of first girl and she is mixing dough using her hands and she touched her face once using her left hand.", "The girl wearing a purple top is putting the dough down in the bowl."], "durations": null, "exact_frames_per_prompt": [58, 22], "background": "In the background, girls are speaking. There are brown cabinets, a light, white ceiling, a yellow wall, counter top with some stuff on it, a window, a glass bowl, a blue bowl and a spatula.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/0Y4zTYoHg4Y.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_906_1"}, {"texts": ["A man wearing a black cloth is holding a bowl with one hand and holding a chicken marinated in spices with another hand.", "The man is putting it on the plate."], "durations": null, "exact_frames_per_prompt": [69, 11], "background": "In the background, there is a white cupboards, some stuff, a coffee machine, a tissue roll, a white tray, plastic, blue plate, silver spoon, a white bowl, a countertop, jug, basket, a microwave machine, light brown wall, voice of the man and a miscellaneous sound is audible.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/0oAJm3f37WU.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_907_0"}, {"texts": ["A boy wearing a yellow-blue dress is sitting on the lap of a girl while a person whose hand is visible is sitting on the right side.", "The boy is eating noodles with a white plastic fork while a girl wearing a frock is sitting behind the boy and a person wearing a blue t-shirt is sitting on the right side."], "durations": null, "exact_frames_per_prompt": [36, 44], "background": "In the background, there is a brown floor, white-black wall, a white plate, a white table, people are shouting, and there is a miscellaneous sound.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/-MytqRwRwBg.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_908_0"}, {"texts": ["A girl wearing a pink dress is sitting with a boy on her lap while a person wearing a blue t-shirt is sitting on the right side.", "The girl is picking noodles from the boy's dress and putting it back on the plate."], "durations": null, "exact_frames_per_prompt": [63, 17], "background": "In the background, there is a brown floor, white-black wall, a white plate, a white table, people are shouting, and there is a miscellaneous sound.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/-MytqRwRwBg.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_908_1"}, {"texts": ["A kid wearing a white cloth is sitting on the chair and eating cake with his finger."], "durations": null, "exact_frames_per_prompt": [80], "background": "In the background, there is a wall, chairs, a brown table, a baby's chair, a black tray, cake, balloons, white surface, people are speaking and laughing sounds are audible.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/4NEOPglw-zM.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_912_0"}, {"texts": ["A person wearing a graphic black t-shirt is sitting behind the table and making a bow.", "The person is cutting the extra pieces of fabrics from the bow."], "durations": null, "exact_frames_per_prompt": [53, 27], "background": "In the background, there is a white table, scissors, a black bow, white drape, white boundary, gray slab and the sound of music is audible.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/-gdyA7pMlBc.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_914_0"}, {"texts": ["A man whose half body is visible wearing a black sweatshirt is standing and tying a knot into a bow tie.", "The man is cutting off the extra cloth in the bow tie with a scissor."], "durations": null, "exact_frames_per_prompt": [50, 29], "background": "In the background, there is a black bow tie, white table, a scissor, white tile wall, white curtains and some music is audible.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/-gdyA7pMlBc.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_915_0"}, {"texts": ["A girl wearing a white designer t-shirt is standing and squeezing red paste from a packet.", "The girl puts the packet back and holds the spoon. ", "The girl is spreading it on a piece of white bread with a spoon.\n"], "durations": null, "exact_frames_per_prompt": [28, 25, 27], "background": "In the background, there is a brown table, a brown sofa, cushions, a plate, a bottle, an orange wall, a blue curtain, a window, some other stuff, and a woman's and a girl's voices are audible.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/-vX6KljYppA.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_927_0"}, {"texts": ["A boy wearing a green and white striped t-shirt is sitting on the muddy surface near the brown rabbit, he is at first looking in the left direction and smiling.", "The boy points to the rabbit.", "The boy starts touching the rabbit."], "durations": null, "exact_frames_per_prompt": [17, 15, 47], "background": "In the background, there is a wooden structure, a lock, muddy surface, and the glass.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/06lRs8M7bSA.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_931_0"}, {"texts": ["A brown rabbit sitting on the muddy surface is chewing the food while a boy wearing a white t -shirt sitting on the ground", "The rabbit is at first caressed by the boy in a striped t-shirt", "The rabbit then moves in the left direction while a boy wearing a white t-shirt goes to pick the cup"], "durations": null, "exact_frames_per_prompt": [33, 30, 17], "background": "In the background, there is a wooden structure, a lock, muddy surface, and the glass.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/06lRs8M7bSA.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_931_1"}, {"texts": ["A boy wearing a green-white t-shirt is sitting on the ground, moving his hand while a brown rabbit is sitting on the ground, facing left, and eating something.", "The boy is touching the rabbit."], "durations": null, "exact_frames_per_prompt": [33, 30], "background": "In the background, a boy, people, and miscellaneous sounds are audible. There is a blue cup, a door lock, a wooden door, wooden walls, a wood square piece, and a soil surface.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/06lRs8M7bSA.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_932_0"}, {"texts": ["A brown rabbit sniffing the ground while a man wearing a white-green t-shirt is sitting and trying to touch the rabbit.", "The rabbit is being touched by a boy."], "durations": null, "exact_frames_per_prompt": [33, 47], "background": "In the background, a boy, people, and miscellaneous sounds are audible. There is a blue cup, a door lock, a wooden door, wooden walls, a wood square piece, and a soil surface.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/06lRs8M7bSA.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_932_1"}, {"texts": ["A kid wearing blue-golden clothes is sitting and crying. ", "The kid wearing blue-golden clothes is keeping his hand on the woman."], "durations": null, "exact_frames_per_prompt": [13, 67], "background": "In the background, there is a pink curtain, a white chair, a shed, some music, a kids crying, and a woman's voice is audible.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/-PNcaCDLaFw.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_934_0"}, {"texts": ["A kid wearing a gray t-shirt is picking up garbage from the floor and is throwing it."], "durations": null, "exact_frames_per_prompt": [80], "background": "In the background, music is playing and miscellaneous sounds are audible. There is a white wall, a brown bed with white mattress and pillow, a side table with a green plant, a bedside lamp, a silver object and a gray object.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/8HVSLiPy8Nw.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_937_0"}, {"texts": ["A boy wearing a gray t-shirt is picking paper confetti from the floor and throwing it in the air."], "durations": null, "exact_frames_per_prompt": [80], "background": "In the background, T. V and miscellaneous sounds are audible. There is a plant, a planter, a table, a bed, a white pillow, a white bed sheet, a brown blanket, white walls, and two silver things.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/8HVSLiPy8Nw.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_938_0"}, {"texts": ["A baby wearing a pink top is sitting in a baby feeding chair, drinking something from a straw while a lady is holding a juice box and a straw.", "The baby is moving while a lady is moving her hands."], "durations": null, "exact_frames_per_prompt": [25, 55], "background": "In the background, a person is audible. There is a baby feeding chair, a straw, a packet, a painting, a photo frame, a brown side table, a black wire, and a white wall.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/2ByBLPJD82Y.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_948_0"}, {"texts": ["A person whose hands are visible is giving the straw to the baby.", "The person is moving."], "durations": null, "exact_frames_per_prompt": [31, 49], "background": "In the background, a person is audible. There is a baby feeding chair, a straw, a packet, a painting, a photo frame, a brown side table, a black wire, and a white wall.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/2ByBLPJD82Y.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_948_1"}, {"texts": ["A woman wearing a black-white top and blue jeans is sitting and combing the hair of a dog."], "durations": null, "exact_frames_per_prompt": [80], "background": "In the background, there is a brown couch, white objects, gray carpeted floor and the sound of music is audible.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/-w3KSGlo1uY.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_94_0"}, {"texts": ["A black-white dog is lying on the gray carpeted floor and getting his hair combed by the woman."], "durations": null, "exact_frames_per_prompt": [80], "background": "In the background, there is a brown couch, white objects, gray carpeted floor and the sound of music is audible.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/-w3KSGlo1uY.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_94_1"}, {"texts": ["A man wearing a white t-shirt and grey pants is hitting a golf ball with a golf stick.", "The man stands on the green grass field holding a golf stick."], "durations": null, "exact_frames_per_prompt": [17, 64], "background": "In the background there is sky, green grass field, trees, soil surface and a person is speaking.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/1V_7ovkjHCg.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_953_0"}, {"texts": ["A man wearing a white t-shirt and gray pants is standing in the golf swing position and hitting a golf ball with a golf stick.", "The man is standing and speaking."], "durations": null, "exact_frames_per_prompt": [21, 60], "background": "In the background, there are trees, green grass, a sand bunker, blue sky, golf stick, golf ball, and the man's voice is audible.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/1V_7ovkjHCg.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_954_0"}, {"texts": ["A girl wearing a white shirt and a grey skirt is sitting on the bench, first holding a blue napkin,then She puts the napkin on the bench and starting to fold it."], "durations": null, "exact_frames_per_prompt": [81], "background": "In the background, there is a blue napkin, a bench, soil surface, a white card with black text, and people are speaking.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/1QeszSopJhc.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_955_0"}, {"texts": ["A boy on the left side wearing a black t-shirt is sitting, moving his hands and speaking while a man wearing a black shirt is sitting on the right while putting his hands on the table and then he holds a glass and laughing while watching towards the boy."], "durations": null, "exact_frames_per_prompt": [80], "background": "In the background there is a wooden table, brown wall, glass bottles, posters, a black box, two glasses with beer, and a boy is speaking and a man laughing.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/-zSEHrWQ2BE.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_961_0"}, {"texts": ["A man on the right side wearing a black shirt is sitting and laughing on the boy's talks.", "The man at the end lifts his glass."], "durations": null, "exact_frames_per_prompt": [69, 9], "background": "In the background there is a wooden table, brown wall, glass bottles, posters, a black box, two glasses with beer, and a boy is speaking and a man laughing.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/-zSEHrWQ2BE.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_961_1"}, {"texts": ["A girl wearing a blue top is sitting on a bed and picking the clothes kept on her right side on the bed."], "durations": null, "exact_frames_per_prompt": [80], "background": "In the background, there are yellow walls, a cabinet, clothes, and toy and the brown sheet. There is sound of female voice in the background.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/-YKDJrc2p5w.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_962_0"}, {"texts": ["A man in black clothing is at first standing on the wooden stage and watching the sheep shearing while another man wearing black and blue outfit is standing on the stage and removing the woolen fleece and the sheep is getting his woolen fleece removed.", "The man in black clothing turns while group of people are standing on the stage and third man wearing white and black outfit is moving towards left side.", "The man in black clothing starts walking in the left direction while fourth man wearing purple outfit is standing near the stage, holding a mop in his hand and he is cleaning and other group of people are standing on the ground and fifth man wearing blue outfit is sitting on the right side."], "durations": null, "exact_frames_per_prompt": [20, 13, 16], "background": "In the background, there is a chair, a table, barricades, cloth ceiling, iron frames, a stage, and a grassy surface.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/-2-nYjSwo8U.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_966_6"}, {"texts": ["A man wearing a black chef dress is standing, pouring something into the pan and mixing it."], "durations": null, "exact_frames_per_prompt": [80], "background": "In the background, a man is audible. There is a pan, a jug, a red cooking pot, a black kitchen stove, a wooden board, a red cutting board, an onion, a knife, a clear glass bowl, a black kitchen shelf, wooden cupboards, windows, a green wall, and bamboo outside a window.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/1ruzPN0fSKw.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_967_0"}, {"texts": ["A boy wearing a vine-colored shirt is standing on the grassy surface and feeding the pigeons sitting on the rock."], "durations": null, "exact_frames_per_prompt": [80], "background": "In the background, there is a seat, a rock , and a grassy surface.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/-kHCPj7_bqU.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_969_0"}, {"texts": ["A person whose hands are visible is bandaging the leg of another person."], "durations": null, "exact_frames_per_prompt": [80], "background": "In the background, there are bandages, and an off-white surface.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/2dE9NFxQijo.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_970_0"}, {"texts": ["A person whose leg is visible is getting his leg wrapped with a bandage while a person on the left side is wrapping the bandage."], "durations": null, "exact_frames_per_prompt": [80], "background": "In the background, there are bandages, and an off-white surface.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/2dE9NFxQijo.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_970_1"}, {"texts": ["A woman is brushing her eyebrows with an eyebrow brush.", "The woman is showing the eyebrow maker pencil."], "durations": null, "exact_frames_per_prompt": [65, 15], "background": "In the background, a woman is speaking. There is an eyebrow marker pencil, white eyebrow brush, and a white wall behind the woman.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/0gTdR-nnLyY.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_971_0"}, {"texts": ["A man wearing specs is holding the newspaper and looking into it."], "durations": null, "exact_frames_per_prompt": [80], "background": "In the background, there is a newspaper, a seat, a banner, and the raindrops.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/6vZS95ajUlo.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_977_0"}, {"texts": ["A man is sitting and reading a newspaper."], "durations": null, "exact_frames_per_prompt": [80], "background": "In the background, miscellaneous sounds are audible. There is a newspaper, a bench, and blue tarpaulins.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/6vZS95ajUlo.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_978_0"}, {"texts": ["A man is wearing black pants is standing in a bending position while a black dog is eating something, a woman wearing a greet t-shirt is holding the dog's leash, a woman at the back is playing with another dog and a person is sitting at the back.", "The man steps back while the woman wearing a green t-shirt is coming towards the dog and caressing him."], "durations": null, "exact_frames_per_prompt": [13, 44], "background": "In the background, people are speaking and laughing. There are trees, soil surface, houses, parked cars, a road surface, shrubs and white sky.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/1EVYUcLNXbE.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_979_0"}, {"texts": ["A woman wearing a green t-shirt is standing holding a black dog's leash as a man wearing a grey t-shirt is bending and looking at the dog and then gets up and steps back to the right on a grey road surface.", "The woman wearing a green t-shirt is walking.", "The woman wearing a green t-shirt is caressing the dog and a group of people in which one is sitting rest are standing and doing different activities on a grey road surface."], "durations": null, "exact_frames_per_prompt": [41, 14, 25], "background": "In the background, people are speaking and laughing. There are trees, soil surface, houses, parked cars, a road surface, shrubs and white sky.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/1EVYUcLNXbE.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_979_1"}, {"texts": ["A woman wearing gray pants is playing with the third dog while a black dog is eating and being caressed by the woman, a man is looking at it, two people are playing with the white dog, a woman is playing with the brown dog and a person is sitting on the path."], "durations": null, "exact_frames_per_prompt": [80], "background": "In the background, people are speaking and laughing. There are trees, soil surface, houses, parked cars, a road surface, shrubs and white sky.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/1EVYUcLNXbE.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_979_2"}, {"texts": ["A black dog is standing and eating from a white bowl while a man wearing a blue t-shirt is leaning forward and looking at the dog, he then stands and steps back, and a woman wearing a green top is standing on the road holding the dog's belt, She moves her hand, starts scratching her neck, then walks forward and starts caressing the dog, A group of people are standing on the road in the backside while holding their dog's belt doing different activity."], "durations": null, "exact_frames_per_prompt": [80], "background": "In the background, people are speaking and laughing. There are trees, soil surface, houses, parked cars, a road surface, shrubs and white sky.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/1EVYUcLNXbE.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_979_4"}, {"texts": ["A white-black dog is waking while a person is sitting and a person wearing blue t-shirt is leaning forward, and a woman wearing green t-shirt is standing and looking towards the black dog that is eating something in a bowl, and a man wearing gray t-shirt is standing and looking towards the black dog.", "The dog is playing with the second woman while a group of people are walking on the road, and a dog is moving on the road, and a woman wearing a green t-shirt is coming towards the black dog and petting."], "durations": null, "exact_frames_per_prompt": [24, 56], "background": "In the background, people are speaking and laughing. There are trees, soil surface, houses, parked cars, a road surface, shrubs and white sky.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/1EVYUcLNXbE.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_979_6"}, {"texts": ["A person whose hand is visible, holding a fry pan, a plate and tongs and taking fish fry from the frying pan and putting in the plate."], "durations": null, "exact_frames_per_prompt": [80], "background": "In the background, a person is speaking and sizzling sounds are audible, there is a white plate, frying pans, gas stove and white counter-top.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/02pEmMBwFME.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_97_0"}, {"texts": ["A baby wearing a white t-shirt and purple pajamas is sitting on the floor and reading a book while a man is lying on the carpet next to the baby."], "durations": null, "exact_frames_per_prompt": [80], "background": "In the background, a person is audible. There is a book, a red-white carpet, a yellow marble floor, white walls, a black fireplace, and a wooden box.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/10fd-o-m8lg.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_980_0"}, {"texts": ["A person wearing a yellow t-shirt and green checkered pajamas is laying on the floor next to the baby."], "durations": null, "exact_frames_per_prompt": [80], "background": "In the background, a person is audible. There is a book, a red-white carpet, a yellow marble floor, white walls, a black fireplace, and a wooden box.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/10fd-o-m8lg.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_980_1"}, {"texts": ["A woman wearing a striped cloth is standing, ironing the shirt with the iron machine on the ironing board.", "The woman wearing a striped cloth is showing the iron machine in the front.", "The woman wearing a striped cloth is standing while holding the baby on the counter with her hands and speaking."], "durations": null, "exact_frames_per_prompt": [34, 14, 32], "background": "In the background, there is an ironing board, a shirt on the ironing board, an iron machine on the ironing board, walls, windows covered with curtains and window blind, counters, cupboards, a ceiling lamp hanging, and a flower pot on the counter.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/2z-a1d-w5zQ.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_981_0"}, {"texts": ["A baby wearing printed clothes is sitting on the counter while a woman is holding the baby and she is talking.", "The baby wearing printed clothes starts holding curtains with it's right hand."], "durations": null, "exact_frames_per_prompt": [15, 16], "background": "In the background, there is an ironing board, a shirt on the ironing board, an iron machine on the ironing board, walls, windows covered with curtains and window blind, counters, cupboards, a ceiling lamp hanging, and a flower pot on the counter.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/2z-a1d-w5zQ.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_981_1"}, {"texts": ["A black horse on the right side is moving and getting its feet cleaned by a woman and putting its mouth in the black-pink basket and another horse is standing on the left side and sniffing on the floor."], "durations": null, "exact_frames_per_prompt": [80], "background": "In the background, there is gray surface, a black-pink basket, a sliding door, wooden structure, some other stuff, and the sound of music is audible.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/1tEJRzqqalM.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_986_0"}, {"texts": ["A brown-white horse on the left side is moving and eating something while a woman wearing a blue sweater is brushing another horse then moves back and goes to the right then again starts brushing the horse."], "durations": null, "exact_frames_per_prompt": [80], "background": "In the background, there is gray surface, a black-pink basket, a sliding door, wooden structure, some other stuff, and the sound of music is audible.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/1tEJRzqqalM.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_986_1"}, {"texts": ["A woman wearing blue clothes is walking, touching and cleaning the feet of horse one while a white-brown horse is moving and eating something."], "durations": null, "exact_frames_per_prompt": [80], "background": "In the background, there is gray surface, a black-pink basket, a sliding door, wooden structure, some other stuff, and the sound of music is audible.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/1tEJRzqqalM.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_986_2"}, {"texts": ["A man wearing a white cloth and a pink turban is sitting on the back of the first elephant while a man is sitting wearing a red turban, and a man in black t-shirt is riding another black elephant at the front.", "The man is riding while smoking while a man wearing a white shirt is standing on the wall on the right side next to the other black elephant."], "durations": null, "exact_frames_per_prompt": [23, 57], "background": "In the background, there is a blackish boundary, a gray boundary, gray road, a white bucket, a gray slab like structure, gray slides of rocks, soil surface, the voices of the people and some miscellaneous sounds are audible.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/05hKGU5928E.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_987_0"}, {"texts": ["A man wearing black clothes is sitting on the back of the second elephant and riding while another man wearing white clothes is riding the first elephant."], "durations": null, "exact_frames_per_prompt": [80], "background": "In the background, there is a blackish boundary, a gray boundary, gray road, a white bucket, a gray slab like structure, gray slides of rocks, soil surface, the voices of the people and some miscellaneous sounds are audible.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/05hKGU5928E.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_987_1"}, {"texts": ["A black elephant is walking on the right side of the second elephant, in the opposite direction of the second elephant, carrying the first man on his back while a group of people are sitting on the back of elephants and a man is standing on the right side with an elephant."], "durations": null, "exact_frames_per_prompt": [62], "background": "In the background, there is a blackish boundary, a gray boundary, gray road, a white bucket, a gray slab like structure, gray slides of rocks, soil surface, the voices of the people and some miscellaneous sounds are audible.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/05hKGU5928E.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_987_4"}, {"texts": ["A black elephant is walking on the right side of the blackish boundary in the opposite direction of the first elephant and carrying the second man on his back while a black elephant standing beside the wall on the ride side"], "durations": null, "exact_frames_per_prompt": [80], "background": "In the background, there is a blackish boundary, a gray boundary, gray road, a white bucket, a gray slab like structure, gray slides of rocks, soil surface, the voices of the people and some miscellaneous sounds are audible.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/05hKGU5928E.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_987_5"}, {"texts": ["A woman wearing a purple top and blue jeans is sitting on the dark brown horse's back while holding the horse harness and riding from left to right on the soil surface."], "durations": null, "exact_frames_per_prompt": [80], "background": "In the background, there is a miscellaneous sound, green trees, a shed house on the green grass surface, a wooden fence boundary, a green grass surface, the sky, and a soil surface.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/5-qOl012I-4.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_988_0"}, {"texts": ["A dark brown horse is walking from left to right on the soil surface while carrying a woman on his back."], "durations": null, "exact_frames_per_prompt": [80], "background": "In the background, there is a miscellaneous sound, green trees, a shed house on the green grass surface, a wooden fence boundary, a green grass surface, the sky, and a soil surface.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/5-qOl012I-4.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_988_1"}, {"texts": ["A baby wearing black cloth is sitting in the lap of a man and crying."], "durations": null, "exact_frames_per_prompt": [80], "background": "In the background, the baby is crying and a person is speaking, there is a gray wall, and a black sofa.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/0cnlFOiCSO8.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_991_0"}, {"texts": ["A man wearing a black cap is sitting and holding a baby and watching the baby."], "durations": null, "exact_frames_per_prompt": [80], "background": "In the background, the baby is crying and a person is speaking, there is a gray wall, and a black sofa.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/0cnlFOiCSO8.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_991_1"}, {"texts": ["A man wearing a white shirt and blue jeans is standing on a boat, holding a fishing rod, and pulling it."], "durations": null, "exact_frames_per_prompt": [80], "background": "In the background, people and miscellaneous sounds are audible. There is a black fishing rod, a boat, blue water, and a blue sky with white clouds.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/2E0x_34Fnps.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_996_0"}, {"texts": ["A man whose upper half-body is visible, wearing a black t-shirt and black gloves, is standing on the left side and is holding a ham meat in his hand, and is speaking while putting the ham meat on the griddle."], "durations": null, "exact_frames_per_prompt": [80], "background": "In the background, a man is speaking; there is a house with doors and windows, white railing, a tree, plants, a griddle with the sweet corn cobs and a green grass surface.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/13ba6HzNUlk.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_997_1"}, {"texts": ["A person in a black clothing is at first spreading a black paper with his hands.", "The person picks up the tape and applies it on the black paper."], "durations": null, "exact_frames_per_prompt": [39, 41], "background": "In the background, there is a black paper, a tape, and a graph paper. There is the sound of a female voice in the background.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/11dU4SaFA-w.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_999_0"}, {"texts": ["A person wearing a black top is dancing on a green surface."], "durations": null, "exact_frames_per_prompt": [80], "background": "In the background, a song is playing, a person is laughing. There is a blur background, a car, a fuel station, fuel hose, green surface and white black background.", "npz_video": "storybench/npy_96x160pix_8fps/uvo-valid/videos/1qNpl-VZjM0.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 0, "npz_gt_video_start_frame": 0, "npz_gt_video_end_frame": null, "skip_frames_after_generation": null, "storybench_mode": "story_gen", "indices_to_select": null, "comment": "UVO_sparse_val_9_0"}]