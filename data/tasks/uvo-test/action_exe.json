[{"texts": ["A man wearing a white t-shirt is sitting behind the table, eating a burger and enjoying it while giving a thumbs up when it tastes good while a person whose hand is visible is holding a fork and picking up the food."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-FbSzomWtWw.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_dense_val_100_0_0"}, {"texts": ["A person, whose hand is visible, is holding a fork and picking some food with it while a man wearing a white t-shirt is sitting and eating food and looking at it."], "durations": null, "exact_frames_per_prompt": [29], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-FbSzomWtWw.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 6, "npz_gt_video_start_frame": 6, "npz_gt_video_end_frame": 35, "skip_frames_after_generation": 6, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_dense_val_100_1_0"}, {"texts": ["A man wearing a brown hat is speaking, and looking at a printed piece of paper while holding it in his hands."], "durations": null, "exact_frames_per_prompt": [69], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/0LH6n81tyds.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 73, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_dense_val_102_0_0"}, {"texts": ["A lady wearing green cloth is running and guiding a dog in a dog competition on a green grass surface while a man wearing white clothes standing on the right side watching the dog activity"], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-g1KcIki0ww.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_dense_val_103_0_0"}, {"texts": ["A white-brown dog is running with the lady in a dog competition on a green grass surface.\n while others are watching them."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-g1KcIki0ww.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_dense_val_103_1_0"}, {"texts": ["A man wearing a pink t-shirt sitting on a black couch opens the cap of a glass bottle."], "durations": null, "exact_frames_per_prompt": [15], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/0E85GLeQgas.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 19, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_dense_val_107_0_0"}, {"texts": ["The man places the bottle on a table."], "durations": null, "exact_frames_per_prompt": [18], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/0E85GLeQgas.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 19, "npz_gt_video_start_frame": 19, "npz_gt_video_end_frame": 37, "skip_frames_after_generation": 19, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_dense_val_107_0_1"}, {"texts": ["The man takes the bottle and hits it with another person's bottle."], "durations": null, "exact_frames_per_prompt": [22], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/0E85GLeQgas.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 37, "npz_gt_video_start_frame": 37, "npz_gt_video_end_frame": 59, "skip_frames_after_generation": 37, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_dense_val_107_0_2"}, {"texts": ["The man starts drinking liquid from the bottle."], "durations": null, "exact_frames_per_prompt": [18], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/0E85GLeQgas.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 59, "npz_gt_video_start_frame": 59, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 59, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_dense_val_107_0_3"}, {"texts": ["A person whose hands are visible is holding a brown cardboard piece."], "durations": null, "exact_frames_per_prompt": [34], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/0Ec2-bgw3Bw.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 38, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_dense_val_109_0_0"}, {"texts": ["The person is measuring the brown cardboard piece on a piece of white cardboard on the green table top."], "durations": null, "exact_frames_per_prompt": [42], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/0Ec2-bgw3Bw.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 38, "npz_gt_video_start_frame": 38, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 38, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_dense_val_109_0_1"}, {"texts": ["A person whose hands are visible is jerking the blanket and then he folds the blanket and puts a green cover on the bed."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-2hpTtCgbEU.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_dense_val_10_0_0"}, {"texts": ["A person wearing yellow clothes is standing and cutting a watermelon with a knife."], "durations": null, "exact_frames_per_prompt": [77], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-5M3vGKRUWQ.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 81, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_dense_val_110_0_0"}, {"texts": ["A person whose only hands are visible picks up the potato and the peeler."], "durations": null, "exact_frames_per_prompt": [14], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/0ctdGZygYeU.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 18, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_dense_val_111_0_0"}, {"texts": ["The person is peeling the potato."], "durations": null, "exact_frames_per_prompt": [62], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/0ctdGZygYeU.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 18, "npz_gt_video_start_frame": 18, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 18, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_dense_val_111_0_1"}, {"texts": ["A man is sitting, speaking, and moving his hand while looking toward the woman while a man wearing a grey suit is standing at the back and another man wearing a red shirt on the right is looking at the first man."], "durations": null, "exact_frames_per_prompt": [63], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-emx5qjikEc.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 17, "npz_gt_video_start_frame": 17, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 17, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_dense_val_112_1_0"}, {"texts": ["A woman is standing while holding the podium and giving a speech while a group of people are standing on the stage, few are holding banners."], "durations": null, "exact_frames_per_prompt": [10], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-emx5qjikEc.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 14, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_dense_val_112_2_0"}, {"texts": ["A person whose only hands are visible is cutting a pineapple crown with a scissor"], "durations": null, "exact_frames_per_prompt": [37], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-gJlTcI3jRI.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 22, "npz_gt_video_start_frame": 22, "npz_gt_video_end_frame": 59, "skip_frames_after_generation": 22, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_dense_val_113_0_0"}, {"texts": ["The person whose only hands are visible is cutting a pineapple with a knife."], "durations": null, "exact_frames_per_prompt": [21], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-gJlTcI3jRI.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 59, "npz_gt_video_start_frame": 59, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 59, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_dense_val_113_0_1"}, {"texts": ["A baby wearing blue clothes first touches the girl's ice cream while the first girl is eating her ice cream."], "durations": null, "exact_frames_per_prompt": [23], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/0qxkk9oBOW8.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 27, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_dense_val_114_1_0"}, {"texts": ["The baby turns back."], "durations": null, "exact_frames_per_prompt": [21], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/0qxkk9oBOW8.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 27, "npz_gt_video_start_frame": 27, "npz_gt_video_end_frame": 48, "skip_frames_after_generation": 27, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_dense_val_114_1_1"}, {"texts": ["The baby starts climbing on the back side of the seat."], "durations": null, "exact_frames_per_prompt": [19], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/0qxkk9oBOW8.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 48, "npz_gt_video_start_frame": 48, "npz_gt_video_end_frame": 67, "skip_frames_after_generation": 48, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_dense_val_114_1_2"}, {"texts": ["A girl wearing a black top is sitting and eating a hot dog and moving her hand."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-qicnRe0k4E.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_dense_val_116_0_0"}, {"texts": ["A person wearing a gray top is walking along with the dogs on the road while holding the leash of the dogs while a black vehicle is moving on the grey road surface on the left side."], "durations": null, "exact_frames_per_prompt": [62], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-1RxG3SJZfY.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 66, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_dense_val_117_0_0"}, {"texts": ["A white-brown dog is walking on the left side of the person while the dog is held by the person through a rope while another brown colored dog, is walking on the right side of the person while the dog is held by the person through a rope."], "durations": null, "exact_frames_per_prompt": [62], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-1RxG3SJZfY.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 66, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_dense_val_117_1_0"}, {"texts": ["A black dog is walking on the right side of the person while the dog is held by the person through a rope.\n while a white-brown dog is walking on the road."], "durations": null, "exact_frames_per_prompt": [62], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-1RxG3SJZfY.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 66, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_dense_val_117_2_0"}, {"texts": ["A woman wearing a vest is walking on the road with two dogs."], "durations": null, "exact_frames_per_prompt": [62], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-1RxG3SJZfY.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 66, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_dense_val_118_0_0"}, {"texts": ["A dark brown dog is also walking on the road with the woman while the white dog is walking with them."], "durations": null, "exact_frames_per_prompt": [62], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-1RxG3SJZfY.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 66, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_dense_val_118_2_0"}, {"texts": ["A woman wearing a white vest is walking on a road surface, holding the dogs' leashes."], "durations": null, "exact_frames_per_prompt": [62], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-1RxG3SJZfY.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 66, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_dense_val_119_0_0"}, {"texts": ["A brown dog is walking on a road surface on the right side being held by the woman while a white-brown dog is walking on a road surface on the left side while being held by the woman."], "durations": null, "exact_frames_per_prompt": [62], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-1RxG3SJZfY.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 66, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_dense_val_119_1_0"}, {"texts": ["A white brown dog is walking on a road surface on the left side held by the woman while the black dog on the right side."], "durations": null, "exact_frames_per_prompt": [62], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-1RxG3SJZfY.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 66, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_dense_val_119_2_0"}, {"texts": ["A man wearing a black jacket is sitting on a brown horse on the left side while a person wearing light blue clothes is standing and looking towards the right."], "durations": null, "exact_frames_per_prompt": [17], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-9Rcf4Fzed8.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 63, "npz_gt_video_start_frame": 63, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 63, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_dense_val_120_1_0"}, {"texts": ["A brown horse is standing on the left side with the first man sitting on its back while a group of people are sitting on the backs of horses behind the first man."], "durations": null, "exact_frames_per_prompt": [32], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-9Rcf4Fzed8.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 48, "npz_gt_video_start_frame": 48, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 48, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_dense_val_120_6_0"}, {"texts": ["A man wearing yellow female clothes first moves toward the iron board."], "durations": null, "exact_frames_per_prompt": [15], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/4Wx2DN6dSbw.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 19, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_dense_val_122_0_0"}, {"texts": ["The man wearing yellow female clothes picks up the iron, and starts ironing the cloth."], "durations": null, "exact_frames_per_prompt": [61], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/4Wx2DN6dSbw.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 19, "npz_gt_video_start_frame": 19, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 19, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_dense_val_122_0_1"}, {"texts": ["A man wearing a red t-shirt is cutting food and moving aside a food pan filled with food with another man."], "durations": null, "exact_frames_per_prompt": [38], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-43p5IQaY18.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 42, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_dense_val_123_0_0"}, {"texts": ["A girl wearing a pink outfit is sitting on a chair on the right side of a man, and holding and opening a brown object."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-LISB_b8rIw.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_dense_val_126_0_0"}, {"texts": ["A man wearing a black t-shirt is sitting on the left side of the first girl, and looking here and there."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-LISB_b8rIw.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_dense_val_126_1_0"}, {"texts": ["A man wearing a white shirt is tying a tie around his neck."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-irXG0K3Wfo.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_dense_val_128_0_0"}, {"texts": ["A man wearing a white shirt is tying a tie around his neck."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-irXG0K3Wfo.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_dense_val_129_0_0"}, {"texts": ["A brown horse is standing on the grass and getting its hair combed by the man."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/1y_BRfvi3lU.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_dense_val_130_0_0"}, {"texts": ["A man wearing a gray t-shirt is standing next to the horse and combing its hair."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/1y_BRfvi3lU.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_dense_val_130_1_0"}, {"texts": ["A man wearing a gray t-shirt is standing on the green grass surface and brushing the hair of a horse."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/1y_BRfvi3lU.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_dense_val_131_0_0"}, {"texts": ["A brown horse is standing on the green grass surface and getting its hair brushed by the man."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/1y_BRfvi3lU.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_dense_val_131_1_0"}, {"texts": ["A man wearing a grey t-shirt is standing on the ground and brushing the hair of the horse."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/1y_BRfvi3lU.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_dense_val_132_0_0"}, {"texts": ["A brown horse is standing on the ground and getting its hair brushed by the man."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/1y_BRfvi3lU.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_dense_val_132_1_0"}, {"texts": ["A person wearing a white top sitting on a white tile surface is burning a brown stick with a lighter while another person wearing printed pants is sitting near the person on the white tiled surface."], "durations": null, "exact_frames_per_prompt": [12], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-0edhdVNyYk.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 16, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_dense_val_134_0_0"}, {"texts": ["The person is setting fire to a plate through that burning stick."], "durations": null, "exact_frames_per_prompt": [28], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-0edhdVNyYk.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 16, "npz_gt_video_start_frame": 16, "npz_gt_video_end_frame": 44, "skip_frames_after_generation": 16, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_dense_val_134_0_1"}, {"texts": ["The person starts roasting a sausage."], "durations": null, "exact_frames_per_prompt": [36], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-0edhdVNyYk.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 44, "npz_gt_video_start_frame": 44, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 44, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_dense_val_134_0_2"}, {"texts": ["A kid wearing a dark blue top is sitting on the right side of the person."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-0edhdVNyYk.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_dense_val_134_1_0"}, {"texts": ["A man wearing a black suit is moving his hands in the air while walking while a group of people are sitting and listening to him."], "durations": null, "exact_frames_per_prompt": [12], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/1vcbRgLeQds.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 16, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_dense_val_136_0_0"}, {"texts": ["A man is standing in front of people behind the podium and speaking."], "durations": null, "exact_frames_per_prompt": [63], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/1vcbRgLeQds.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 17, "npz_gt_video_start_frame": 17, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 17, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_dense_val_136_1_0"}, {"texts": ["A boy wearing black pants is sitting on the floor, holding fake paper money counting it and putting it on the floor."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-2vdj0y_rI4.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_dense_val_138_0_0"}, {"texts": ["A man wearing a white shirt is standing in front of the beehive box and hits a wooden object on the metal object."], "durations": null, "exact_frames_per_prompt": [11], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-18X6h92xpw.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 15, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_dense_val_139_0_0"}, {"texts": ["The man holding an object and putting it at the corner of the beehive box."], "durations": null, "exact_frames_per_prompt": [17], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-18X6h92xpw.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 15, "npz_gt_video_start_frame": 15, "npz_gt_video_end_frame": 32, "skip_frames_after_generation": 15, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_dense_val_139_0_1"}, {"texts": ["The man picks up the beehive frame and watching it."], "durations": null, "exact_frames_per_prompt": [48], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-18X6h92xpw.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 32, "npz_gt_video_start_frame": 32, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 32, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_dense_val_139_0_2"}, {"texts": ["A person whose only hands are visible is moving his hands."], "durations": null, "exact_frames_per_prompt": [70], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/48549nP_RtU.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 74, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_dense_val_141_0_0"}, {"texts": ["A person wearing a brown sweater and black pants is standing behind a grey machine and moving his hands on the machine."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/0N3bmU-kCPI.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_dense_val_143_0_0"}, {"texts": ["A man wearing brown clothes is sitting, holding a red-black fish rod."], "durations": null, "exact_frames_per_prompt": [30], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/0zMVDwEJvBo.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 34, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_dense_val_147_0_0"}, {"texts": ["The man wearing brown clothes is moving his hand toward the gray surface."], "durations": null, "exact_frames_per_prompt": [46], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/0zMVDwEJvBo.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 34, "npz_gt_video_start_frame": 34, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 34, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_dense_val_147_0_1"}, {"texts": ["A person whose hands are visible is wrapping a thread around the metal wire."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/1AxGI4tOVLk.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_dense_val_148_0_0"}, {"texts": ["A yellow parrot is sitting on the girl's hand and moving his neck in different directions."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-AhebSCahUM.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_dense_val_149_1_0"}, {"texts": ["A person whose hand is visible is holding a white spatula and picking up the cookies on the spatula from the baking tray and put it on the left side while a brown dog is standing on the brown floor is watching the woman and moving his head."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/06HBsMpx9SM.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_dense_val_150_0_0"}, {"texts": ["A brown dog is standing on the brown surface and looking at the spatula and moving it's head while a man is holding the spatula and he is moving it left and right."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/06HBsMpx9SM.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_dense_val_150_1_0"}, {"texts": ["A person whose only hands are visible is inserting a bolt into a mechanical part."], "durations": null, "exact_frames_per_prompt": [40], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-pu2XnPx3BM.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 44, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_dense_val_151_0_0"}, {"texts": ["The person is tightening the bolt with an allen key."], "durations": null, "exact_frames_per_prompt": [35], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-pu2XnPx3BM.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 44, "npz_gt_video_start_frame": 44, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 44, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_dense_val_151_0_1"}, {"texts": ["A person whose hands are visible is holding a screw and putting in the disc brake of a bike wheel."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-pu2XnPx3BM.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_dense_val_152_0_0"}, {"texts": ["A girl wearing a floral shirt is eating a doughnut and holding a soft drink bottle.\n while a man on the right side in a colorful cap is sitting and a girl on the left side is sitting then starts speaking."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/3Qzzlg_SNGM.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_dense_val_153_0_0"}, {"texts": ["A boy wearing a cap on the right is looking at person one and two while a girl in the middle is talking with another girl and takes a bottle from her while eating and shows it to the camera."], "durations": null, "exact_frames_per_prompt": [42], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/3Qzzlg_SNGM.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 46, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_dense_val_153_2_0"}, {"texts": ["A girl wearing floral shirt is eating donut."], "durations": null, "exact_frames_per_prompt": [26], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/3Qzzlg_SNGM.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 30, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_dense_val_154_0_0"}, {"texts": ["The girl wearing floral shirt is holding a soft drink bottle."], "durations": null, "exact_frames_per_prompt": [50], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/3Qzzlg_SNGM.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 30, "npz_gt_video_start_frame": 30, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 30, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_dense_val_154_0_1"}, {"texts": ["A person wearing black clothes is taking out the fuel nozzle from the fuel dispenser."], "durations": null, "exact_frames_per_prompt": [26], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-LsxVKO78Fg.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 30, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_dense_val_156_0_0"}, {"texts": ["The person is going to the left side with it."], "durations": null, "exact_frames_per_prompt": [6], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-LsxVKO78Fg.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 30, "npz_gt_video_start_frame": 30, "npz_gt_video_end_frame": 36, "skip_frames_after_generation": 30, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_dense_val_156_0_1"}, {"texts": ["A man is pointing his finger towards the fuel dispenser while speaking while a yellow color bus and a car is moving towards the left side behind him."], "durations": null, "exact_frames_per_prompt": [35], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-LsxVKO78Fg.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 45, "npz_gt_video_start_frame": 45, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 45, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_dense_val_156_1_0"}, {"texts": ["An old man wearing a towel is washing the cloth by hitting it on a rock slab."], "durations": null, "exact_frames_per_prompt": [58], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/2xFTAu9ermA.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 22, "npz_gt_video_start_frame": 22, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 22, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_dense_val_163_1_0"}, {"texts": ["A person wearing a grey t-shirt is standing and cooking yellow food."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-5EKnCsR9A8.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_dense_val_165_0_0"}, {"texts": ["A woman whose hands are visible is flipping the black napkin."], "durations": null, "exact_frames_per_prompt": [62], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/0zeq-hN8X-U.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 66, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_dense_val_166_0_0"}, {"texts": ["The woman is putting the fork in the napkin."], "durations": null, "exact_frames_per_prompt": [14], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/0zeq-hN8X-U.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 66, "npz_gt_video_start_frame": 66, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 66, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_dense_val_166_0_1"}, {"texts": ["A man wearing an orange t-shirt is sitting and eating a piece of carrot."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-1vQEXONh3Y.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_dense_val_168_0_0"}, {"texts": ["A man wearing an orange t-shirt is sitting, holding food in his right hand and eating it."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-1vQEXONh3Y.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_dense_val_169_0_0"}, {"texts": ["A person, whose hands are visible, is holding a packet of a green toy."], "durations": null, "exact_frames_per_prompt": [22], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/3C4OGrKPpUk.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 26, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_dense_val_16_0_0"}, {"texts": ["The person is putting it back on the wooden table."], "durations": null, "exact_frames_per_prompt": [28], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/3C4OGrKPpUk.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 26, "npz_gt_video_start_frame": 26, "npz_gt_video_end_frame": 54, "skip_frames_after_generation": 26, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_dense_val_16_0_1"}, {"texts": ["The person is holding another box and showing it."], "durations": null, "exact_frames_per_prompt": [27], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/3C4OGrKPpUk.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 54, "npz_gt_video_start_frame": 54, "npz_gt_video_end_frame": 81, "skip_frames_after_generation": 54, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_dense_val_16_0_2"}, {"texts": ["A girl wearing a grey t-shirt is standing and clicking pictures of a group of people."], "durations": null, "exact_frames_per_prompt": [9], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/0pEjYh3BF1o.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 13, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_dense_val_171_0_0"}, {"texts": ["The girl wearing a grey t-shirt starts walking while others are in different activities"], "durations": null, "exact_frames_per_prompt": [67], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/0pEjYh3BF1o.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 13, "npz_gt_video_start_frame": 13, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 13, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_dense_val_171_0_1"}, {"texts": ["A man wearing a black t-shirt is holding a tray filled with food in his hands and walking forward while a woman wearing a grey top is holding a camera, clicking pictures, and walking forward, a group of people are sitting in different directions; some are standing to the left."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/0pEjYh3BF1o.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_dense_val_171_1_0"}, {"texts": ["A kid is crawling on the bed."], "durations": null, "exact_frames_per_prompt": [27], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-HiMSqjiJxk.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 31, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_dense_val_173_0_0"}, {"texts": ["The kid is lifting the bed-sheet."], "durations": null, "exact_frames_per_prompt": [33], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-HiMSqjiJxk.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 31, "npz_gt_video_start_frame": 31, "npz_gt_video_end_frame": 64, "skip_frames_after_generation": 31, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_dense_val_173_0_1"}, {"texts": ["The kid is getting down from the bed."], "durations": null, "exact_frames_per_prompt": [15], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-HiMSqjiJxk.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 64, "npz_gt_video_start_frame": 64, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 64, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_dense_val_173_0_2"}, {"texts": ["A man wearing a white cloth is sitting on the chair over the white platform."], "durations": null, "exact_frames_per_prompt": [23], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/2lBUaUBD9JE.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 57, "npz_gt_video_start_frame": 57, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 57, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_dense_val_175_1_0"}, {"texts": ["A baby wearing a pink t-shirt is sitting on the bed, holding a book in his hands, making cooing sounds while looking at the book"], "durations": null, "exact_frames_per_prompt": [35], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-CQPR7e8xKE.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 39, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_dense_val_176_0_0"}, {"texts": ["The baby lies down on the cushions."], "durations": null, "exact_frames_per_prompt": [41], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-CQPR7e8xKE.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 39, "npz_gt_video_start_frame": 39, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 39, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_dense_val_176_0_1"}, {"texts": ["A woman wearing a gray t-shirt is closing the plastic cup lid."], "durations": null, "exact_frames_per_prompt": [51], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/0RnlrsxG1hY.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 55, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_dense_val_178_0_0"}, {"texts": ["The woman is mixing the food in the white bowl."], "durations": null, "exact_frames_per_prompt": [25], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/0RnlrsxG1hY.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 55, "npz_gt_video_start_frame": 55, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 55, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_dense_val_178_0_1"}, {"texts": ["A woman wearing a gray t-shirt is closing the plastic cup."], "durations": null, "exact_frames_per_prompt": [50], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/0RnlrsxG1hY.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 54, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_dense_val_179_0_0"}, {"texts": ["The woman wearing a gray t-shirt is mixing the food in the white bowl."], "durations": null, "exact_frames_per_prompt": [26], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/0RnlrsxG1hY.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 54, "npz_gt_video_start_frame": 54, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 54, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_dense_val_179_0_1"}, {"texts": ["A woman wearing a red top and black sweater is showing a hand sign while a person in black clothes is pouring some liquid from a can into a glass."], "durations": null, "exact_frames_per_prompt": [17], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/4GCihEtEtIg.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 21, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_dense_val_180_0_0"}, {"texts": ["The woman is drinking something from the glass."], "durations": null, "exact_frames_per_prompt": [24], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/4GCihEtEtIg.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 21, "npz_gt_video_start_frame": 21, "npz_gt_video_end_frame": 45, "skip_frames_after_generation": 21, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_dense_val_180_0_1"}, {"texts": ["A man wearing a black shirt is pouring drinks into the glass while a woman in a red top is drinking from the glass and then giving a pose."], "durations": null, "exact_frames_per_prompt": [59], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/4GCihEtEtIg.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 10, "npz_gt_video_start_frame": 10, "npz_gt_video_end_frame": 69, "skip_frames_after_generation": 10, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_dense_val_180_1_0"}, {"texts": ["A woman wearing a black dress is walking toward the left while holding a glass and a man in a black t-shirt is standing and holding a snake."], "durations": null, "exact_frames_per_prompt": [32], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-NljOk6doRg.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 15, "npz_gt_video_start_frame": 15, "npz_gt_video_end_frame": 47, "skip_frames_after_generation": 15, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_dense_val_181_0_0"}, {"texts": ["A man wearing a black t-shirt is standing while holding a snake while the woman behind starts walking"], "durations": null, "exact_frames_per_prompt": [74], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-NljOk6doRg.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 78, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_dense_val_181_1_0"}, {"texts": ["The man is touched by another man."], "durations": null, "exact_frames_per_prompt": [3], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-NljOk6doRg.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 78, "npz_gt_video_start_frame": 78, "npz_gt_video_end_frame": 81, "skip_frames_after_generation": 78, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_dense_val_181_1_1"}, {"texts": ["A snake is in the hands of the first man while a woman wearing a black top is standing at the backside and then walk toward the left while holding a glass in her hand and a person on the right side touches the man on the back."], "durations": null, "exact_frames_per_prompt": [77], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-NljOk6doRg.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 81, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_dense_val_181_3_0"}, {"texts": ["A person whose hands are visible is punching the paper with the help of a binding machine."], "durations": null, "exact_frames_per_prompt": [32], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-3otOHOo7kY.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 36, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_dense_val_182_0_0"}, {"texts": ["The person is picking up the papers from the white surface."], "durations": null, "exact_frames_per_prompt": [43], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-3otOHOo7kY.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 36, "npz_gt_video_start_frame": 36, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 36, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_dense_val_182_0_1"}, {"texts": ["A man wearing a white shirt is running away from a man wearing a blazer while others are looking at them and start laughing."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/0fp_LpMTbfg.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_dense_val_184_0_0"}, {"texts": ["A man wearing a blazer is chasing the first man and pointing his finger toward him while a group of people are sitting and laughing while watching the two men."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/0fp_LpMTbfg.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_dense_val_184_1_0"}, {"texts": ["A person whose hands are only visible lifts a white box of i-pad with his hands."], "durations": null, "exact_frames_per_prompt": [70], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/0OGDlNNH14Y.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 10, "npz_gt_video_start_frame": 10, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 10, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_dense_val_185_0_0"}, {"texts": ["A boy wearing a white t-shirt is sitting and drinking something from a glass and then holds a glass in his right hand."], "durations": null, "exact_frames_per_prompt": [25], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/06A7IVDdaXo.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 29, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_dense_val_187_0_0"}, {"texts": ["The boy then shifts the glass in his left hand and again starts drinking and looking here and there."], "durations": null, "exact_frames_per_prompt": [51], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/06A7IVDdaXo.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 29, "npz_gt_video_start_frame": 29, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 29, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_dense_val_187_0_1"}, {"texts": ["A woman wearing red clothes is tying a ribbon around a gift wrapped box with her hands and her toe while the cat watches it."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/0AkA2Ru9qG0.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_dense_val_18_0_0"}, {"texts": ["A white cat is looking at the box while the woman is packing the gift."], "durations": null, "exact_frames_per_prompt": [8], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/0AkA2Ru9qG0.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 12, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_dense_val_18_1_0"}, {"texts": ["The cat is moving."], "durations": null, "exact_frames_per_prompt": [68], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/0AkA2Ru9qG0.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 12, "npz_gt_video_start_frame": 12, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 12, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_dense_val_18_1_1"}, {"texts": ["A woman wearing blue jeans is sitting on a brown horse and riding it on a gray surface.\n while a brown and white color dog is walking along with the horse on the road."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/0x6wV-RDB4A.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_dense_val_190_0_0"}, {"texts": ["A brown horse is walking on a gray surface with a woman sitting on its back while a white-brown dog is walking on the road with them."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/0x6wV-RDB4A.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_dense_val_190_1_0"}, {"texts": ["A white brown dog is walking on the right side of the horse and then in the front on a gray surface while a girl wearing blue jeans is sitting on the horse and riding it."], "durations": null, "exact_frames_per_prompt": [63], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/0x6wV-RDB4A.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 67, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_dense_val_190_2_0"}, {"texts": ["A person wearing a navy blue t-shirt is mixing some brown stuff in a metal bowl."], "durations": null, "exact_frames_per_prompt": [48], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-CYI2yJHIj4.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 52, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_dense_val_191_0_0"}, {"texts": ["The person takes a white thing on a piece of paper from a jar."], "durations": null, "exact_frames_per_prompt": [21], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-CYI2yJHIj4.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 52, "npz_gt_video_start_frame": 52, "npz_gt_video_end_frame": 73, "skip_frames_after_generation": 52, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_dense_val_191_0_1"}, {"texts": ["The person starts rubbing the paper inside a metal tray."], "durations": null, "exact_frames_per_prompt": [7], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-CYI2yJHIj4.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 73, "npz_gt_video_start_frame": 73, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 73, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_dense_val_191_0_2"}, {"texts": ["A person whose only hands are visible, wearing gloves is holding a ruler scale on the sheet and cutting the sheet with a cutter"], "durations": null, "exact_frames_per_prompt": [53], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-4m_QzxDvxc.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 57, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_dense_val_192_0_0"}, {"texts": ["The person is folding the edge of the sheet on the ruler scale."], "durations": null, "exact_frames_per_prompt": [23], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-4m_QzxDvxc.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 57, "npz_gt_video_start_frame": 57, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 57, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_dense_val_192_0_1"}, {"texts": ["A person whose only hands are visible is drawing lines on a white paper with a scale and a white object."], "durations": null, "exact_frames_per_prompt": [25], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-4m_QzxDvxc.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 30, "npz_gt_video_start_frame": 30, "npz_gt_video_end_frame": 55, "skip_frames_after_generation": 30, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_dense_val_193_0_0"}, {"texts": ["The person folding the white paper against the scale."], "durations": null, "exact_frames_per_prompt": [25], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-4m_QzxDvxc.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 55, "npz_gt_video_start_frame": 55, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 55, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_dense_val_193_0_1"}, {"texts": ["A man wearing a shirt is walking toward the woman while a person is moving towards the left side."], "durations": null, "exact_frames_per_prompt": [8], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/0lya888dgZ0.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 12, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_dense_val_194_1_0"}, {"texts": ["The man wearing a shirt takes the award from the woman, and then stands while looking toward the left and smiles while a man is taking pictures on his phone and then starts moving towards the left."], "durations": null, "exact_frames_per_prompt": [67], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/0lya888dgZ0.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 12, "npz_gt_video_start_frame": 12, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 12, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_dense_val_194_1_1"}, {"texts": ["A man wearing black clothes is standing on the right, capturing the first man and the woman."], "durations": null, "exact_frames_per_prompt": [19], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/0lya888dgZ0.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 23, "npz_gt_video_start_frame": 23, "npz_gt_video_end_frame": 42, "skip_frames_after_generation": 23, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_dense_val_194_3_0"}, {"texts": ["The man walks while the first man and a woman is standing on the stage while carrying an award."], "durations": null, "exact_frames_per_prompt": [11], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/0lya888dgZ0.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 42, "npz_gt_video_start_frame": 42, "npz_gt_video_end_frame": 53, "skip_frames_after_generation": 42, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_dense_val_194_3_1"}, {"texts": ["A woman wearing red cloth is holding the threads.  while a brown cat is standing near the gift and looking into it."], "durations": null, "exact_frames_per_prompt": [21], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/0AkA2Ru9qG0.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 25, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_dense_val_19_0_0"}, {"texts": ["The woman wearing red cloth is putting her legs on the box."], "durations": null, "exact_frames_per_prompt": [21], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/0AkA2Ru9qG0.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 25, "npz_gt_video_start_frame": 25, "npz_gt_video_end_frame": 46, "skip_frames_after_generation": 25, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_dense_val_19_0_1"}, {"texts": ["The woman wearing red cloth is tying the threads."], "durations": null, "exact_frames_per_prompt": [34], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/0AkA2Ru9qG0.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 46, "npz_gt_video_start_frame": 46, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 46, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_dense_val_19_0_2"}, {"texts": ["A white cat is sitting beside the box.\n while the woman is packing the gift."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/0AkA2Ru9qG0.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_dense_val_19_1_0"}, {"texts": ["A man wearing a black t-shirt is standing with the boy and putting both hands in the grey bowl and mixing a white object while the other person wearing black top is also mixing something in the bowl."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-0eUJLYCmbs.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_dense_val_201_0_0"}, {"texts": ["A boy wearing a purple t-shirt is standing with the first man, putting both hands in the grey bowl and mixing a white object while a man wearing black t-shirt mixing something in the bowl"], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-0eUJLYCmbs.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_dense_val_201_1_0"}, {"texts": ["A boy wearing a green t-shirt and blue pants is holding an apple in his hands and running forward on the grass field."], "durations": null, "exact_frames_per_prompt": [28], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-nTtZiY1He0.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 32, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_dense_val_202_0_0"}, {"texts": ["A man is wearing a black outfit is standing on the grass while holding a golf club."], "durations": null, "exact_frames_per_prompt": [37], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/08Exugf5OvU.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 41, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_dense_val_203_0_0"}, {"texts": ["The man is playing golf."], "durations": null, "exact_frames_per_prompt": [39], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/08Exugf5OvU.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 41, "npz_gt_video_start_frame": 41, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 41, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_dense_val_203_0_1"}, {"texts": ["A man wearing a black t-shirt is standing in the front and cheering while moving his hands."], "durations": null, "exact_frames_per_prompt": [42], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/2QBk2LE-mbI.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 46, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_dense_val_204_0_0"}, {"texts": ["A man wearing a watch is holding a knife and peeling a watermelon.\n"], "durations": null, "exact_frames_per_prompt": [77], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-CHrri2fxkk.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 81, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_dense_val_205_0_0"}, {"texts": ["A woman wearing a blue jacket holding a brush is brushing the black horse while a white horse on the right side is standing."], "durations": null, "exact_frames_per_prompt": [16], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/1RYKyKAYZOQ.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 20, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_dense_val_207_0_0"}, {"texts": ["The woman is then walking towards his tail to brush it."], "durations": null, "exact_frames_per_prompt": [60], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/1RYKyKAYZOQ.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 20, "npz_gt_video_start_frame": 20, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 20, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_dense_val_207_0_1"}, {"texts": ["A black horse is standing on a gray surface and getting brushed by the woman."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/1RYKyKAYZOQ.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_dense_val_207_1_0"}, {"texts": ["A man wearing a black shirt is standing, showing and tying a black-white striped bow tie."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/2ujJI-dSrG0.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_dense_val_208_0_0"}, {"texts": ["A man wearing a black jacket is standing on the left side holding a camera and then moves back while a man is adjusting the front right tire of a car; a man on the left side is raising the car up with a jack; a woman is standing; and a man is walking and then stands."], "durations": null, "exact_frames_per_prompt": [77], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/0BhVDzsdGUM.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 81, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_dense_val_209_2_0"}, {"texts": ["The third man wearing green-black clothes is standing and fixing a car wheel while a group of people are standing around the car and looking at the car."], "durations": null, "exact_frames_per_prompt": [77], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/0BhVDzsdGUM.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 81, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_dense_val_209_3_0"}, {"texts": ["A man wearing green-black clothes is standing and removing a car wheel while a man wearing yellow-black clothes walks to the right, a woman wearing yellow-black clothes stands on the road holding a camera, a person wearing red-black clothes is standing on the left and moves back when the wheel falls, and a man wearing green-black clothes is standing and removing the back wheel of the car."], "durations": null, "exact_frames_per_prompt": [77], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/0BhVDzsdGUM.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 81, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_dense_val_209_4_0"}, {"texts": ["A person is getting a tattoo in his arm another person wearing gloves is making a tattoo on the person's hand."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-0COMyPvkHU.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_dense_val_211_0_0"}, {"texts": ["A person whose hands are visible is wearing a pair of white rubber gloves and making a tattoo using a tattoo machine on a person's arm."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-0COMyPvkHU.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_dense_val_211_1_0"}, {"texts": ["A person whose only hands and legs are visible is making a tattoo on another person's hand with a tattoo machine."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-0COMyPvkHU.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_dense_val_212_0_0"}, {"texts": ["A person whose hand is visible is getting a tattoo on his hand by another person."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-0COMyPvkHU.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_dense_val_212_1_0"}, {"texts": ["A boy wearing a black jacket is standing and picking a doughnut from a white box."], "durations": null, "exact_frames_per_prompt": [29], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-3L9Zso_oHg.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 33, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_dense_val_216_0_0"}, {"texts": ["The boy holds it in his mouth and eat it at once."], "durations": null, "exact_frames_per_prompt": [47], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-3L9Zso_oHg.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 33, "npz_gt_video_start_frame": 33, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 33, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_dense_val_216_0_1"}, {"texts": ["A woman wearing a beige colored dress is standing, holding an award in her left hand and speaking into the mic."], "durations": null, "exact_frames_per_prompt": [70], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/18GDi4O5VXA.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 10, "npz_gt_video_start_frame": 10, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 10, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_dense_val_218_0_0"}, {"texts": ["A woman wearing a black dress is standing and pulling a brown-white fitness bandage"], "durations": null, "exact_frames_per_prompt": [27], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/4-dI5vRgGWQ.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 31, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_dense_val_220_0_0"}, {"texts": ["The woman is putting it around her wrist."], "durations": null, "exact_frames_per_prompt": [48], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/4-dI5vRgGWQ.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 31, "npz_gt_video_start_frame": 31, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 31, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_dense_val_220_0_1"}, {"texts": ["A person wearing black t-shirt and black hat is sitting on the left side on the brown sofa while a man wearing a black t-shirt is sitting in the middle on a sofa and a man wearing a gray t-shirt is sitting on the right side of the sofa."], "durations": null, "exact_frames_per_prompt": [29], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-h1tKOt3PI0.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 33, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_dense_val_221_0_0"}, {"texts": ["The person lifts his glass and drinks it."], "durations": null, "exact_frames_per_prompt": [47], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-h1tKOt3PI0.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 33, "npz_gt_video_start_frame": 33, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 33, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_dense_val_221_0_1"}, {"texts": ["A person wearing black t-shirt and fox cap is sitting in the middle on the brown sofa.  while a man wearing a black shirt is sitting and picking a glass on the left side and another man wearing a gray t-shirt is sitting and looking towards the right side on the right side."], "durations": null, "exact_frames_per_prompt": [67], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-h1tKOt3PI0.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 71, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_dense_val_221_1_0"}, {"texts": ["The person is moving forward and touching the glass bottle while the man on the left side is putting the glass on the table."], "durations": null, "exact_frames_per_prompt": [9], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-h1tKOt3PI0.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 71, "npz_gt_video_start_frame": 71, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 71, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_dense_val_221_1_1"}, {"texts": ["A person wearing a grey t-shirt is sitting on the right side on the brown sofa while a man wearing black shirt is sitting on the left and he starts drinking and another man is sitting in the middle."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-h1tKOt3PI0.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_dense_val_221_2_0"}, {"texts": ["A baby is sitting on a green chair, and getting fed by the person with a spoon."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/88lYiEMZ9fI.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_dense_val_227_0_0"}, {"texts": ["A person whose hand is visible is holding a spoon and feeding the baby with it."], "durations": null, "exact_frames_per_prompt": [46], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/88lYiEMZ9fI.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 34, "npz_gt_video_start_frame": 34, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 34, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_dense_val_227_1_0"}, {"texts": ["A baby is sitting on a green chair."], "durations": null, "exact_frames_per_prompt": [29], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/88lYiEMZ9fI.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 33, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_dense_val_228_0_0"}, {"texts": ["The baby is getting fed by a person with a spoon."], "durations": null, "exact_frames_per_prompt": [46], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/88lYiEMZ9fI.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 33, "npz_gt_video_start_frame": 33, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 33, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_dense_val_228_0_1"}, {"texts": ["A person whose hand is visible is holding a spoon and feeding the baby with it."], "durations": null, "exact_frames_per_prompt": [44], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/88lYiEMZ9fI.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 36, "npz_gt_video_start_frame": 36, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 36, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_dense_val_228_1_0"}, {"texts": ["A man wearing a t-shirt and a red cap is standing."], "durations": null, "exact_frames_per_prompt": [14], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/0L8ZcrRtFxI.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 18, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_dense_val_22_0_0"}, {"texts": ["The man is speaking in the microphone while shaking a spray can in his hand."], "durations": null, "exact_frames_per_prompt": [62], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/0L8ZcrRtFxI.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 18, "npz_gt_video_start_frame": 18, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 18, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_dense_val_22_0_1"}, {"texts": ["A lady wearing a blue t-shirt and white pants is standing on the green grass field."], "durations": null, "exact_frames_per_prompt": [9], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-khmO1mLnoA.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 13, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_dense_val_230_0_0"}, {"texts": ["The lady is hitting the golf ball with a golf stick while a man in a blue t-shirt is moving on the grass surface near the woman."], "durations": null, "exact_frames_per_prompt": [67], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-khmO1mLnoA.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 13, "npz_gt_video_start_frame": 13, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 13, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_dense_val_230_0_1"}, {"texts": ["A man wearing a blue t-shirt and black pants is standing on the left side of the lady on the green grass field."], "durations": null, "exact_frames_per_prompt": [29], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-khmO1mLnoA.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 51, "npz_gt_video_start_frame": 51, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 51, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_dense_val_230_1_0"}, {"texts": ["A person whose only hand is visible is rubbing his hand on the black tire."], "durations": null, "exact_frames_per_prompt": [47], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/62joIFmFImo.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 51, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_dense_val_231_0_0"}, {"texts": ["A woman wearing black clothes is sitting near the bed and doing hand gestures."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/2qxlxhL8wIg.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_dense_val_232_0_0"}, {"texts": ["A person wearing a black t-shirt and blue shorts is standing while a girl wearing a black outfits in the centre holding cow nipple in her left hand"], "durations": null, "exact_frames_per_prompt": [13], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-x6_TtmCCig.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 17, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_dense_val_234_0_0"}, {"texts": ["The person wearing a black t-shirt and blue shorts is moving his hand toward the cow while a girl wearing a black outfits moved away from there"], "durations": null, "exact_frames_per_prompt": [63], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-x6_TtmCCig.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 17, "npz_gt_video_start_frame": 17, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 17, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_dense_val_234_0_1"}, {"texts": ["A girl wearing black clothes is standing and milking a black cow while the person wearing blue pants is instructing the girl"], "durations": null, "exact_frames_per_prompt": [57], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-x6_TtmCCig.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 61, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_dense_val_234_2_0"}, {"texts": ["The girl wearing black clothes starts walking while the person wearing blue pants pats the cow"], "durations": null, "exact_frames_per_prompt": [19], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-x6_TtmCCig.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 61, "npz_gt_video_start_frame": 61, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 61, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_dense_val_234_2_1"}, {"texts": ["A girl wearing a white top and green shorts is standing and watching the black cow while another girl wearing black clothes is milking the cow, a person wearing a black t-shirt and blue shorts is standing and touching the cow, and a third girl and another person are standing on the right."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-x6_TtmCCig.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_dense_val_234_3_0"}, {"texts": ["A person is cutting a watermelon."], "durations": null, "exact_frames_per_prompt": [8], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/0-NbSopwDlk.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 12, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_dense_val_236_0_0"}, {"texts": ["The person is putting its pieces in a steel bowl."], "durations": null, "exact_frames_per_prompt": [34], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/0-NbSopwDlk.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 12, "npz_gt_video_start_frame": 12, "npz_gt_video_end_frame": 46, "skip_frames_after_generation": 12, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_dense_val_236_0_1"}, {"texts": ["The person starts cutting the other part of a watermelon."], "durations": null, "exact_frames_per_prompt": [34], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/0-NbSopwDlk.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 46, "npz_gt_video_start_frame": 46, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 46, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_dense_val_236_0_2"}, {"texts": ["A person wearing white clothes is standing near the wooden box, taking out a metal object with a scraper."], "durations": null, "exact_frames_per_prompt": [31], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/0xS32LBYiWA.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 35, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_dense_val_237_0_0"}, {"texts": ["The person is putting it on the other metal object."], "durations": null, "exact_frames_per_prompt": [23], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/0xS32LBYiWA.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 35, "npz_gt_video_start_frame": 35, "npz_gt_video_end_frame": 58, "skip_frames_after_generation": 35, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_dense_val_237_0_1"}, {"texts": ["The person is showing it."], "durations": null, "exact_frames_per_prompt": [22], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/0xS32LBYiWA.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 58, "npz_gt_video_start_frame": 58, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 58, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_dense_val_237_0_2"}, {"texts": ["A man wearing a white shirt is on the television, speaking something on a mic.\n"], "durations": null, "exact_frames_per_prompt": [36], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/oVMHriEp7f0.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 40, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_dense_val_238_0_0"}, {"texts": ["A boy on the left side wearing a yellow t-shirt is sitting and eating watermelon while the other boy in white is looking t him"], "durations": null, "exact_frames_per_prompt": [72], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/0G2A0x2-vf4.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 76, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_dense_val_239_0_0"}, {"texts": ["The boy takes another piece of watermelon."], "durations": null, "exact_frames_per_prompt": [4], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/0G2A0x2-vf4.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 76, "npz_gt_video_start_frame": 76, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 76, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_dense_val_239_0_1"}, {"texts": ["A boy on the right side wearing a white vest is sitting and eating watermelon in speed while another boy wearing yellow outfit and a blue cap is sitting on the left side of the first boy and eating watermelon and a kid is running towards the right side."], "durations": null, "exact_frames_per_prompt": [53], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/0G2A0x2-vf4.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 57, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_dense_val_239_1_0"}, {"texts": ["The boy wearing a white vest wipes his mouth with his vest while group of people are doing different activities, some are sitting and some are standing."], "durations": null, "exact_frames_per_prompt": [23], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/0G2A0x2-vf4.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 57, "npz_gt_video_start_frame": 57, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 57, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_dense_val_239_1_1"}, {"texts": ["A person whose only head is visible is sitting and eating watermelon while some people are standing and walking, others are eating watermelons."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/0G2A0x2-vf4.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_dense_val_239_2_0"}, {"texts": ["A person wearing red clothes is standing while holding food in his right hand and then puts the food into the silver container."], "durations": null, "exact_frames_per_prompt": [22], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-_3E3GBXAUc.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 26, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_dense_val_23_0_0"}, {"texts": ["The person wearing red clothes picks an orange food from the white container."], "durations": null, "exact_frames_per_prompt": [14], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-_3E3GBXAUc.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 26, "npz_gt_video_start_frame": 26, "npz_gt_video_end_frame": 40, "skip_frames_after_generation": 26, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_dense_val_23_0_1"}, {"texts": ["The person wearing red clothes puts the orange food into another white container, and mixes it with food with his right hand."], "durations": null, "exact_frames_per_prompt": [39], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-_3E3GBXAUc.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 40, "npz_gt_video_start_frame": 40, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 40, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_dense_val_23_0_2"}, {"texts": ["A girl wearing a pink top is sitting on the right side while the girl in blue t-shirt takes a slip from the box by using mouth."], "durations": null, "exact_frames_per_prompt": [12], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/0hZBaym-z8M.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 16, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_dense_val_240_0_0"}, {"texts": ["The girl watching the second girl and laughing while the girl in blue t-shirt takes the slip from the mouth and open it."], "durations": null, "exact_frames_per_prompt": [64], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/0hZBaym-z8M.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 16, "npz_gt_video_start_frame": 16, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 16, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_dense_val_240_0_1"}, {"texts": ["A girl wearing a sky blue top is sitting on the left side while a girl wearing a pink top is sitting on the right side."], "durations": null, "exact_frames_per_prompt": [8], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/0hZBaym-z8M.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 12, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_dense_val_240_1_0"}, {"texts": ["The girl is picking a piece of paper from a bowl with her mouth while a girl wearing a pink top is looking in the direction of the girl and laughing."], "durations": null, "exact_frames_per_prompt": [23], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/0hZBaym-z8M.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 12, "npz_gt_video_start_frame": 12, "npz_gt_video_end_frame": 35, "skip_frames_after_generation": 12, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_dense_val_240_1_1"}, {"texts": ["The girl is removing it from her mouth and opening it while laughing."], "durations": null, "exact_frames_per_prompt": [45], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/0hZBaym-z8M.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 35, "npz_gt_video_start_frame": 35, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 35, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_dense_val_240_1_2"}, {"texts": ["A woman wearing a blue-grey outfit is sitting on the grey floor and pointing on her left side while the other woman sat with a baby and a fish."], "durations": null, "exact_frames_per_prompt": [31], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-15UpGVyV4w.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 35, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_dense_val_242_0_0"}, {"texts": ["The woman wearing a blue-grey outfit starts feeding fishes."], "durations": null, "exact_frames_per_prompt": [19], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-15UpGVyV4w.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 35, "npz_gt_video_start_frame": 35, "npz_gt_video_end_frame": 54, "skip_frames_after_generation": 35, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_dense_val_242_0_1"}, {"texts": ["A girl wearing a white-pink dress is sitting on the left side of a woman and having a boy on her lap and throwing food in the water."], "durations": null, "exact_frames_per_prompt": [50], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-15UpGVyV4w.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 54, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_dense_val_242_1_0"}, {"texts": ["A boy wearing an orange dress is sitting on the girl's lap and moving his hands while a woman in a gray tank top is sitting next to them feeding the fishes, a few fishes are swimming in the pond, and a few people are walking at the back."], "durations": null, "exact_frames_per_prompt": [50], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-15UpGVyV4w.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 54, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_dense_val_242_2_0"}, {"texts": ["A man on the left side wearing a black t-shirt is sitting and speaking something and moving his hand while another man in a black shirt is sitting on the right side and holding a glass of drink."], "durations": null, "exact_frames_per_prompt": [35], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-ErtRXry_Ns.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 39, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_dense_val_247_0_0"}, {"texts": ["The man is picking up the glass and drink something while another man starts drinking from the glass."], "durations": null, "exact_frames_per_prompt": [41], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-ErtRXry_Ns.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 39, "npz_gt_video_start_frame": 39, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 39, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_dense_val_247_0_1"}, {"texts": ["A boy on the right side wearing a black t-shirt is sitting and holding a glass while a man on the left side wearing a black t-shirt is sitting and speaking while looking down and moving his hand."], "durations": null, "exact_frames_per_prompt": [16], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-ErtRXry_Ns.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 20, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_dense_val_247_1_0"}, {"texts": ["The boy is then picking it up while a man on the left side wearing a black t-shirt then picking up his glass."], "durations": null, "exact_frames_per_prompt": [32], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-ErtRXry_Ns.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 20, "npz_gt_video_start_frame": 20, "npz_gt_video_end_frame": 52, "skip_frames_after_generation": 20, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_dense_val_247_1_1"}, {"texts": ["The boy is drink something while a man on the left side wearing a black t-shirt is drinking from the glass."], "durations": null, "exact_frames_per_prompt": [28], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-ErtRXry_Ns.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 52, "npz_gt_video_start_frame": 52, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 52, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_dense_val_247_1_2"}, {"texts": ["A boy wearing a green t-shirt and black jeans is standing on the brown floor and tearing a wrapping paper of a toy box while a woman is sitting and watching the boy and a dog is walking from right to left."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/1DAtchstb_4.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_dense_val_249_0_0"}, {"texts": ["A woman wearing a white top and blue jeans is sitting on a sofa and looking at the first boy while a small white dog is roaming around on the carpet surface."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/1DAtchstb_4.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_dense_val_249_1_0"}, {"texts": ["A woman wearing a black coat is holding her chin and speaking something."], "durations": null, "exact_frames_per_prompt": [20], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/1ejgHKw8E3Y.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 24, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_dense_val_24_0_0"}, {"texts": ["A boy wearing a orange t-shirt is sitting on witness stand and speaking something on a mic while a woman wearing black clothes is standing and speaking something, and some other people are sitting and standing."], "durations": null, "exact_frames_per_prompt": [55], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/1ejgHKw8E3Y.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 25, "npz_gt_video_start_frame": 25, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 25, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_dense_val_24_1_0"}, {"texts": ["A girl wearing a blue coat is sitting on a chair and using computer.\n a man dressed in red sits in front and speaks into the microphone; a man dressed in black sits to the left; and a man dressed in grey stands to the right."], "durations": null, "exact_frames_per_prompt": [38], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/1ejgHKw8E3Y.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 25, "npz_gt_video_start_frame": 25, "npz_gt_video_end_frame": 63, "skip_frames_after_generation": 25, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_dense_val_24_2_0"}, {"texts": ["A judge is sitting on the left side of the boy and looking at the boy as a woman wearing a grey suit is working on the desktop while sitting on a chair."], "durations": null, "exact_frames_per_prompt": [38], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/1ejgHKw8E3Y.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 25, "npz_gt_video_start_frame": 25, "npz_gt_video_end_frame": 63, "skip_frames_after_generation": 25, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_dense_val_24_4_0"}, {"texts": ["A person whose legs are visible is wearing white shoes and is standing."], "durations": null, "exact_frames_per_prompt": [31], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-L1mPUcEHQY.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 35, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_dense_val_250_0_0"}, {"texts": ["The person is playing golf on the green grass surface."], "durations": null, "exact_frames_per_prompt": [43], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-L1mPUcEHQY.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 35, "npz_gt_video_start_frame": 35, "npz_gt_video_end_frame": 78, "skip_frames_after_generation": 35, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_dense_val_250_0_1"}, {"texts": ["A man on the left is sitting, holding a cigarette in his left hand."], "durations": null, "exact_frames_per_prompt": [16], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/4JLJ6YTCaJc.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 20, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_dense_val_251_0_0"}, {"texts": ["The man is getting a tattoo on his right hand from another man."], "durations": null, "exact_frames_per_prompt": [60], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/4JLJ6YTCaJc.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 20, "npz_gt_video_start_frame": 20, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 20, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_dense_val_251_0_1"}, {"texts": ["A man on the right is sitting on a chair and making a tattoo on the first man's hand with a tattoo machine."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/4JLJ6YTCaJc.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_dense_val_251_1_0"}, {"texts": ["A boy wearing a black t-shirt is sitting on a red carpet and wrapping something with wrapping paper in fast forward."], "durations": null, "exact_frames_per_prompt": [59], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-om14lXEcNs.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 63, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_dense_val_252_0_0"}, {"texts": ["A baby on the left side is bathing inside the blue tub while a baby on the right side is sitting inside the blue tub, hitting on the water and putting his hand on his mouth."], "durations": null, "exact_frames_per_prompt": [55], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-d_UnNREYO4.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 59, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_dense_val_255_1_0"}, {"texts": ["The baby on the left side is putting his finger inside the nose while a baby on the right side is also trying to put his fingers inside his nose."], "durations": null, "exact_frames_per_prompt": [21], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-d_UnNREYO4.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 59, "npz_gt_video_start_frame": 59, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 59, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_dense_val_255_1_1"}, {"texts": ["A girl wearing a check top and black shorts is hitting a golf ball forward using a golf stick and then moving forward on the green grass field."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-uq6sWx0g_w.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_dense_val_256_0_0"}, {"texts": ["A girl wearing a green dress is sitting on the right side of the grey surface while a boy wearing a red t-shirt sits on the left, a person whose hand is visible is moving his hand on the girl's hair, and there are four rabbits near the grill on a gray surface, and people are moving on the back side."], "durations": null, "exact_frames_per_prompt": [64], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-o0JCIfoeqs.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 68, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_dense_val_258_0_0"}, {"texts": ["The girl stands up and starts moving her hand while a group of people are present on the back side."], "durations": null, "exact_frames_per_prompt": [12], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-o0JCIfoeqs.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 68, "npz_gt_video_start_frame": 68, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 68, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_dense_val_258_0_1"}, {"texts": ["A boy wearing a red outfit is sitting on the left side of the grey surface and touching the white rabbit and a girl wearing green top is sitting right to him and watching white rabbit and stands up"], "durations": null, "exact_frames_per_prompt": [69], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-o0JCIfoeqs.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 73, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_dense_val_258_1_0"}, {"texts": ["The boy wearing a red outfit stands up."], "durations": null, "exact_frames_per_prompt": [6], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-o0JCIfoeqs.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 73, "npz_gt_video_start_frame": 73, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 73, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_dense_val_258_1_1"}, {"texts": ["A white rabbit sitting on the right side of the red bucket is moving his head while a boy in a green t-shirt is caressing it, a person caresses the girl, some animals are on the right side and a group of people in which some are standing and some are walking on the brown surface."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-o0JCIfoeqs.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_dense_val_258_2_0"}, {"texts": ["A person, whose only hand is visible, is pressing the button of the shredder machine."], "durations": null, "exact_frames_per_prompt": [14], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-FsB_A4A1Ks.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 18, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_dense_val_25_0_0"}, {"texts": ["The person, whose only hand is visible taking out the paper and showing it."], "durations": null, "exact_frames_per_prompt": [40], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-FsB_A4A1Ks.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 18, "npz_gt_video_start_frame": 18, "npz_gt_video_end_frame": 58, "skip_frames_after_generation": 18, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_dense_val_25_0_1"}, {"texts": ["The person, whose only hand is visible is pressing the button again."], "durations": null, "exact_frames_per_prompt": [22], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-FsB_A4A1Ks.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 58, "npz_gt_video_start_frame": 58, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 58, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_dense_val_25_0_2"}, {"texts": ["A girl is eating noodles."], "durations": null, "exact_frames_per_prompt": [23], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-beyXnxwTao.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 27, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_dense_val_260_0_0"}, {"texts": ["The girl licks her hand."], "durations": null, "exact_frames_per_prompt": [16], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-beyXnxwTao.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 27, "npz_gt_video_start_frame": 27, "npz_gt_video_end_frame": 43, "skip_frames_after_generation": 27, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_dense_val_260_0_1"}, {"texts": ["The girl picks up the bottle."], "durations": null, "exact_frames_per_prompt": [16], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-beyXnxwTao.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 43, "npz_gt_video_start_frame": 43, "npz_gt_video_end_frame": 59, "skip_frames_after_generation": 43, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_dense_val_260_0_2"}, {"texts": ["A girl wearing a grey t-shirt is holding a book and reading it."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-5EyYMSM5eo.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_dense_val_261_0_0"}, {"texts": ["A person of whom only hands are visible is moving his fingers while a girl in the front is sitting with a blindfold on her eyes."], "durations": null, "exact_frames_per_prompt": [27], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-6KfcOhyCtI.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 34, "npz_gt_video_start_frame": 34, "npz_gt_video_end_frame": 61, "skip_frames_after_generation": 34, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_dense_val_262_1_0"}, {"texts": ["The person of whom only hands are visible is putting something into the spoon."], "durations": null, "exact_frames_per_prompt": [19], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-6KfcOhyCtI.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 61, "npz_gt_video_start_frame": 61, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 61, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_dense_val_262_1_1"}, {"texts": ["A person wearing a red t-shirt and grey lower is standing on the blue surface and mixing something in the drink using a white spoon."], "durations": null, "exact_frames_per_prompt": [29], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-6BGjxMc1BU.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 33, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_dense_val_263_0_0"}, {"texts": ["A girl wearing a black blazer is standing and looking to the right as a group of people is standing and sitting at the backside."], "durations": null, "exact_frames_per_prompt": [34], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/--kbTDDIiP0.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 38, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_dense_val_265_0_0"}, {"texts": ["The girl wearing a black blazer starts walking and everyone starts clapping."], "durations": null, "exact_frames_per_prompt": [18], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/--kbTDDIiP0.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 38, "npz_gt_video_start_frame": 38, "npz_gt_video_end_frame": 56, "skip_frames_after_generation": 38, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_dense_val_265_0_1"}, {"texts": ["The girl wearing a black blazer is hugging the woman."], "durations": null, "exact_frames_per_prompt": [24], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/--kbTDDIiP0.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 56, "npz_gt_video_start_frame": 56, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 56, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_dense_val_265_0_2"}, {"texts": ["A woman wearing a black-blue-yellow gown is standing and hugging the girl while a group of people are clapping."], "durations": null, "exact_frames_per_prompt": [27], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/--kbTDDIiP0.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 53, "npz_gt_video_start_frame": 53, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 53, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_dense_val_265_1_0"}, {"texts": ["A man wearing a green t-shirt is sitting next to the horse legs and holding a white plaster in his hands."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/0IPSrG2K83M.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_dense_val_266_1_0"}, {"texts": ["A person wearing a black t-shirt and black pants is riding a yellow camel from right to left on the grey surface while a man wearing white clothes is walking to the left holding the camel leash and other people are doing different activity."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/1WEhv0Znmaw.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_dense_val_268_0_0"}, {"texts": ["A person wearing white clothes is walking on the left of the camel and holding the camel rope on which the person is riding while a man wearing a white t-shirt is standing an watching the camel and the third person behind the camel is wearing a white outfit is walking on the grey surface."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/1WEhv0Znmaw.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_dense_val_268_1_0"}, {"texts": ["A yellow camel is walking from right to left on the grey surface while carrying a person on its back while a man wearing a white outfit is walking ahead while holding the leash of the camel and other people are standing and watching the man."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/1WEhv0Znmaw.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_dense_val_268_4_0"}, {"texts": ["A person wearing a red t-shirt is sitting on the back of a camel and taking a camel ride and the man wearing white shirt is holding the camel's rope and moving in forward direction"], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/3KMngq0EbG8.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_dense_val_271_0_0"}, {"texts": ["A brown camel is walking in a forward direction on a grey road while taking the person on its back.\n while a man wearing a white shirt is walking on the road while holding the leash of the camel, a group of people are walking on a road surface in the back, two people are riding bikes on a road surface, and a car is moving in the forward direction."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/3KMngq0EbG8.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_dense_val_271_1_0"}, {"texts": ["A man wearing a white shirt is walking on the road in front of the camel while the man in red t-shirt sits on the camel."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/3KMngq0EbG8.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_dense_val_271_2_0"}, {"texts": ["A man wearing a white shirt is walking on the road in front of the camel while a man wearing red t-shirt riding the camel"], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/3KMngq0EbG8.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_dense_val_271_3_0"}, {"texts": ["A boy wearing a blue t-shirt is sitting and shredding papers in a shredding machine while a man, wearing a dark blue t-shirt, is sitting besides the boy and giving some papers to him."], "durations": null, "exact_frames_per_prompt": [36], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/1G-iF_ldr3A.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 40, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_dense_val_273_0_0"}, {"texts": ["A man wearing gray shorts is sitting as a boy wearing a blue t-shirt is sitting and putting a paper into a paper shredding machine."], "durations": null, "exact_frames_per_prompt": [33], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/1G-iF_ldr3A.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 37, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_dense_val_273_1_0"}, {"texts": ["The man is giving papers to the boy with his right hand."], "durations": null, "exact_frames_per_prompt": [43], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/1G-iF_ldr3A.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 37, "npz_gt_video_start_frame": 37, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 37, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_dense_val_273_1_1"}, {"texts": ["A woman wearing a golden-black dress is standing on the left side, explaining the weather forecasting report while touching the big display."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/03dk7mneDU0.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_dense_val_275_0_0"}, {"texts": ["A white-black dog is getting groomed by a person."], "durations": null, "exact_frames_per_prompt": [48], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/2EuRhc8zCwg.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 32, "npz_gt_video_start_frame": 32, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 32, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_dense_val_276_1_0"}, {"texts": ["A woman wearing a black top and denim jeans is standing and doing hand gestures."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/2-J1ADkUN2c.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_dense_val_277_0_0"}, {"texts": ["A girl wearing a green top is sitting and pointing towards the cow udder while a person in the front wearing a white t-shirt is standing."], "durations": null, "exact_frames_per_prompt": [33], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-2Fv5u2YO5s.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 37, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_dense_val_278_0_0"}, {"texts": ["The girl wearing a green top holds a bucket."], "durations": null, "exact_frames_per_prompt": [39], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-2Fv5u2YO5s.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 37, "npz_gt_video_start_frame": 37, "npz_gt_video_end_frame": 76, "skip_frames_after_generation": 37, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_dense_val_278_0_1"}, {"texts": ["The girl wearing a green top and touches the udder of the cow and explaining about milking."], "durations": null, "exact_frames_per_prompt": [4], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-2Fv5u2YO5s.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 76, "npz_gt_video_start_frame": 76, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 76, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_dense_val_278_0_2"}, {"texts": ["A black-and-white cow is standing on a gray surface and its udders are getting touched by the girl while a person in a white dress is standing on the left side."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-2Fv5u2YO5s.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_dense_val_278_2_0"}, {"texts": ["A man wearing a green t-shirt is holding a mic."], "durations": null, "exact_frames_per_prompt": [16], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/0L8ZcrRtFxI.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 20, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_dense_val_280_1_0"}, {"texts": ["The man is speaking something while standing on a white surface."], "durations": null, "exact_frames_per_prompt": [60], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/0L8ZcrRtFxI.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 20, "npz_gt_video_start_frame": 20, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 20, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_dense_val_280_1_1"}, {"texts": ["A man wearing a blue shirt is standing behind the first man."], "durations": null, "exact_frames_per_prompt": [19], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-ejLPB4J4SM.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 23, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_dense_val_281_1_0"}, {"texts": ["The man is patting the back of the first man."], "durations": null, "exact_frames_per_prompt": [7], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-ejLPB4J4SM.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 23, "npz_gt_video_start_frame": 23, "npz_gt_video_end_frame": 30, "skip_frames_after_generation": 23, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_dense_val_281_1_1"}, {"texts": ["The man is shaking his hand with him."], "durations": null, "exact_frames_per_prompt": [23], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-ejLPB4J4SM.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 30, "npz_gt_video_start_frame": 30, "npz_gt_video_end_frame": 53, "skip_frames_after_generation": 30, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_dense_val_281_1_2"}, {"texts": ["The man stands."], "durations": null, "exact_frames_per_prompt": [27], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-ejLPB4J4SM.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 53, "npz_gt_video_start_frame": 53, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 53, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_dense_val_281_1_3"}, {"texts": ["A baby wearing green clothes is sitting on a bed and holding a white paper in his hands and playing with it."], "durations": null, "exact_frames_per_prompt": [48], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-94nreFhQRg.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 52, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_dense_val_286_0_0"}, {"texts": ["A baby wearing green patterned cloth is sitting on a bed and holding a white paper and playing with it."], "durations": null, "exact_frames_per_prompt": [48], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-94nreFhQRg.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 52, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_dense_val_287_1_0"}, {"texts": ["A person whose only hand is visible is feeding the group of fish."], "durations": null, "exact_frames_per_prompt": [74], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-LHB2aRRP5Q.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 6, "npz_gt_video_start_frame": 6, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 6, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_dense_val_288_0_0"}, {"texts": ["A woman whose hands are visible wearing a purple printed dress is trimming the hair of a Shih Tzu dog with a trimmer and the dog moves his head"], "durations": null, "exact_frames_per_prompt": [35], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-3zcsBnDVzU.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 42, "npz_gt_video_start_frame": 42, "npz_gt_video_end_frame": 77, "skip_frames_after_generation": 42, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_dense_val_289_0_0"}, {"texts": ["A brown-white shih tzu dog is sitting on the right side.  while a woman wearing a blue printed top is standing on the left side of the dog."], "durations": null, "exact_frames_per_prompt": [38], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-3zcsBnDVzU.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 42, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_dense_val_289_1_0"}, {"texts": ["The dog is getting its hair trimmed by the woman then sticking its tongue out."], "durations": null, "exact_frames_per_prompt": [38], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-3zcsBnDVzU.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 42, "npz_gt_video_start_frame": 42, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 42, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_dense_val_289_1_1"}, {"texts": ["A person wearing black sweater and grey pants is riding a brown horse from right to left on the grey road surface."], "durations": null, "exact_frames_per_prompt": [52], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/4luNHEnvxNo.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 56, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_dense_val_292_0_0"}, {"texts": ["A brown horse is walking from right to left on the grey road surface while carrying a person on his back."], "durations": null, "exact_frames_per_prompt": [52], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/4luNHEnvxNo.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 56, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_dense_val_292_1_0"}, {"texts": ["A person whose hands are visible is flaming a paper ball and putting it into the container and arranging it using a wooden stick."], "durations": null, "exact_frames_per_prompt": [37], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-3q1QUUKZ0c.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 7, "npz_gt_video_start_frame": 7, "npz_gt_video_end_frame": 44, "skip_frames_after_generation": 7, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_dense_val_2_1_0"}, {"texts": ["A boy wearing black shorts is standing on the grey surface and doing his t-shirt up while a group of people are walking at the backside on the grey surface."], "durations": null, "exact_frames_per_prompt": [31], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-Aw1GOyNtlI.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 35, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_dense_val_30_0_0"}, {"texts": ["The boy walks toward the black cow on the right side while the girls are sitting while holding a glass on his hands and watching toward the boy."], "durations": null, "exact_frames_per_prompt": [37], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-Aw1GOyNtlI.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 35, "npz_gt_video_start_frame": 35, "npz_gt_video_end_frame": 72, "skip_frames_after_generation": 35, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_dense_val_30_0_1"}, {"texts": ["The boy is touching the black cow."], "durations": null, "exact_frames_per_prompt": [8], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-Aw1GOyNtlI.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 72, "npz_gt_video_start_frame": 72, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 72, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_dense_val_30_0_2"}, {"texts": ["A girl on the right side wearing a grey top is sitting on a black cow and holding a glass and a girl wearing green t-shirt is pointing towards right and drinking from glass while a kid in white t-shirt is moving towards another black cow and pulling his t-shirt up and looking back"], "durations": null, "exact_frames_per_prompt": [42], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-Aw1GOyNtlI.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 38, "npz_gt_video_start_frame": 38, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 38, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_dense_val_30_1_0"}, {"texts": ["A black cow sitting on the dry grass on the right side a boy wearing white clothes walks towards the right; a girl wearing a grey top sits on the black cow and holds a glass; a woman wearing a greenish top sits on the green frame and drinks from the glass."], "durations": null, "exact_frames_per_prompt": [27], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-Aw1GOyNtlI.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 43, "npz_gt_video_start_frame": 43, "npz_gt_video_end_frame": 70, "skip_frames_after_generation": 43, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_dense_val_30_5_0"}, {"texts": ["The black cow is touched by a boy."], "durations": null, "exact_frames_per_prompt": [10], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-Aw1GOyNtlI.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 70, "npz_gt_video_start_frame": 70, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 70, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_dense_val_30_5_1"}, {"texts": ["A man in a blue cap and white t-shirt is first standing while holding a golf stick and he bends forward and angles the golf stick in front of the golf ball."], "durations": null, "exact_frames_per_prompt": [64], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/4WmFKoJghYM.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 68, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_dense_val_31_0_0"}, {"texts": ["The man hits the golf ball."], "durations": null, "exact_frames_per_prompt": [12], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/4WmFKoJghYM.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 68, "npz_gt_video_start_frame": 68, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 68, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_dense_val_31_0_1"}, {"texts": ["A girl carrying a baby in her lap and is sitting beside the lady on the outside of a pond on a grey surface and feeding food to the fishes while a few people are walking at the back."], "durations": null, "exact_frames_per_prompt": [50], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-15UpGVyV4w.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 54, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_dense_val_33_1_0"}, {"texts": ["A baby wearing orange cloth is sitting in the lap of the girl on the outside of a pond on a grey surface and feeding food to the fishes while the woman is pointing towards her left and showing something"], "durations": null, "exact_frames_per_prompt": [50], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-15UpGVyV4w.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 54, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_dense_val_33_2_0"}, {"texts": [" A person wearing scuba diving gear is swimming in the aquarium."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/0jg1cEhzFL4.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_dense_val_34_0_0"}, {"texts": ["A woman wearing a pink headband is applying make-up above her eyebrows with a make-up brush.\n"], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/1MkGBIeHNgY.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_dense_val_36_0_0"}, {"texts": ["A woman whose only head and hand are visible, is applying makeup near her eyebrow with a makeup brush.\n"], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/1MkGBIeHNgY.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_dense_val_37_0_0"}, {"texts": ["A person wearing a red sweatshirt is standing and pouring oil from the bottle onto the spoon."], "durations": null, "exact_frames_per_prompt": [13], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-e_w1rF4RGg.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 17, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_dense_val_3_0_0"}, {"texts": ["The person pours it into the big bowl."], "durations": null, "exact_frames_per_prompt": [63], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-e_w1rF4RGg.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 17, "npz_gt_video_start_frame": 17, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 17, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_dense_val_3_0_1"}, {"texts": ["A man wearing a white printed t-shirt is sitting and holding the drill machine with an apple on the iron object, and peeling the apple with the help of a peeler."], "durations": null, "exact_frames_per_prompt": [21], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-6GGk0vasfU.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 25, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_dense_val_40_0_0"}, {"texts": ["A boy wearing a green t-shirt is holding a brown box."], "durations": null, "exact_frames_per_prompt": [20], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/0i1lP-WyXLU.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 24, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_dense_val_41_0_0"}, {"texts": ["The boy is showing the brown box."], "durations": null, "exact_frames_per_prompt": [19], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/0i1lP-WyXLU.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 24, "npz_gt_video_start_frame": 24, "npz_gt_video_end_frame": 43, "skip_frames_after_generation": 24, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_dense_val_41_0_1"}, {"texts": ["The boy is turning the box."], "durations": null, "exact_frames_per_prompt": [20], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/0i1lP-WyXLU.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 43, "npz_gt_video_start_frame": 43, "npz_gt_video_end_frame": 63, "skip_frames_after_generation": 43, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_dense_val_41_0_2"}, {"texts": ["The boy is folding the top cover of the box."], "durations": null, "exact_frames_per_prompt": [17], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/0i1lP-WyXLU.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 63, "npz_gt_video_start_frame": 63, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 63, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_dense_val_41_0_3"}, {"texts": ["A girl wearing a black-white striped t-shirt is standing and tossing food in a frying pan."], "durations": null, "exact_frames_per_prompt": [28], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/--33Lscn6sk.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 32, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_dense_val_43_0_0"}, {"texts": ["A person wearing a t-shirt is sitting, pressing his hand on the white paper, and then turns the white paper and starts folding it."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-6c-sV_gmq0.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_dense_val_44_0_0"}, {"texts": ["A man wearing white cloth is holding a salt plate and speaking."], "durations": null, "exact_frames_per_prompt": [19], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/4G__B9iEOC8.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 23, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_dense_val_46_0_0"}, {"texts": ["The man is picking a tray with roasted chicken."], "durations": null, "exact_frames_per_prompt": [20], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/4G__B9iEOC8.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 23, "npz_gt_video_start_frame": 23, "npz_gt_video_end_frame": 43, "skip_frames_after_generation": 23, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_dense_val_46_0_1"}, {"texts": ["The man is holding a tong and then heating a salt plate."], "durations": null, "exact_frames_per_prompt": [37], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/4G__B9iEOC8.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 43, "npz_gt_video_start_frame": 43, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 43, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_dense_val_46_0_2"}, {"texts": ["A woman wearing an off-white trousers bends over the bed."], "durations": null, "exact_frames_per_prompt": [12], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-_Oni-SybW0.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 16, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_dense_val_49_0_0"}, {"texts": ["The woman wearing an off-white trousers makes the bed."], "durations": null, "exact_frames_per_prompt": [35], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-_Oni-SybW0.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 16, "npz_gt_video_start_frame": 16, "npz_gt_video_end_frame": 51, "skip_frames_after_generation": 16, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_dense_val_49_0_1"}, {"texts": ["The woman wearing an off-white trousers stands straight and looks in the straight direction."], "durations": null, "exact_frames_per_prompt": [29], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-_Oni-SybW0.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 51, "npz_gt_video_start_frame": 51, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 51, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_dense_val_49_0_2"}, {"texts": ["A person wearing a black t-shirt is standing on the left and whisking a brown liquid in a white bowl with a whisk."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-16TZQLfXUc.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_dense_val_4_0_0"}, {"texts": ["A person whose hands are visible is showing frozen vegetables pack."], "durations": null, "exact_frames_per_prompt": [8], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-eJVHxmkm-4.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 12, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_dense_val_50_0_0"}, {"texts": ["The person is putting it in a wok."], "durations": null, "exact_frames_per_prompt": [58], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-eJVHxmkm-4.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 12, "npz_gt_video_start_frame": 12, "npz_gt_video_end_frame": 71, "skip_frames_after_generation": 12, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_dense_val_50_0_1"}, {"texts": ["The person is sauteing it."], "durations": null, "exact_frames_per_prompt": [9], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-eJVHxmkm-4.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 71, "npz_gt_video_start_frame": 71, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 71, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_dense_val_50_0_2"}, {"texts": ["A white-brown goat is in the lap of a boy and drinking a liquid from a bottle.\n while the dog comes towards the man."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/1EATaLAG2Dc.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_dense_val_54_0_0"}, {"texts": ["A boy wearing spectacles is standing and holding a goat while a person whose hand is visible is feeding the goat with the bottle, and a white dog comes from the left and jumps forward on the person and is being caressed by him."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/1EATaLAG2Dc.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_dense_val_54_1_0"}, {"texts": ["A person whose hands are visible is feeding a goat with a bottle while a man wearing a cream-colored jacket is holding the goat, and a white puppy comes from the left and starts jumping."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/1EATaLAG2Dc.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_dense_val_54_2_0"}, {"texts": ["A woman on the right side wearing a grey top is holding a knife and a boiled potato."], "durations": null, "exact_frames_per_prompt": [39], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-_hu_Ld-ddk.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 43, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_dense_val_55_0_0"}, {"texts": ["The woman on the right side wearing a grey top close the tap."], "durations": null, "exact_frames_per_prompt": [15], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-_hu_Ld-ddk.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 43, "npz_gt_video_start_frame": 43, "npz_gt_video_end_frame": 58, "skip_frames_after_generation": 43, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_dense_val_55_0_1"}, {"texts": ["The woman on the right side wearing a grey top peels the boiled potato with a knife."], "durations": null, "exact_frames_per_prompt": [22], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-_hu_Ld-ddk.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 58, "npz_gt_video_start_frame": 58, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 58, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_dense_val_55_0_2"}, {"texts": ["A baby wearing a white-pink dress is standing and eating food."], "durations": null, "exact_frames_per_prompt": [41], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-GU9hPMO3mg.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 45, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_dense_val_58_0_0"}, {"texts": ["A man wearing a dark gray snow suit is leaning forward, holding a fishing rod and pushing the fishing rope into the ice fishing hole while a person wearing a dark gray snow suit is standing on the snow surface,and a person wearing black outfit is standing on the snow surface."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-QfN6xAteC4.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_dense_val_5_0_0"}, {"texts": ["A man on the left side is wearing a black jacket and red pants is standing and pushing the rope into the ice fishing hole while a person wearing black clothes is standing at the back."], "durations": null, "exact_frames_per_prompt": [48], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-QfN6xAteC4.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 32, "npz_gt_video_start_frame": 32, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 32, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_dense_val_5_1_0"}, {"texts": ["A man is standing on the backside and doing ice fishing."], "durations": null, "exact_frames_per_prompt": [50], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-QfN6xAteC4.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 30, "npz_gt_video_start_frame": 30, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 30, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_dense_val_5_2_0"}, {"texts": ["A person whose hands are visible is chopping pineapple and making designs on it with the help of a knife."], "durations": null, "exact_frames_per_prompt": [36], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-5NObVvJedg.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 40, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_dense_val_63_0_0"}, {"texts": ["A white brown cat is walking on the green grass surface."], "durations": null, "exact_frames_per_prompt": [41], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/2eigAnTVfTU.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 39, "npz_gt_video_start_frame": 39, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 39, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_dense_val_68_0_0"}, {"texts": ["A person on the left side wearing a blue t-shirt is riding on an elephant while a person on the right wearing white t-shirt is ridding on an elephant."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/0pi94mAZXAU.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_dense_val_69_0_0"}, {"texts": ["A person on the right of the first person wearing a white t-shirt is riding on an elephant while third person wearing white outfit and a cap is also riding on the elephant and he is sitting in front of other persons."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/0pi94mAZXAU.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_dense_val_69_1_0"}, {"texts": ["A mahout wearing a white t-shirt is sitting on an elephant and controlling the elephant while a man wearing a blue t-shirt and a woman wearing a white t-shirt is sitting on the back of the elephant."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/0pi94mAZXAU.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_dense_val_69_2_0"}, {"texts": ["An elephant is walking on the grey surface to the left side while carrying two people on its back while the other person is also on the back of elephant."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/0pi94mAZXAU.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_dense_val_69_4_0"}, {"texts": ["A person is sitting on a brown couch with a child in his lap, holding a book."], "durations": null, "exact_frames_per_prompt": [44], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/4qVBgiGwi2c.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 48, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_dense_val_6_1_0"}, {"texts": ["The person opens that book while a child wearing a pink cloth is sitting and close a book."], "durations": null, "exact_frames_per_prompt": [32], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/4qVBgiGwi2c.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 48, "npz_gt_video_start_frame": 48, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 48, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_dense_val_6_1_1"}, {"texts": ["A man wearing a dark blue suit is standing and tying a bow tie around his neck."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-u1Cj9edMUI.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_dense_val_70_0_0"}, {"texts": ["A man wearing cream shorts is standing on a boat and fishing in the river."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-FPZjV_G6LU.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_dense_val_71_0_0"}, {"texts": ["A crying baby wearing a red trunk is being held by a woman. The woman put some water on the baby's face, and the baby's left hand is being held by another woman."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-73Kg-MKmwE.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_dense_val_72_0_0"}, {"texts": ["A woman on the right side wearing an orange-yellow saree, holding the left hand of the baby while sitting while a woman wearing a yellow saree is holding the baby and a group of people are sitting around the baby."], "durations": null, "exact_frames_per_prompt": [26], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-73Kg-MKmwE.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 30, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_dense_val_72_2_0"}, {"texts": ["A baby wearing a grey t-shirt is sitting on a brown chair and trying to take food from the hand of the person."], "durations": null, "exact_frames_per_prompt": [44], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/2aSMcnH9l2Q.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 48, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_dense_val_73_0_0"}, {"texts": ["The baby takes the food."], "durations": null, "exact_frames_per_prompt": [13], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/2aSMcnH9l2Q.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 48, "npz_gt_video_start_frame": 48, "npz_gt_video_end_frame": 61, "skip_frames_after_generation": 48, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_dense_val_73_0_1"}, {"texts": ["The baby puts it in his mouth."], "durations": null, "exact_frames_per_prompt": [19], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/2aSMcnH9l2Q.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 61, "npz_gt_video_start_frame": 61, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 61, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_dense_val_73_0_2"}, {"texts": ["A person whose hand is only visible is holding the food in the finger while a baby wearing a grey sweatshirt is trying to hold the food, and other people are moving."], "durations": null, "exact_frames_per_prompt": [23], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/2aSMcnH9l2Q.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 27, "npz_gt_video_start_frame": 27, "npz_gt_video_end_frame": 50, "skip_frames_after_generation": 27, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_dense_val_73_2_0"}, {"texts": ["The person whose hand is only visible is giving the food to the baby."], "durations": null, "exact_frames_per_prompt": [5], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/2aSMcnH9l2Q.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 50, "npz_gt_video_start_frame": 50, "npz_gt_video_end_frame": 55, "skip_frames_after_generation": 50, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_dense_val_73_2_1"}, {"texts": ["A woman wearing a black top with white stripes is applying something on her upper lip."], "durations": null, "exact_frames_per_prompt": [42], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-mouPHwsYD8.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 46, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_dense_val_74_0_0"}, {"texts": ["Later the woman is applying lip gloss on her lips."], "durations": null, "exact_frames_per_prompt": [34], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-mouPHwsYD8.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 46, "npz_gt_video_start_frame": 46, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 46, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_dense_val_74_0_1"}, {"texts": ["A person whose only hand is visible is holding a fruit in his hand."], "durations": null, "exact_frames_per_prompt": [39], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-h0RIu0ggZs.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 43, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_dense_val_75_0_0"}, {"texts": ["A person whose hand is visible is plucking a fruit from a tree and then showing it."], "durations": null, "exact_frames_per_prompt": [36], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-h0RIu0ggZs.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 40, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_dense_val_76_0_0"}, {"texts": ["A person wearing transparent glove is standing on the white tiled floor and holding the meat mincer machine."], "durations": null, "exact_frames_per_prompt": [24], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/1cUETMJ8jBM.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 25, "npz_gt_video_start_frame": 25, "npz_gt_video_end_frame": 49, "skip_frames_after_generation": 25, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_dense_val_77_0_0"}, {"texts": ["The person is pushing the outlet of meat mincer machine."], "durations": null, "exact_frames_per_prompt": [31], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/1cUETMJ8jBM.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 49, "npz_gt_video_start_frame": 49, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 49, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_dense_val_77_0_1"}, {"texts": ["A person whose half body is visible is folding a nori sheet then putting it aside."], "durations": null, "exact_frames_per_prompt": [18], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-BGvHxisFII.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 22, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_dense_val_83_0_0"}, {"texts": ["The person takes another nori sheet and puts it on the aluminium foil paper."], "durations": null, "exact_frames_per_prompt": [58], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-BGvHxisFII.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 22, "npz_gt_video_start_frame": 22, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 22, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_dense_val_83_0_1"}, {"texts": ["A man is standing, holding a baby in his lap, and picking up the pancake from the pan and putting it into a plate with a spatula while speaking."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/1bLrVWuHby0.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_dense_val_84_0_0"}, {"texts": ["A baby wearing a gray dress is held by the man, and the baby is also holding the spatula with the man, and he is putting the pancake into the plate with the man while smiling."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/1bLrVWuHby0.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_dense_val_84_1_0"}, {"texts": ["A boy on the right side wearing a grey t-shirt is standing while putting his hands over the table while a boy wearing shorts sits on the chair and presses the yellow container over the pieces of bread."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/5cbC1OGsnjk.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_dense_val_86_0_0"}, {"texts": ["A man wearing a black suit and white shirt is screwing in a cork bottle opener."], "durations": null, "exact_frames_per_prompt": [37], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/--TSmh3wj9I.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 41, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_dense_val_87_0_0"}, {"texts": ["The man opening a bottle."], "durations": null, "exact_frames_per_prompt": [39], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/--TSmh3wj9I.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 41, "npz_gt_video_start_frame": 41, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 41, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_dense_val_87_0_1"}, {"texts": ["A person whose hand is only visible is showing his thumb."], "durations": null, "exact_frames_per_prompt": [29], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/2GjXVYEc7J8.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 33, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_dense_val_89_0_0"}, {"texts": ["The person touches the egg on the plate."], "durations": null, "exact_frames_per_prompt": [3], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/2GjXVYEc7J8.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 33, "npz_gt_video_start_frame": 33, "npz_gt_video_end_frame": 36, "skip_frames_after_generation": 33, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_dense_val_89_0_1"}, {"texts": ["A person whose hands are visible is cutting pineapple with a knife and putting it in a maroon container."], "durations": null, "exact_frames_per_prompt": [49], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/0D2YY8fs0ok.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 53, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_dense_val_8_0_0"}, {"texts": ["The person is washing his hand."], "durations": null, "exact_frames_per_prompt": [14], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/0D2YY8fs0ok.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 53, "npz_gt_video_start_frame": 53, "npz_gt_video_end_frame": 67, "skip_frames_after_generation": 53, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_dense_val_8_0_1"}, {"texts": ["The person lifts the maroon container and starts cleaning the sink."], "durations": null, "exact_frames_per_prompt": [12], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/0D2YY8fs0ok.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 67, "npz_gt_video_start_frame": 67, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 67, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_dense_val_8_0_2"}, {"texts": ["A woman wearing a brown jacket is standing on the green grass surface and plucking fruits from a tree with a fruit picker then turns back while a group of people is standing at the back and watching her."], "durations": null, "exact_frames_per_prompt": [14], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/0azWFsmoz38.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 18, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_dense_val_92_0_0"}, {"texts": ["The woman again starts plucking fruits from the tree while a girl in a blue top is standing at the back and moving toward the left with a fruit picker in her hand."], "durations": null, "exact_frames_per_prompt": [62], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/0azWFsmoz38.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 18, "npz_gt_video_start_frame": 18, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 18, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_dense_val_92_0_1"}, {"texts": ["A person wearing grey pants is standing on the backside and plucking fruits from the tree with a fruit picker while two women in the front are also plucking fruits with a fruit picker and another two women are standing on the left."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/0azWFsmoz38.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_dense_val_92_4_0"}, {"texts": ["A woman is smiling and applying makeup on her eyebrow with a black make-up tool."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/--bGIeO_7Jo.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_dense_val_93_0_0"}, {"texts": ["A lady wearing pink clothes is standing and buttoning up a brown shirt and then spreading it on a grey table and speaking while doing this."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-BABPsEWA54.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_dense_val_94_0_0"}, {"texts": ["A baby is sitting on a baby chair."], "durations": null, "exact_frames_per_prompt": [10], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-ZYl4Sw2SSk.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 14, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_dense_val_95_0_0"}, {"texts": ["The baby is eating food with hand."], "durations": null, "exact_frames_per_prompt": [44], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-ZYl4Sw2SSk.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 14, "npz_gt_video_start_frame": 14, "npz_gt_video_end_frame": 58, "skip_frames_after_generation": 14, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_dense_val_95_0_1"}, {"texts": ["The baby starts hitting the tray with his hand."], "durations": null, "exact_frames_per_prompt": [22], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-ZYl4Sw2SSk.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 58, "npz_gt_video_start_frame": 58, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 58, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_dense_val_95_0_2"}, {"texts": ["A person wearing a blue t-shirt and black trousers first bends over the dog and fixes its collar belt while a group of people and dogs are standing at the back."], "durations": null, "exact_frames_per_prompt": [26], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/4VV5L7lhOjE.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 18, "npz_gt_video_start_frame": 18, "npz_gt_video_end_frame": 44, "skip_frames_after_generation": 18, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_dense_val_96_0_0"}, {"texts": ["The person wearing a blue t-shirt and black trousers turns and starts walking in a straight direction along with the dog."], "durations": null, "exact_frames_per_prompt": [18], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/4VV5L7lhOjE.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 44, "npz_gt_video_start_frame": 44, "npz_gt_video_end_frame": 62, "skip_frames_after_generation": 44, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_dense_val_96_0_1"}, {"texts": ["A brown-colored dog first stands on the grassy surface while group of dogs are on the grass surface and a person on the left side is standing and starts walking."], "durations": null, "exact_frames_per_prompt": [30], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/4VV5L7lhOjE.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 21, "npz_gt_video_start_frame": 21, "npz_gt_video_end_frame": 51, "skip_frames_after_generation": 21, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_dense_val_96_1_0"}, {"texts": ["The dog walks along with the person in a blue t-shirt."], "durations": null, "exact_frames_per_prompt": [29], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/4VV5L7lhOjE.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 51, "npz_gt_video_start_frame": 51, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 51, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_dense_val_96_1_1"}, {"texts": ["A man wearing a white shirt is standing on the left side then starts walking to the right side while another man in a black jacket is sitting and drinking, then starts licking a piece of lime."], "durations": null, "exact_frames_per_prompt": [34], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/1BkmCXHttEQ.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 38, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_dense_val_98_1_0"}, {"texts": ["A man wearing a white shirt is looking in the right side and smiling."], "durations": null, "exact_frames_per_prompt": [28], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/0fmNdKx4cdI.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 32, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_dense_val_9_0_0"}, {"texts": ["The man wearing a white shirt is drinking from a white glass."], "durations": null, "exact_frames_per_prompt": [48], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/0fmNdKx4cdI.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 32, "npz_gt_video_start_frame": 32, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 32, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_dense_val_9_0_1"}, {"texts": ["A man wearing white-black clothes is sitting and speaking on the mic."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/2s_Sv0gngEc.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_0_0_0"}, {"texts": ["A man wearing a grey t-shirt is holding a craft book and lying on the right side of the bed with a kid on resting on his left shoulder"], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/0gkr8N4nCnU.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1000_0_0"}, {"texts": ["A boy wearing a white t-shirt is lying in the middle of the bed and his two fingers are in his mouth while a man wearing grey t-shirt is lying on the bed and looking in front and a woman is sleeping in the right while covering herself from the sheet."], "durations": null, "exact_frames_per_prompt": [45], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/0gkr8N4nCnU.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 7, "npz_gt_video_start_frame": 7, "npz_gt_video_end_frame": 52, "skip_frames_after_generation": 7, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1000_1_0"}, {"texts": ["The boy wearing a white t-shirt takes them out and starts speaking while the man wearing grey t-shirt starts speaking to the boy while looking in front."], "durations": null, "exact_frames_per_prompt": [28], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/0gkr8N4nCnU.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 52, "npz_gt_video_start_frame": 52, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 52, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1000_1_1"}, {"texts": ["A woman covered in a blanket is sleeping on the left side of the bed along with the boy and the man."], "durations": null, "exact_frames_per_prompt": [71], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/0gkr8N4nCnU.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 9, "npz_gt_video_start_frame": 9, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 9, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1000_2_0"}, {"texts": ["A man wearing a serape poncho is standing and drinking a shot from the shot glass."], "durations": null, "exact_frames_per_prompt": [26], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/61dpykxqid4.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 30, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1006_0_0"}, {"texts": ["The man is holding a whisky bottle with his right hand and a tequila bottle with his left hand, speaking while showing the bottles and putting the bottles on the counter."], "durations": null, "exact_frames_per_prompt": [25], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/61dpykxqid4.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 30, "npz_gt_video_start_frame": 30, "npz_gt_video_end_frame": 55, "skip_frames_after_generation": 30, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1006_0_1"}, {"texts": ["The man is falling down when an object hits his face."], "durations": null, "exact_frames_per_prompt": [10], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/61dpykxqid4.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 55, "npz_gt_video_start_frame": 55, "npz_gt_video_end_frame": 65, "skip_frames_after_generation": 55, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1006_0_2"}, {"texts": ["A person wearing a blue t-shirt is binding a book with a binding machine and a glue machine."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-6xAzSJcp_A.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1007_0_0"}, {"texts": ["A man wearing a white t-shirt is opening the green beer bottle with a silver bottle opener."], "durations": null, "exact_frames_per_prompt": [47], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/3wYAQ_sctuk.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 51, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1008_0_0"}, {"texts": ["The man wearing a white t-shirt is drinking the beer."], "durations": null, "exact_frames_per_prompt": [29], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/3wYAQ_sctuk.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 51, "npz_gt_video_start_frame": 51, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 51, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1008_0_1"}, {"texts": ["A man wearing a yellow-green t-shirt is standing and drinking beer from a glass, and he is getting tapped by the people."], "durations": null, "exact_frames_per_prompt": [50], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/8mYOrvp0nv4.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 54, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_100_0_0"}, {"texts": ["A man whose hand is visible, is wearing a green glove and is tapping on the head of the first man while a man wearing yellow t-shirt drinking a beer"], "durations": null, "exact_frames_per_prompt": [56], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/8mYOrvp0nv4.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 8, "npz_gt_video_start_frame": 8, "npz_gt_video_end_frame": 64, "skip_frames_after_generation": 8, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_100_2_0"}, {"texts": ["A woman wearing a black t-shirt is sitting on a chair and moving pieces of paper in front of the child."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/0W8QlCxJFkI.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1012_0_0"}, {"texts": ["A child wearing a white-blue striped romper is sitting on a table while a woman wearing a black top is sitting in a chair and tearing the newspaper."], "durations": null, "exact_frames_per_prompt": [40], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/0W8QlCxJFkI.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 44, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1012_1_0"}, {"texts": ["The child wearing a white-blue striped romper is laughing and moving legs and hands."], "durations": null, "exact_frames_per_prompt": [36], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/0W8QlCxJFkI.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 44, "npz_gt_video_start_frame": 44, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 44, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1012_1_1"}, {"texts": ["A woman wearing a black t-shirt is sitting on the chair and moving the pieces of paper in front of a child."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/0W8QlCxJFkI.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1013_0_0"}, {"texts": ["A kid wearing a blue-white striped romper is sitting on the table, laughing and moving their legs while a woman wearing black clothes is holding a baby and playing with the baby."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/0W8QlCxJFkI.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1013_1_0"}, {"texts": ["A girl wearing a blue vest and shorts is standing on the right side of a black car."], "durations": null, "exact_frames_per_prompt": [16], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/1dHvbrCDeCw.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 20, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1014_0_0"}, {"texts": ["The girl is putting her hand towards the fuel tank."], "durations": null, "exact_frames_per_prompt": [17], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/1dHvbrCDeCw.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 20, "npz_gt_video_start_frame": 20, "npz_gt_video_end_frame": 37, "skip_frames_after_generation": 20, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1014_0_1"}, {"texts": ["A man wearing gloves and black trousers is putting the papers in the gray shredding machine."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/3rDg75gkelk.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1015_0_0"}, {"texts": ["A girl wearing a white dress is pointing her finger towards the cooking eggs and putting a bottle aside."], "durations": null, "exact_frames_per_prompt": [41], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-b9DAD0puU4.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 45, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1016_0_0"}, {"texts": ["The girl is standing and moving her hands."], "durations": null, "exact_frames_per_prompt": [35], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-b9DAD0puU4.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 45, "npz_gt_video_start_frame": 45, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 45, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1016_0_1"}, {"texts": ["A person whose a hand is visible, is at first puts a cloth between the filter and the frame."], "durations": null, "exact_frames_per_prompt": [40], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/1LdhD1TRgYY.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 44, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1018_0_0"}, {"texts": ["The person unfastens the filter with a filter wrench."], "durations": null, "exact_frames_per_prompt": [35], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/1LdhD1TRgYY.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 44, "npz_gt_video_start_frame": 44, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 44, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1018_0_1"}, {"texts": ["A person of whom only hand is visible wearing a red-black glove is placing a cloth."], "durations": null, "exact_frames_per_prompt": [38], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/1LdhD1TRgYY.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 42, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1019_0_0"}, {"texts": ["The person of whom only hand is visible is tightening a blue machine part."], "durations": null, "exact_frames_per_prompt": [38], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/1LdhD1TRgYY.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 42, "npz_gt_video_start_frame": 42, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 42, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1019_0_1"}, {"texts": ["A baby wearing black cloth is sitting in the lap of a man and crying."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/14ekd4nkpwc.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_101_0_0"}, {"texts": ["A man wearing white clothes is sitting, holding a baby, and tapping his hand on the mouth of the baby."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/14ekd4nkpwc.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_101_1_0"}, {"texts": ["A woman wearing a maroon t-shirt is holding bird food in her hand and feeding the pigeons while a woman wearing a cap is standing on a gray surface, holding bird food in her hand, and a group of people are standing, sitting, and walking on a gray surface."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-98DdKiwFi0.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1020_0_0"}, {"texts": ["A woman wearing a red and white striped cloth is holding the bird food in her hand and raising it front of the pigeons while another girl is also giving food to the pigeons, a group of them is roaming on the grey surface and some are eating food from her hand, while a crowd of people is walking here and there and some are sitting."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-98DdKiwFi0.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1020_1_0"}, {"texts": ["A woman wearing blue clothes is standing."], "durations": null, "exact_frames_per_prompt": [36], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/AGUASZftBfY.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 40, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1024_0_0"}, {"texts": ["The woman is holding a knife."], "durations": null, "exact_frames_per_prompt": [10], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/AGUASZftBfY.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 40, "npz_gt_video_start_frame": 40, "npz_gt_video_end_frame": 50, "skip_frames_after_generation": 40, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1024_0_1"}, {"texts": ["The woman is cutting the watermelon."], "durations": null, "exact_frames_per_prompt": [30], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/AGUASZftBfY.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 50, "npz_gt_video_start_frame": 50, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 50, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1024_0_2"}, {"texts": ["A group of people are standing and clapping on the brown surface, some people are sitting on chairs."], "durations": null, "exact_frames_per_prompt": [34], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-ieVw_C-v-E.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 29, "npz_gt_video_start_frame": 29, "npz_gt_video_end_frame": 63, "skip_frames_after_generation": 29, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1028_1_ms_0"}, {"texts": ["A woman wearing a blue top is sitting on a wooden floor while the small baby throws the clothes out of drawer and lifts another cloth from ground"], "durations": null, "exact_frames_per_prompt": [37], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-Z1qu3KdiI8.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 41, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1029_0_0"}, {"texts": ["The woman is taking clothes from the baby and holding it and the baby sits down holding the drawer and tries to grab a green colored cloth"], "durations": null, "exact_frames_per_prompt": [34], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-Z1qu3KdiI8.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 41, "npz_gt_video_start_frame": 41, "npz_gt_video_end_frame": 75, "skip_frames_after_generation": 41, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1029_0_1"}, {"texts": ["The woman is putting the clothes in the cupboard while the baby looks here and there"], "durations": null, "exact_frames_per_prompt": [5], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-Z1qu3KdiI8.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 75, "npz_gt_video_start_frame": 75, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 75, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1029_0_2"}, {"texts": ["A baby wearing a diaper is picking clothes from the floor while the woman arrange the clothes in cupboard."], "durations": null, "exact_frames_per_prompt": [28], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-Z1qu3KdiI8.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 32, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1029_1_0"}, {"texts": ["The baby is giving it to the woman."], "durations": null, "exact_frames_per_prompt": [48], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-Z1qu3KdiI8.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 32, "npz_gt_video_start_frame": 32, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 32, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1029_1_1"}, {"texts": ["A man wearing a white chef coat is standing and peeling an apple with a knife."], "durations": null, "exact_frames_per_prompt": [27], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/2sywFP6JCzM.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 31, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_102_0_0"}, {"texts": ["The man putting the apple on the table."], "durations": null, "exact_frames_per_prompt": [4], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/2sywFP6JCzM.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 31, "npz_gt_video_start_frame": 31, "npz_gt_video_end_frame": 35, "skip_frames_after_generation": 31, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_102_0_1"}, {"texts": ["A person whose only hands are visible is cutting a watermelon on the red cardboard."], "durations": null, "exact_frames_per_prompt": [66], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/9Tyyi5UKTzw.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 70, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1030_0_0"}, {"texts": ["A man wearing a grayish-blue t-shirt is standing near the washing machine and tapping and knocking on the machine."], "durations": null, "exact_frames_per_prompt": [45], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/5r6rxFx8ofs.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 49, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1031_0_0"}, {"texts": ["The man turns in the front direction and speaks while doing hand gestures."], "durations": null, "exact_frames_per_prompt": [31], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/5r6rxFx8ofs.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 49, "npz_gt_video_start_frame": 49, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 49, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1031_0_1"}, {"texts": ["A person whose hand is visible is giving food in the mouth of a girl."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-rmKQkOPZ6c.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1034_0_0"}, {"texts": ["A girl wearing light green clothes and her eyes are closed by a cloth is sitting and eating and then laughing while a person whom hand is visible is putting food in girl's mouth."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-rmKQkOPZ6c.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1034_1_0"}, {"texts": ["A man wearing a black suit is walking backwards on the stage while holding a trophy while a group of people are sitting facing backwards, a lady in a blue dress is standing up, and a lady in a blue dress starts speaking into the microphone."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-dM5ar6vxB4.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1035_1_0"}, {"texts": ["A woman wearing a blue blazer and blue short skirt is tilting her body to pick up the papers and then standing on the stage holding papers in her hands while another woman wearing a blue outfit moves towards the podium and stands there, a man in a tuxedo is moving back and then stands near the woman in a blue blazer."], "durations": null, "exact_frames_per_prompt": [29], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-dM5ar6vxB4.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 33, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1035_2_0"}, {"texts": ["A woman whose upper half-body is visible wearing a blue top is standing behind the podium on the stage while a man and another woman are standing on the stage and a group of people are sitting in the audience sitting area."], "durations": null, "exact_frames_per_prompt": [36], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-dM5ar6vxB4.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 40, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1035_3_0"}, {"texts": ["The woman is speaking on the mic."], "durations": null, "exact_frames_per_prompt": [40], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-dM5ar6vxB4.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 40, "npz_gt_video_start_frame": 40, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 40, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1035_3_1"}, {"texts": ["A man wearing a blue t-shirt is sitting and taking food from the barbeque."], "durations": null, "exact_frames_per_prompt": [42], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/1u_0SfvUUpE.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 46, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1039_0_0"}, {"texts": ["The man is putting the food into the plate."], "durations": null, "exact_frames_per_prompt": [34], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/1u_0SfvUUpE.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 46, "npz_gt_video_start_frame": 46, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 46, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1039_0_1"}, {"texts": ["A person wearing a white chef coat is peeling a green apple with a knife."], "durations": null, "exact_frames_per_prompt": [19], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/2sywFP6JCzM.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 23, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_103_0_0"}, {"texts": ["A man wearing a yellow t-shirt, black jeans, and white shoes is standing on the green grass surface and playing golf while hitting with a golf stick."], "durations": null, "exact_frames_per_prompt": [62], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-_LzAw2WGlM.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 66, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1042_0_0"}, {"texts": ["The man wearing a yellow t-shirt, black jeans, and white shoes stops playing golf and starts speaking while moving his left hand."], "durations": null, "exact_frames_per_prompt": [14], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-_LzAw2WGlM.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 66, "npz_gt_video_start_frame": 66, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 66, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1042_0_1"}, {"texts": ["A man whose only hands are visible is adding cheese on the pizza base in a tray."], "durations": null, "exact_frames_per_prompt": [34], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-jClNNfcK5c.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 38, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1043_0_0"}, {"texts": ["The man is holding a cheese packet in his hand."], "durations": null, "exact_frames_per_prompt": [42], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-jClNNfcK5c.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 38, "npz_gt_video_start_frame": 38, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 38, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1043_0_1"}, {"texts": ["A boy wearing a black t-shirt is sitting on the left side of the chair, putting the Oreo packet on the table while another boy wearing a grey t-shirt is sitting on the right side and talking."], "durations": null, "exact_frames_per_prompt": [13], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/3_lAkUQnUPg.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 17, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1046_0_0"}, {"texts": ["The boy holding the camera attached to the tripod."], "durations": null, "exact_frames_per_prompt": [25], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/3_lAkUQnUPg.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 17, "npz_gt_video_start_frame": 17, "npz_gt_video_end_frame": 42, "skip_frames_after_generation": 17, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1046_0_1"}, {"texts": ["A boy is sitting on the right side wearing a grey t-shirt, first keeps his hand on the table while a boy in the black T-shirt puts oreo biscuit on the table and starts looking towards the left side."], "durations": null, "exact_frames_per_prompt": [38], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/3_lAkUQnUPg.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 42, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1046_1_0"}, {"texts": ["The boy is mashing the oreo biscuit into the bowl while standing."], "durations": null, "exact_frames_per_prompt": [39], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/3_lAkUQnUPg.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 42, "npz_gt_video_start_frame": 42, "npz_gt_video_end_frame": 81, "skip_frames_after_generation": 42, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1046_1_1"}, {"texts": ["A man wearing a white-grey striped t-shirt is sitting on the left side and watching the other man."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-lks3iTsQx4.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1047_0_0"}, {"texts": ["A man wearing a black coat is sitting on the white-brown sofa on the right side and eating food while a man wearing a multicolored t-shirt is sitting on a sofa and watching the man wearing a black coat."], "durations": null, "exact_frames_per_prompt": [45], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-lks3iTsQx4.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 49, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1047_1_0"}, {"texts": ["The man is wiping his mouth with a napkin."], "durations": null, "exact_frames_per_prompt": [31], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-lks3iTsQx4.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 49, "npz_gt_video_start_frame": 49, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 49, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1047_1_1"}, {"texts": ["A woman wearing an orange coat is coming from the right side and stands in front of a digital screen."], "durations": null, "exact_frames_per_prompt": [27], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-TjobiLNiBk.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 31, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1048_0_0"}, {"texts": ["The woman wearing an orange coat is giving a weather forecast report."], "durations": null, "exact_frames_per_prompt": [49], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-TjobiLNiBk.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 31, "npz_gt_video_start_frame": 31, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 31, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1048_0_1"}, {"texts": ["A girl whose only hand is visible is playing with a toy cat."], "durations": null, "exact_frames_per_prompt": [17], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-hr7cgupd_M.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 21, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1050_0_0"}, {"texts": ["A man whose half body is visible wearing a blue t-shirt is putting pizza into the oven."], "durations": null, "exact_frames_per_prompt": [36], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-hr7cgupd_M.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 44, "npz_gt_video_start_frame": 44, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 44, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1050_1_0"}, {"texts": ["A person whose hand is visible is putting a white liquid on the bike tire."], "durations": null, "exact_frames_per_prompt": [18], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/3sM19pzsjRo.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 10, "npz_gt_video_start_frame": 10, "npz_gt_video_end_frame": 28, "skip_frames_after_generation": 10, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1051_0_0"}, {"texts": ["A man wearing a blue denim shirt and black shorts standing in the water."], "durations": null, "exact_frames_per_prompt": [45], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/17VOafB2HfQ.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 49, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1054_0_0"}, {"texts": ["The man wearing a blue denim shirt and black shorts is feeding stingrays."], "durations": null, "exact_frames_per_prompt": [31], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/17VOafB2HfQ.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 49, "npz_gt_video_start_frame": 49, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 49, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1054_0_1"}, {"texts": ["A woman in greyish-black clothing is standing near the platform of the kitchen, she is at first working on her mobile phone."], "durations": null, "exact_frames_per_prompt": [55], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-W53pl92itE.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 59, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1057_0_0"}, {"texts": ["The woman looking in the straight direction."], "durations": null, "exact_frames_per_prompt": [21], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-W53pl92itE.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 59, "npz_gt_video_start_frame": 59, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 59, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1057_0_1"}, {"texts": ["A man wearing gray pants is sitting and counting the banknotes with his left hand."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-sU5IiBuOO4.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1058_0_0"}, {"texts": ["A person whose only hand and legs are visible, wearing jeans is sliding some dollar currency notes with his thumb."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-sU5IiBuOO4.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1059_0_0"}, {"texts": ["A man wearing a black t-shirt and gray shorts is standing while a woman is coming towards the man and a group of people are moving on the grass surface, some of whom are eating something."], "durations": null, "exact_frames_per_prompt": [30], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/2QFGQlwx3wo.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 34, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1060_0_0"}, {"texts": ["The man wearing a black t-shirt and gray shorts is moving hands while talking."], "durations": null, "exact_frames_per_prompt": [14], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/2QFGQlwx3wo.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 34, "npz_gt_video_start_frame": 34, "npz_gt_video_end_frame": 48, "skip_frames_after_generation": 34, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1060_0_1"}, {"texts": ["A man wearing a dark gray t-shirt and checkered shorts is leaning forward, sticking his tongue out while a woman wearing blue jeans comes from the back and sticks her tongue out."], "durations": null, "exact_frames_per_prompt": [20], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/2QFGQlwx3wo.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 24, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1060_1_0"}, {"texts": ["The man starts walking while a group of people are standing around on the green grass surface."], "durations": null, "exact_frames_per_prompt": [10], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/2QFGQlwx3wo.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 24, "npz_gt_video_start_frame": 24, "npz_gt_video_end_frame": 34, "skip_frames_after_generation": 24, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1060_1_1"}, {"texts": ["A woman wearing an orange top and blue jeans walks forward while a group of men is standing with their mouth open."], "durations": null, "exact_frames_per_prompt": [11], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/2QFGQlwx3wo.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 15, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1060_2_0"}, {"texts": ["The woman leans, opens her mouth, and starts shouting while a group of people and kids are standing in the yard."], "durations": null, "exact_frames_per_prompt": [19], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/2QFGQlwx3wo.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 15, "npz_gt_video_start_frame": 15, "npz_gt_video_end_frame": 34, "skip_frames_after_generation": 15, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1060_2_1"}, {"texts": ["A man wearing black-blue clothes is standing and speaking."], "durations": null, "exact_frames_per_prompt": [14], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/2QFGQlwx3wo.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 34, "npz_gt_video_start_frame": 34, "npz_gt_video_end_frame": 48, "skip_frames_after_generation": 34, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1061_1_0"}, {"texts": ["A woman wearing a dark gray t-shirt and black checked pajama is sitting on the floor."], "durations": null, "exact_frames_per_prompt": [21], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-6fbWHuoX1s.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 25, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1063_0_0"}, {"texts": ["The woman wearing a dark gray t-shirt and black checked pajama is folding a wrapping paper."], "durations": null, "exact_frames_per_prompt": [55], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-6fbWHuoX1s.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 25, "npz_gt_video_start_frame": 25, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 25, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1063_0_1"}, {"texts": ["A man wearing a white printed t-shirt is sitting on the brown sofa chair with a man wearing yellow top who is holding glass in his hand with another man wearing blue top who cheers his glass with second man"], "durations": null, "exact_frames_per_prompt": [73], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/2kJmwT7t7iM.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 77, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1064_0_0"}, {"texts": ["The man is picking up the bottle from the glass table."], "durations": null, "exact_frames_per_prompt": [2], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/2kJmwT7t7iM.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 77, "npz_gt_video_start_frame": 77, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 77, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1064_0_1"}, {"texts": ["A man wearing a dark blue clothes is sitting on the left side while another man wearing a white t-shirt is sitting on a sofa and putting both hands on his mouth."], "durations": null, "exact_frames_per_prompt": [38], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/2kJmwT7t7iM.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 42, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1064_1_0"}, {"texts": ["The man wearing a dark blue clothes do cheers while the third man sitting on the right side also cheers."], "durations": null, "exact_frames_per_prompt": [18], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/2kJmwT7t7iM.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 42, "npz_gt_video_start_frame": 42, "npz_gt_video_end_frame": 60, "skip_frames_after_generation": 42, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1064_1_1"}, {"texts": ["The man wearing a dark blue clothes starts drinking a shot then puts the shot glass on the table while the third man is sitting and drinking, and then puts the shot glass on the table."], "durations": null, "exact_frames_per_prompt": [20], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/2kJmwT7t7iM.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 60, "npz_gt_video_start_frame": 60, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 60, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1064_1_2"}, {"texts": ["A man wearing a green t-shirt is sitting on the right side while another man wearing white and grey outfit is sitting on a sofa and he is first leaning forward then puts both his hands on his face."], "durations": null, "exact_frames_per_prompt": [40], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/2kJmwT7t7iM.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 44, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1064_2_0"}, {"texts": ["The man does cheers while third man wearing blue outfit, sitting on the left side does cheers with the first man."], "durations": null, "exact_frames_per_prompt": [12], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/2kJmwT7t7iM.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 44, "npz_gt_video_start_frame": 44, "npz_gt_video_end_frame": 57, "skip_frames_after_generation": 44, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1064_2_1"}, {"texts": ["The man starts drinking a shot."], "durations": null, "exact_frames_per_prompt": [15], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/2kJmwT7t7iM.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 57, "npz_gt_video_start_frame": 57, "npz_gt_video_end_frame": 72, "skip_frames_after_generation": 57, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1064_2_2"}, {"texts": ["The man puts the shot glass on the table."], "durations": null, "exact_frames_per_prompt": [8], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/2kJmwT7t7iM.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 72, "npz_gt_video_start_frame": 72, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 72, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1064_2_3"}, {"texts": ["A man wearing a white t-shirt and blue jeans is fishing with a rod and pulls the fish out of the water while another man is fishing with the boy and a group of people in which some people are standing on the gray surface and one man is running away."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/1GL_7nMxYVA.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1065_0_0"}, {"texts": ["A man wearing a gray t-shirt and black trousers is teaching a kid how to fish while another man wearing a white t-shirt is fishing along with the man and catches a fish and pulls it out, other people get scared and move back."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/1GL_7nMxYVA.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1065_1_0"}, {"texts": ["A kid wearing a black-yellow t-shirt is learning and fishing from the second person while a man wearing white t-shirt is also fishing on the left and people are standing behind."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/1GL_7nMxYVA.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1065_2_0"}, {"texts": ["A person wearing a white t-shirt and blue jeans in the back starts running while a man pulls out the fish from the pond and another man is fishing with the boy and some people are standing on the gray surface."], "durations": null, "exact_frames_per_prompt": [50], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/1GL_7nMxYVA.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 30, "npz_gt_video_start_frame": 30, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 30, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1065_6_0"}, {"texts": ["A man wearing a blue t-shirt is cleaning the leg of a cow with a white cloth."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/1fH8Hd8dxyk.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1066_0_0"}, {"texts": ["A grey-white cow is standing and getting its leg and udder cleaned by the man."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/1fH8Hd8dxyk.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1066_1_0"}, {"texts": ["A person whose only lower body is visible is cutting a fish on a white chopping board with a knife."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/4qMCU-sYC44.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1068_0_0"}, {"texts": ["A person wearing a gray t-shirt is standing and cutting a salmon with a knife."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/4qMCU-sYC44.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1069_0_0"}, {"texts": ["A boy wearing a white t-shirt is standing and holding a snake."], "durations": null, "exact_frames_per_prompt": [36], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/49l1HeXO_88.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 40, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_106_0_0"}, {"texts": ["The boy is transferring it to a container while a person whose hands are visible is holding the container."], "durations": null, "exact_frames_per_prompt": [24], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/49l1HeXO_88.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 40, "npz_gt_video_start_frame": 40, "npz_gt_video_end_frame": 64, "skip_frames_after_generation": 40, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_106_0_1"}, {"texts": ["A brown white snake is sitting on the boy's hands."], "durations": null, "exact_frames_per_prompt": [39], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/49l1HeXO_88.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 43, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_106_2_0"}, {"texts": ["The boy is transferring the snake to a container"], "durations": null, "exact_frames_per_prompt": [21], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/49l1HeXO_88.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 43, "npz_gt_video_start_frame": 43, "npz_gt_video_end_frame": 64, "skip_frames_after_generation": 43, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_106_2_1"}, {"texts": ["A woman wearing an orange t-shirt is standing behind the wooden desk and forecasting a weather report on the screen and speaking."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/58r5Y4G32X4.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1071_0_0"}, {"texts": ["A woman wearing a dark purple top and white shorts is sitting on the left side of the girl and spreading a red paste on the bread with a spoon while a woman wearing a floral shirt moved the transparent container in front of the girl, then stood up and held the spoon."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/0EvFP-wbYVM.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1072_0_0"}, {"texts": ["A woman wearing a graphic pink top and black pants is sitting in front of both people."], "durations": null, "exact_frames_per_prompt": [36], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/0EvFP-wbYVM.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 40, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1072_1_0"}, {"texts": ["The woman is putting a container of food on the table while a woman wearing a maroon top is sitting on a green chair and garnishing red sauce on bread with a spoon, and a girl in a red t-shirt is sitting on a blue chair on the left."], "durations": null, "exact_frames_per_prompt": [24], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/0EvFP-wbYVM.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 40, "npz_gt_video_start_frame": 40, "npz_gt_video_end_frame": 64, "skip_frames_after_generation": 40, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1072_1_1"}, {"texts": ["The woman is standing up and holding the bread while the girl wearing a red t-shirt is looking at the bread."], "durations": null, "exact_frames_per_prompt": [16], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/0EvFP-wbYVM.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 64, "npz_gt_video_start_frame": 64, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 64, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1072_1_2"}, {"texts": ["A girl wearing a red t-shirt is sitting on the right side of the first woman and looking at the bread while a woman in purple top spreading sauce on the dough."], "durations": null, "exact_frames_per_prompt": [13], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/0EvFP-wbYVM.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 17, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1072_3_0"}, {"texts": ["The girl is looking at the container of food another woman in pink top stood up and took the spoon from the first woman."], "durations": null, "exact_frames_per_prompt": [63], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/0EvFP-wbYVM.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 17, "npz_gt_video_start_frame": 17, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 17, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1072_3_1"}, {"texts": ["A person whose hands are visible only, is holding a wooden spatula in one hand and whisking the paste in a pan at the stove."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/1kaSAklKMoo.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1073_0_0"}, {"texts": ["A person whose only hands are visible is at first sprinkling on the pizza which is kept on a pizza lifter."], "durations": null, "exact_frames_per_prompt": [24], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/0UJMIJ8BJJA.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 28, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1077_0_0"}, {"texts": ["The person shakes the pizza lifter and then picks it up."], "durations": null, "exact_frames_per_prompt": [41], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/0UJMIJ8BJJA.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 28, "npz_gt_video_start_frame": 28, "npz_gt_video_end_frame": 69, "skip_frames_after_generation": 28, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1077_0_1"}, {"texts": ["The person is holding a knife and chopping the basil leaves."], "durations": null, "exact_frames_per_prompt": [11], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/0UJMIJ8BJJA.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 69, "npz_gt_video_start_frame": 69, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 69, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1077_0_2"}, {"texts": ["A boy wearing a black t-shirt is sitting on a brown chair and drinking shots from small glasses in the restaurant."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/3u0zGTdFCsw.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1078_0_0"}, {"texts": ["A man wearing a black cloth is holding a container in his hand."], "durations": null, "exact_frames_per_prompt": [47], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/28bQP2GtXrA.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 51, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1079_0_0"}, {"texts": ["The man takes out food from the container and starts eating."], "durations": null, "exact_frames_per_prompt": [18], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/28bQP2GtXrA.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 51, "npz_gt_video_start_frame": 51, "npz_gt_video_end_frame": 69, "skip_frames_after_generation": 51, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1079_0_1"}, {"texts": ["The man then holds a plastic packet."], "durations": null, "exact_frames_per_prompt": [11], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/28bQP2GtXrA.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 69, "npz_gt_video_start_frame": 69, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 69, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1079_0_2"}, {"texts": ["A person wearing black clothes is pouring the hot water from the clay tea pot into a ceramic jug."], "durations": null, "exact_frames_per_prompt": [44], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/0gabWfDz5JI.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 48, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1082_0_0"}, {"texts": ["The man closes the lid of the jug."], "durations": null, "exact_frames_per_prompt": [32], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/0gabWfDz5JI.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 48, "npz_gt_video_start_frame": 48, "npz_gt_video_end_frame": 81, "skip_frames_after_generation": 48, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1082_0_1"}, {"texts": ["A person whose lower body is visible is pouring hot water into a white dish from a brown kettle."], "durations": null, "exact_frames_per_prompt": [77], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/0gabWfDz5JI.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 81, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1083_0_0"}, {"texts": ["A woman, whose hand is visible, holding a pen, is writing on the white paper with a black pen."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/3OYhq6HbJ4U.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1084_0_0"}, {"texts": ["A man wearing a green shirt walks away while a while dog is standing on the pet table and then shakes his head."], "durations": null, "exact_frames_per_prompt": [45], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/1J-1hoGEvCU.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 49, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1085_0_0"}, {"texts": ["The man starts cutting the hair of the white dog with a scissor."], "durations": null, "exact_frames_per_prompt": [31], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/1J-1hoGEvCU.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 49, "npz_gt_video_start_frame": 49, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 49, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1085_0_1"}, {"texts": ["A small white dog is standing on a black table is cleaning itself with a man wearing checked shirt and giving dog a haircut"], "durations": null, "exact_frames_per_prompt": [45], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/1J-1hoGEvCU.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 49, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1085_1_0"}, {"texts": ["The small white dog is getting its hair cut by person one."], "durations": null, "exact_frames_per_prompt": [31], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/1J-1hoGEvCU.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 49, "npz_gt_video_start_frame": 49, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 49, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1085_1_1"}, {"texts": ["A man wearing a white stripe green shirt is throwing a black object onto the sofa chair."], "durations": null, "exact_frames_per_prompt": [16], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/1J-1hoGEvCU.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 20, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1086_0_0"}, {"texts": ["The man starts walking while the dog moves and shakes his body and head"], "durations": null, "exact_frames_per_prompt": [29], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/1J-1hoGEvCU.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 20, "npz_gt_video_start_frame": 20, "npz_gt_video_end_frame": 49, "skip_frames_after_generation": 20, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1086_0_1"}, {"texts": ["The man is cutting the dog's hair with the scissor while the dog turns his head to the right"], "durations": null, "exact_frames_per_prompt": [31], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/1J-1hoGEvCU.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 49, "npz_gt_video_start_frame": 49, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 49, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1086_0_2"}, {"texts": ["A white dog is standing on the table while the man is standing on the left side, holding a black object in his hands, and then puts it on the sofa."], "durations": null, "exact_frames_per_prompt": [36], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/1J-1hoGEvCU.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 40, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1086_1_0"}, {"texts": ["The white dog starts shaking its body."], "durations": null, "exact_frames_per_prompt": [8], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/1J-1hoGEvCU.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 40, "npz_gt_video_start_frame": 40, "npz_gt_video_end_frame": 48, "skip_frames_after_generation": 40, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1086_1_1"}, {"texts": ["The white dog is getting the hair cut by the man."], "durations": null, "exact_frames_per_prompt": [32], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/1J-1hoGEvCU.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 48, "npz_gt_video_start_frame": 48, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 48, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1086_1_2"}, {"texts": ["A man wearing a green sweatshirt is at first adding the sauce on the meat kept in a barbeque machine."], "durations": null, "exact_frames_per_prompt": [28], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/1bM_yDcYJwo.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 32, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1088_0_0"}, {"texts": ["The man is applying the sauce with the brush."], "durations": null, "exact_frames_per_prompt": [48], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/1bM_yDcYJwo.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 32, "npz_gt_video_start_frame": 32, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 32, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1088_0_1"}, {"texts": ["A man wearing a brown jacket is standing on a gray surface, holding a bottle and putting sauce on a piece of food."], "durations": null, "exact_frames_per_prompt": [24], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/1bM_yDcYJwo.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 28, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1089_0_0"}, {"texts": ["The man picks a brush and spreads it."], "durations": null, "exact_frames_per_prompt": [52], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/1bM_yDcYJwo.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 28, "npz_gt_video_start_frame": 28, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 28, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1089_0_1"}, {"texts": ["A woman wearing colorful dotted white clothes is sitting on the sofa while holding a baby on her lap and holding a box with her left hand, and is speaking and tapping with her right hand on the baby's back, and then touching her own hair and looking up."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/1-I3bZnxQIY.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_108_0_0"}, {"texts": ["A baby wearing a colorful striped cloth and socks is sitting on a woman's lap while holding a box, looking up and smiling, and then opens the box."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/1-I3bZnxQIY.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_108_1_0"}, {"texts": ["A girl is speaking facing front, near a person while standing next to the person in blue clothes, later changing the camera angle, and recording the group of goats moving and grazing in the barn."], "durations": null, "exact_frames_per_prompt": [37], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/07xXjxsohjA.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 44, "npz_gt_video_start_frame": 44, "npz_gt_video_end_frame": 81, "skip_frames_after_generation": 44, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1091_0_0"}, {"texts": ["A goat is standing on wooden shavings in front of the person while another two goats are also standing on the wooden shavings."], "durations": null, "exact_frames_per_prompt": [77], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/07xXjxsohjA.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 81, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1091_2_0"}, {"texts": ["A baby wearing a black t-shirt with white stripes is sitting on the floor, tearing the paper."], "durations": null, "exact_frames_per_prompt": [43], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/0pznVln9yhw.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 47, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1094_0_0"}, {"texts": ["The baby starts moving."], "durations": null, "exact_frames_per_prompt": [20], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/0pznVln9yhw.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 47, "npz_gt_video_start_frame": 47, "npz_gt_video_end_frame": 67, "skip_frames_after_generation": 47, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1094_0_1"}, {"texts": ["A person whose hands are visible is folding a paper and making something."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/0bpyGQJk6hs.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1097_0_0"}, {"texts": ["A woman whose hands are visible is cooking food in the black pan with the help of a spatula."], "durations": null, "exact_frames_per_prompt": [53], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/1cSHWdIzbJg.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 57, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1098_0_0"}, {"texts": ["The woman is putting the food into the white bowl."], "durations": null, "exact_frames_per_prompt": [23], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/1cSHWdIzbJg.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 57, "npz_gt_video_start_frame": 57, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 57, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1098_0_1"}, {"texts": ["A woman wearing black clothes is putting a pillowcase on a pillow."], "durations": null, "exact_frames_per_prompt": [32], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/I-43Z0AB2Js.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 25, "npz_gt_video_start_frame": 25, "npz_gt_video_end_frame": 57, "skip_frames_after_generation": 25, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1099_0_0"}, {"texts": ["A man wearing a blue t-shirt is holding a dough and the two girls wearing blue and black top is watching him"], "durations": null, "exact_frames_per_prompt": [28], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/0kP-26H_9uI.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 32, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1100_0_0"}, {"texts": ["The man is at first putting the dough on the wooden surface."], "durations": null, "exact_frames_per_prompt": [16], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/0kP-26H_9uI.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 32, "npz_gt_video_start_frame": 32, "npz_gt_video_end_frame": 48, "skip_frames_after_generation": 32, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1100_0_1"}, {"texts": ["The man spreads it with his hands."], "durations": null, "exact_frames_per_prompt": [15], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/0kP-26H_9uI.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 48, "npz_gt_video_start_frame": 48, "npz_gt_video_end_frame": 63, "skip_frames_after_generation": 48, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1100_0_2"}, {"texts": ["The man flips it on a black pan."], "durations": null, "exact_frames_per_prompt": [17], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/0kP-26H_9uI.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 63, "npz_gt_video_start_frame": 63, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 63, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1100_0_3"}, {"texts": ["A woman wearing a blue top is standing at the right side of the man, and watching the man spreading the dough while another woman, wearing a black top, is also standing on the right side and watching the man."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/0kP-26H_9uI.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1100_1_0"}, {"texts": ["A man wearing a blue t-shirt is holding a dough while a woman in a blue top is standing on the left side of the man and watching him."], "durations": null, "exact_frames_per_prompt": [59], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/0kP-26H_9uI.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 63, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1101_0_0"}, {"texts": ["The man is flips it onto the black pan while a woman in a black top is standing on the extreme right and watching him."], "durations": null, "exact_frames_per_prompt": [17], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/0kP-26H_9uI.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 63, "npz_gt_video_start_frame": 63, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 63, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1101_0_1"}, {"texts": ["A woman wearing a blue top is standing and is looking at the dough while a man wearing a blue t-shirt is also standing and preparing the dough."], "durations": null, "exact_frames_per_prompt": [57], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/0kP-26H_9uI.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 61, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1101_1_0"}, {"texts": ["The woman is smiling while the woman wearing a black top is standing on the right and looking at the dough."], "durations": null, "exact_frames_per_prompt": [19], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/0kP-26H_9uI.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 61, "npz_gt_video_start_frame": 61, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 61, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1101_1_1"}, {"texts": ["A woman on the right wearing a black top is also standing and is looking at the white dough while a man wearing blue t-shirt preparing some food"], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/0kP-26H_9uI.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1101_2_0"}, {"texts": ["A man wearing a blue t-shirt is standing; he lifts the dough while a woman wearing a blue t-shirt is standing in the middle and watching the man, another woman wearing a black top is standing on the right side and watching the man."], "durations": null, "exact_frames_per_prompt": [10], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/0kP-26H_9uI.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 14, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1102_0_0"}, {"texts": ["The man stretches it."], "durations": null, "exact_frames_per_prompt": [48], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/0kP-26H_9uI.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 14, "npz_gt_video_start_frame": 14, "npz_gt_video_end_frame": 62, "skip_frames_after_generation": 14, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1102_0_1"}, {"texts": ["The man put it on the black tray."], "durations": null, "exact_frames_per_prompt": [18], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/0kP-26H_9uI.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 62, "npz_gt_video_start_frame": 62, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 62, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1102_0_2"}, {"texts": ["A woman wearing a gray top and black trousers is standing in the middle and watching the process."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/0kP-26H_9uI.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1102_1_0"}, {"texts": ["A boy wearing a blue jacket is standing and opening the bolts of the wheel."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/3zMPpr7iZMU.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1103_0_0"}, {"texts": ["A boy wearing a blue t-shirt is washing his hands in a towel."], "durations": null, "exact_frames_per_prompt": [29], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/1btAwIjGwfs.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 33, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1104_0_0"}, {"texts": ["Then the boy picks a glass cup."], "durations": null, "exact_frames_per_prompt": [47], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/1btAwIjGwfs.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 33, "npz_gt_video_start_frame": 33, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 33, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1104_0_1"}, {"texts": ["A woman whose only hands are visible is wearing green cloth is sitting on the red sofa, carrying a black dog on her lap and tickling a finger."], "durations": null, "exact_frames_per_prompt": [77], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/1pxvzGMCvoA.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 81, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1105_0_0"}, {"texts": ["A black dog stands on its own two legs and tickles a woman's finger with two legs on the woman lap."], "durations": null, "exact_frames_per_prompt": [77], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/1pxvzGMCvoA.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 81, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1105_1_0"}, {"texts": ["A baby wearing a white t-shirt and blue pants is crouching down to pat the rabbit while a brown animal is moving on the left side."], "durations": null, "exact_frames_per_prompt": [29], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-KX8ZEdKlX4.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 33, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1106_0_0"}, {"texts": ["The baby stands straight.  while another animal walks behind the baby on the left side."], "durations": null, "exact_frames_per_prompt": [47], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-KX8ZEdKlX4.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 33, "npz_gt_video_start_frame": 33, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 33, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1106_0_1"}, {"texts": ["A white rabbit is sitting on the ground and is being petted by the baby while a brown rabbit is sitting on the other side of the fence, a brown animal and a black animal are standing on the soil surface."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-KX8ZEdKlX4.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1106_1_0"}, {"texts": ["A man wearing a black t-shirt is holding a mike and speaking on it."], "durations": null, "exact_frames_per_prompt": [41], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/3iFh4yqDtvA.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 39, "npz_gt_video_start_frame": 39, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 39, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1107_2_0"}, {"texts": ["A black-brown dog is walking forward on the road surface with a baby girl who is holding its leash."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/0sL5rRoMgLs.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1108_0_0"}, {"texts": ["A baby girl wearing a multicolored dress is holding the leash of the dog and walking behind him."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/0sL5rRoMgLs.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1108_1_0"}, {"texts": ["A girl wearing a red dress is at first sitting on the wooden floor, near the blue gift box while a boy wearing a black sweater is holding an object, he moves towards another girl who is sitting on the right and sits there."], "durations": null, "exact_frames_per_prompt": [10], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-koP2KpBnnQ.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 14, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_110_0_0"}, {"texts": ["The girl gets up and starts walking towards the other blue gift box while the boy pulls the blue gift towards him."], "durations": null, "exact_frames_per_prompt": [21], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-koP2KpBnnQ.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 14, "npz_gt_video_start_frame": 14, "npz_gt_video_end_frame": 35, "skip_frames_after_generation": 14, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_110_0_1"}, {"texts": ["The girl sits in front of the other gift box while another girl picks up a box, throws it and moves towards the boy, and the boy is unwrapping the gift."], "durations": null, "exact_frames_per_prompt": [20], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-koP2KpBnnQ.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 35, "npz_gt_video_start_frame": 35, "npz_gt_video_end_frame": 55, "skip_frames_after_generation": 35, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_110_0_2"}, {"texts": ["The girl stands up and starts moving towards the gift box kept in front of the Christmas tree while another girl pushes the blue gift box."], "durations": null, "exact_frames_per_prompt": [25], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-koP2KpBnnQ.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 55, "npz_gt_video_start_frame": 55, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 55, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_110_0_3"}, {"texts": ["A girl in multi-colored clothing is at first kneeling on the floor.  while a boy puts the stick down and holds a gift box and another girl in a red top stands up."], "durations": null, "exact_frames_per_prompt": [24], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-koP2KpBnnQ.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 28, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_110_1_0"}, {"texts": ["The girl picks and drops the box kept on the floor and the boy is unwrapping the gift box."], "durations": null, "exact_frames_per_prompt": [20], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-koP2KpBnnQ.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 28, "npz_gt_video_start_frame": 28, "npz_gt_video_end_frame": 48, "skip_frames_after_generation": 28, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_110_1_1"}, {"texts": ["The girl starts moving in the front direction from behind the boy."], "durations": null, "exact_frames_per_prompt": [26], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-koP2KpBnnQ.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 48, "npz_gt_video_start_frame": 48, "npz_gt_video_end_frame": 74, "skip_frames_after_generation": 48, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_110_1_2"}, {"texts": ["The girl starts pushing the gift."], "durations": null, "exact_frames_per_prompt": [5], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-koP2KpBnnQ.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 74, "npz_gt_video_start_frame": 74, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 74, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_110_1_3"}, {"texts": ["A woman wearing white top is sitting on a black couch and is making a paper box with a multi-colored paper with a man wearing blue colour check shirt and speaking"], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/1Ve41j9BnP8.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1110_0_0"}, {"texts": ["A man on the left wearing a checkered shirt is also sitting on the black couch, while the woman wearing white top is doing some work."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/1Ve41j9BnP8.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1110_1_0"}, {"texts": ["A man wearing a black t-shirt is sitting and brushing his face while holding a mirror."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/3vBY_Uy4moQ.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1111_0_0"}, {"texts": ["A man wearing a black suit is standing, and moving his hands toward the screen.\n"], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-GKxkTG6O2E.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1112_0_0"}, {"texts": ["A person whose hand is visible is holding a transparent cup while a kid wearing a red jacket is sitting and eating the ice-cream, a group of people are sitting at the back and a person is walking at the back."], "durations": null, "exact_frames_per_prompt": [62], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-ujutn8Alpw.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 18, "npz_gt_video_start_frame": 18, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 18, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1113_0_0"}, {"texts": ["A baby wearing red clothes is sitting and holding a red spoon and eating ice-cream from the cup while a group of people is standing and sitting at the back and a kid starts walking toward the front."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-ujutn8Alpw.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1113_1_0"}, {"texts": ["A baby wearing pink and grey jacket is eating ice cream with a red spoon."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-ujutn8Alpw.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1114_0_0"}, {"texts": ["A man wearing a grey pant is standing on the green grass surface, putting the golf stick near the golf ball."], "durations": null, "exact_frames_per_prompt": [54], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/058qIczCDm0.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 58, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1115_0_0"}, {"texts": ["The man is hitting the golf ball with the golf stick."], "durations": null, "exact_frames_per_prompt": [22], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/058qIczCDm0.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 58, "npz_gt_video_start_frame": 58, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 58, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1115_0_1"}, {"texts": ["A woman at the back, wearing a purple jacket is walking and running on the road with her black and white dog while another woman wearing a blue sweater is walking on the road with her black-white dog."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/0M4ScBpCmig.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1116_1_0"}, {"texts": ["A black and white dog at the back is walking and pulling person two while first person wearing blue outfit is walking on road in front of the person two and a white color dog is walking with the first person then the dog stops on the grass surface."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/0M4ScBpCmig.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1116_3_0"}, {"texts": ["A person whose only hand is visible opens a washing machine door."], "durations": null, "exact_frames_per_prompt": [18], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/7hdSoP4GK64.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 11, "npz_gt_video_start_frame": 11, "npz_gt_video_end_frame": 29, "skip_frames_after_generation": 11, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1117_0_0"}, {"texts": ["The person puts clothes in a washing machine."], "durations": null, "exact_frames_per_prompt": [45], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/7hdSoP4GK64.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 29, "npz_gt_video_start_frame": 29, "npz_gt_video_end_frame": 74, "skip_frames_after_generation": 29, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1117_0_1"}, {"texts": ["A person whom hand is visible is opening a washing machine."], "durations": null, "exact_frames_per_prompt": [24], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/7hdSoP4GK64.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 28, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1118_0_0"}, {"texts": ["The person whom hand is visible is picking the clothes from the floor."], "durations": null, "exact_frames_per_prompt": [10], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/7hdSoP4GK64.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 28, "npz_gt_video_start_frame": 28, "npz_gt_video_end_frame": 38, "skip_frames_after_generation": 28, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1118_0_1"}, {"texts": ["The person whom hand is visible is putting it in the washing machine."], "durations": null, "exact_frames_per_prompt": [42], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/7hdSoP4GK64.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 38, "npz_gt_video_start_frame": 38, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 38, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1118_0_2"}, {"texts": ["A person sitting on the soil surface."], "durations": null, "exact_frames_per_prompt": [13], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/2Rg0ZmxYdrs.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 17, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1119_0_0"}, {"texts": ["The person is integrating wooden sticks for fire."], "durations": null, "exact_frames_per_prompt": [50], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/2Rg0ZmxYdrs.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 17, "npz_gt_video_start_frame": 17, "npz_gt_video_end_frame": 67, "skip_frames_after_generation": 17, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1119_0_1"}, {"texts": ["The person is placing a pot on a burning fire."], "durations": null, "exact_frames_per_prompt": [13], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/2Rg0ZmxYdrs.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 67, "npz_gt_video_start_frame": 67, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 67, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1119_0_2"}, {"texts": ["A girl wearing a red sweater and black trousers is sitting on the floor while a boy wearing a blue sweater is holding a plastic object and then he sits on the brown floor."], "durations": null, "exact_frames_per_prompt": [13], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-koP2KpBnnQ.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 17, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_111_0_0"}, {"texts": ["The girl stands while a boy starts touching the gift packet."], "durations": null, "exact_frames_per_prompt": [4], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-koP2KpBnnQ.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 17, "npz_gt_video_start_frame": 17, "npz_gt_video_end_frame": 22, "skip_frames_after_generation": 17, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_111_0_1"}, {"texts": ["The girl starts walking and she sits while a boy sits and starts unwrapping the gift pack."], "durations": null, "exact_frames_per_prompt": [36], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-koP2KpBnnQ.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 22, "npz_gt_video_start_frame": 22, "npz_gt_video_end_frame": 58, "skip_frames_after_generation": 22, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_111_0_2"}, {"texts": ["The girl stands again, and again starts walking in the room"], "durations": null, "exact_frames_per_prompt": [22], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-koP2KpBnnQ.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 58, "npz_gt_video_start_frame": 58, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 58, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_111_0_3"}, {"texts": ["A girl wearing a purple sweater and blue skirt is walking around and starts unwrapping the gift along with other kids."], "durations": null, "exact_frames_per_prompt": [34], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-koP2KpBnnQ.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 46, "npz_gt_video_start_frame": 46, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 46, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_111_2_0"}, {"texts": ["A woman wearing red clothes is standing near the podium and speaking while holding a paper a woman is standing near the wall."], "durations": null, "exact_frames_per_prompt": [22], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/04ahWwa-zRM.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 26, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1122_0_0"}, {"texts": ["The woman wearing red clothes moves and hugs another woman a woman is hugging and group of people stands up and start clapping."], "durations": null, "exact_frames_per_prompt": [54], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/04ahWwa-zRM.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 26, "npz_gt_video_start_frame": 26, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 26, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1122_0_1"}, {"texts": ["A woman is standing next to the first woman."], "durations": null, "exact_frames_per_prompt": [35], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/04ahWwa-zRM.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 39, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1122_1_0"}, {"texts": ["The woman is hugging the first woman."], "durations": null, "exact_frames_per_prompt": [16], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/04ahWwa-zRM.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 39, "npz_gt_video_start_frame": 39, "npz_gt_video_end_frame": 55, "skip_frames_after_generation": 39, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1122_1_1"}, {"texts": ["A man wearing a blue shirt and brown pants is sitting on a sofa and holding a baby with his hands in his lap while a black and white animal is also sitting in the front on the sofa."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/32n6826bJ20.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1123_0_0"}, {"texts": ["A baby wearing a pink t-shirt and pajamas is lying on the lap of the man, and the baby is caressing a cat with his hand."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/32n6826bJ20.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1123_1_0"}, {"texts": ["A baby wearing a pink t-shirt is lying on a person's lap."], "durations": null, "exact_frames_per_prompt": [22], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/32n6826bJ20.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 26, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1124_0_0"}, {"texts": ["The baby wearing a pink t-shirt is moving her hand on a cat's head."], "durations": null, "exact_frames_per_prompt": [54], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/32n6826bJ20.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 26, "npz_gt_video_start_frame": 26, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 26, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1124_0_1"}, {"texts": ["A person wearing brown pants is sitting and holding the baby with his hands on his lap while the baby is touching the cat's head."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/32n6826bJ20.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1124_1_0"}, {"texts": ["A white and black cat is sitting on a grey sofa and a person is sitting on the couch while holding a baby on its lap."], "durations": null, "exact_frames_per_prompt": [18], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/32n6826bJ20.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 22, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1124_2_0"}, {"texts": ["The cat is getting caressed by a baby."], "durations": null, "exact_frames_per_prompt": [58], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/32n6826bJ20.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 22, "npz_gt_video_start_frame": 22, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 22, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1124_2_1"}, {"texts": ["A man wearing blue trousers is standing, speaking, and doing hand gestures towards the weather showing screen."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/0KQfqV0QOSM.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1126_0_0"}, {"texts": ["A person whose hand is visible is touching the cat."], "durations": null, "exact_frames_per_prompt": [33], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/2PYSV3M7Woc.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 37, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1127_0_0"}, {"texts": ["A cat is sitting and touched by a person."], "durations": null, "exact_frames_per_prompt": [32], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/2PYSV3M7Woc.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 36, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1127_1_0"}, {"texts": ["The cat lifts her leg."], "durations": null, "exact_frames_per_prompt": [12], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/2PYSV3M7Woc.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 36, "npz_gt_video_start_frame": 36, "npz_gt_video_end_frame": 48, "skip_frames_after_generation": 36, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1127_1_1"}, {"texts": ["A girl wearing a multicolored frock is walking on the left side of the road along with the brown dog."], "durations": null, "exact_frames_per_prompt": [58], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-6VzhYhWDMY.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 62, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1128_0_0"}, {"texts": ["A brown colored dog is walking on the road along with the girl in multicolor frock."], "durations": null, "exact_frames_per_prompt": [58], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-6VzhYhWDMY.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 62, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1128_1_0"}, {"texts": ["A person whose fingers are visible is mixing the egg yolk in a pan with a silicone spatula."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/09UVMOpKTRo.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1129_0_0"}, {"texts": ["A girl wearing white clothes is standing near the barrier and watching the goat while a white goat left side standing inside harrier and playing with a girl"], "durations": null, "exact_frames_per_prompt": [25], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-CT7VD7r3gQ.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 29, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_112_2_0"}, {"texts": ["A white goat is standing near the barrier and watching the girl."], "durations": null, "exact_frames_per_prompt": [25], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-CT7VD7r3gQ.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 29, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_112_3_0"}, {"texts": ["A person whose only hand is visible is mixing a yellow paste in a silver bowl with a spatula."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/09UVMOpKTRo.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1130_0_0"}, {"texts": ["A man is wearing a black t-shirt holding a fishing rod."], "durations": null, "exact_frames_per_prompt": [31], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/0QILb4g09ZI.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 35, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1132_0_0"}, {"texts": ["The man is trying to catch a fish."], "durations": null, "exact_frames_per_prompt": [45], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/0QILb4g09ZI.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 35, "npz_gt_video_start_frame": 35, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 35, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1132_0_1"}, {"texts": ["A man wearing a blue t-shirt and black shorts is standing."], "durations": null, "exact_frames_per_prompt": [26], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/17tNN4BrJsQ.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 30, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1133_0_0"}, {"texts": ["The man wearing a blue t-shirt and black shorts is hitting a golf ball with the golf stick."], "durations": null, "exact_frames_per_prompt": [50], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/17tNN4BrJsQ.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 30, "npz_gt_video_start_frame": 30, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 30, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1133_0_1"}, {"texts": ["A group of mules are walking and carrying the people.\n"], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/4VZUCNBf1P4.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1137_1_ms_0"}, {"texts": ["A man wearing a white t-shirt is standing on the grass surface while another man wearing brown and grey outfit, holding a golf stick in his hand is standing on grass surface."], "durations": null, "exact_frames_per_prompt": [25], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/2AINoaJm_Wg.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 29, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1139_0_ms_0"}, {"texts": ["A man wearing a violet t-shirt is also standing."], "durations": null, "exact_frames_per_prompt": [15], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/2AINoaJm_Wg.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 26, "npz_gt_video_start_frame": 26, "npz_gt_video_end_frame": 41, "skip_frames_after_generation": 26, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1139_1_0"}, {"texts": ["The man is playing golf on the grass surface."], "durations": null, "exact_frames_per_prompt": [39], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/2AINoaJm_Wg.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 41, "npz_gt_video_start_frame": 41, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 41, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1139_1_1"}, {"texts": ["A child wearing a white shirt with a white sweater and green striped pants is standing on the right side of the soil surface and is babbling while moving the right hand towards the first goat."], "durations": null, "exact_frames_per_prompt": [25], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-CT7VD7r3gQ.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 29, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_113_2_0"}, {"texts": ["A person whose half body is visible, wearing a black jacket and orange pants is standing and drilling into ice with an ice auger machine."], "durations": null, "exact_frames_per_prompt": [46], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/35TXceM7rnE.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 50, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1145_0_0"}, {"texts": ["A person whose legs are visible is brushing the horse."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-iAt-PFC52M.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1147_1_0"}, {"texts": ["A man whose only half body is visible is wearing a gray-black shirt and black pants is standing and taking out money from the cash drawer."], "durations": null, "exact_frames_per_prompt": [42], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/0jhDVW0pmvk.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 38, "npz_gt_video_start_frame": 38, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 38, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1148_0_ms_0"}, {"texts": ["A person wearing a grey shirt is standing and taking cash out from the cash drawer."], "durations": null, "exact_frames_per_prompt": [43], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/0jhDVW0pmvk.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 37, "npz_gt_video_start_frame": 37, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 37, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1149_0_ms_0"}, {"texts": ["A woman wearing a purple top is standing in the kitchen while a kid wearing a white sweatshirt lifts her hands and puts on the chopping board, and a girl wearing a pink sweatshirt taps a green food."], "durations": null, "exact_frames_per_prompt": [43], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/10YyfNp68iI.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 47, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_114_0_0"}, {"texts": ["The woman is picking up the white container while a girl wearing a pink sweatshirt puts her hand on the side of her face and a kid wearing a white sweatshirt points her index finger at the woman."], "durations": null, "exact_frames_per_prompt": [33], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/10YyfNp68iI.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 47, "npz_gt_video_start_frame": 47, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 47, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_114_0_1"}, {"texts": ["A girl wearing a pink top is speaking and is tapping on the white butter paper."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/10YyfNp68iI.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_114_1_0"}, {"texts": ["A small girl wearing a white top is doing hand gestures and playing with a girl wearing pink top who is tapping some green thing on the counter while the woman wearing purple dress is explaining them something"], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/10YyfNp68iI.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_114_2_0"}, {"texts": ["A man wearing white clothes is standing near the counter and decorating food."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/49l11Vs3WRY.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1150_0_0"}, {"texts": ["A woman wearing a printed black t-shirt is removing the pillows from the bed while moving on a white surface."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/j9qdhiMFv90.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1151_0_0"}, {"texts": ["A man wearing a denim jeans is holding a dog leash and washing the black dog with a hose."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-r9d_D8UxZo.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1152_0_0"}, {"texts": ["A black dog is standing on the gray surface and getting washed by a man."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-r9d_D8UxZo.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1152_1_0"}, {"texts": ["A person whose hand is only visible is speaking while moving her hands over a pizza."], "durations": null, "exact_frames_per_prompt": [69], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/0z5EFz8eysI.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 11, "npz_gt_video_start_frame": 11, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 11, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1153_0_0"}, {"texts": ["A woman wearing a white top is sitting on a ride and laughing.\n"], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/1uFmC13MA3E.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1156_0_0"}, {"texts": ["A person wearing gray clothes is standing and holding the rope of a horse."], "durations": null, "exact_frames_per_prompt": [30], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/0HHxzWE4sCE.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 34, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1157_0_0"}, {"texts": ["The person wearing gray clothes is untying it and speaking while the brown horse is standing on the green grass surface."], "durations": null, "exact_frames_per_prompt": [46], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/0HHxzWE4sCE.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 34, "npz_gt_video_start_frame": 34, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 34, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1157_0_1"}, {"texts": ["A black cat is sitting on a bed."], "durations": null, "exact_frames_per_prompt": [18], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-xRULl4FhNw.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 22, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1159_1_0"}, {"texts": ["The cat stands up and the person whose hand is visible touching the cat."], "durations": null, "exact_frames_per_prompt": [58], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-xRULl4FhNw.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 22, "npz_gt_video_start_frame": 22, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 22, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1159_1_1"}, {"texts": ["A girl wearing a pink top is sitting, holding a pink bottle, and coughing."], "durations": null, "exact_frames_per_prompt": [13], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/3bgdfs9-43o.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 17, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_115_0_0"}, {"texts": ["The girl is drinking from the bottle. "], "durations": null, "exact_frames_per_prompt": [63], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/3bgdfs9-43o.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 17, "npz_gt_video_start_frame": 17, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 17, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_115_0_1"}, {"texts": ["A big black cat is growling while walking on the bed."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-xRULl4FhNw.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1160_0_0"}, {"texts": ["A man wearing a white t-shirt is opening a plastic container in which a small snake is sitting."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/2Hq2ZiAFwlw.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1162_0_0"}, {"texts": ["A small snake is sitting inside a plastic container and the man wearing white tank top tries to open the plastic container"], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/2Hq2ZiAFwlw.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1162_1_0"}, {"texts": ["A woman wearing black clothes is applying cream on her face and under eyes and speaking."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/4MLTZwhxfKE.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1164_0_0"}, {"texts": ["A person whose only hand is visible is cutting a watermelon with a knife."], "durations": null, "exact_frames_per_prompt": [28], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/A5ZkpwjQU48.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 52, "npz_gt_video_start_frame": 52, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 52, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1165_0_ms_0"}, {"texts": ["A cat which is standing on the floor is getting its back caress by the person in black clothing."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-K7Vi40ZcR0.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1166_1_0"}, {"texts": ["A man whose upper half-body is visible, wearing a black-white t-shirt, is sitting while holding a trophy in his left hand and speaking while moving his right hand."], "durations": null, "exact_frames_per_prompt": [47], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-m9CFI5QHmk.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 33, "npz_gt_video_start_frame": 33, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 33, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1167_0_ms_0"}, {"texts": ["A person wearing light brown clothes is walking at the front while holding the camel leash while another person wearing a blue-white striped t-shirt is riding the camel."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/0t-YfvZNdks.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1168_0_0"}, {"texts": ["A person wearing a sky blue-white striped shirt is sitting on the camel and riding on the sand surface while a man wearing brown clothes is walking on the sand surface while holding the camel's leash and another man wearing pants is also walking on the sand surface towards the back."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/0t-YfvZNdks.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1168_1_0"}, {"texts": ["A brown camel is walking behind the first person, who is holding a leash and carrying a person on his back while some people are standing and some are walking."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/0t-YfvZNdks.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1168_3_0"}, {"texts": ["A man wearing light brown clothes is walking at the front while holding the camel leash while a woman is wearing blue pants sitting on a camel and going."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/0t-YfvZNdks.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1169_0_0"}, {"texts": ["A woman wearing a sky blue-white striped shirt is sitting on the camel and riding on the sand surface while a man wearing brown clothes is walking on the sand surface, holding the camel's leash, and another man wearing white pants is walking on the sand surface at the back."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/0t-YfvZNdks.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1169_1_0"}, {"texts": ["A camel is walking just behind the first person who holding the leash and carrying a person on his back."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/0t-YfvZNdks.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1169_4_0"}, {"texts": ["A baby wearing a red printed t-shirt is lying on the baby seat and laughing while watching the woman tearing a piece of paper."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/17OqptEZ-w8.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_116_0_0"}, {"texts": ["A boy wearing blue shorts is standing on the grey surface, holding a fuel pump nozzle and pressing the handle of the fuel pump nozzle."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/0NwqwjDdmgc.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1178_0_0"}, {"texts": ["A man wearing a white chef coat is standing in the kitchen."], "durations": null, "exact_frames_per_prompt": [11], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/3RzGHMkOZcI.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 15, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1179_0_0"}, {"texts": ["The man is mixing a salad in a white bowl with a tong."], "durations": null, "exact_frames_per_prompt": [65], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/3RzGHMkOZcI.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 15, "npz_gt_video_start_frame": 15, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 15, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1179_0_1"}, {"texts": ["A man wearing a black t-shirt and pants is standing in front of a table and performing flair bartending."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/3u_jAWaj0zI.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_117_0_0"}, {"texts": ["A group of three people are sitting on chairs and moving their hands on the table."], "durations": null, "exact_frames_per_prompt": [29], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/--UW3CcUqaU.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 33, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1181_0_ms_0"}, {"texts": ["A person whose hands are visible is showing his hands."], "durations": null, "exact_frames_per_prompt": [10], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/2R2Xfv0IuV4.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 14, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1182_0_0"}, {"texts": ["The person is picking up the tray from the table and showing it."], "durations": null, "exact_frames_per_prompt": [45], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/2R2Xfv0IuV4.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 14, "npz_gt_video_start_frame": 14, "npz_gt_video_end_frame": 59, "skip_frames_after_generation": 14, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1182_0_1"}, {"texts": ["A woman wearing a black top is standing and moving on the soil surface then feeding food to the goats while a child in a blue t-shirt is terrified of the goat and a crowd moves around the cages."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/1G_YL6ruA2E.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1184_0_0"}, {"texts": ["A brown-white goat on the right is eating food given by a woman and standing on the soil surface while a kid wearing blue and orange clothes is walking away from the goats and people are doing different activity."], "durations": null, "exact_frames_per_prompt": [40], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/1G_YL6ruA2E.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 44, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1184_6_0"}, {"texts": ["A brown-white goat on the left is eating food given by the woman and standing on the soil surface while a kid in blue-orange shorts gets scared of the goat, and another brown goat moves towards the right."], "durations": null, "exact_frames_per_prompt": [55], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/1G_YL6ruA2E.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 25, "npz_gt_video_start_frame": 25, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 25, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1184_7_0"}, {"texts": ["A girl wearing a black top is sitting and lip sync with a song and doing sign language."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/5rtumYrWXAg.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1185_0_0"}, {"texts": ["A woman wearing a black dress is sitting on a dark gray seat and doing the sign language of a song with her hands while looking in front and singing."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/5rtumYrWXAg.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1186_0_0"}, {"texts": ["A man wearing a shirt and hat is walking."], "durations": null, "exact_frames_per_prompt": [26], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/0JTO1HrK8kY.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 30, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1188_0_0"}, {"texts": ["The man sits, opening the cap of the bottle."], "durations": null, "exact_frames_per_prompt": [12], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/0JTO1HrK8kY.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 30, "npz_gt_video_start_frame": 30, "npz_gt_video_end_frame": 42, "skip_frames_after_generation": 30, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1188_0_1"}, {"texts": ["The man is drinking water."], "durations": null, "exact_frames_per_prompt": [33], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/0JTO1HrK8kY.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 42, "npz_gt_video_start_frame": 42, "npz_gt_video_end_frame": 75, "skip_frames_after_generation": 42, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1188_0_2"}, {"texts": ["A man wearing a white t-shirt opens a glass bottle cap while tapping his hand on the bottle while putting the edge of the bottle on the white object."], "durations": null, "exact_frames_per_prompt": [17], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/SD_z-98Au80.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 21, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1189_0_0"}, {"texts": ["The man starts drinking."], "durations": null, "exact_frames_per_prompt": [39], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/SD_z-98Au80.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 21, "npz_gt_video_start_frame": 21, "npz_gt_video_end_frame": 60, "skip_frames_after_generation": 21, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1189_0_1"}, {"texts": ["A woman wearing a checkered top is standing in the kitchen, and putting cooked vegetables from a pan to a white bowl with a spatula."], "durations": null, "exact_frames_per_prompt": [77], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/27DE0MrKb2A.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 81, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1192_0_0"}, {"texts": ["A girl wearing a blue top is drinking milk from the udder of the cow and then starts cleaning her mouth with her hand."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-3N2gRXnmsE.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1193_0_0"}, {"texts": ["A white-brown cow is standing on the grass, getting milked by a person while a woman wearing a blue top drank a cow milk"], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-3N2gRXnmsE.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1193_2_0"}, {"texts": ["A woman wearing a white t-shirt is standing behind the table. "], "durations": null, "exact_frames_per_prompt": [12], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/3zsqUBED5LM.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 16, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1197_1_0"}, {"texts": ["The woman is picking up the sausage with her right hand."], "durations": null, "exact_frames_per_prompt": [10], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/3zsqUBED5LM.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 16, "npz_gt_video_start_frame": 16, "npz_gt_video_end_frame": 26, "skip_frames_after_generation": 16, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1197_1_1"}, {"texts": ["The woman starts to put the sausage into her mouth."], "durations": null, "exact_frames_per_prompt": [6], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/3zsqUBED5LM.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 26, "npz_gt_video_start_frame": 26, "npz_gt_video_end_frame": 32, "skip_frames_after_generation": 26, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1197_1_2"}, {"texts": ["The woman is laughing while taking out the sausage from her mouth."], "durations": null, "exact_frames_per_prompt": [22], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/3zsqUBED5LM.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 32, "npz_gt_video_start_frame": 32, "npz_gt_video_end_frame": 54, "skip_frames_after_generation": 32, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1197_1_3"}, {"texts": ["A boy wearing a white t-shirt is sitting on the brown carpet, he first puts his hand in the green santa cap while another boy wearing a blue t-shirt is sitting on the left, holding a red cap, and speaking while shaking the cap."], "durations": null, "exact_frames_per_prompt": [38], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/11qz8wWjBYs.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 42, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1198_0_0"}, {"texts": ["The boy takes out a red object and showing it with his left hand while smiling, and the boy in the blue t-shirt takes out an object from the red cap."], "durations": null, "exact_frames_per_prompt": [38], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/11qz8wWjBYs.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 42, "npz_gt_video_start_frame": 42, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 42, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1198_0_1"}, {"texts": ["A boy wearing a blue t-shirt is sitting on the brown carpet, looking inside the red Santa cap while another boy wearing a white vest is sitting, holding a green Santa cap, and taking out a red gift."], "durations": null, "exact_frames_per_prompt": [73], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/11qz8wWjBYs.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 77, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1198_1_0"}, {"texts": ["The boy is taking out the chocolate while another boy is showing the red gift."], "durations": null, "exact_frames_per_prompt": [3], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/11qz8wWjBYs.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 77, "npz_gt_video_start_frame": 77, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 77, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1198_1_1"}, {"texts": ["A woman wearing a blue top is standing and cutting watermelon."], "durations": null, "exact_frames_per_prompt": [10], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/8c8MObAsjAk.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 14, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_119_0_0"}, {"texts": ["The woman is holding the pieces of watermelon in her hands."], "durations": null, "exact_frames_per_prompt": [45], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/8c8MObAsjAk.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 14, "npz_gt_video_start_frame": 14, "npz_gt_video_end_frame": 59, "skip_frames_after_generation": 14, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_119_0_1"}, {"texts": ["A man wearing a shirt is holding a frying pan and whisking a yellow liquid food with a whisk."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/2-TaWGruYyY.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_11_0_0"}, {"texts": ["A man wearing a gray suit and specs is sitting and speaking on the mic."], "durations": null, "exact_frames_per_prompt": [42], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/1blenBMZQgY.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 46, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1204_0_0"}, {"texts": ["A man wearing a black suit is also sitting and speaking while a group of people are sitting and looking at the man."], "durations": null, "exact_frames_per_prompt": [34], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/1blenBMZQgY.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 46, "npz_gt_video_start_frame": 46, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 46, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1204_1_0"}, {"texts": ["A person wearing a bee suit is spraying smoke inside a yellow bee house with a bee hive smoker."], "durations": null, "exact_frames_per_prompt": [56], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/1WOHCPQEvg0.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 60, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1207_0_0"}, {"texts": ["A man carrying a black bag is walking on the left side on the soil surface."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/0GpOe8tgrRo.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1208_0_0"}, {"texts": ["A man wearing a cap is sitting on the back is riding on the elephant back while the other person who is sitting in front of the man."], "durations": null, "exact_frames_per_prompt": [58], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/0GpOe8tgrRo.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 18, "npz_gt_video_start_frame": 18, "npz_gt_video_end_frame": 76, "skip_frames_after_generation": 18, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1208_3_0"}, {"texts": ["An elephant is walking on the soil surface while carrying people on its back while other people are walking on the soil surface."], "durations": null, "exact_frames_per_prompt": [71], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/0GpOe8tgrRo.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 9, "npz_gt_video_start_frame": 9, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 9, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1208_4_0"}, {"texts": ["A small white and brown dog is playing with another dog and a person comes from the right side and moves its hand toward the dog."], "durations": null, "exact_frames_per_prompt": [50], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/39TaxrZTdYE.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 54, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1210_0_0"}, {"texts": ["The small white and brown dog is being fed by a person and another dog is also being fed by the person."], "durations": null, "exact_frames_per_prompt": [26], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/39TaxrZTdYE.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 54, "npz_gt_video_start_frame": 54, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 54, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1210_0_1"}, {"texts": ["A big white and brown dog is also playing with dog one while a man wearing black pants is standing on a carpet and feeding the dogs."], "durations": null, "exact_frames_per_prompt": [65], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/39TaxrZTdYE.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 69, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1210_1_0"}, {"texts": ["The dog is also being fed by the person."], "durations": null, "exact_frames_per_prompt": [11], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/39TaxrZTdYE.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 69, "npz_gt_video_start_frame": 69, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 69, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1210_1_1"}, {"texts": ["A woman wearing black pants on the right comes while the small puppy takes the food from woman."], "durations": null, "exact_frames_per_prompt": [51], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/39TaxrZTdYE.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 55, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1210_2_0"}, {"texts": ["The woman wearing black pants feeds both the dogs."], "durations": null, "exact_frames_per_prompt": [25], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/39TaxrZTdYE.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 55, "npz_gt_video_start_frame": 55, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 55, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1210_2_1"}, {"texts": ["A person whose hands are visible is holding a tray and whisking a liquid in a tray with a whisk."], "durations": null, "exact_frames_per_prompt": [17], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/0YZnqJIOCE0.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 21, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1214_0_ms_0"}, {"texts": ["A group of people are sitting, one person is holding a bottle and one person is sitting on a table."], "durations": null, "exact_frames_per_prompt": [20], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/0YZnqJIOCE0.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 60, "npz_gt_video_start_frame": 60, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 60, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1214_1_ms_0"}, {"texts": ["A person wearing a black suit is standing, folding a white cloth and making a boat on the black surface."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/2dRSJcaJIXw.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1215_0_0"}, {"texts": ["A person wearing a black suit is making a boat with a cloth."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/2dRSJcaJIXw.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1216_0_0"}, {"texts": ["A woman whose hands are visible is holding a white-grey spotted gift and cutting a paper strap from the gift with a pair of scissors."], "durations": null, "exact_frames_per_prompt": [14], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-LbmsAhfpmk.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 18, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1220_0_0"}, {"texts": ["The woman is pressing the paper strap with her fingers."], "durations": null, "exact_frames_per_prompt": [23], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-LbmsAhfpmk.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 18, "npz_gt_video_start_frame": 18, "npz_gt_video_end_frame": 41, "skip_frames_after_generation": 18, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1220_0_1"}, {"texts": ["A man wearing a black shirt and black pants is sitting on the elephant's back and is unloading the girl from the elephant while the man wearing black half jacket is helping the girl to get down"], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/0eVbNhwl8_8.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1222_1_0"}, {"texts": ["A man wearing an orange t-shirt is standing on the road, and is holding the girl while his hands are up while another man sitting on an elephant is helping the girl get down from the elephant."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/0eVbNhwl8_8.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1222_2_0"}, {"texts": ["A girl wearing a purple jacket and printed gray pants is being unloaded from the elephant by the first man and the other man is holding the girl and helping her get down the elephant, and some people are moving behind the elephant."], "durations": null, "exact_frames_per_prompt": [50], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/0eVbNhwl8_8.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 54, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1222_3_0"}, {"texts": ["A brown elephant is standing on the road while carrying the first man and the girl on the back while a man wearing a black coat is taking down the girl from the back of an elephant, tapping on the elephant, and a group of people are standing on the right side of the elephant."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/0eVbNhwl8_8.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1222_4_0"}, {"texts": ["A person whose only hands are visible is holding a peeler, and peeling a green apple."], "durations": null, "exact_frames_per_prompt": [32], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/28FryKFFCg0.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 36, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1224_0_0"}, {"texts": ["The person throws the peels in a paper bag."], "durations": null, "exact_frames_per_prompt": [44], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/28FryKFFCg0.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 36, "npz_gt_video_start_frame": 36, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 36, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1224_0_1"}, {"texts": ["A woman wearing a black top is standing behind the kitchen counter-top holding a chicken meat slice in her hand."], "durations": null, "exact_frames_per_prompt": [42], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/147Zp0WzvKk.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 46, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1226_0_0"}, {"texts": ["The woman puts the chicken meat slice into the corn flour."], "durations": null, "exact_frames_per_prompt": [34], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/147Zp0WzvKk.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 46, "npz_gt_video_start_frame": 46, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 46, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1226_0_1"}, {"texts": ["A woman wearing a black clothes is standing just behind the horse and puts her hand on the horse."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/1eXnOzELWQI.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1227_0_0"}, {"texts": ["A brown horse is standing on the brown surface while it is being cleaned by a woman."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/1eXnOzELWQI.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1227_1_0"}, {"texts": ["A woman wearing a ring on her finger. "], "durations": null, "exact_frames_per_prompt": [42], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-g9TIDtHVOA.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 6, "npz_gt_video_start_frame": 6, "npz_gt_video_end_frame": 48, "skip_frames_after_generation": 6, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_122_1_ms_0"}, {"texts": ["The woman is sitting on a brown chair."], "durations": null, "exact_frames_per_prompt": [32], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-g9TIDtHVOA.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 48, "npz_gt_video_start_frame": 48, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 48, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_122_1_ms_1"}, {"texts": ["A boy wearing a printed white t-shirt is standing."], "durations": null, "exact_frames_per_prompt": [40], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/1kMxaTdtQP0.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 44, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1230_0_0"}, {"texts": ["The boy is eating crunchy food."], "durations": null, "exact_frames_per_prompt": [36], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/1kMxaTdtQP0.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 44, "npz_gt_video_start_frame": 44, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 44, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1230_0_1"}, {"texts": ["A man wearing a red vest is standing in the front, opening cans and drinking from it."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/2XLhI47SOdo.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1233_0_0"}, {"texts": ["A boy sitting on a black chair, moving the chair and eating chips from a packet."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/2fDjeY7IVi8.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1235_0_0"}, {"texts": ["A man whose only hands are visible is caressing the body of the cat."], "durations": null, "exact_frames_per_prompt": [40], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/0n5JR_xxcTw.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 44, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1237_0_0"}, {"texts": ["The cat bites the man's hand when he is caressing the cat."], "durations": null, "exact_frames_per_prompt": [36], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/0n5JR_xxcTw.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 44, "npz_gt_video_start_frame": 44, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 44, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1237_0_1"}, {"texts": ["A brown striped cat is lying on the carpet while a hand of a person is touching the cat."], "durations": null, "exact_frames_per_prompt": [47], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/0n5JR_xxcTw.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 51, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1237_1_0"}, {"texts": ["The cat is biting the man's hand while being caressed by the man."], "durations": null, "exact_frames_per_prompt": [29], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/0n5JR_xxcTw.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 51, "npz_gt_video_start_frame": 51, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 51, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1237_1_1"}, {"texts": ["A man wearing a gray suit is standing and is talking about the weather."], "durations": null, "exact_frames_per_prompt": [39], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-hlAfZNh-uU.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 41, "npz_gt_video_start_frame": 41, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 41, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1238_0_0"}, {"texts": ["A man wearing a grey suit and a white shirt is standing in front of the digital screen, speaking and moving his hands."], "durations": null, "exact_frames_per_prompt": [39], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-hlAfZNh-uU.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 41, "npz_gt_video_start_frame": 41, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 41, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1239_0_0"}, {"texts": ["A person whose only hands are visible is holding, and pulling an orange rope."], "durations": null, "exact_frames_per_prompt": [36], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-hs2X8EDjf0.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 40, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_123_0_ms_0"}, {"texts": ["A person whose hand is only visible is holding a box and removes the cover of the box."], "durations": null, "exact_frames_per_prompt": [33], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/1e5GEtumSp4.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 18, "npz_gt_video_start_frame": 18, "npz_gt_video_end_frame": 51, "skip_frames_after_generation": 18, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1241_0_0"}, {"texts": ["The person whose hand is only visible puts the cover on the table."], "durations": null, "exact_frames_per_prompt": [25], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/1e5GEtumSp4.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 51, "npz_gt_video_start_frame": 51, "npz_gt_video_end_frame": 76, "skip_frames_after_generation": 51, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1241_0_1"}, {"texts": ["The person whose hand is only visible pushes the cover in the right side."], "durations": null, "exact_frames_per_prompt": [4], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/1e5GEtumSp4.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 76, "npz_gt_video_start_frame": 76, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 76, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1241_0_2"}, {"texts": ["A man wearing a blue t-shirt is standing and doing a sheep shearing on the brown cloth surface with the help of a sheep shearing machine while a sheep is lying down on a brown cloth and is being sheered by the man and another man wearing pink outfit and a cap, holding a mobile phone in his hand is standing on the right side and filming the first man and group of people are standing on the backside and watching the sheep shearing."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/0Ibiej393Xo.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1243_0_0"}, {"texts": ["A white sheep is getting sheared by the first man while the others are watching him."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/0Ibiej393Xo.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1243_3_0"}, {"texts": ["A person whose hand is visible, holding the hands of another person, and giving a massage."], "durations": null, "exact_frames_per_prompt": [40], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/28CFs15XQ8I.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 44, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1244_0_0"}, {"texts": ["A person whose hands are only visible is cutting a peeled watermelon piece, with a knife."], "durations": null, "exact_frames_per_prompt": [70], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/8MI8_eZJQ-I.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 74, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1246_0_0"}, {"texts": ["A boy wearing a gray t-shirt and gray shorts is sitting on a sofa and unwrapping a gift."], "durations": null, "exact_frames_per_prompt": [58], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-YGIiAr8SLc.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 62, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1248_0_0"}, {"texts": ["A woman wearing a blue shirt is dancing"], "durations": null, "exact_frames_per_prompt": [41], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/7Zw46FSOciI.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 45, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_124_0_0"}, {"texts": ["The woman starts scraping a watermelon piece with a knife."], "durations": null, "exact_frames_per_prompt": [34], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/7Zw46FSOciI.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 45, "npz_gt_video_start_frame": 45, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 45, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_124_0_1"}, {"texts": ["A person wearing a white top and black-white pajamas is chopping bread slices, making a sandwich, and walking around."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/6cA-Z6kmYKs.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1253_0_0"}, {"texts": ["A girl wearing black dress is sitting on a brown chair."], "durations": null, "exact_frames_per_prompt": [58], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/1vP6NS3C6Ug.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 62, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1255_0_0"}, {"texts": ["The girl is folding a green square paper diagonally."], "durations": null, "exact_frames_per_prompt": [18], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/1vP6NS3C6Ug.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 62, "npz_gt_video_start_frame": 62, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 62, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1255_0_1"}, {"texts": ["A woman whom upper half-body is visible, wearing a black top and specs, is sitting and speaking."], "durations": null, "exact_frames_per_prompt": [22], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/1vP6NS3C6Ug.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 26, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1256_0_0"}, {"texts": ["The woman wearing black top is moving her right hand."], "durations": null, "exact_frames_per_prompt": [12], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/1vP6NS3C6Ug.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 26, "npz_gt_video_start_frame": 26, "npz_gt_video_end_frame": 38, "skip_frames_after_generation": 26, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1256_0_1"}, {"texts": ["The woman wearing black top is holding a green paper on the bed study table."], "durations": null, "exact_frames_per_prompt": [21], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/1vP6NS3C6Ug.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 38, "npz_gt_video_start_frame": 38, "npz_gt_video_end_frame": 59, "skip_frames_after_generation": 38, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1256_0_2"}, {"texts": ["The woman wearing black top starts folding the green paper with her hands."], "durations": null, "exact_frames_per_prompt": [21], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/1vP6NS3C6Ug.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 59, "npz_gt_video_start_frame": 59, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 59, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1256_0_3"}, {"texts": ["A man wearing a blue shirt is moving."], "durations": null, "exact_frames_per_prompt": [39], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/6_9opM_N8sQ.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 43, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1257_0_0"}, {"texts": ["The man starts hanging a pink bow tie around his neck."], "durations": null, "exact_frames_per_prompt": [37], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/6_9opM_N8sQ.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 43, "npz_gt_video_start_frame": 43, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 43, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1257_0_1"}, {"texts": ["A person whose hands are visible is putting a silver can in the black shredder."], "durations": null, "exact_frames_per_prompt": [40], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/1yR4YXE2gCQ.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 44, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_125_0_0"}, {"texts": ["The person squeezes the can with his leg."], "durations": null, "exact_frames_per_prompt": [20], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/1yR4YXE2gCQ.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 44, "npz_gt_video_start_frame": 44, "npz_gt_video_end_frame": 64, "skip_frames_after_generation": 44, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_125_0_1"}, {"texts": ["The person puts the can in the shredder."], "durations": null, "exact_frames_per_prompt": [16], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/1yR4YXE2gCQ.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 64, "npz_gt_video_start_frame": 64, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 64, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_125_0_2"}, {"texts": ["A person whose only half-body is visible is standing and adding cheese to the garlic bread in a bowl."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-N8bMIwJ6YU.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1262_0_0"}, {"texts": ["A person whose hands are visible is holding a bowl and putting cheese in another bowl."], "durations": null, "exact_frames_per_prompt": [49], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-N8bMIwJ6YU.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 8, "npz_gt_video_start_frame": 8, "npz_gt_video_end_frame": 57, "skip_frames_after_generation": 8, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1263_0_0"}, {"texts": ["The person is spreading it."], "durations": null, "exact_frames_per_prompt": [22], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-N8bMIwJ6YU.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 57, "npz_gt_video_start_frame": 57, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 57, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1263_0_1"}, {"texts": ["A boy wearing a black t-shirt is standing on the left side and is holding the glass bottle while the woman wearing black top is trying to open the bottle and the girl watches it"], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/RztPkOKdNLs.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1265_0_0"}, {"texts": ["A girl wearing a black top is standing in the middle and trying to open the glass bottle cap with the help of a bottle opener tool while a boy wearing a black t-shirt is standing on the left is holding a bottle."], "durations": null, "exact_frames_per_prompt": [20], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/RztPkOKdNLs.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 24, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1265_1_0"}, {"texts": ["Then the girl touches the bottle while a boy start looking at the girl and laugh."], "durations": null, "exact_frames_per_prompt": [56], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/RztPkOKdNLs.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 24, "npz_gt_video_start_frame": 24, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 24, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1265_1_1"}, {"texts": ["A girl wearing a pink t-shirt is standing on the right side and watching the first girl while the boy holding the glass bottle"], "durations": null, "exact_frames_per_prompt": [32], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/RztPkOKdNLs.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 48, "npz_gt_video_start_frame": 48, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 48, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1265_2_0"}, {"texts": ["A bare-chested man is lying and is getting a tattoo on his chest by another person.\n while a group of people in which some people are walking and some people are standing on the backside."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/1gjjfUJV8Ms.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1267_0_0"}, {"texts": ["A man wearing a black t-shirt is making a tattoo on the chest of person one using a tattoo making machine while two people are standing on the backside."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/1gjjfUJV8Ms.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1267_1_0"}, {"texts": ["A man wearing a black printed t-shirt is standing and making a tattoo on the neck of another man.\n while a group of people are standing and moving at the back."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/1gjjfUJV8Ms.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1268_0_0"}, {"texts": ["A man is lying down on the surface."], "durations": null, "exact_frames_per_prompt": [14], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/1gjjfUJV8Ms.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 18, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1268_1_0"}, {"texts": ["The man is getting a tattoo from the first man while two men are standing behind."], "durations": null, "exact_frames_per_prompt": [62], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/1gjjfUJV8Ms.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 18, "npz_gt_video_start_frame": 18, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 18, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1268_1_1"}, {"texts": ["A woman wearing a white t-shirt is applying bandage on the hand of the other woman while a woman wearing white t-shirt standing in behind talking with someone"], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/1he9fSWoeHs.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1269_0_0"}, {"texts": ["A woman wearing a black t-shirt is standing and getting a bandage on her hand by the first woman."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/1he9fSWoeHs.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1269_1_0"}, {"texts": ["A man is picking up the tin can, pressing it with his hand."], "durations": null, "exact_frames_per_prompt": [40], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/1yR4YXE2gCQ.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 44, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_126_0_0"}, {"texts": ["The man is pressing the can with his leg."], "durations": null, "exact_frames_per_prompt": [27], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/1yR4YXE2gCQ.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 44, "npz_gt_video_start_frame": 44, "npz_gt_video_end_frame": 71, "skip_frames_after_generation": 44, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_126_0_1"}, {"texts": ["The man is inserting the can into the shredding machine."], "durations": null, "exact_frames_per_prompt": [9], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/1yR4YXE2gCQ.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 71, "npz_gt_video_start_frame": 71, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 71, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_126_0_2"}, {"texts": ["A man wearing a blue-white t-shirt and black trousers is teaching how to hit a golf shot to another man while the third man on the left side wearing a white t-shirt is hitting the golf ball."], "durations": null, "exact_frames_per_prompt": [51], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/0bvZn3gQZa4.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 55, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1270_0_0"}, {"texts": ["The man is handing over the golf club while another man wearing a red t-shirt take the golf club."], "durations": null, "exact_frames_per_prompt": [11], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/0bvZn3gQZa4.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 55, "npz_gt_video_start_frame": 55, "npz_gt_video_end_frame": 66, "skip_frames_after_generation": 55, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1270_0_1"}, {"texts": ["The man is sitting on the ground."], "durations": null, "exact_frames_per_prompt": [13], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/0bvZn3gQZa4.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 66, "npz_gt_video_start_frame": 66, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 66, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1270_0_2"}, {"texts": ["A man wearing a red t-shirt and black shorts is standing, watching the first man."], "durations": null, "exact_frames_per_prompt": [55], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/0bvZn3gQZa4.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 59, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1270_1_0"}, {"texts": ["The man is holding the golf club, and moving while the first man sits down and explains something"], "durations": null, "exact_frames_per_prompt": [21], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/0bvZn3gQZa4.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 59, "npz_gt_video_start_frame": 59, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 59, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1270_1_1"}, {"texts": ["A man wearing a white t-shirt and brown shorts is standing while a boy wearing a red t-shirt is standing in front and looking down then looks to the man, a man wearing a blue-white t-shirt is speaking to the boy while holding a golf stick and explaining then gives the stick to the boy."], "durations": null, "exact_frames_per_prompt": [54], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/0bvZn3gQZa4.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 58, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1270_2_0"}, {"texts": ["The man is hitting a golf shot in the back while the man wearing white-blue t-shirt comes around in the left to the boy and then sits."], "durations": null, "exact_frames_per_prompt": [22], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/0bvZn3gQZa4.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 58, "npz_gt_video_start_frame": 58, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 58, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1270_2_1"}, {"texts": ["A brown cow is standing, and her udders are touched by the girl and the man."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/0fdzNn6qT6o.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1271_0_0"}, {"texts": ["A man wearing a light gray t-shirt is holding the hand of the girl while the woman watches it."], "durations": null, "exact_frames_per_prompt": [30], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/0fdzNn6qT6o.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 34, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1271_2_0"}, {"texts": ["The man wearing a light gray t-shirt is touching the udders of the cow."], "durations": null, "exact_frames_per_prompt": [17], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/0fdzNn6qT6o.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 34, "npz_gt_video_start_frame": 34, "npz_gt_video_end_frame": 51, "skip_frames_after_generation": 34, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1271_2_1"}, {"texts": ["The man wearing a light gray t-shirt is moving his hand back."], "durations": null, "exact_frames_per_prompt": [12], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/0fdzNn6qT6o.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 51, "npz_gt_video_start_frame": 51, "npz_gt_video_end_frame": 63, "skip_frames_after_generation": 51, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1271_2_2"}, {"texts": ["A woman wearing yellow-white clothes is holding a newspaper."], "durations": null, "exact_frames_per_prompt": [31], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/68k8hqesAaY.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 35, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1272_0_0"}, {"texts": ["The woman wearing yellow-white clothes is starts speaking."], "durations": null, "exact_frames_per_prompt": [46], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/68k8hqesAaY.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 35, "npz_gt_video_start_frame": 35, "npz_gt_video_end_frame": 81, "skip_frames_after_generation": 35, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1272_0_1"}, {"texts": ["A person wearing black shirt is walking to the left side while a man wearing a blue shirt is walking ahead of the person wearing a black shirt and then standing, another man wearing a blue suit is standing on the left side, another man wearing a brown coat is standing on the right side, and a group of people are sitting."], "durations": null, "exact_frames_per_prompt": [19], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-mDkex29ylc.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 30, "npz_gt_video_start_frame": 30, "npz_gt_video_end_frame": 49, "skip_frames_after_generation": 30, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1273_1_0"}, {"texts": ["The person is handing over a frame."], "durations": null, "exact_frames_per_prompt": [31], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-mDkex29ylc.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 49, "npz_gt_video_start_frame": 49, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 49, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1273_1_1"}, {"texts": ["A person wearing a dark blue coat is taking a frame from another person while two people are standing on his left side and looking at him, and a group of people are looking at him from the front."], "durations": null, "exact_frames_per_prompt": [35], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-mDkex29ylc.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 45, "npz_gt_video_start_frame": 45, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 45, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1273_2_0"}, {"texts": ["A person wearing a blue shirt is walking to the left side while a man wearing grey and blue suit is standing on the left side and moving his hands and another man wearing an army uniform is first moving to the left side then stops in the middle and third man wearing black outfit, holding a frame in his hand is moving to the left side and handover the frame to the first man and group of people are sitting on the front side and looking at the men."], "durations": null, "exact_frames_per_prompt": [69], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-mDkex29ylc.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 11, "npz_gt_video_start_frame": 11, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 11, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1273_3_0"}, {"texts": ["A person wearing dark green coat is walking to the left side a man wearing blue shirt is walking to the left side, and a man wearing black shirt is coming from the right and giving a photo frame to a man and then moving towards the right and group of people are sitting at the front and a man is standing on the left side."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-mDkex29ylc.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1273_4_0"}, {"texts": ["A man wearing a blue t-shirt is standing and is holding a snake and he hands over the snake to the boy."], "durations": null, "exact_frames_per_prompt": [29], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/4E75lDljah8.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 33, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1277_0_0"}, {"texts": ["The man takes back the snake in his hands then the boy is touching the snake."], "durations": null, "exact_frames_per_prompt": [46], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/4E75lDljah8.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 33, "npz_gt_video_start_frame": 33, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 33, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1277_0_1"}, {"texts": ["A boy wearing a dark gray t-shirt is also standing. He takes the snake from person one."], "durations": null, "exact_frames_per_prompt": [27], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/4E75lDljah8.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 31, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1277_1_0"}, {"texts": ["The boy gives back the snake to person one."], "durations": null, "exact_frames_per_prompt": [28], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/4E75lDljah8.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 31, "npz_gt_video_start_frame": 31, "npz_gt_video_end_frame": 59, "skip_frames_after_generation": 31, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1277_1_1"}, {"texts": ["A brown and black snake is being held by person one and person two."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/4E75lDljah8.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1277_2_0"}, {"texts": ["A man wearing a blue t-shirt, light brown pants, and white shoes is standing on the right side, holding a snake in his hands, giving the snake to the boy."], "durations": null, "exact_frames_per_prompt": [29], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/4E75lDljah8.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 33, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1278_0_0"}, {"texts": ["The man again holding the snake and the boy is touching it."], "durations": null, "exact_frames_per_prompt": [47], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/4E75lDljah8.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 33, "npz_gt_video_start_frame": 33, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 33, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1278_0_1"}, {"texts": ["A boy wearing a gray t-shirt, blue jeans, and brown shoes is standing on the left side and holding a snake while a man in a blue t-shirt is standing, passing the snake to the boy, and later taking it back."], "durations": null, "exact_frames_per_prompt": [51], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/4E75lDljah8.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 55, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1278_1_0"}, {"texts": ["The boy touches the snake."], "durations": null, "exact_frames_per_prompt": [25], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/4E75lDljah8.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 55, "npz_gt_video_start_frame": 55, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 55, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1278_1_1"}, {"texts": ["A brown snake is being held into the boy's and the man's hands."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/4E75lDljah8.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1278_2_0"}, {"texts": ["A man wearing a blue t-shirt is at first cutting the watermelon in a cube with a knife."], "durations": null, "exact_frames_per_prompt": [20], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/AIe-33zv2_0.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 24, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_127_0_0"}, {"texts": ["The man wearing a blue t-shirt picks up the poly bag."], "durations": null, "exact_frames_per_prompt": [44], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/AIe-33zv2_0.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 24, "npz_gt_video_start_frame": 24, "npz_gt_video_end_frame": 68, "skip_frames_after_generation": 24, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_127_0_1"}, {"texts": ["The man wearing a blue t-shirt put a cube of a watermelon in the polybag."], "durations": null, "exact_frames_per_prompt": [13], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/AIe-33zv2_0.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 68, "npz_gt_video_start_frame": 68, "npz_gt_video_end_frame": 81, "skip_frames_after_generation": 68, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_127_0_2"}, {"texts": ["A boy wearing a black tracksuit, and red cap, and red-white shoes is standing while holding a golf stick and looking on the right side."], "durations": null, "exact_frames_per_prompt": [27], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/0Ff6-dYO3SM.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 31, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1280_0_0"}, {"texts": ["The boy playing golf while hitting the golf ball with the golf stick."], "durations": null, "exact_frames_per_prompt": [26], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/0Ff6-dYO3SM.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 31, "npz_gt_video_start_frame": 31, "npz_gt_video_end_frame": 57, "skip_frames_after_generation": 31, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1280_0_1"}, {"texts": ["The boy starts walking towards the right."], "durations": null, "exact_frames_per_prompt": [12], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/0Ff6-dYO3SM.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 57, "npz_gt_video_start_frame": 57, "npz_gt_video_end_frame": 69, "skip_frames_after_generation": 57, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1280_0_2"}, {"texts": ["A boy wearing a purple t-shirt is running on the floor."], "durations": null, "exact_frames_per_prompt": [26], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/2Pu2qAn_0_0.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 30, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1281_0_0"}, {"texts": ["The boy is moving his leg, and bending in the washing machine."], "durations": null, "exact_frames_per_prompt": [51], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/2Pu2qAn_0_0.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 30, "npz_gt_video_start_frame": 30, "npz_gt_video_end_frame": 81, "skip_frames_after_generation": 30, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1281_0_1"}, {"texts": ["A person wearing a blue jeans is standing and teaching to check the puncture of a tube."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/3HmJL8AnGMs.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1282_0_0"}, {"texts": ["A woman wearing a lavender purple t-shirt and a colorful striped apron is standing and speaking while moving her hands."], "durations": null, "exact_frames_per_prompt": [19], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/0voxHKytcZs.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 23, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1284_0_0"}, {"texts": ["The woman is holding a spatula in her right hand, and holding a mixing bowl with her left hand, and starts mixing the vegetables with the spatula."], "durations": null, "exact_frames_per_prompt": [56], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/0voxHKytcZs.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 23, "npz_gt_video_start_frame": 23, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 23, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1284_0_1"}, {"texts": ["A person whose hand is visible, holding a white pen and writing on a white paper."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/3KXsfxdUAJ4.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1285_0_0"}, {"texts": ["A person wearing a white honey bee suit is cleaning an artificial wooden bee hive with a brush."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/6iVQYogSg4s.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1286_0_0"}, {"texts": ["A woman wearing a blue dress is lying on the bed and looking at the person while the other woman stands and lay her hands on the first woman and looking at the person"], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/Jo0hXIsDvlU.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1288_0_0"}, {"texts": ["A person wearing a blue dress is standing on the left side of the white bed sheets while another person wearing blue dress is laying on the bed"], "durations": null, "exact_frames_per_prompt": [21], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/Jo0hXIsDvlU.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 9, "npz_gt_video_start_frame": 9, "npz_gt_video_end_frame": 30, "skip_frames_after_generation": 9, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1288_2_0"}, {"texts": ["The person is picking up a folded white bedsheet."], "durations": null, "exact_frames_per_prompt": [13], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/Jo0hXIsDvlU.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 30, "npz_gt_video_start_frame": 30, "npz_gt_video_end_frame": 43, "skip_frames_after_generation": 30, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1288_2_1"}, {"texts": ["The person is walking towards the bed."], "durations": null, "exact_frames_per_prompt": [26], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/Jo0hXIsDvlU.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 43, "npz_gt_video_start_frame": 43, "npz_gt_video_end_frame": 69, "skip_frames_after_generation": 43, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1288_2_2"}, {"texts": ["The person is starting to unfold the bedsheet."], "durations": null, "exact_frames_per_prompt": [11], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/Jo0hXIsDvlU.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 69, "npz_gt_video_start_frame": 69, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 69, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1288_2_3"}, {"texts": ["A man wearing a blue dress is picking up a white cloth while another man sleeping on the bed"], "durations": null, "exact_frames_per_prompt": [48], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/Jo0hXIsDvlU.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 9, "npz_gt_video_start_frame": 9, "npz_gt_video_end_frame": 57, "skip_frames_after_generation": 9, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1289_0_0"}, {"texts": ["The man is spreading it."], "durations": null, "exact_frames_per_prompt": [23], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/Jo0hXIsDvlU.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 57, "npz_gt_video_start_frame": 57, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 57, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1289_0_1"}, {"texts": ["A woman wearing a blue dress is lying on the right side of a bed while another woman in blue clothes is standing next to her and another person in blue clothes is picking up a white sheet, walking, and opening it."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/Jo0hXIsDvlU.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1289_1_0"}, {"texts": ["A man wearing a white t-shirt is standing on the right side and cutting a potato with a knife while another man wearing a purple t-shirt is standing beside the man and is cutting a potato with a knife."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/4H5q9XIPsOM.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_128_0_0"}, {"texts": ["A man wearing a blue t-shirt is standing on the left side and cutting potatoes with a knife along with the white tshirt person."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/4H5q9XIPsOM.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_128_1_0"}, {"texts": ["A person in blue clothes is taking out a white bedsheet."], "durations": null, "exact_frames_per_prompt": [33], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/Jo0hXIsDvlU.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 9, "npz_gt_video_start_frame": 9, "npz_gt_video_end_frame": 42, "skip_frames_after_generation": 9, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1290_0_0"}, {"texts": ["The person in blue clothes starts unfolding it, while moving towards the bed."], "durations": null, "exact_frames_per_prompt": [38], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/Jo0hXIsDvlU.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 42, "npz_gt_video_start_frame": 42, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 42, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1290_0_1"}, {"texts": ["A man wearing a white chef coat is standing and is preparing the red meat."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/1rjBNrQvRcI.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1292_0_0"}, {"texts": ["A man wearing a white chef's dress is standing on the gray floor and pouring the brown food powder on the meat pieces with his right hand."], "durations": null, "exact_frames_per_prompt": [27], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/1rjBNrQvRcI.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 31, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1293_0_0"}, {"texts": ["The man is flipping the meat pieces with his hands."], "durations": null, "exact_frames_per_prompt": [49], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/1rjBNrQvRcI.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 31, "npz_gt_video_start_frame": 31, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 31, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1293_0_1"}, {"texts": ["A man wearing a black t-shirt and white shorts is standing in a kitchen, holding a pan while the girl wearing white top is sitting on the chair then stands and watch the man"], "durations": null, "exact_frames_per_prompt": [31], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/2e_XbnvZ6qY.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 35, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1295_0_0"}, {"texts": ["The man wearing a black t-shirt and white shorts puts the pan on the stove."], "durations": null, "exact_frames_per_prompt": [11], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/2e_XbnvZ6qY.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 35, "npz_gt_video_start_frame": 35, "npz_gt_video_end_frame": 46, "skip_frames_after_generation": 35, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1295_0_1"}, {"texts": ["A girl wearing a white top and purple shorts is sitting on the right while a man wearing a black t-shirt is standing and holding a frying pan and moves a pan toward a girl."], "durations": null, "exact_frames_per_prompt": [11], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/2e_XbnvZ6qY.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 15, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1295_1_0"}, {"texts": ["The girl stands while a man puts the pan on the gas stove and then he bends toward the floor."], "durations": null, "exact_frames_per_prompt": [31], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/2e_XbnvZ6qY.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 15, "npz_gt_video_start_frame": 15, "npz_gt_video_end_frame": 46, "skip_frames_after_generation": 15, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1295_1_1"}, {"texts": ["A man wearing a silver watch, whose hand is visible only, is putting some notes in the cash counting machine."], "durations": null, "exact_frames_per_prompt": [25], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/2xIhtl7rU9E.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 29, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1296_0_0"}, {"texts": ["The man taking one note and putting it aside."], "durations": null, "exact_frames_per_prompt": [51], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/2xIhtl7rU9E.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 29, "npz_gt_video_start_frame": 29, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 29, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1296_0_1"}, {"texts": ["A man wearing a dark gray t-shirt and spectacles is standing on the right side of the woman and looking at the cooking food."], "durations": null, "exact_frames_per_prompt": [50], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/3tR8XQp4hDc.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 54, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1297_1_0"}, {"texts": ["The man wearing a dark gray t-shirt starts looking in front while the woman is standing and speaking."], "durations": null, "exact_frames_per_prompt": [26], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/3tR8XQp4hDc.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 54, "npz_gt_video_start_frame": 54, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 54, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1297_1_1"}, {"texts": ["A man wearing a gray sweater is standing next to the woman and watching food."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/3tR8XQp4hDc.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1298_0_0"}, {"texts": ["A woman wearing a black shirt is standing, holding a wooden spatula, doing hand gestures, and pouring vegetables into the pan while the other person wearing a grey top watches it."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/3tR8XQp4hDc.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1298_1_0"}, {"texts": ["A boy wearing a brown t-shirt and light brown pyjamas is standing on a green grass field."], "durations": null, "exact_frames_per_prompt": [13], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/12la0850LX8.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 17, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1299_0_0"}, {"texts": ["The boy is hitting a golf ball."], "durations": null, "exact_frames_per_prompt": [22], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/12la0850LX8.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 17, "npz_gt_video_start_frame": 17, "npz_gt_video_end_frame": 39, "skip_frames_after_generation": 17, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1299_0_1"}, {"texts": ["A boy wearing a grey t-shirt is sitting on a black chair."], "durations": null, "exact_frames_per_prompt": [38], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/19xlAHl3ZOs.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 42, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_12_0_0"}, {"texts": ["The boy is eating ice cream from a white-yellow bowl with a spoon."], "durations": null, "exact_frames_per_prompt": [38], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/19xlAHl3ZOs.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 42, "npz_gt_video_start_frame": 42, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 42, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_12_0_1"}, {"texts": ["A boy wearing a peach colour t-shirt is sitting on a chair and holding a newspaper in his hands and looking at the newspaper while other two persons doing some activity behind him."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-axI2H57avE.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1300_0_0"}, {"texts": ["A boy wearing a light-blue shirt is crawling on the backside while another boy on the left side wearing a brown t-shirt is sitting and reading newspaper and the third boy on the right side wearing a blue t-shirt is bends down."], "durations": null, "exact_frames_per_prompt": [68], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-axI2H57avE.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 72, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1300_1_0"}, {"texts": ["The boy puts his hand on his mouth."], "durations": null, "exact_frames_per_prompt": [8], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-axI2H57avE.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 72, "npz_gt_video_start_frame": 72, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 72, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1300_1_1"}, {"texts": ["A boy wearing a sky blue t-shirt is standing behind the first boy while another boy with with light colored shirt is kneeling down"], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-axI2H57avE.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1300_2_0"}, {"texts": ["A woman wearing a green sari is standing on the right side, holding a trophy and the three men walks from the left side"], "durations": null, "exact_frames_per_prompt": [46], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/11_jBe6j-lQ.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 50, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1303_0_0"}, {"texts": ["The woman is giving the trophy to the other lady and the other lady is giving the trophy to those three men"], "durations": null, "exact_frames_per_prompt": [30], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/11_jBe6j-lQ.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 50, "npz_gt_video_start_frame": 50, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 50, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1303_0_1"}, {"texts": ["A woman wearing a white sari is studying and taking the trophy from the first woman while a group of people are standing on the stage and a group of men walks towards the woman."], "durations": null, "exact_frames_per_prompt": [55], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/11_jBe6j-lQ.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 59, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1303_1_0"}, {"texts": ["The woman is giving it to the group of men."], "durations": null, "exact_frames_per_prompt": [21], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/11_jBe6j-lQ.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 59, "npz_gt_video_start_frame": 59, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 59, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1303_1_1"}, {"texts": ["A woman wearing a green saree is standing on the right side, holding a trophy while a woman wearing a cream sari is standing on the left side, a group of people is standing and then clapping, another group of people is holding cameras, and three men are coming towards the right side."], "durations": null, "exact_frames_per_prompt": [49], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/11_jBe6j-lQ.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 53, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1304_0_0"}, {"texts": ["A woman is giving the trophy to the other woman while three men stand on the right side."], "durations": null, "exact_frames_per_prompt": [27], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/11_jBe6j-lQ.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 53, "npz_gt_video_start_frame": 53, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 53, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1304_0_1"}, {"texts": ["A woman wearing a white saree is standing group of people are standing on the stage."], "durations": null, "exact_frames_per_prompt": [36], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/11_jBe6j-lQ.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 40, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1304_1_0"}, {"texts": ["The woman is taking the trophy from the woman wearing green saree."], "durations": null, "exact_frames_per_prompt": [34], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/11_jBe6j-lQ.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 40, "npz_gt_video_start_frame": 40, "npz_gt_video_end_frame": 74, "skip_frames_after_generation": 40, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1304_1_1"}, {"texts": ["The woman is giving it to the group of men."], "durations": null, "exact_frames_per_prompt": [6], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/11_jBe6j-lQ.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 74, "npz_gt_video_start_frame": 74, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 74, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1304_1_2"}, {"texts": ["A man wearing a white shirt is standing, holding a mic, speaking and moving."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/2K0bqbK1PWU.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1308_0_0"}, {"texts": ["A man wearing a white shirt is moving in the front, holding a microphone in his hand and speaking on the microphone."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/2K0bqbK1PWU.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1309_0_0"}, {"texts": ["A brown cat is lying on the blue-white surface and getting caressed by the person."], "durations": null, "exact_frames_per_prompt": [54], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/2QeK3fdugD0.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 58, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1311_1_0"}, {"texts": ["A girl wearing a white t-shirt is sitting on a bed and counting the banknotes."], "durations": null, "exact_frames_per_prompt": [16], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/06FgcE0nW9s.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 20, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1315_0_0"}, {"texts": ["The girl is moving the camera toward the banknotes."], "durations": null, "exact_frames_per_prompt": [60], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/06FgcE0nW9s.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 20, "npz_gt_video_start_frame": 20, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 20, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1315_0_1"}, {"texts": ["A man wearing a gray suit is standing, holding an iPad, and talking.\n"], "durations": null, "exact_frames_per_prompt": [60], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-axhvgcHZ4Y.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 20, "npz_gt_video_start_frame": 20, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 20, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1316_0_0"}, {"texts": ["A man wearing a white t-shirt is standing on the floor, taking out a screw from the wheel of a green-black micro scooter."], "durations": null, "exact_frames_per_prompt": [11], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/2SQu5VqswX8.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 15, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1317_0_0"}, {"texts": ["The man shows the screw."], "durations": null, "exact_frames_per_prompt": [9], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/2SQu5VqswX8.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 15, "npz_gt_video_start_frame": 15, "npz_gt_video_end_frame": 25, "skip_frames_after_generation": 15, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1317_0_1"}, {"texts": ["The man pushes out a screw from the wheel."], "durations": null, "exact_frames_per_prompt": [41], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/2SQu5VqswX8.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 25, "npz_gt_video_start_frame": 25, "npz_gt_video_end_frame": 67, "skip_frames_after_generation": 25, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1317_0_2"}, {"texts": ["The man pulls the screw out."], "durations": null, "exact_frames_per_prompt": [12], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/2SQu5VqswX8.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 67, "npz_gt_video_start_frame": 67, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 67, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1317_0_3"}, {"texts": ["A man wearing a suit is sitting and is having a drink."], "durations": null, "exact_frames_per_prompt": [22], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/8HyQrQVfGQQ.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 26, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1319_0_0"}, {"texts": ["A child wearing a brown-green printed light brown cloth is sitting on a chair on the left side and is eating the brown muffin cake while licking it with his finger."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/1rLgHXNOoqI.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1320_0_0"}, {"texts": ["A man wearing white shirt is wrapping the gift."], "durations": null, "exact_frames_per_prompt": [31], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/0-NDFExmeBI.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 35, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1324_0_ms_0"}, {"texts": ["The man is showing the gift."], "durations": null, "exact_frames_per_prompt": [20], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/0-NDFExmeBI.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 35, "npz_gt_video_start_frame": 35, "npz_gt_video_end_frame": 55, "skip_frames_after_generation": 35, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1324_0_ms_1"}, {"texts": ["The man is standing at the left side of the woman and folding his hands while posing while a woman wearing red Santa cap is in the right side and  showing her open palms."], "durations": null, "exact_frames_per_prompt": [25], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/0-NDFExmeBI.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 55, "npz_gt_video_start_frame": 55, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 55, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1324_0_ms_2"}, {"texts": ["A woman is holding her hands and standing with a man and then she is wearing a red Christmas hat showing her hands while posing."], "durations": null, "exact_frames_per_prompt": [24], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/0-NDFExmeBI.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 56, "npz_gt_video_start_frame": 56, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 56, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1324_1_ms_0"}, {"texts": ["A person standing in a black t-shirt is cutting a pineapple into pieces on a wooden chopping board."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-zf9RikeKy4.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1326_0_0"}, {"texts": ["A baby wearing a red t-shirt is sitting on a baby chair and eating cake with his hands."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-7yNDPMfE4Q.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1327_0_0"}, {"texts": ["A man wearing a blue shirt standing on a brown soil surface is shearing the wool from sheep while a group of sheep are standing behind the wooden fence."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/0JH2PXAdwV4.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1328_0_0"}, {"texts": ["A sheep is laid down on the brown soil surface and a man shearing the wool from his body."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/0JH2PXAdwV4.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1328_2_0"}, {"texts": ["A man wearing a white t-shirt is sitting on the left side of the girl on an elephant saddle and speaking and a man and woman are sitting on another elephant."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-7yP-JTSHj0.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1329_0_0"}, {"texts": ["A girl wearing black dress is sitting on the right side of the man on an elephant saddle."], "durations": null, "exact_frames_per_prompt": [53], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-7yP-JTSHj0.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 27, "npz_gt_video_start_frame": 27, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 27, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1329_1_0"}, {"texts": ["A man wearing a blue shirt is sitting on an elephant and riding the elephant while another man wearing white outfit is standing in front of the first man and making video and a woman wearing white outfit is also sitting on an elephant behind the first man."], "durations": null, "exact_frames_per_prompt": [28], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-7yP-JTSHj0.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 32, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1329_2_0"}, {"texts": ["A girl wearing a white top is sitting on the elephant saddle behind the second man and riding on the elephant while a man wearing a white t-shirt is self-vlogging and a girl in a black dress is standing near him on the right side."], "durations": null, "exact_frames_per_prompt": [30], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-7yP-JTSHj0.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 34, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1329_3_0"}, {"texts": ["A man wearing a grey t-shirt and blue pants is sitting on the chair and shearing a white sheep with a sheep shearing machine while a black and white animal is moving from right to left."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/0R64iSmFYFQ.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_132_0_0"}, {"texts": ["A white sheep is standing near the man on the green grass surface while a man wearing a blue denim is sitting and shearing the sheep."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/0R64iSmFYFQ.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_132_1_0"}, {"texts": ["A kid wearing a blue t-shirt is sitting on an off white floor, keeping her finger on the number book."], "durations": null, "exact_frames_per_prompt": [56], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/1GKU1DHuD0k.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 60, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1334_0_0"}, {"texts": ["The kid is holding her leg."], "durations": null, "exact_frames_per_prompt": [19], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/1GKU1DHuD0k.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 60, "npz_gt_video_start_frame": 60, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 60, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1334_0_1"}, {"texts": ["A person whose hands are visible is putting their hands on the book and then flipping pages of it."], "durations": null, "exact_frames_per_prompt": [71], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-VdZYb-SC2o.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 9, "npz_gt_video_start_frame": 9, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 9, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1335_0_0"}, {"texts": ["A boy wearing a blue shirt is standing on the wooden floor while holding a blue wrapping paper."], "durations": null, "exact_frames_per_prompt": [18], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/1dJW9_6zy5w.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 22, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1336_0_0"}, {"texts": ["The boy at first holds the toy box which is in the hand of a child in black clothing while the girls in right side are looking at him."], "durations": null, "exact_frames_per_prompt": [48], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/1dJW9_6zy5w.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 22, "npz_gt_video_start_frame": 22, "npz_gt_video_end_frame": 70, "skip_frames_after_generation": 22, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1336_0_1"}, {"texts": ["The boy leaves the box."], "durations": null, "exact_frames_per_prompt": [10], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/1dJW9_6zy5w.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 70, "npz_gt_video_start_frame": 70, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 70, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1336_0_2"}, {"texts": ["A person wearing a dark green jacket and blue pants is standing on the right side of the barbeque, holding a meat fork and grilling the meats on the barbeque."], "durations": null, "exact_frames_per_prompt": [66], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/0LXCkmzhT_g.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 14, "npz_gt_video_start_frame": 14, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 14, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1337_0_0"}, {"texts": ["A man wearing denim jeans is standing and cooking with a grilling fork.\n"], "durations": null, "exact_frames_per_prompt": [73], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/0LXCkmzhT_g.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 7, "npz_gt_video_start_frame": 7, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 7, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1338_0_0"}, {"texts": ["A fish is caught and dragged out of the water while the man with black jacket dragging the thread"], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-Weo4lxnFY8.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1339_0_0"}, {"texts": ["A man wearing a black jacket is standing on the right side and pulling the fish hook while a group of people are standing on the ice surface."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-Weo4lxnFY8.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1339_1_0"}, {"texts": ["A woman wearing maroon cloth is standing and holding a boy while a group of people including kids are standing and sitting."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/0r1tBheuaXU.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_133_0_0"}, {"texts": ["A man wearing black cloth is standing and bent while a group of goats are sitting on the surface and group of people are standing on the surface and another man wearing maroon and black outfit is sitting on the right side."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/0r1tBheuaXU.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_133_3_0"}, {"texts": ["A woman wearing a multicolored sweater is standing while a man in a white t-shirt is standing on the right side and recording."], "durations": null, "exact_frames_per_prompt": [50], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/0wphRCtKfMs.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 54, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1340_1_0"}, {"texts": ["The woman is picking a gingerbread from the counter-top."], "durations": null, "exact_frames_per_prompt": [26], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/0wphRCtKfMs.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 54, "npz_gt_video_start_frame": 54, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 54, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1340_1_1"}, {"texts": ["A woman wearing a black dress is standing in the front and speaking."], "durations": null, "exact_frames_per_prompt": [59], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/0HAX8-Z9JmQ.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 63, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1342_0_0"}, {"texts": ["The woman shows the weather forecast."], "durations": null, "exact_frames_per_prompt": [17], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/0HAX8-Z9JmQ.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 63, "npz_gt_video_start_frame": 63, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 63, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1342_0_1"}, {"texts": ["A baby boy with white-golden hairs is sitting on a red chair and eating noodles with his hands from a white plate."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/0UTA7zg8huM.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1345_0_0"}, {"texts": ["A man wearing a white shirt is sitting, holding a glass of wine, keeps his hands on the table, and lifts the glass up and puts it on the white table while a man wearing a blue t-shirt sitting in left side holding a wine glass"], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/2HxkXRg7lCQ.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1346_0_0"}, {"texts": ["A man wearing a blue t-shirt is sitting, keeping his leg on the chair, hand on the table, and holding a glass of wine while a man wearing a white shirt is sitting on the right side while holding a glass of wine in his hand."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/2HxkXRg7lCQ.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1346_1_0"}, {"texts": ["A boy wearing white t-shirt is bending on the left side and eating watermelon and two boys are also bending and eating the watermelon."], "durations": null, "exact_frames_per_prompt": [41], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/1KxRr-R4QbI.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 45, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1347_0_0"}, {"texts": ["A boy in the middle wearing a red t-shirt is bending and eating watermelon while the other two boys does the same."], "durations": null, "exact_frames_per_prompt": [39], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/1KxRr-R4QbI.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 43, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1347_1_0"}, {"texts": ["The boy is then standing on the soil surface."], "durations": null, "exact_frames_per_prompt": [2], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/1KxRr-R4QbI.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 43, "npz_gt_video_start_frame": 43, "npz_gt_video_end_frame": 45, "skip_frames_after_generation": 43, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1347_1_1"}, {"texts": ["A boy wearing white vest is bending and eating watermelon along with other boys and the middle boy stand straight from his position."], "durations": null, "exact_frames_per_prompt": [41], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/1KxRr-R4QbI.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 45, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1347_2_0"}, {"texts": ["A woman wearing black clothes and a black helmet is walking with a horse towards the forest while another horse is walking behind her on the soil pathway."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/1B9dWbCIYY0.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1349_0_0"}, {"texts": ["A brown horse tied to a leash is walking after the first woman while another horse whose head is visible is walking behind the horse."], "durations": null, "exact_frames_per_prompt": [48], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/1B9dWbCIYY0.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 32, "npz_gt_video_start_frame": 32, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 32, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1349_1_0"}, {"texts": ["A person whose hands are visible is putting tissue paper on the hole."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/0uYz-1Xj5YQ.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1350_0_0"}, {"texts": ["A man wearing a light grey shirt is standing and tying a tie."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/071a92n2wNQ.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1352_0_0"}, {"texts": ["A boy wearing a black t-shirt and black shorts is walking to the stage."], "durations": null, "exact_frames_per_prompt": [54], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/1sR4p0bsQpk.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 58, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1353_0_0"}, {"texts": ["The boy is receiving something from a woman while group of people claps at him"], "durations": null, "exact_frames_per_prompt": [16], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/1sR4p0bsQpk.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 58, "npz_gt_video_start_frame": 58, "npz_gt_video_end_frame": 74, "skip_frames_after_generation": 58, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1353_0_1"}, {"texts": ["The boy is walking away."], "durations": null, "exact_frames_per_prompt": [6], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/1sR4p0bsQpk.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 74, "npz_gt_video_start_frame": 74, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 74, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1353_0_2"}, {"texts": ["A woman wearing a black top and black trousers is standing on the stage while a group of people are standing on the stage one of them wearing black cloth starts to clap, a group of people are sitting in the audiences sitting area and starts to clap while few kids are walking, a boy wearing black clothes climbs on the stage while holding a paper."], "durations": null, "exact_frames_per_prompt": [51], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/1sR4p0bsQpk.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 55, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1353_1_0"}, {"texts": ["The woman wearing a black top and black trousers is giving something to the boy."], "durations": null, "exact_frames_per_prompt": [10], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/1sR4p0bsQpk.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 55, "npz_gt_video_start_frame": 55, "npz_gt_video_end_frame": 65, "skip_frames_after_generation": 55, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1353_1_1"}, {"texts": ["The woman wearing a black top and black trousers is walking away while a boy wearing black clothes is getting down from the stage."], "durations": null, "exact_frames_per_prompt": [15], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/1sR4p0bsQpk.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 65, "npz_gt_video_start_frame": 65, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 65, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1353_1_2"}, {"texts": ["A man wearing a black t-shirt is sitting on the left."], "durations": null, "exact_frames_per_prompt": [24], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/1sR4p0bsQpk.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 28, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1353_6_0"}, {"texts": ["The man is wiping his face with his arm."], "durations": null, "exact_frames_per_prompt": [9], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/1sR4p0bsQpk.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 28, "npz_gt_video_start_frame": 28, "npz_gt_video_end_frame": 37, "skip_frames_after_generation": 28, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1353_6_1"}, {"texts": ["The man starts clapping."], "durations": null, "exact_frames_per_prompt": [43], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/1sR4p0bsQpk.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 37, "npz_gt_video_start_frame": 37, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 37, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1353_6_2"}, {"texts": ["A boy wearing a light blue t-shirt is sitting and eating with a spoon."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/03fd3rnupoE.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1354_0_0"}, {"texts": ["A man wearing a red t-shirt and brown pants is taking out the front wheel of the black bike."], "durations": null, "exact_frames_per_prompt": [31], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/0V066qnmi1A.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 50, "npz_gt_video_start_frame": 50, "npz_gt_video_end_frame": 81, "skip_frames_after_generation": 50, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1355_0_0"}, {"texts": ["A man wearing gray trousers is sitting on the chair, fixing a bike brake mechanism."], "durations": null, "exact_frames_per_prompt": [60], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/0V066qnmi1A.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 64, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1356_0_0"}, {"texts": ["The man is lifting the bike tire."], "durations": null, "exact_frames_per_prompt": [17], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/0V066qnmi1A.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 64, "npz_gt_video_start_frame": 64, "npz_gt_video_end_frame": 81, "skip_frames_after_generation": 64, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1356_0_1"}, {"texts": ["A man whose hand is visible only, is holding a spatula and sauteing the sausages in a pan."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/3yO4vPaddgo.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_135_0_0"}, {"texts": ["A person with blue trousers is standing and cleaning the horse's hoof with a metal piece."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/09_WmlNUCuA.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1360_0_0"}, {"texts": ["A horse is standing and getting its hoof cleaned by the person."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/09_WmlNUCuA.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1360_1_0"}, {"texts": ["A woman wearing a red and green jacket is standing while holding a black kettle."], "durations": null, "exact_frames_per_prompt": [11], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/1Vqf0N2wDmk.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 15, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1361_0_0"}, {"texts": ["The woman wearing a red and green jacket is smiling while a woman wearing a red sweatshirt turns to the left, keeps her hand on her mouth, and starts laughing."], "durations": null, "exact_frames_per_prompt": [14], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/1Vqf0N2wDmk.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 15, "npz_gt_video_start_frame": 15, "npz_gt_video_end_frame": 29, "skip_frames_after_generation": 15, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1361_0_1"}, {"texts": ["The woman wearing a red and green jacket pours the hot water in the cup."], "durations": null, "exact_frames_per_prompt": [51], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/1Vqf0N2wDmk.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 29, "npz_gt_video_start_frame": 29, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 29, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1361_0_2"}, {"texts": ["A woman in a red sweater is standing in the right side and smiling while looking at the woman in red and green jacket and the woman in the red and green jacket is holding an electric jug and then pouring water into a cup."], "durations": null, "exact_frames_per_prompt": [67], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/1Vqf0N2wDmk.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 13, "npz_gt_video_start_frame": 13, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 13, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1361_1_0"}, {"texts": ["A man wearing a red shirt and black pants is standing, holding a golf stick and moving his hand."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/1XfGd7ejcew.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1362_0_0"}, {"texts": ["A woman wearing a designer top is sitting on the floor and unwrapping a gift.  while a kid wearing striped clothes is also sitting on the floor and looking at the gift."], "durations": null, "exact_frames_per_prompt": [29], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-Q83wlepp48.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 33, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1364_0_0"}, {"texts": ["The woman picks the gift in her hand while the kid is touching the gift."], "durations": null, "exact_frames_per_prompt": [27], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-Q83wlepp48.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 33, "npz_gt_video_start_frame": 33, "npz_gt_video_end_frame": 60, "skip_frames_after_generation": 33, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1364_0_1"}, {"texts": ["The woman puts it down."], "durations": null, "exact_frames_per_prompt": [13], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-Q83wlepp48.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 60, "npz_gt_video_start_frame": 60, "npz_gt_video_end_frame": 73, "skip_frames_after_generation": 60, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1364_0_2"}, {"texts": ["The woman starts pressing the gift wrapping paper while the kid is looking here and there."], "durations": null, "exact_frames_per_prompt": [7], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-Q83wlepp48.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 73, "npz_gt_video_start_frame": 73, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 73, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1364_0_3"}, {"texts": ["A kid wearing kids-wear is sitting on the floor watching the gifts while a woman wearing a black printed top is helping with opening the gift and picking up the gift."], "durations": null, "exact_frames_per_prompt": [57], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-Q83wlepp48.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 61, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1364_1_0"}, {"texts": ["The kid then starts touching the gift which is put there by the woman."], "durations": null, "exact_frames_per_prompt": [19], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-Q83wlepp48.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 61, "npz_gt_video_start_frame": 61, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 61, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1364_1_1"}, {"texts": ["A person whose hand is visible is holding a coin. "], "durations": null, "exact_frames_per_prompt": [53], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/2nwSCOdvy8E.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 57, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1366_0_0"}, {"texts": ["The person is inserting the coin in the Tyre tread."], "durations": null, "exact_frames_per_prompt": [23], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/2nwSCOdvy8E.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 57, "npz_gt_video_start_frame": 57, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 57, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1366_0_1"}, {"texts": ["A person wearing a white black t-shirt is standing and tearing a white paper into pieces."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/1cCusXOOuLg.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1367_0_0"}, {"texts": ["A man wearing a black suit is moving on the left and moving his hand while speaking."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/0oNQEL5lBns.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1368_0_0"}, {"texts": ["A man wearing a black coat is speaking while facing front."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-3x5jizdxd8.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1369_0_0"}, {"texts": ["A man wearing a printed white shirt is tying a maroon tie on the collar with his hands."], "durations": null, "exact_frames_per_prompt": [46], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-3x5jizdxd8.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 50, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1369_1_0"}, {"texts": ["A person wearing a dress is standing and cutting a pineapple with a knife on the wooden cardboard."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/0ubF6XT9jXU.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_136_0_0"}, {"texts": ["A man wearing a grey t-shirt is sitting on a brown sofa with a box on his lap and showing a t-shirt."], "durations": null, "exact_frames_per_prompt": [34], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/1ZOBR74E7Uo.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 38, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1371_0_0"}, {"texts": ["The man puts it on his right side."], "durations": null, "exact_frames_per_prompt": [17], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/1ZOBR74E7Uo.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 38, "npz_gt_video_start_frame": 38, "npz_gt_video_end_frame": 55, "skip_frames_after_generation": 38, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1371_0_1"}, {"texts": ["The man moves his goggles up."], "durations": null, "exact_frames_per_prompt": [25], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/1ZOBR74E7Uo.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 55, "npz_gt_video_start_frame": 55, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 55, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1371_0_2"}, {"texts": ["A man wearing black clothes is sitting and eating a burger."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/2B4NQC26en8.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1373_0_0"}, {"texts": ["A man wearing a black t-shirt is sitting and eating a burger with his hands."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/2B4NQC26en8.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1374_0_0"}, {"texts": ["A woman wearing a grey hoodie is standing and plucking up apples from the tree and then putting them into the carry bag."], "durations": null, "exact_frames_per_prompt": [44], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/1jIMg9gK1s0.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 48, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1375_0_0"}, {"texts": ["The woman tilting her body towards the big apple storage container."], "durations": null, "exact_frames_per_prompt": [32], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/1jIMg9gK1s0.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 48, "npz_gt_video_start_frame": 48, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 48, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1375_0_1"}, {"texts": ["A girl wearing a green top is eating something."], "durations": null, "exact_frames_per_prompt": [26], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/0jpmhkj8RrY.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 30, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1377_0_0"}, {"texts": ["The girl wearing a green top is tying her hair."], "durations": null, "exact_frames_per_prompt": [16], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/0jpmhkj8RrY.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 30, "npz_gt_video_start_frame": 30, "npz_gt_video_end_frame": 48, "skip_frames_after_generation": 30, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1377_0_1"}, {"texts": ["The girl wearing a green top is picking up a white packet."], "durations": null, "exact_frames_per_prompt": [32], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/0jpmhkj8RrY.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 48, "npz_gt_video_start_frame": 48, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 48, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1377_0_2"}, {"texts": ["A man wearing a light gray dress is standing, holding a knife in his left hand, and cutting a pineapple on a wooden board."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/0ubF6XT9jXU.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_137_0_0"}, {"texts": ["A white-brown dog is sitting on a black surface, moving, petted, and combed by a women."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/1NkiX_rp2J4.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1381_1_0"}, {"texts": ["A woman wearing a blue dress is caressing a dog."], "durations": null, "exact_frames_per_prompt": [17], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/1NkiX_rp2J4.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 21, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1382_0_0"}, {"texts": ["The woman takes a pet comb from a brown table."], "durations": null, "exact_frames_per_prompt": [12], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/1NkiX_rp2J4.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 21, "npz_gt_video_start_frame": 21, "npz_gt_video_end_frame": 33, "skip_frames_after_generation": 21, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1382_0_1"}, {"texts": ["The woman starts showing the comb while the dog turn his head towards right."], "durations": null, "exact_frames_per_prompt": [17], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/1NkiX_rp2J4.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 33, "npz_gt_video_start_frame": 33, "npz_gt_video_end_frame": 50, "skip_frames_after_generation": 33, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1382_0_2"}, {"texts": ["The woman holds the dog."], "durations": null, "exact_frames_per_prompt": [27], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/1NkiX_rp2J4.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 50, "npz_gt_video_start_frame": 50, "npz_gt_video_end_frame": 77, "skip_frames_after_generation": 50, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1382_0_3"}, {"texts": ["The woman is moving the comb towards the dog."], "durations": null, "exact_frames_per_prompt": [3], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/1NkiX_rp2J4.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 77, "npz_gt_video_start_frame": 77, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 77, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1382_0_4"}, {"texts": ["A brown-white shih tzu dog is sitting on a brown table, getting caressed by the woman."], "durations": null, "exact_frames_per_prompt": [61], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/1NkiX_rp2J4.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 65, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1382_1_0"}, {"texts": ["The brown-white shih tzu dog is getting held by the woman."], "durations": null, "exact_frames_per_prompt": [15], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/1NkiX_rp2J4.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 65, "npz_gt_video_start_frame": 65, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 65, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1382_1_1"}, {"texts": ["A machine is moving and opening a beer bottle with a bottle opener attached to the machine."], "durations": null, "exact_frames_per_prompt": [32], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/reWziqKu-Tk.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 36, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1383_0_0"}, {"texts": ["A woman sitting on the left side is holding some food, eating it while another woman sitting on the right side is also holding some food and eating it."], "durations": null, "exact_frames_per_prompt": [31], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/2buPVKtWU5M.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 35, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1385_0_0"}, {"texts": ["The woman is licking her finger."], "durations": null, "exact_frames_per_prompt": [46], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/2buPVKtWU5M.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 35, "npz_gt_video_start_frame": 35, "npz_gt_video_end_frame": 81, "skip_frames_after_generation": 35, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1385_0_1"}, {"texts": ["A woman is sitting on the right side while another woman is also sitting on the left side."], "durations": null, "exact_frames_per_prompt": [30], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/2buPVKtWU5M.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 34, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1385_1_0"}, {"texts": ["The woman is also holding some food and eating it along with the woman on the left side."], "durations": null, "exact_frames_per_prompt": [47], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/2buPVKtWU5M.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 34, "npz_gt_video_start_frame": 34, "npz_gt_video_end_frame": 81, "skip_frames_after_generation": 34, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1385_1_1"}, {"texts": ["A woman wearing blue cloth is standing and holding a knife and cutting sushi roll while a girl wearing a pink top is standing beside the"], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-DZeoj_11pU.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1389_0_0"}, {"texts": ["A girl wearing pink clothes is standing while a man is standing on the right and he is cutting the sushi rolls."], "durations": null, "exact_frames_per_prompt": [43], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-DZeoj_11pU.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 47, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1389_1_0"}, {"texts": ["The girl is arranging the sushi rolls."], "durations": null, "exact_frames_per_prompt": [33], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-DZeoj_11pU.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 47, "npz_gt_video_start_frame": 47, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 47, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1389_1_1"}, {"texts": ["A baby whose only upper body is visible, wearing a pink-printed blue feeding bib on the neck, is sitting on the baby-hug booster chair and eating red food while picking red food from the table."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/0nbRLi_nHEQ.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_138_0_0"}, {"texts": ["A man whose hand is visible is holding the fish food in his hand."], "durations": null, "exact_frames_per_prompt": [27], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/08gPg6wvJU4.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 17, "npz_gt_video_start_frame": 17, "npz_gt_video_end_frame": 44, "skip_frames_after_generation": 17, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1390_2_0"}, {"texts": ["A woman wearing black clothes is sitting and holding a baby and a paper piece and playing with the baby."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/0_k6XiBEnBg.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1391_0_0"}, {"texts": ["A baby wearing white clothes is sitting in the lap of a woman and playing."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/0_k6XiBEnBg.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1391_1_0"}, {"texts": ["A person whose only a hand is visible is picking the chopped vegetables from a bowl and putting it in the pan."], "durations": null, "exact_frames_per_prompt": [75], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/0rNqlVsdlwE.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 5, "npz_gt_video_start_frame": 5, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 5, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1392_0_0"}, {"texts": ["A person whose hand is visible, picking vegetables from a plate and putting in a frying pan."], "durations": null, "exact_frames_per_prompt": [69], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/0rNqlVsdlwE.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 11, "npz_gt_video_start_frame": 11, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 11, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1393_0_0"}, {"texts": ["A person whose hands are visible is rapping a white thread on a green thread."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-zID4lhAHqQ.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1394_0_0"}, {"texts": ["A girl wearing a pink top is sitting in a car, holding a white paper while the boy beside showing a round shaped green colored paper with some black text and symbols printed on it"], "durations": null, "exact_frames_per_prompt": [47], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/2ib1idaGAc4.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 51, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1397_0_0"}, {"texts": ["The girl wearing a pink top is trying to tear the white paper while the boy turning his head"], "durations": null, "exact_frames_per_prompt": [29], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/2ib1idaGAc4.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 51, "npz_gt_video_start_frame": 51, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 51, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1397_0_1"}, {"texts": ["A boy wearing a black t-shirt is sitting in a car, holding a round yellow paper and showing it while a kid wearing a pink t-shirt is sitting on the seat of the car and a person is pointing its hand."], "durations": null, "exact_frames_per_prompt": [14], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/2ib1idaGAc4.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 18, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1397_1_0"}, {"texts": ["The boy wearing a black t-shirt keeps it aside while a girl wearing a pink t-shirt is breaking white paper and touching her head."], "durations": null, "exact_frames_per_prompt": [62], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/2ib1idaGAc4.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 18, "npz_gt_video_start_frame": 18, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 18, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1397_1_1"}, {"texts": ["A boy wearing a black t-shirt is sitting on the seat and showing a poster while a girl wearing pink clothes is sitting and holding a white paper in her hand and a person whose hand is visible is on the left side."], "durations": null, "exact_frames_per_prompt": [26], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/2ib1idaGAc4.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 30, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1398_0_0"}, {"texts": ["The boy is keeping it on his thighs while the girl is tearing the white paper and a white vehicle is moving from the front to back side."], "durations": null, "exact_frames_per_prompt": [50], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/2ib1idaGAc4.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 30, "npz_gt_video_start_frame": 30, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 30, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1398_0_1"}, {"texts": ["A girl wearing a pink t-shirt is sitting on the seat, holding a piece of paper and a boy in a black t-shirt is holding a inflatable cushion and showing it."], "durations": null, "exact_frames_per_prompt": [17], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/2ib1idaGAc4.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 21, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1398_1_0"}, {"texts": ["The girl is doing her hair back."], "durations": null, "exact_frames_per_prompt": [59], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/2ib1idaGAc4.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 21, "npz_gt_video_start_frame": 21, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 21, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1398_1_1"}, {"texts": ["A man whose upper body is visible, wearing a black shirt, is standing on the right side and wrapping the bandage around the other man's shoulder."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/1AeH62ERnQs.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1399_0_0"}, {"texts": ["A man wearing blue jeans is standing on the left side with his right hand up and getting a bandage wrapped on his shoulder from the first man."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/1AeH62ERnQs.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1399_1_0"}, {"texts": ["A person wearing white trousers is lifting the frame from the beehives box while the other man wearing blue jeans is holding something in his hand which he put down"], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-Cg72tgUTBI.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_139_0_0"}, {"texts": ["A man wearing blue trousers is standing on the right side and holding a bee hive smoker while another man wearing yellow gloves is doing something with the wooden box."], "durations": null, "exact_frames_per_prompt": [26], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-Cg72tgUTBI.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 30, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_139_1_0"}, {"texts": ["The man wearing blue trousers bends and put the bee hive smoker on the floor."], "durations": null, "exact_frames_per_prompt": [19], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-Cg72tgUTBI.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 30, "npz_gt_video_start_frame": 30, "npz_gt_video_end_frame": 49, "skip_frames_after_generation": 30, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_139_1_1"}, {"texts": ["The man wearing blue trousers then gets up while another man pulls out an object out of the wooden box."], "durations": null, "exact_frames_per_prompt": [31], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-Cg72tgUTBI.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 49, "npz_gt_video_start_frame": 49, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 49, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_139_1_2"}, {"texts": ["A girl on the left side wearing a black t-shirt and blue leggings is sitting on a sofa and taking out pulp from the avocado with a fork while a girl on the right side is holding a piece of avocado and speaking."], "durations": null, "exact_frames_per_prompt": [23], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/3nO2FQ-R5jQ.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 27, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_13_0_0"}, {"texts": ["The girl is putting the pulp into the box while the girl on the right picks up the fork and takes out the pulp from the avocado."], "durations": null, "exact_frames_per_prompt": [53], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/3nO2FQ-R5jQ.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 27, "npz_gt_video_start_frame": 27, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 27, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_13_0_1"}, {"texts": ["A girl on the right side wearing a black t-shirt and camouflage leggings is sitting on the sofa and holding an Avogadro while a girl on the left side wearing blue pants is removing the pulp of the avocado."], "durations": null, "exact_frames_per_prompt": [67], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/3nO2FQ-R5jQ.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 71, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_13_1_0"}, {"texts": ["The girl on the right side wearing a black t-shirt and camouflage leggings is taking out pulp from the Avogadro with a fork and putting it into the box."], "durations": null, "exact_frames_per_prompt": [9], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/3nO2FQ-R5jQ.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 71, "npz_gt_video_start_frame": 71, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 71, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_13_1_1"}, {"texts": ["A woman whose hands are visible is bandaging the tail of the animal."], "durations": null, "exact_frames_per_prompt": [40], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/0bLehW9HFsU.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 44, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1400_0_0"}, {"texts": ["A woman wearing a green top and black apron is preparing food stuffing in a big bowl."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-HLcbff_gMk.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1404_0_0"}, {"texts": ["A woman wearing a red top is talking loudly, crying, and moving her hands."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/0F6UInVYdMo.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1405_0_0"}, {"texts": ["A person wearing white and black clothes is standing and speaking and showing on a screen."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/0MCOnOpNEzo.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1408_0_0"}, {"texts": ["A man wearing a black suit is moving on the left side."], "durations": null, "exact_frames_per_prompt": [18], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/0MCOnOpNEzo.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 22, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1409_0_0"}, {"texts": ["The man wearing a black suit is putting his hand on the display."], "durations": null, "exact_frames_per_prompt": [58], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/0MCOnOpNEzo.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 22, "npz_gt_video_start_frame": 22, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 22, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1409_0_1"}, {"texts": ["A man wearing a maroon apron is holding a meat in one hand and crushing the corn flakes from the other hand."], "durations": null, "exact_frames_per_prompt": [21], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/063UXQ46xmA.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 25, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_140_0_0"}, {"texts": ["The man coats the meat in the crushed cornflakes."], "durations": null, "exact_frames_per_prompt": [55], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/063UXQ46xmA.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 25, "npz_gt_video_start_frame": 25, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 25, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_140_0_1"}, {"texts": ["A man wearing a gray-black jacket is sitting on his knee on a snowy surface and washing his hands into an ice fishing hole."], "durations": null, "exact_frames_per_prompt": [12], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/3QnfxMNaNuU.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 16, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1410_0_ms_0"}, {"texts": ["The man is speaking while moving his hands."], "durations": null, "exact_frames_per_prompt": [38], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/3QnfxMNaNuU.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 16, "npz_gt_video_start_frame": 16, "npz_gt_video_end_frame": 54, "skip_frames_after_generation": 16, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1410_0_ms_1"}, {"texts": ["The man is leaning forward."], "durations": null, "exact_frames_per_prompt": [11], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/3QnfxMNaNuU.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 54, "npz_gt_video_start_frame": 54, "npz_gt_video_end_frame": 65, "skip_frames_after_generation": 54, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1410_0_ms_2"}, {"texts": ["A man wearing a gray-black jacket is sitting on his knee on the snowy surface and washing his hands into an ice fishing hole."], "durations": null, "exact_frames_per_prompt": [11], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/3QnfxMNaNuU.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 15, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1411_0_0"}, {"texts": ["The man is then speaking while moving his hands."], "durations": null, "exact_frames_per_prompt": [43], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/3QnfxMNaNuU.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 15, "npz_gt_video_start_frame": 15, "npz_gt_video_end_frame": 58, "skip_frames_after_generation": 15, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1411_0_1"}, {"texts": ["The man is then leaning forward."], "durations": null, "exact_frames_per_prompt": [7], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/3QnfxMNaNuU.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 58, "npz_gt_video_start_frame": 58, "npz_gt_video_end_frame": 65, "skip_frames_after_generation": 58, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1411_0_2"}, {"texts": ["A woman whose only hand is visible is touching a black sheep."], "durations": null, "exact_frames_per_prompt": [71], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/07diMt4U1lw.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 9, "npz_gt_video_start_frame": 9, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 9, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1412_0_0"}, {"texts": ["A person whose only hand is visible is folding the envelope and cello taping on the white table."], "durations": null, "exact_frames_per_prompt": [40], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/1F5sUAkqy5s.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 44, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1413_0_0"}, {"texts": ["A child wearing a blue t-shirt is sitting on a white chair, holding a newspaper."], "durations": null, "exact_frames_per_prompt": [40], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/4oLcWABGhBY.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 44, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1414_0_0"}, {"texts": ["The child throws it."], "durations": null, "exact_frames_per_prompt": [12], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/4oLcWABGhBY.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 44, "npz_gt_video_start_frame": 44, "npz_gt_video_end_frame": 56, "skip_frames_after_generation": 44, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1414_0_1"}, {"texts": ["A man wearing a grey shiny suit is standing behind the podium and speaking on the mic group of people are standing and smiling."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/11gjjHfzCc0.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1416_0_0"}, {"texts": ["A man on the left side wearing a dark grey suit is holding a white thing while few other men are standing with him and the person wearing printed suit speaking something on the mic"], "durations": null, "exact_frames_per_prompt": [14], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/11gjjHfzCc0.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 18, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1416_1_0"}, {"texts": ["The man is taking it close to a man's nose who is standing next to him on the right side."], "durations": null, "exact_frames_per_prompt": [62], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/11gjjHfzCc0.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 18, "npz_gt_video_start_frame": 18, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 18, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1416_1_1"}, {"texts": ["A woman wearing black top and white leggings is standing and picking meat pieces and putting them into a meat grinding machine."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-nAzbltKpUE.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1423_0_0"}, {"texts": ["A boy wearing a red-blue t-shirt and blue capris is sitting on the floor rug and moving a toy car box."], "durations": null, "exact_frames_per_prompt": [36], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/1ehDWXzIGFU.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 40, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1424_0_0"}, {"texts": ["The boy wearing a red-blue t-shirt and blue capris stands up while holding the box and starts looking at the box."], "durations": null, "exact_frames_per_prompt": [40], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/1ehDWXzIGFU.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 40, "npz_gt_video_start_frame": 40, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 40, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1424_0_1"}, {"texts": ["A man wearing a white t-shirt is standing and moving on a green golf course, holding a golf club while a man in a blue t-shirt holds a red object and looks at it, another man in a black t-shirt walks to the right and poses for hitting the golf ball."], "durations": null, "exact_frames_per_prompt": [59], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-7a1j_Q6PUk.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 63, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1428_0_0"}, {"texts": ["The man wearing a white t-shirt is playing golf."], "durations": null, "exact_frames_per_prompt": [17], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-7a1j_Q6PUk.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 63, "npz_gt_video_start_frame": 63, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 63, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1428_0_1"}, {"texts": ["A man wearing black t-shirt is walking in the golf course.  a man wearing white t-shirt on the right side playing golf"], "durations": null, "exact_frames_per_prompt": [44], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-7a1j_Q6PUk.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 6, "npz_gt_video_start_frame": 6, "npz_gt_video_end_frame": 50, "skip_frames_after_generation": 6, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1428_2_0"}, {"texts": ["The man wearing black t-shirt is playing golf while a man wearing white t-shirt hit a ball"], "durations": null, "exact_frames_per_prompt": [30], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-7a1j_Q6PUk.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 50, "npz_gt_video_start_frame": 50, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 50, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1428_2_1"}, {"texts": ["A person whose hands are visible is holding a pan with eggs on the stove and mixing the eggs in the pan with chopsticks."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/0w7YOvqgO2I.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_142_0_0"}, {"texts": ["A man in the front wearing specs is doing shots."], "durations": null, "exact_frames_per_prompt": [22], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/1Uw8hT3pS5A.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 26, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1430_0_0"}, {"texts": ["A man wearing a white t-shirt and blue jeans is sitting on a horse and riding it."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/0ojl8BOp3DU.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1432_0_0"}, {"texts": ["A brown horse is standing while wearing a white t-shirt man is sitting on the horse."], "durations": null, "exact_frames_per_prompt": [44], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/0ojl8BOp3DU.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 48, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1432_1_0"}, {"texts": ["The brown horse starts walking, and is being ridden by a man."], "durations": null, "exact_frames_per_prompt": [32], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/0ojl8BOp3DU.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 48, "npz_gt_video_start_frame": 48, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 48, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1432_1_1"}, {"texts": ["A person wearing a white protective gear is standing on the silver ladder."], "durations": null, "exact_frames_per_prompt": [37], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/6ORPtrQ6Dh0.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 41, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1434_0_0"}, {"texts": ["The person is spraying on the tree on the right corner using a silver sprayer."], "durations": null, "exact_frames_per_prompt": [27], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/6ORPtrQ6Dh0.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 41, "npz_gt_video_start_frame": 41, "npz_gt_video_end_frame": 68, "skip_frames_after_generation": 41, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1434_0_1"}, {"texts": ["A child wearing a black t-shirt and blue pants is sitting and holding a pen."], "durations": null, "exact_frames_per_prompt": [50], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/0k9t_KPR2ro.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 54, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_143_0_0"}, {"texts": ["The child is pointing with the pen in the book while looking down."], "durations": null, "exact_frames_per_prompt": [26], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/0k9t_KPR2ro.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 54, "npz_gt_video_start_frame": 54, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 54, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_143_0_1"}, {"texts": ["A man wearing a light green shirt and jeans is sitting near the vehicle holding a pressure gauge, opening the tire stem cover."], "durations": null, "exact_frames_per_prompt": [47], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/32E949vdg_U.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 51, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1440_0_0"}, {"texts": ["The man is checking tire pressure with a tire pressure gauge."], "durations": null, "exact_frames_per_prompt": [29], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/32E949vdg_U.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 51, "npz_gt_video_start_frame": 51, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 51, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1440_0_1"}, {"texts": ["A man whose upper half-body is visible wearing a black full-sleeve t-shirt is sitting on the left side and is putting an object on the left side while speaking."], "durations": null, "exact_frames_per_prompt": [72], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/0HkH6W9vyuQ.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 76, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1441_0_0"}, {"texts": ["The man starts standing."], "durations": null, "exact_frames_per_prompt": [4], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/0HkH6W9vyuQ.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 76, "npz_gt_video_start_frame": 76, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 76, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1441_0_1"}, {"texts": ["A man, whose upper half-body is visible, wearing a light brown full-sleeve t-shirt, is sitting on a chair on the right side and is drinking yellow liquid from the rum glass and then speaking while sitting on the left, a man in a black t-shirt speaks and stands up at the end."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/0HkH6W9vyuQ.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1441_1_0"}, {"texts": ["A woman is holding a black object and applying a blush on her cheeks."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/4sHAfisTWCQ.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1442_0_0"}, {"texts": ["A person whose hands are visible is peeling a potato with a peeler."], "durations": null, "exact_frames_per_prompt": [77], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-N7_cQKqUWE.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 81, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1443_0_0"}, {"texts": ["A woman is wearing a maroon top. The woman is sitting.\n while a woman in a white top is sitting and trying to drink water from a closed bottle, a woman in a blue shirt is sitting and picking up ice cubes, and a man in a white t-shirt is sitting and trying to open the bottle for the woman."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/0d47EHINWMI.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1444_0_0"}, {"texts": ["A man wearing a white t-shirt is sitting and coughing while the other three women sitting beside him and laughing and the woman in the middle wearing white top grabs the water bottle to drink water"], "durations": null, "exact_frames_per_prompt": [67], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/0d47EHINWMI.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 71, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1444_2_0"}, {"texts": ["The man takes a bottle from the second person."], "durations": null, "exact_frames_per_prompt": [9], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/0d47EHINWMI.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 71, "npz_gt_video_start_frame": 71, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 71, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1444_2_1"}, {"texts": ["A woman wearing a blue top is sitting between the second person and the third person and all are going crazy because of chili."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/0d47EHINWMI.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1444_4_0"}, {"texts": ["A woman wearing a white top is sitting and coughing with one man and two women."], "durations": null, "exact_frames_per_prompt": [23], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/0d47EHINWMI.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 27, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1445_1_0"}, {"texts": ["The woman is moving her hands towards her face and a person whose hand is visible passes a bottle of water."], "durations": null, "exact_frames_per_prompt": [25], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/0d47EHINWMI.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 27, "npz_gt_video_start_frame": 27, "npz_gt_video_end_frame": 52, "skip_frames_after_generation": 27, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1445_1_1"}, {"texts": ["The woman takes a bottle from a person."], "durations": null, "exact_frames_per_prompt": [19], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/0d47EHINWMI.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 52, "npz_gt_video_start_frame": 52, "npz_gt_video_end_frame": 71, "skip_frames_after_generation": 52, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1445_1_2"}, {"texts": ["The woman gives it to the man."], "durations": null, "exact_frames_per_prompt": [9], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/0d47EHINWMI.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 71, "npz_gt_video_start_frame": 71, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 71, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1445_1_3"}, {"texts": ["A woman wearing a blue jacket is sitting between the second woman and the man, takes ice from the glass and starts eating while the third woman wearing a red top is sitting and put her hand on the mouth, the second woman wearing a white top is sitting scream for water and trying to drink it and a man wearing a white t-shirt is sitting and snatching the water bottle from the second woman."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/0d47EHINWMI.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1445_2_0"}, {"texts": ["A man wearing a white t-shirt with written words is sitting on the right and coughing while the others are doing different activities."], "durations": null, "exact_frames_per_prompt": [71], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/0d47EHINWMI.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 75, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1445_3_0"}, {"texts": ["The man wearing a white t-shirt takes a bottle from the second woman."], "durations": null, "exact_frames_per_prompt": [5], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/0d47EHINWMI.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 75, "npz_gt_video_start_frame": 75, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 75, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1445_3_1"}, {"texts": ["A person on the left side wearing a printed white shirt is standing and holding a piglet in his hands while others are playing with it."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-fVOpScdSPg.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1446_0_0"}, {"texts": ["A girl wearing a printed pink t-shirt is standing and touching the nose of a piglet.\n while the other girl in white skirt touch the piglet."], "durations": null, "exact_frames_per_prompt": [51], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-fVOpScdSPg.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 55, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1446_2_0"}, {"texts": ["A girl wearing a brown top is standing while a person is holding a piglet and the piglet is eating from the hands of the girl who is wearing a pink dress, a woman wearing a white vest is standing behind the girl wearing a pink dress and holding her hand."], "durations": null, "exact_frames_per_prompt": [17], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-fVOpScdSPg.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 21, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1446_3_0"}, {"texts": ["The girl is touching the piglet with her hand while a person wearing grey clothes is standing behind the girl."], "durations": null, "exact_frames_per_prompt": [59], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-fVOpScdSPg.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 21, "npz_gt_video_start_frame": 21, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 21, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1446_3_1"}, {"texts": ["A woman wearing a black t-shirt is standing with a brown dog who is also standing"], "durations": null, "exact_frames_per_prompt": [53], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/061sX9nz-Ug.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 57, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1448_0_0"}, {"texts": ["The woman is snipping a dog's hair with the scissors."], "durations": null, "exact_frames_per_prompt": [23], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/061sX9nz-Ug.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 57, "npz_gt_video_start_frame": 57, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 57, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1448_0_1"}, {"texts": ["A brown dog is standing on the black table while a woman in black top is taken scissor and tries to cut dogs hair."], "durations": null, "exact_frames_per_prompt": [56], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/061sX9nz-Ug.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 60, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1448_1_0"}, {"texts": ["The dog is getting a haircut by the woman."], "durations": null, "exact_frames_per_prompt": [19], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/061sX9nz-Ug.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 60, "npz_gt_video_start_frame": 60, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 60, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1448_1_1"}, {"texts": ["A person whose hands are visible only, is folding a paper into a triangle."], "durations": null, "exact_frames_per_prompt": [54], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/1joFLLaLSKw.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 58, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1449_0_0"}, {"texts": ["The person starts writing on the paper with a sketch pen."], "durations": null, "exact_frames_per_prompt": [22], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/1joFLLaLSKw.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 58, "npz_gt_video_start_frame": 58, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 58, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1449_0_1"}, {"texts": ["A person whose hand is visible is crafting a triangle."], "durations": null, "exact_frames_per_prompt": [58], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/1joFLLaLSKw.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 62, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1450_0_0"}, {"texts": ["The person is writing the degree of angle."], "durations": null, "exact_frames_per_prompt": [18], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/1joFLLaLSKw.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 62, "npz_gt_video_start_frame": 62, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 62, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1450_0_1"}, {"texts": ["A man wearing a black jacket is holding a stick and plucking the fruit with the stick.\n"], "durations": null, "exact_frames_per_prompt": [9], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-PSBh7Qbcmc.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 13, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_145_0_ms_0"}, {"texts": ["A person whose only hands are visible is rolling the handle."], "durations": null, "exact_frames_per_prompt": [10], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-PSBh7Qbcmc.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 14, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_145_1_0"}, {"texts": ["The person whose only hands are visible is picking up the fruit."], "durations": null, "exact_frames_per_prompt": [62], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-PSBh7Qbcmc.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 14, "npz_gt_video_start_frame": 14, "npz_gt_video_end_frame": 76, "skip_frames_after_generation": 14, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_145_1_1"}, {"texts": ["The person whose only hands are visible is putting the fruit pieces in the yellow bucket."], "durations": null, "exact_frames_per_prompt": [4], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-PSBh7Qbcmc.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 76, "npz_gt_video_start_frame": 76, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 76, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_145_1_2"}, {"texts": ["A man wearing a white striped green t-shirt is standing behind the table and helping the first boy to spread the red sauce on the pizza while others are watching it."], "durations": null, "exact_frames_per_prompt": [15], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/0J_ykamxQUU.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 19, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1460_0_0"}, {"texts": ["The man wearing a white striped green t-shirt walks  and stands and puts the meat pieces on the bread."], "durations": null, "exact_frames_per_prompt": [61], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/0J_ykamxQUU.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 19, "npz_gt_video_start_frame": 19, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 19, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1460_0_1"}, {"texts": ["A boy wearing a green t-shirt, black jeans and a white apron is standing on the left side and spreading the red sauce on the pizza base while a man wearing green t-shirt is spreading the red sauce on the the pizza base and speaking while coming left side, a girl wearing blue clothes is coming to the left side, a boy wearing black clothes and a white apron is standing on the right side and spreading pizza sauce from the spoon on the other pizza base, a boy wearing red-black t-shirt is standing on the left side looking at the pizza."], "durations": null, "exact_frames_per_prompt": [30], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/0J_ykamxQUU.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 34, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1460_2_0"}, {"texts": ["The boy is standing behind the second child, tilting his body and is looking at the other pizza base on the right side of the table while the man wearing green t-shirt is adding toppings on the pizza base and a kid wearing pink clothes is touching the pizza with the sauce spoon and adding toppings, a boy wearing red-black t-shirt standing on the right side and looking at the pizza while moving his body."], "durations": null, "exact_frames_per_prompt": [46], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/0J_ykamxQUU.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 34, "npz_gt_video_start_frame": 34, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 34, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1460_2_1"}, {"texts": ["A boy wearing a black t-shirt and a white apron is standing on the right side of the woman and is spreading the red sauce on the pizza base with the spoon while the boy wearing green t-shirt is also spreading sauce on pizza base while others are watching them"], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/0J_ykamxQUU.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1460_3_0"}, {"texts": ["A child wearing a blue-red t-shirt and a white chef cap is standing on the left side of the table while two boys are spreading something on flattened dough, a woman in a blue t-shirt and a man in a green t-shirt are helping the boys."], "durations": null, "exact_frames_per_prompt": [31], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/0J_ykamxQUU.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 35, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1460_4_0"}, {"texts": ["The child stands on the right side of the man and looks at the pizza base."], "durations": null, "exact_frames_per_prompt": [45], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/0J_ykamxQUU.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 35, "npz_gt_video_start_frame": 35, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 35, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1460_4_1"}, {"texts": ["A girl wearing a white shirt is talking while having food."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/0VqathWO8mU.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1461_0_0"}, {"texts": ["A person whose fingers are visible is ironing a shoe."], "durations": null, "exact_frames_per_prompt": [53], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/1AxBnmEOZ2Q.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 12, "npz_gt_video_start_frame": 12, "npz_gt_video_end_frame": 65, "skip_frames_after_generation": 12, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1462_0_0"}, {"texts": ["A person whose fingers are visible is ironing the green cloth which is put on the show with the ironing machine."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/1AxBnmEOZ2Q.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1463_0_0"}, {"texts": ["A man wearing a gray t-shirt and blue shorts is standing, moving, and making a bed while something is moving under the blanket."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/j28R51DEVq8.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1465_0_0"}, {"texts": ["An animal is moving under the sheet while a man wearing a grey t-shirt and blue shorts is covering the sheet on the bed."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/j28R51DEVq8.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1465_1_0"}, {"texts": ["A person whose hand is visible making an omelet in a pan."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/02bfGyFuidk.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1468_0_0"}, {"texts": ["A person whose hand is visible is steering the yellow food in a frying pan with the help of a black spatula."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/02bfGyFuidk.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1469_0_0"}, {"texts": ["A man whose only hand is visible is holding a balance adapter board."], "durations": null, "exact_frames_per_prompt": [58], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/1Dkr-Xk-aEU.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 62, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1470_0_0"}, {"texts": ["The man puts the board aside."], "durations": null, "exact_frames_per_prompt": [11], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/1Dkr-Xk-aEU.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 62, "npz_gt_video_start_frame": 62, "npz_gt_video_end_frame": 73, "skip_frames_after_generation": 62, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1470_0_1"}, {"texts": ["The man picks up the wire from a box."], "durations": null, "exact_frames_per_prompt": [7], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/1Dkr-Xk-aEU.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 73, "npz_gt_video_start_frame": 73, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 73, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1470_0_2"}, {"texts": ["A boy wearing a black t-shirt is sitting on the right and eating food and a group of people are also eating food and some people are standing on the backside and a boy is making a recording with his phone."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/07dVkTTGXv4.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1472_0_0"}, {"texts": ["A boy wearing a blue-yellow striped t-shirt is sitting on the left and eating food while some people are also sitting and eating food, and some other people are standing at the back."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/07dVkTTGXv4.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1472_1_0"}, {"texts": ["A boy wearing a green t-shirt is sitting on the back and eating food while in a group of people, some people are sitting around the table and eating food, and the rest are standing and looking here and there."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/07dVkTTGXv4.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1472_2_0"}, {"texts": ["A boy wearing a grey t-shirt is standing on the right and holding a mobile phone in his hands while a group of boys is sitting and eating, and a few people are standing at the back."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/07dVkTTGXv4.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1472_4_0"}, {"texts": ["A girl wearing a pink jacket and black trousers is jumping."], "durations": null, "exact_frames_per_prompt": [27], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-WvBTcrfKKI.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 31, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1474_0_0"}, {"texts": ["The girl wearing a pink jacket and black trousers is walking away."], "durations": null, "exact_frames_per_prompt": [35], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-WvBTcrfKKI.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 31, "npz_gt_video_start_frame": 31, "npz_gt_video_end_frame": 66, "skip_frames_after_generation": 31, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1474_0_1"}, {"texts": ["A girl wearing a pink jacket and black trousers is jumping."], "durations": null, "exact_frames_per_prompt": [35], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-WvBTcrfKKI.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 39, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1475_0_0"}, {"texts": ["The girl walking away."], "durations": null, "exact_frames_per_prompt": [27], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-WvBTcrfKKI.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 39, "npz_gt_video_start_frame": 39, "npz_gt_video_end_frame": 66, "skip_frames_after_generation": 39, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1475_0_1"}, {"texts": ["A woman wearing a grey t-shirt is standing on the right side and holding a snake in her hand while other people are standing and touching the snake."], "durations": null, "exact_frames_per_prompt": [77], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/4NTGeHgXrgY.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 81, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1476_0_0"}, {"texts": ["A boy wearing a black t-shirt is standing and touching the snake while a man wearing a black t-shirt is standing in the middle, a woman wearing a grey top is standing and touching the snake."], "durations": null, "exact_frames_per_prompt": [77], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/4NTGeHgXrgY.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 81, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1476_1_0"}, {"texts": ["A man wearing a black t-shirt is standing in the middle while a person wearing a cap is standing on the left side holding a snake, and a woman wearing a gray t-shirt is standing on the right side taking a snake from the person."], "durations": null, "exact_frames_per_prompt": [77], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/4NTGeHgXrgY.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 81, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1476_2_0"}, {"texts": ["A snake is hanging on the hand of the woman while a man is standing in front and looking at the snake, a person wearing red and black clothes is also touching the snake and a group of people are sitting at the back."], "durations": null, "exact_frames_per_prompt": [55], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/4NTGeHgXrgY.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 26, "npz_gt_video_start_frame": 26, "npz_gt_video_end_frame": 81, "skip_frames_after_generation": 26, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1476_4_0"}, {"texts": ["A man wearing brown clothes is walking while holding a dog with a dog leash."], "durations": null, "exact_frames_per_prompt": [44], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/0eahjw_NZ44.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 48, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_147_0_0"}, {"texts": ["The man wearing brown clothes is touching the dog."], "durations": null, "exact_frames_per_prompt": [32], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/0eahjw_NZ44.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 48, "npz_gt_video_start_frame": 48, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 48, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_147_0_1"}, {"texts": ["A black-brown dog is sitting, walking, and tail wagging on the grey surface while the man holds the dog."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/0eahjw_NZ44.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_147_1_0"}, {"texts": ["A man wearing a white shirt and pant is standing on the right side and holding a white bed-sheet while a woman also wearing white shirt and pants is holding the bed-sheet."], "durations": null, "exact_frames_per_prompt": [43], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/IYfzqUKGWFo.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 47, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1482_0_0"}, {"texts": ["The man is putting it on the bed while a man wearing a suit is standing at the back and a person wearing a green shirt is also standing and holding a mic."], "durations": null, "exact_frames_per_prompt": [33], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/IYfzqUKGWFo.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 47, "npz_gt_video_start_frame": 47, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 47, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1482_0_1"}, {"texts": ["A woman wearing a white shirt and pants is standing on the left side and holding a white bed-sheet while the other woman on the right is holding the other end of the bed-sheet"], "durations": null, "exact_frames_per_prompt": [38], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/IYfzqUKGWFo.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 42, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1482_1_0"}, {"texts": ["The woman is putting it on the bed."], "durations": null, "exact_frames_per_prompt": [38], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/IYfzqUKGWFo.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 42, "npz_gt_video_start_frame": 42, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 42, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1482_1_1"}, {"texts": ["A man wearing a green shirt and black pant is standing while holding a mic in his hand while a man wearing black clothes is standing and looking in the direction of the bed, a woman wearing white clothes and a man wearing white clothes are spreading the white bed sheet on the bed."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/IYfzqUKGWFo.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1482_2_0"}, {"texts": ["A woman wearing a blue jacket is holding a white handheld mixer and mixing the food in a big black container while holding a food box in the other hand along with the child standing near her."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/0IDsLy_Elno.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1484_0_0"}, {"texts": ["A child wearing a purple t-shirt is standing on a chair near the woman and watching the mixer mixing the food."], "durations": null, "exact_frames_per_prompt": [46], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/0IDsLy_Elno.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 50, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1484_1_0"}, {"texts": ["The child is holding the food box and the handheld mixer with the woman."], "durations": null, "exact_frames_per_prompt": [30], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/0IDsLy_Elno.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 50, "npz_gt_video_start_frame": 50, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 50, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1484_1_1"}, {"texts": ["A woman wearing a white top is standing and ironing a multi color dress with an iron."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/0FK9vQbe8Ik.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1485_0_0"}, {"texts": ["A person whose only finger is visible is tickling the cat in the neck."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-uQEPIgJjO4.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1486_0_0"}, {"texts": ["A black brown cat is sitting and getting tickled by another person."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-uQEPIgJjO4.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1486_1_0"}, {"texts": ["A man wearing a white vest is holding a can."], "durations": null, "exact_frames_per_prompt": [37], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/1Jvl-MR9bOI.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 41, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1487_0_0"}, {"texts": ["The man is drinking and speaking while looking in front."], "durations": null, "exact_frames_per_prompt": [39], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/1Jvl-MR9bOI.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 41, "npz_gt_video_start_frame": 41, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 41, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1487_0_1"}, {"texts": ["A man wearing a white vest is sitting, talking."], "durations": null, "exact_frames_per_prompt": [37], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/1Jvl-MR9bOI.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 41, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1488_0_0"}, {"texts": ["The man wearing a white vest is drinking something out of a can."], "durations": null, "exact_frames_per_prompt": [39], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/1Jvl-MR9bOI.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 41, "npz_gt_video_start_frame": 41, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 41, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1488_0_1"}, {"texts": ["A girl wearing a black spotted dress is feeding food to the group of koi fish from a white food container."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/0zINhjQnpYE.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_148_0_0"}, {"texts": ["A man wearing a blue t-shirt is sitting."], "durations": null, "exact_frames_per_prompt": [13], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/5CvyVOTBYP4.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 17, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1493_0_0"}, {"texts": ["The man drinking with a bottle."], "durations": null, "exact_frames_per_prompt": [64], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/5CvyVOTBYP4.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 17, "npz_gt_video_start_frame": 17, "npz_gt_video_end_frame": 81, "skip_frames_after_generation": 17, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1493_0_1"}, {"texts": ["A child wearing a red graphic t-shirt and blue pants is standing on the left side of the table and pointing with her hands on the book while a person wearing a purple dress whose lower body is visible is standing beside the child, and another person whose hand is visible touches the book then points her hand towards a picture."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-DsMpC5_-c4.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_149_1_0"}, {"texts": ["A group of people are running in a green field and one person is picking the other person."], "durations": null, "exact_frames_per_prompt": [70], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/2bZRDuw_RTI.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 74, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_14_0_ms_0"}, {"texts": ["A man wearing black clothes is standing and playing golf on the ground."], "durations": null, "exact_frames_per_prompt": [58], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/1AKg356qfMg.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 62, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1500_0_0"}, {"texts": ["A person whose hand is visible is feeding the baby with an orange and silver spoon."], "durations": null, "exact_frames_per_prompt": [67], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-dl427XRsOE.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 5, "npz_gt_video_start_frame": 5, "npz_gt_video_end_frame": 72, "skip_frames_after_generation": 5, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1501_0_0"}, {"texts": ["A baby wearing an orange t-shirt is sitting and is being fed by a person with an orange and silver spoon."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-dl427XRsOE.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1501_1_0"}, {"texts": ["A woman wearing a cap is lying on a bed and getting a microblading treatment another woman on the left side wearing a white outfit is doing microblading treatment of first woman's eyebrow."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/0zqmuRpChLw.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1502_0_0"}, {"texts": ["A woman wearing white clothes is giving a microblading treatment to the woman lying on the bed a woman wearing lying on the bed taking treatment"], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/0zqmuRpChLw.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1502_1_0"}, {"texts": ["A girl wearing pink underwear is holding a pan upward while a kid sitting on the couch is looking at the girl."], "durations": null, "exact_frames_per_prompt": [17], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-8S87U-FVJI.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 21, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1505_0_0"}, {"texts": ["The girl wearing pink underwear is putting it on the table while a white animal is lying on the gray couch."], "durations": null, "exact_frames_per_prompt": [19], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-8S87U-FVJI.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 21, "npz_gt_video_start_frame": 21, "npz_gt_video_end_frame": 40, "skip_frames_after_generation": 21, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1505_0_1"}, {"texts": ["The girl wearing pink underwear is putting some food into the pan."], "durations": null, "exact_frames_per_prompt": [28], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-8S87U-FVJI.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 40, "npz_gt_video_start_frame": 40, "npz_gt_video_end_frame": 68, "skip_frames_after_generation": 40, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1505_0_2"}, {"texts": ["The girl wearing pink underwear starts walking."], "durations": null, "exact_frames_per_prompt": [12], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-8S87U-FVJI.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 68, "npz_gt_video_start_frame": 68, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 68, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1505_0_3"}, {"texts": ["A boy wearing red-white underwear is lying on the gray couch and watching the girl."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-8S87U-FVJI.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1505_1_0"}, {"texts": ["A woman whose only lower body is visible is cooking sausage in a black pan."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/0HXY5yOs4vM.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1506_0_0"}, {"texts": ["A person whose hand is visible is pressing the paper on the yellow surface while another person's hand is also visible, holding a drill machine and drilling the yellow surface."], "durations": null, "exact_frames_per_prompt": [17], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/2o_Bgm1MsP0.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 21, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1507_0_ms_0"}, {"texts": ["A person wearing black cloth whose only hands are visible is applying the paste on the paper."], "durations": null, "exact_frames_per_prompt": [37], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/2o_Bgm1MsP0.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 29, "npz_gt_video_start_frame": 29, "npz_gt_video_end_frame": 66, "skip_frames_after_generation": 29, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1507_1_0"}, {"texts": ["A girl wearing a red shirt and white pants is reading a book."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-DsMpC5_-c4.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_150_0_0"}, {"texts": ["A man whose only hands are visible is tightening the nut bolt of the car with a wrench, hitting the wrench with a hammer."], "durations": null, "exact_frames_per_prompt": [24], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/2JjD84z3l8c.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 28, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1513_0_0"}, {"texts": ["The man is moving the white tub with his hand."], "durations": null, "exact_frames_per_prompt": [33], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/2JjD84z3l8c.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 28, "npz_gt_video_start_frame": 28, "npz_gt_video_end_frame": 61, "skip_frames_after_generation": 28, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1513_0_1"}, {"texts": ["The man again tightened the nut bolt with a wrench."], "durations": null, "exact_frames_per_prompt": [19], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/2JjD84z3l8c.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 61, "npz_gt_video_start_frame": 61, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 61, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1513_0_2"}, {"texts": ["A man wearing a suit is standing and is reading the weather forecast."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/0Hzm1WjTU9U.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1516_0_0"}, {"texts": ["A man standing in front of a screen wearing a black suit is presenting the weather forecast report."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/0Hzm1WjTU9U.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1517_0_0"}, {"texts": ["A girl wearing a red-black printed t-shirt is standing in the front and moving her finger on a book while a woman whose only hand is visible pointing her finger on a book."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-DsMpC5_-c4.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_151_0_0"}, {"texts": ["A woman whose hand is visible is putting her finger on the book while other women put their finger and show words and read."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-DsMpC5_-c4.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_151_2_0"}, {"texts": ["A man wearing a blue t-shirt makes a drink while a man wearing black t-shirt is standing on the left and he is doing something."], "durations": null, "exact_frames_per_prompt": [42], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/0TkcgzlKlOM.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 46, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1523_0_0"}, {"texts": ["The man wearing a blue t-shirt serves it while people are standing and they are doing different activity."], "durations": null, "exact_frames_per_prompt": [21], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/0TkcgzlKlOM.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 46, "npz_gt_video_start_frame": 46, "npz_gt_video_end_frame": 67, "skip_frames_after_generation": 46, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1523_0_1"}, {"texts": ["The man wearing a blue t-shirt lifts a glass."], "durations": null, "exact_frames_per_prompt": [13], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/0TkcgzlKlOM.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 67, "npz_gt_video_start_frame": 67, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 67, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1523_0_2"}, {"texts": ["A man wearing a black t-shirt and black trousers is standing in the back and cleaning the glasses while another man in a blue t-shirt is making a drink, then picking some glasses and some people are standing on the right side and looking at them."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/0TkcgzlKlOM.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1523_1_0"}, {"texts": ["An old man is lying on the bed and holding a book and speaking while a girl wearing a red top is sitting on the bed and looking towards the book."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/0xHJzEftR5s.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1524_0_0"}, {"texts": ["A girl is sitting behind the old man and watching the book."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/0xHJzEftR5s.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1524_1_0"}, {"texts": ["A person wearing white clothes is standing and making a tie."], "durations": null, "exact_frames_per_prompt": [52], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-vFkMmod2K8.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 8, "npz_gt_video_start_frame": 8, "npz_gt_video_end_frame": 60, "skip_frames_after_generation": 8, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1525_0_0"}, {"texts": ["A man wearing black cloth is standing near the counter."], "durations": null, "exact_frames_per_prompt": [61], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/3MK4LUiDKpI.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 65, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1526_0_0"}, {"texts": ["The man is taking food from the bowl and spreading on a black paper on the wooden board."], "durations": null, "exact_frames_per_prompt": [15], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/3MK4LUiDKpI.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 65, "npz_gt_video_start_frame": 65, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 65, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1526_0_1"}, {"texts": ["A man wearing a black chef uniform is standing behind the silver counter top."], "durations": null, "exact_frames_per_prompt": [10], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/3MK4LUiDKpI.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 14, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1527_0_0"}, {"texts": ["The man wearing a black chef uniform is spreading cooked rice on the nori."], "durations": null, "exact_frames_per_prompt": [46], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/3MK4LUiDKpI.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 14, "npz_gt_video_start_frame": 14, "npz_gt_video_end_frame": 60, "skip_frames_after_generation": 14, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1527_0_1"}, {"texts": ["The man wearing a black chef uniform is then taking cooked rice from a big maroon bowl."], "durations": null, "exact_frames_per_prompt": [20], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/3MK4LUiDKpI.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 60, "npz_gt_video_start_frame": 60, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 60, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1527_0_2"}, {"texts": ["A baby wearing a white-blue t-shirt is sitting in the lap of the person and eating cake."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/3xnLJ3bjOD0.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1528_0_0"}, {"texts": ["A person wearing a green t-shirt is sitting carrying a baby and eating cake."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/3xnLJ3bjOD0.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1528_1_0"}, {"texts": ["A person wearing a green jacket is spreading money on the grey seat of a car."], "durations": null, "exact_frames_per_prompt": [77], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/2F0DvdY8sac.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 81, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1529_0_0"}, {"texts": ["A woman wearing a black full-sleeve t-shirt and black jeans is standing and holding the girl's hand."], "durations": null, "exact_frames_per_prompt": [16], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/2P2z9dhQTFw.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 20, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_152_0_0"}, {"texts": ["The woman shifting the girl and holding a spatula."], "durations": null, "exact_frames_per_prompt": [22], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/2P2z9dhQTFw.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 20, "npz_gt_video_start_frame": 20, "npz_gt_video_end_frame": 42, "skip_frames_after_generation": 20, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_152_0_1"}, {"texts": ["The woman starts flipping the pancake on the griddle with the spatula."], "durations": null, "exact_frames_per_prompt": [31], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/2P2z9dhQTFw.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 42, "npz_gt_video_start_frame": 42, "npz_gt_video_end_frame": 73, "skip_frames_after_generation": 42, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_152_0_2"}, {"texts": ["A girl wearing a pink full-sleeve t-shirt and white jeans is standing, holding a spatula in her right hand and flipping the pancake with the help of the woman."], "durations": null, "exact_frames_per_prompt": [25], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/2P2z9dhQTFw.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 29, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_152_1_0"}, {"texts": ["The girl is shifting on the right side, and looking at the pancake while the woman holds the spatula and flips the pancake"], "durations": null, "exact_frames_per_prompt": [37], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/2P2z9dhQTFw.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 29, "npz_gt_video_start_frame": 29, "npz_gt_video_end_frame": 66, "skip_frames_after_generation": 29, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_152_1_1"}, {"texts": ["A woman wearing a black top and pink shorts is lying down and getting a tattoo by a man."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-eE4tUHysUA.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1531_0_0"}, {"texts": ["A man wearing a black t-shirt is sitting on the left side of the woman and holding a tattoo machine."], "durations": null, "exact_frames_per_prompt": [33], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-eE4tUHysUA.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 37, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1531_1_0"}, {"texts": ["The man is tattooing it on the body of the woman."], "durations": null, "exact_frames_per_prompt": [43], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-eE4tUHysUA.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 37, "npz_gt_video_start_frame": 37, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 37, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1531_1_1"}, {"texts": ["A person whose only hands are visible is holding a fish."], "durations": null, "exact_frames_per_prompt": [17], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-gyWb33UPKk.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 21, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1532_0_ms_0"}, {"texts": ["A white-green fish is being held by a person."], "durations": null, "exact_frames_per_prompt": [17], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-gyWb33UPKk.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 21, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1532_1_ms_0"}, {"texts": ["A man wearing a blue jumpsuit is sitting on a white bucket, holding a fishing rod, and rolling it."], "durations": null, "exact_frames_per_prompt": [21], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-gyWb33UPKk.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 21, "npz_gt_video_start_frame": 21, "npz_gt_video_end_frame": 42, "skip_frames_after_generation": 21, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1532_2_ms_0"}, {"texts": ["The man is pulling out the fish from the hole, and holding it."], "durations": null, "exact_frames_per_prompt": [39], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-gyWb33UPKk.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 42, "npz_gt_video_start_frame": 42, "npz_gt_video_end_frame": 81, "skip_frames_after_generation": 42, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1532_2_ms_1"}, {"texts": ["A green fish is being pulled out from the hole."], "durations": null, "exact_frames_per_prompt": [18], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-gyWb33UPKk.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 46, "npz_gt_video_start_frame": 46, "npz_gt_video_end_frame": 64, "skip_frames_after_generation": 46, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1532_3_ms_0"}, {"texts": ["The fish is being held by a man."], "durations": null, "exact_frames_per_prompt": [17], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-gyWb33UPKk.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 64, "npz_gt_video_start_frame": 64, "npz_gt_video_end_frame": 81, "skip_frames_after_generation": 64, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1532_3_ms_1"}, {"texts": ["A girl wearing a pink t-shirt is standing and holding a snake with one hand."], "durations": null, "exact_frames_per_prompt": [40], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/5m6YcW7sC9U.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 44, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1537_0_0"}, {"texts": ["The girl is caressing the snake with the other hand while speaking."], "durations": null, "exact_frames_per_prompt": [36], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/5m6YcW7sC9U.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 44, "npz_gt_video_start_frame": 44, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 44, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1537_0_1"}, {"texts": ["A cream-brown snake is being held by a girl and pressing it."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/5m6YcW7sC9U.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1537_1_0"}, {"texts": ["A man wearing a black t-shirt is tattooing a woman's back."], "durations": null, "exact_frames_per_prompt": [63], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/1fmaEo3wzxg.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 67, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_153_0_0"}, {"texts": ["A woman is sitting and being tattooed by the man."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/1fmaEo3wzxg.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_153_1_0"}, {"texts": ["A man wearing a gray t-shirt is standing, then opens the fuel tank cap of a vehicle."], "durations": null, "exact_frames_per_prompt": [17], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/53YdQLCYoEc.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 21, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1540_0_0"}, {"texts": ["The man wearing a gray t-shirt picks up the fuel dispenser nozzle."], "durations": null, "exact_frames_per_prompt": [18], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/53YdQLCYoEc.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 21, "npz_gt_video_start_frame": 21, "npz_gt_video_end_frame": 39, "skip_frames_after_generation": 21, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1540_0_1"}, {"texts": ["The man wearing a gray t-shirt moves his hand towards the vehicle."], "durations": null, "exact_frames_per_prompt": [22], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/53YdQLCYoEc.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 39, "npz_gt_video_start_frame": 39, "npz_gt_video_end_frame": 61, "skip_frames_after_generation": 39, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1540_0_2"}, {"texts": ["The man wearing a gray t-shirt moves his left hand towards the fuel dispensing machine."], "durations": null, "exact_frames_per_prompt": [19], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/53YdQLCYoEc.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 61, "npz_gt_video_start_frame": 61, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 61, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1540_0_3"}, {"texts": ["A man wearing a green t-shirt is standing and opening the fuel cap of a car."], "durations": null, "exact_frames_per_prompt": [65], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/53YdQLCYoEc.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 69, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1541_0_0"}, {"texts": ["The man is pressing buttons on a fuel dispenser."], "durations": null, "exact_frames_per_prompt": [11], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/53YdQLCYoEc.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 69, "npz_gt_video_start_frame": 69, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 69, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1541_0_1"}, {"texts": ["A woman wearing blue clothes is at first sitting silently on a bed while a person whose hands are visible comes from the right and is holding a paper then starts tearing it."], "durations": null, "exact_frames_per_prompt": [35], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/20J-6g7YIbQ.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 39, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1542_0_0"}, {"texts": ["The woman starts to laugh by watching the tearing of a white paper by a person."], "durations": null, "exact_frames_per_prompt": [41], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/20J-6g7YIbQ.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 39, "npz_gt_video_start_frame": 39, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 39, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1542_0_1"}, {"texts": ["A person whose hands are visible is tearing a paper in front of the woman in blue clothes."], "durations": null, "exact_frames_per_prompt": [62], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/20J-6g7YIbQ.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 18, "npz_gt_video_start_frame": 18, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 18, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1542_1_0"}, {"texts": ["A baby wearing a white printed vest is sitting on the person's lap and laughing while a kid wearing a grey vest is tearing out the book pages and putting them in a small red container."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/0p5Ph99ZFXM.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1543_1_0"}, {"texts": ["A baby wearing a gray vest is sitting while a baby wearing a white printed t-shirt is held by a man from the backside is moving his head."], "durations": null, "exact_frames_per_prompt": [23], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/0p5Ph99ZFXM.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 27, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1543_2_0"}, {"texts": ["The baby is  tearing a paper while a boy sitting and laughing."], "durations": null, "exact_frames_per_prompt": [53], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/0p5Ph99ZFXM.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 27, "npz_gt_video_start_frame": 27, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 27, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1543_2_1"}, {"texts": ["A woman wearing black clothes is climbing on a tree and plucking a gooseberry from the tree while a woman wearing a red t-shirt is standing and plucking gooseberry from the tree."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-mPOZ2pvyRU.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1544_0_0"}, {"texts": ["A woman wearing red clothes is climbing on a tree and plucking a gooseberry from the tree while another woman wearing a black top and shorts is standing on a tree, holding a black plastic bag, and plucking a gooseberry."], "durations": null, "exact_frames_per_prompt": [18], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-mPOZ2pvyRU.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 22, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1544_1_0"}, {"texts": ["A white-black dog is playing with a white ball."], "durations": null, "exact_frames_per_prompt": [17], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/1OK0Dz8YkDQ.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 21, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1548_1_0"}, {"texts": ["The white-black dog picks up the ball from the mouth starts walking towards the man."], "durations": null, "exact_frames_per_prompt": [33], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/1OK0Dz8YkDQ.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 21, "npz_gt_video_start_frame": 21, "npz_gt_video_end_frame": 54, "skip_frames_after_generation": 21, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1548_1_1"}, {"texts": ["The white-black dog is sitting on the green grass surface."], "durations": null, "exact_frames_per_prompt": [26], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/1OK0Dz8YkDQ.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 54, "npz_gt_video_start_frame": 54, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 54, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1548_1_2"}, {"texts": ["A woman wearing a orange t-shirt is sitting on the left side, watching the baby a baby wearing green t-shirt sitting on a right side watching his watch"], "durations": null, "exact_frames_per_prompt": [62], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/0g_pK2gsW3c.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 66, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_154_0_0"}, {"texts": ["The woman is feeding him with an ice cream a baby wearing green t-shirt sitting on right side eating ice cream"], "durations": null, "exact_frames_per_prompt": [14], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/0g_pK2gsW3c.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 66, "npz_gt_video_start_frame": 66, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 66, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_154_0_1"}, {"texts": ["A baby wearing a green t-shirt is sitting on a sofa while holding a straw in his right hand and watching his toy watch while a woman on the left side in an orange top is sitting and talking."], "durations": null, "exact_frames_per_prompt": [40], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/0g_pK2gsW3c.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 44, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_154_1_0"}, {"texts": ["A baby wearing a green t-shirt is watching the woman."], "durations": null, "exact_frames_per_prompt": [21], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/0g_pK2gsW3c.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 44, "npz_gt_video_start_frame": 44, "npz_gt_video_end_frame": 65, "skip_frames_after_generation": 44, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_154_1_1"}, {"texts": ["A baby wearing a green t-shirt is licking an ice cream while the ice cream is held by the woman."], "durations": null, "exact_frames_per_prompt": [15], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/0g_pK2gsW3c.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 65, "npz_gt_video_start_frame": 65, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 65, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_154_1_2"}, {"texts": ["A man wearing a grey shirt is attaching a metal handle on the train wheel with the other man."], "durations": null, "exact_frames_per_prompt": [42], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/29o24GLpYmA.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 46, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1550_0_0"}, {"texts": ["The man starts pulling the handle with another man while a group of people is watching them and sitting in a squat position on the other side of then train."], "durations": null, "exact_frames_per_prompt": [34], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/29o24GLpYmA.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 46, "npz_gt_video_start_frame": 46, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 46, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1550_0_1"}, {"texts": ["The man wearing a blue-red-grey t-shirt is pulling the metal handle with the first man."], "durations": null, "exact_frames_per_prompt": [33], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/29o24GLpYmA.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 47, "npz_gt_video_start_frame": 47, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 47, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1550_2_0"}, {"texts": ["A woman wearing a red apron is holding a knife and peeling a pineapple."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/2HG3vWdmvBk.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1551_0_0"}, {"texts": ["A man wearing a white shirt, a yellow-black-white tie and spectacles is sitting on a black chair and holding a paper while speaking and moving his hand here and there."], "durations": null, "exact_frames_per_prompt": [20], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-g3JhkJRVY4.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 24, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1552_0_0"}, {"texts": ["The man is taking out his spectacles and speaking while moving his other hand here and there."], "durations": null, "exact_frames_per_prompt": [56], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-g3JhkJRVY4.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 24, "npz_gt_video_start_frame": 24, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 24, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1552_0_1"}, {"texts": ["A man whose hands are visible is putting a white microwavable egg cooker on the wooden table."], "durations": null, "exact_frames_per_prompt": [18], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/1tg7VPXV8eI.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 15, "npz_gt_video_start_frame": 15, "npz_gt_video_end_frame": 33, "skip_frames_after_generation": 15, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1553_0_0"}, {"texts": ["The man opens the microwavable egg cooker."], "durations": null, "exact_frames_per_prompt": [26], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/1tg7VPXV8eI.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 33, "npz_gt_video_start_frame": 33, "npz_gt_video_end_frame": 60, "skip_frames_after_generation": 33, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1553_0_1"}, {"texts": ["A woman wearing red clothes is opening a yellow thread."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-ZUXh0uhVwM.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1554_0_0"}, {"texts": ["A woman wearing a pink top is crafting something with paper money while another woman is showing money and then picking up the money from the front."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-ZUXh0uhVwM.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1555_0_0"}, {"texts": ["A person wearing a black shirt is lifting the paper money object while a lady on the left side wearing a red top is sewing a golden strip."], "durations": null, "exact_frames_per_prompt": [34], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-ZUXh0uhVwM.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 46, "npz_gt_video_start_frame": 46, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 46, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1555_1_0"}, {"texts": ["A man wearing a grey shirt is standing on the backside, holding a writing pad, shaking his body while a man wearing a off white clothes moves to the left side with folded hands and a group of people are standing while holding some papers in their hands."], "durations": null, "exact_frames_per_prompt": [71], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/5BnL6kF7wnY.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 75, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1556_0_0"}, {"texts": ["The man wearing a grey shirt raises his hand."], "durations": null, "exact_frames_per_prompt": [5], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/5BnL6kF7wnY.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 75, "npz_gt_video_start_frame": 75, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 75, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1556_0_1"}, {"texts": ["A man wearing a brown jacket is standing and a group of people are standing and a man in a gray shirt is moving his shoulder while speaking."], "durations": null, "exact_frames_per_prompt": [18], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/5BnL6kF7wnY.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 22, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1556_1_0"}, {"texts": ["The man starts walking towards the left side."], "durations": null, "exact_frames_per_prompt": [22], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/5BnL6kF7wnY.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 22, "npz_gt_video_start_frame": 22, "npz_gt_video_end_frame": 44, "skip_frames_after_generation": 22, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1556_1_1"}, {"texts": ["A person whose only is hand is visible holding the pencil and writing on the paper."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/0vBcF691jaU.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1557_0_0"}, {"texts": ["A woman wearing white clothes is sitting and holding a boy and watching the fish."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/00jZej9_xh8.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1558_0_0"}, {"texts": ["A boy wearing black clothes is standing and watching the fish while a woman wearing a lined t-shirt is sitting and is pointing towards the fishes, and a group of fishes is swimming in the pond."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/00jZej9_xh8.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1558_1_0"}, {"texts": ["A woman wearing a black cap is milking a white cow by hand."], "durations": null, "exact_frames_per_prompt": [37], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/18WuF8bnEjY.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 41, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_155_0_0"}, {"texts": ["A man wearing a t-shirt is standing and speaking."], "durations": null, "exact_frames_per_prompt": [38], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/18WuF8bnEjY.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 42, "npz_gt_video_start_frame": 42, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 42, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_155_2_ms_0"}, {"texts": ["A man wearing a shirt is standing and is filming while group of people are standing and looking at the front."], "durations": null, "exact_frames_per_prompt": [38], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/18WuF8bnEjY.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 42, "npz_gt_video_start_frame": 42, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 42, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_155_3_0"}, {"texts": ["A woman wearing an aqua-colored top is standing, taking out some butter from the container."], "durations": null, "exact_frames_per_prompt": [14], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-sU7pxYaj70.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 18, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1560_0_0"}, {"texts": ["The woman is applying butter to a bun with a knife."], "durations": null, "exact_frames_per_prompt": [62], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-sU7pxYaj70.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 18, "npz_gt_video_start_frame": 18, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 18, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1560_0_1"}, {"texts": ["A person wearing a green t-shirt is inserting a drill in the green apple."], "durations": null, "exact_frames_per_prompt": [46], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/1_jfz7DZgtA.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 50, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1564_0_0"}, {"texts": ["The person wearing a green t-shirt is peeling the apple with a peeler and the drill."], "durations": null, "exact_frames_per_prompt": [30], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/1_jfz7DZgtA.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 50, "npz_gt_video_start_frame": 50, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 50, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1564_0_1"}, {"texts": ["A man wearing a sky blue t-shirt is putting a drill bit in an apple."], "durations": null, "exact_frames_per_prompt": [29], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/1_jfz7DZgtA.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 33, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1565_0_0"}, {"texts": ["The man is picking a peeler."], "durations": null, "exact_frames_per_prompt": [18], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/1_jfz7DZgtA.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 33, "npz_gt_video_start_frame": 33, "npz_gt_video_end_frame": 52, "skip_frames_after_generation": 33, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1565_0_1"}, {"texts": ["The man is peeling the apple with a drill and a peeler."], "durations": null, "exact_frames_per_prompt": [28], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/1_jfz7DZgtA.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 52, "npz_gt_video_start_frame": 52, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 52, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1565_0_2"}, {"texts": ["A man wearing protective gear is walking."], "durations": null, "exact_frames_per_prompt": [36], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/0NAooWbXC-E.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 40, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1567_1_0"}, {"texts": ["The man leaning forward and spraying some liquid on the bees."], "durations": null, "exact_frames_per_prompt": [41], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/0NAooWbXC-E.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 40, "npz_gt_video_start_frame": 40, "npz_gt_video_end_frame": 81, "skip_frames_after_generation": 40, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1567_1_1"}, {"texts": ["A man wearing a green t-shirt is standing and giving the snake to a woman and the girl wearing blue top looks at the woman"], "durations": null, "exact_frames_per_prompt": [54], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/2HSFfjEXKlU.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 58, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1568_0_0"}, {"texts": ["A woman wearing black clothes is standing and taking the snake in her hands while the person in the green tshirt is handing over the snake, the child is touching it."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/2HSFfjEXKlU.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1568_1_0"}, {"texts": ["A girl wearing a blue top is standing and a girl is taking a snake from the man and a group of people are standing on the backside."], "durations": null, "exact_frames_per_prompt": [64], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/2HSFfjEXKlU.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 68, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1568_2_0"}, {"texts": ["The girl is touching the snake."], "durations": null, "exact_frames_per_prompt": [12], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/2HSFfjEXKlU.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 68, "npz_gt_video_start_frame": 68, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 68, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1568_2_1"}, {"texts": ["A snake is in the hands of a man then getting lifted by a woman while a few people are sitting, standing, and moving at the back and later a girl in blue frock in front is touching the snake."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/2HSFfjEXKlU.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1568_3_0"}, {"texts": ["A girl wearing a black cloth is sitting and touching a cow teat and milking while a person whose hand is visible is holding a bucket."], "durations": null, "exact_frames_per_prompt": [37], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/18WuF8bnEjY.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 41, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_156_0_0"}, {"texts": ["A man wearing a grey printed t-shirt is standing and holding a steel bucket while a girl is milking a cow while another man is standing on the left side and talking, and some other people are standing at the back."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/18WuF8bnEjY.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_156_1_0"}, {"texts": ["A group of three people wearing blue clothes is standing just behind the man."], "durations": null, "exact_frames_per_prompt": [38], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/18WuF8bnEjY.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 42, "npz_gt_video_start_frame": 42, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 42, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_156_2_ms_0"}, {"texts": ["A black-white cow is standing and touched by a girl while milking and a man is holding the bucket and some men are standing and looking in front."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/18WuF8bnEjY.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_156_4_0"}, {"texts": ["A person whose hands are visible is drilling a book with a black object."], "durations": null, "exact_frames_per_prompt": [31], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/1goca_GPniA.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 35, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1573_0_0"}, {"texts": ["A person whose hands are visible is sewing a book."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/1goca_GPniA.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1573_1_0"}, {"texts": ["A person whom hands are visible is putting a thread in a book."], "durations": null, "exact_frames_per_prompt": [29], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/1goca_GPniA.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 35, "npz_gt_video_start_frame": 35, "npz_gt_video_end_frame": 64, "skip_frames_after_generation": 35, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1573_2_ms_0"}, {"texts": ["A person whose only hand is visible is giving instruction on how to make a loop on tie with one hand, by keeping the tie on the gray surface."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-NMs4aSNWK8.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1574_0_0"}, {"texts": ["A woman wearing a black t-shirt is speaking and holding a coin."], "durations": null, "exact_frames_per_prompt": [29], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-h9Ad_W4ZTs.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 33, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1577_0_0"}, {"texts": ["The woman is touching a black tyre."], "durations": null, "exact_frames_per_prompt": [35], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-h9Ad_W4ZTs.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 33, "npz_gt_video_start_frame": 33, "npz_gt_video_end_frame": 68, "skip_frames_after_generation": 33, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1577_0_1"}, {"texts": ["A person whose only hand is visible is making food and steering with the help of a non-stick spatula."], "durations": null, "exact_frames_per_prompt": [51], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-JzjRAceuAk.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 20, "npz_gt_video_start_frame": 20, "npz_gt_video_end_frame": 71, "skip_frames_after_generation": 20, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1578_0_0"}, {"texts": ["A man wearing a black t-shirt is standing behind the podium while in a group of people some are sitting, some are standing, and one man is walking."], "durations": null, "exact_frames_per_prompt": [41], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/4oy8HrY_FtE.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 45, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_157_0_0"}, {"texts": ["The man is speaking on the mike."], "durations": null, "exact_frames_per_prompt": [35], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/4oy8HrY_FtE.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 45, "npz_gt_video_start_frame": 45, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 45, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_157_0_1"}, {"texts": ["A man wearing a black flannel jacket is catching a fish with a black net while another person on the right in a gray checked shirt is holding a fishing rod and moving back."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/0RBWX9VRzaU.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1581_0_0"}, {"texts": ["A person whose only a hand is visible is holding a fishing rod while a man in white and black hoodies is holding a scoop net and grabbing the fish in the water."], "durations": null, "exact_frames_per_prompt": [9], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/0RBWX9VRzaU.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 13, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1581_1_0"}, {"texts": ["A man wearing a white shirt, black pants and a yellow tie is standing, holding a beer bottle in his right hand and the other in his left hand."], "durations": null, "exact_frames_per_prompt": [35], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/QZPZ8p9eNDs.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 39, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1582_0_0"}, {"texts": ["The man starts opening the right hand beer bottle from the left hand beer bottle."], "durations": null, "exact_frames_per_prompt": [41], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/QZPZ8p9eNDs.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 39, "npz_gt_video_start_frame": 39, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 39, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1582_0_1"}, {"texts": ["A person wearing a white shirt and a yellow tie is standing, holding the green bottles in their hands."], "durations": null, "exact_frames_per_prompt": [20], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/QZPZ8p9eNDs.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 24, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1583_0_0"}, {"texts": ["The person starts moving the green bottle in their left hand."], "durations": null, "exact_frames_per_prompt": [56], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/QZPZ8p9eNDs.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 24, "npz_gt_video_start_frame": 24, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 24, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1583_0_1"}, {"texts": ["A man wearing a black shirt is standing on the left side of the first man, holding a white t-shirt while a man wearing black t-shirt is speaking on mic."], "durations": null, "exact_frames_per_prompt": [45], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/4oy8HrY_FtE.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 49, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_158_1_0"}, {"texts": ["The man is opening a bag while some people are sitting and they are doing different activity."], "durations": null, "exact_frames_per_prompt": [31], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/4oy8HrY_FtE.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 49, "npz_gt_video_start_frame": 49, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 49, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_158_1_1"}, {"texts": ["A man wearing a green t-shirt is standing and talking with the other man while a group of people are standing then starts walking and few are sitting."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/4oy8HrY_FtE.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_158_2_0"}, {"texts": ["A woman wearing a white black striped dress is standing, moving her hands and speaking."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/0BQk_d6Pjwg.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1591_0_0"}, {"texts": ["A man wearing a black t-shirt is standing behind the countertop."], "durations": null, "exact_frames_per_prompt": [13], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/0mIwaOD8fVI.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 17, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1592_0_0"}, {"texts": ["The man is stirring ice cubes and wine in a big glass jar with the help of a bar spoon."], "durations": null, "exact_frames_per_prompt": [63], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/0mIwaOD8fVI.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 17, "npz_gt_video_start_frame": 17, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 17, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1592_0_1"}, {"texts": ["A black bird is eating an earthworm."], "durations": null, "exact_frames_per_prompt": [38], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/0GKFIzTU680.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 42, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1594_0_0"}, {"texts": ["The bird starts hopping."], "durations": null, "exact_frames_per_prompt": [38], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/0GKFIzTU680.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 42, "npz_gt_video_start_frame": 42, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 42, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1594_0_1"}, {"texts": ["An earthworm is being eaten by the black bird."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/0GKFIzTU680.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1594_1_0"}, {"texts": ["A worm on the green surface is eaten by a black bird."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/0GKFIzTU680.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1595_0_0"}, {"texts": ["A black bird is eating a worm."], "durations": null, "exact_frames_per_prompt": [39], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/0GKFIzTU680.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 43, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1595_1_0"}, {"texts": ["The black bird is walking."], "durations": null, "exact_frames_per_prompt": [37], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/0GKFIzTU680.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 43, "npz_gt_video_start_frame": 43, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 43, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1595_1_1"}, {"texts": ["A girl wearing a blue and pink top is shredding some papers while another girl in a printed pink top is picking paper from a box."], "durations": null, "exact_frames_per_prompt": [24], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/8stsrg72N1A.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 28, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_159_0_0"}, {"texts": ["The girl picks up some other papers with another girl."], "durations": null, "exact_frames_per_prompt": [52], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/8stsrg72N1A.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 28, "npz_gt_video_start_frame": 28, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 28, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_159_0_1"}, {"texts": ["A girl wearing a pink and white top is picking up some papers from the box while another girl wearing a pink and blue frock is moving backward and picking up some papers from the box."], "durations": null, "exact_frames_per_prompt": [48], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/8stsrg72N1A.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 52, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_159_1_0"}, {"texts": ["The girl is shredding them and the other girl is coming forward with some papers."], "durations": null, "exact_frames_per_prompt": [28], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/8stsrg72N1A.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 52, "npz_gt_video_start_frame": 52, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 52, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_159_1_1"}, {"texts": ["A baby whose upper half-body is visible wearing a white top and a pink flower printed blue cap is sitting on the woman's lap while holding the watermelon and eating the watermelon."], "durations": null, "exact_frames_per_prompt": [61], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/19fuk7JJkIc.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 65, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_15_1_0"}, {"texts": ["A black dog is walking on the gray surface."], "durations": null, "exact_frames_per_prompt": [31], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-6wkYqjFei0.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 35, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1606_1_0"}, {"texts": ["The dog walks on the green surface."], "durations": null, "exact_frames_per_prompt": [11], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-6wkYqjFei0.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 35, "npz_gt_video_start_frame": 35, "npz_gt_video_end_frame": 46, "skip_frames_after_generation": 35, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1606_1_1"}, {"texts": ["The dog starts sitting there."], "durations": null, "exact_frames_per_prompt": [34], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-6wkYqjFei0.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 46, "npz_gt_video_start_frame": 46, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 46, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1606_1_2"}, {"texts": ["A man wearing light colored clothes is fishing with a fishing rod and is sitting on a chair while a man wearing a black t-shirt is sitting on the left side, a woman wearing a cap is sitting and looking back, and another woman wearing a blue vest is sitting on the right side and taking a net from the back."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/0myrAsWhMBs.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1608_0_0"}, {"texts": ["A man on the right, wearing a blue t-shirt is sitting on a bench while another man wearing a purple shirt is fishing, a woman and the third man are sitting on the bench."], "durations": null, "exact_frames_per_prompt": [30], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/0myrAsWhMBs.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 34, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1608_1_0"}, {"texts": ["The man is picking up a fish net."], "durations": null, "exact_frames_per_prompt": [45], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/0myrAsWhMBs.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 34, "npz_gt_video_start_frame": 34, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 34, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1608_1_1"}, {"texts": ["A man on the left is sitting on the bench and is looking in the front while the other people are involved in other works."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/0myrAsWhMBs.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1608_3_0"}, {"texts": ["A man wearing a black shirt is sitting on a grey seat while another man wearing a grey shirt is sitting and holding a fishing rod, a woman on the right side of the first man wearing a multicolour vest is sitting and another woman on the right side of the second man wearing a blue vest is sitting and picks up a fishing net from behind."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/0myrAsWhMBs.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1609_0_0"}, {"texts": ["A man wearing a white shirt is sitting on a grey seat and holding a fishing rod while another man wearing a blue vest is sitting, he turns around and grabs a fishing net, and another two men are sitting on the left side."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/0myrAsWhMBs.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1609_1_0"}, {"texts": ["A woman wearing a blue-white top is sitting on the grey seat while the woman in blue t-shirt turn back."], "durations": null, "exact_frames_per_prompt": [25], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/0myrAsWhMBs.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 29, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1609_2_0"}, {"texts": ["The woman looks back while the woman in blue t-shirt takes the fishing rod."], "durations": null, "exact_frames_per_prompt": [8], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/0myrAsWhMBs.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 29, "npz_gt_video_start_frame": 29, "npz_gt_video_end_frame": 37, "skip_frames_after_generation": 29, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1609_2_1"}, {"texts": ["A woman wearing a blue vest is sitting on the grey seat while a person is sitting and holding a fishing stick, another two people are sitting on the left side."], "durations": null, "exact_frames_per_prompt": [22], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/0myrAsWhMBs.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 26, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1609_3_0"}, {"texts": ["The woman turns back and takes a net stick from behind."], "durations": null, "exact_frames_per_prompt": [53], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/0myrAsWhMBs.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 26, "npz_gt_video_start_frame": 26, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 26, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1609_3_1"}, {"texts": ["A man is unscrewing a car tire."], "durations": null, "exact_frames_per_prompt": [68], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/376IqO2k6Vs.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 72, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1610_0_0"}, {"texts": ["The man is measuring the pressure with a tire pressure gauge."], "durations": null, "exact_frames_per_prompt": [8], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/376IqO2k6Vs.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 72, "npz_gt_video_start_frame": 72, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 72, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1610_0_1"}, {"texts": ["A girl wearing a white-black sweater is standing and pressing the deer."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/1p2Cdi_BaX8.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1613_0_0"}, {"texts": ["A brown deer is standing and getting caressed by the girl while a group of people are walking towards the right side and looking at the deer."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/1p2Cdi_BaX8.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1613_1_0"}, {"texts": ["A boy wearing a black t-shirt is cleaning a brown horse with a brush."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-9p3_PenmWA.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1614_0_0"}, {"texts": ["A brown horse is standing and getting cleaned by a boy."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-9p3_PenmWA.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1614_1_0"}, {"texts": ["A man wearing a striped t-shirt is peeling a green apple which is inserted in a drilling machine."], "durations": null, "exact_frames_per_prompt": [18], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/1qSZDBpv6lM.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 22, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1616_0_0"}, {"texts": ["The man picks up another apple."], "durations": null, "exact_frames_per_prompt": [9], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/1qSZDBpv6lM.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 22, "npz_gt_video_start_frame": 22, "npz_gt_video_end_frame": 31, "skip_frames_after_generation": 22, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1616_0_1"}, {"texts": ["The man inserted the drilling machine."], "durations": null, "exact_frames_per_prompt": [23], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/1qSZDBpv6lM.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 31, "npz_gt_video_start_frame": 31, "npz_gt_video_end_frame": 54, "skip_frames_after_generation": 31, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1616_0_2"}, {"texts": ["The man starts peeling the apple."], "durations": null, "exact_frames_per_prompt": [26], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/1qSZDBpv6lM.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 54, "npz_gt_video_start_frame": 54, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 54, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1616_0_3"}, {"texts": ["A black-brown dog is walking on the sidewalk and is peeing on the pole."], "durations": null, "exact_frames_per_prompt": [24], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/0K58XC5z9xU.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 28, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1617_0_0"}, {"texts": ["The dog is walking on the sidewalk again."], "durations": null, "exact_frames_per_prompt": [52], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/0K58XC5z9xU.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 28, "npz_gt_video_start_frame": 28, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 28, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1617_0_1"}, {"texts": ["A man wearing green clothes is eating a burger. "], "durations": null, "exact_frames_per_prompt": [61], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/2KadJ3JNKOE.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 65, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1618_0_0"}, {"texts": ["The man is looking around."], "durations": null, "exact_frames_per_prompt": [15], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/2KadJ3JNKOE.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 65, "npz_gt_video_start_frame": 65, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 65, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1618_0_1"}, {"texts": ["A woman wearing an orange t-shirt is sitting on a chair and moving her head while the other man wearing blue suit reading something and putting his spectacles on table."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/03WF4D2tlsU.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1619_0_0"}, {"texts": ["A man wearing a black suit sitting on the right in a chair, moving his hand while a girl is sitting on the right side and a group of people are sitting on the left side."], "durations": null, "exact_frames_per_prompt": [39], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/03WF4D2tlsU.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 43, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1619_1_0"}, {"texts": ["The man wearing a black suit is putting his hand on his mouth."], "durations": null, "exact_frames_per_prompt": [37], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/03WF4D2tlsU.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 43, "npz_gt_video_start_frame": 43, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 43, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1619_1_1"}, {"texts": ["A man also wearing a black suit sitting on the left in a chair with his hand on his mouth on the right side, a man and woman are watching the laptop, and she speaks."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/03WF4D2tlsU.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1619_2_0"}, {"texts": ["A woman wearing a white top is riding a brown horse while horses are standing on the brown surface."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/0H1wFN2EqvM.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1620_0_0"}, {"texts": ["A brown horse is walking on the brown surface and is carrying the woman and a cow and another horse are standing on the backside."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/0H1wFN2EqvM.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1620_1_0"}, {"texts": ["A woman wearing white clothes is holding a fry pan and tossing the food."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/2KRLuQKgS8s.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1621_0_0"}, {"texts": ["A man wearing a gray shirt and black suspenders is sitting and speaking and eating from the bowl while another man in a white t-shirt is sitting on the right, the third man wearing a red shirt is sitting on the left and eating and a woman in a black outfit is sitting and eating from the bowl."], "durations": null, "exact_frames_per_prompt": [54], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/2hH0ZLObjxg.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 26, "npz_gt_video_start_frame": 26, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 26, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1622_2_0"}, {"texts": ["A man wearing white clothes is sitting and speaking and eating from the bowl while a man on the left side in a grey shirt is sitting and moves food towards him."], "durations": null, "exact_frames_per_prompt": [15], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/2hH0ZLObjxg.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 26, "npz_gt_video_start_frame": 26, "npz_gt_video_end_frame": 41, "skip_frames_after_generation": 26, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1622_3_0"}, {"texts": ["A man wearing red-black clothes is sitting and speaking and eating from the bowl while a woman wearing a black cloth is sitting left to the man and speaking while eating from the bowl."], "durations": null, "exact_frames_per_prompt": [38], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/2hH0ZLObjxg.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 42, "npz_gt_video_start_frame": 42, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 42, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1622_4_0"}, {"texts": ["A woman wearing black clothes is sitting and speaking while a boy wearing a red shirt is sitting beside the woman, speaking, and picking something from the bowl."], "durations": null, "exact_frames_per_prompt": [19], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/2hH0ZLObjxg.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 42, "npz_gt_video_start_frame": 42, "npz_gt_video_end_frame": 61, "skip_frames_after_generation": 42, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1622_5_0"}, {"texts": ["The woman is eating from the bowl while the boy also eats something from the bowl."], "durations": null, "exact_frames_per_prompt": [19], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/2hH0ZLObjxg.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 61, "npz_gt_video_start_frame": 61, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 61, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1622_5_1"}, {"texts": ["A man wearing a grey t-shirt is sitting and holding a glass in his hand while the other woman holding the glass reacts."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/25GbIR8P2oc.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1624_0_0"}, {"texts": ["A woman wearing a white t-shirt is sitting on the left side holding a glass and drinking with a man sitting right to her wearing grey t-shirt and drinking"], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/25GbIR8P2oc.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1624_1_0"}, {"texts": ["A boy wearing a blue shirt is in the lap of a woman, moving his hands and touching a woman."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-U4_Srszlsg.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1625_0_0"}, {"texts": ["A woman wearing an orange t-shirt is walking while holding a boy."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-U4_Srszlsg.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1625_1_0"}, {"texts": ["A man wearing a white t-shirt and spectacles is standing and counting the notes."], "durations": null, "exact_frames_per_prompt": [29], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/1IrwPtbW3kM.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 33, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1626_0_0"}, {"texts": ["The man is putting notes on a black slab."], "durations": null, "exact_frames_per_prompt": [12], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/1IrwPtbW3kM.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 33, "npz_gt_video_start_frame": 33, "npz_gt_video_end_frame": 45, "skip_frames_after_generation": 33, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1626_0_1"}, {"texts": ["The man is making them into a bundle."], "durations": null, "exact_frames_per_prompt": [35], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/1IrwPtbW3kM.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 45, "npz_gt_video_start_frame": 45, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 45, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1626_0_2"}, {"texts": ["A woman wearing a white t-shirt and black pants is sitting on the back of the camel and riding while moving her hands up and down in the air while a man wearing a black t-shirt is sitting behind the woman and riding the camel and another man wearing a brown shorts is holding the camel's rope and moving to the right side."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/0Vx6pI74Y8o.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1627_0_0"}, {"texts": ["A man wearing a black t-shirt and graphic white-black pants is sitting behind the first woman on the back of the camel and riding while a man in a green T-shirt is walking along with camel."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/0Vx6pI74Y8o.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1627_1_0"}, {"texts": ["A man wearing a green t-shirt and brown pants is walking in front of the camel while holding the leash of the camel while a man in a black t-shirt sits on the camel's back, a woman in a white shirt sits on the front, and a woman in a black t-shirt stands on the left side, looking at the camel."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/0Vx6pI74Y8o.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1627_2_0"}, {"texts": ["A woman wearing a black vest and gray pants is standing behind the wooden fence and holding something while her hands up while a baby in the pram trying to hold the woman, a woman wearing white top and a man wearing black t-shirt is ridding a camel while moving their hands up, a person wearing a green t-shirt is walking from left to right while holding a camel's string."], "durations": null, "exact_frames_per_prompt": [42], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/0Vx6pI74Y8o.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 32, "npz_gt_video_start_frame": 32, "npz_gt_video_end_frame": 74, "skip_frames_after_generation": 32, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1627_3_0"}, {"texts": ["A brown camel is walking behind the second man who is holding a leash, and carrying two people on his back while a woman wearing black clothes is standing near the wooden barricade and a kid is in the baby trolley."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/0Vx6pI74Y8o.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1627_4_0"}, {"texts": ["A man wearing off white clothes is sitting and chopping mince using a butcher knife on a wooden piece."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/4V6jmKnuW6Y.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_162_0_0"}, {"texts": ["A white dog is walking."], "durations": null, "exact_frames_per_prompt": [50], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-M9djkf2nL4.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 54, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1630_0_0"}, {"texts": ["The dog picks a wood piece, and starts running.\n"], "durations": null, "exact_frames_per_prompt": [26], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-M9djkf2nL4.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 54, "npz_gt_video_start_frame": 54, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 54, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1630_0_1"}, {"texts": ["A person wearing gray clothes is standing and holding a frying grill and moving it."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/3wi4ZNJRrdY.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1632_0_0"}, {"texts": ["A woman wearing a white t-shirt is standing and speaking."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/1VG5sw9lSJs.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1634_0_0"}, {"texts": ["A man wearing a purple t-shirt and blue shorts is standing on a boat and pulling a fishing line while group of people are sitting and standing on boat and they are doing different activities."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-4ZqQgKrfUY.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1636_0_0"}, {"texts": ["A fish is being caught and pulled out of the water by the man and in a group of people, one man is pulling wire, some are standing, and some are sitting on the boat."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-4ZqQgKrfUY.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1636_2_0"}, {"texts": ["A woman, whose hands are visible, is folding a white napkin on the wooden surface."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/1EIKMlqpke4.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_163_0_0"}, {"texts": ["A brown camel is standing on the ground while other camel sits on the ground."], "durations": null, "exact_frames_per_prompt": [58], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/0DdXhnmSEbE.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 62, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1641_1_0"}, {"texts": ["The camel sits while others are watching it."], "durations": null, "exact_frames_per_prompt": [18], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/0DdXhnmSEbE.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 62, "npz_gt_video_start_frame": 62, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 62, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1641_1_1"}, {"texts": ["A man wearing a brown-black jacket comes a woman wearing green top sitting front on the camel"], "durations": null, "exact_frames_per_prompt": [9], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/0DdXhnmSEbE.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 13, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1641_2_0"}, {"texts": ["The man wearing a brown-black jacket gives command to the camel to sit a woman wearing green top sitting top  scared and scream loudly"], "durations": null, "exact_frames_per_prompt": [67], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/0DdXhnmSEbE.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 13, "npz_gt_video_start_frame": 13, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 13, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1641_2_1"}, {"texts": ["A man wearing a white dishdasha is sitting on a camel while in back side 2 people are sitting on camel and man wearing cloth headgear is holding the rope of the camel."], "durations": null, "exact_frames_per_prompt": [43], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/0DdXhnmSEbE.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 47, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1641_4_0"}, {"texts": ["The man is getting down while the other camel sits on the sand surface."], "durations": null, "exact_frames_per_prompt": [33], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/0DdXhnmSEbE.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 47, "npz_gt_video_start_frame": 47, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 47, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1641_4_1"}, {"texts": ["A woman wearing a green vest and gray trousers is sitting in front of a camel while another woman in black outfit is sitting behind the first woman and two men are sitting on the other camel and another man in a scarf comes and guides the camel to sit down."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/0DdXhnmSEbE.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1641_5_0"}, {"texts": ["A woman wearing a black t-shirt and black trousers is sitting in the back on a camel while another woman wearing grey trousers is sitting in front of the woman and another camel is standing while carrying two men, it sits down and the men get off it, and another man wearing a brown-black jacket comes from the left to help the woman."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/0DdXhnmSEbE.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1641_6_0"}, {"texts": ["A man wearing blue pattern cloth is sitting and talking to a girl while a kid on the right side of a man is eating food."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-ebVdJQiRps.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1645_0_0"}, {"texts": ["A girl wearing pink clothes is sitting and talking to the man while a kid wearing white clothes is sitting on the chair and eating food."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-ebVdJQiRps.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1645_1_0"}, {"texts": ["A baby wearing white clothes is sitting and holding food while a man in a blue-white shirt showing food to another baby and looking into the front camera."], "durations": null, "exact_frames_per_prompt": [57], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-ebVdJQiRps.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 61, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1645_2_0"}, {"texts": ["The baby is eating it while a man took his hand back from the table"], "durations": null, "exact_frames_per_prompt": [19], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-ebVdJQiRps.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 61, "npz_gt_video_start_frame": 61, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 61, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1645_2_1"}, {"texts": ["A boy wearing red clothes is sitting in the middle and laughing while a woman wearing a black t-shirt is sitting on the right side and eating, another woman wearing a red t-shirt is sitting on the left side and laughing."], "durations": null, "exact_frames_per_prompt": [63], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-8KaTipBlBY.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 67, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_164_0_0"}, {"texts": ["The boy is eating."], "durations": null, "exact_frames_per_prompt": [14], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-8KaTipBlBY.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 67, "npz_gt_video_start_frame": 67, "npz_gt_video_end_frame": 81, "skip_frames_after_generation": 67, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_164_0_1"}, {"texts": ["A woman on the right side wearing black clothes is sitting, laughing and eating while another woman wearing a maroon top and a man wearing a red t-shirt are also sitting, laughing, and eating."], "durations": null, "exact_frames_per_prompt": [77], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-8KaTipBlBY.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 81, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_164_1_0"}, {"texts": ["A woman on the left side wearing red clothes is sitting, laughing and speaking while a man wearing red t-shirt is sitting in the middle laughing, speaking while leaning forward on the table and eating something, a woman wearing black clothe sitting in the right side and eating, laughing while looking at the man ."], "durations": null, "exact_frames_per_prompt": [77], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-8KaTipBlBY.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 81, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_164_2_0"}, {"texts": ["A man wearing a suit is standing in the middle of the people, and speaking on the mic while holding a notebook in his hand."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/49DOI4q5Egc.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1650_0_0"}, {"texts": ["A man wearing a black suit is standing in the center of the group of people, holding a paper clip board in his hand and speaking on the mic while moving his hand with the paper clip board while in a group of people, some are sitting and some are standing, one is holding a camera and clicking pictures."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/49DOI4q5Egc.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1651_1_0"}, {"texts": ["A woman wearing a black jacket and black jeans is standing while holding a camera and recording the first man while a woman wearing black clothes is walking from right to left."], "durations": null, "exact_frames_per_prompt": [45], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/49DOI4q5Egc.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 49, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1651_2_0"}, {"texts": ["The woman stands and looks at the first man while a group of people are sitting, standing and walking around."], "durations": null, "exact_frames_per_prompt": [30], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/49DOI4q5Egc.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 49, "npz_gt_video_start_frame": 49, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 49, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1651_2_1"}, {"texts": ["A man wearing a black suit is standing while holding a laptop behind the first woman and looking at the first man while a group of people are sitting and watching the man who is speaking into the mic, while one person is standing in the back and some people are walking in the front."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/49DOI4q5Egc.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1651_5_0"}, {"texts": ["A woman wearing a white top and blue shorts is standing in hitting a short position."], "durations": null, "exact_frames_per_prompt": [37], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/0MEz65O4XiU.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 41, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1654_0_ms_0"}, {"texts": ["The woman started celebrating."], "durations": null, "exact_frames_per_prompt": [19], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/0MEz65O4XiU.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 41, "npz_gt_video_start_frame": 41, "npz_gt_video_end_frame": 60, "skip_frames_after_generation": 41, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1654_0_ms_1"}, {"texts": ["A woman wearing a white-black vest is standing, talking, and moving her hand a group of people are sitting and standing behind her, along with a man who is taking a golf shot."], "durations": null, "exact_frames_per_prompt": [18], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/0MEz65O4XiU.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 62, "npz_gt_video_start_frame": 62, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 62, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1654_1_ms_0"}, {"texts": ["A person is hitting a golf shot in the back as a lady wearing a black-white top is talking while looking at the front."], "durations": null, "exact_frames_per_prompt": [18], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/0MEz65O4XiU.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 62, "npz_gt_video_start_frame": 62, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 62, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1654_2_ms_0"}, {"texts": ["A girl wearing a red hoodie is standing."], "durations": null, "exact_frames_per_prompt": [10], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-5_ouPNg-x0.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 14, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1658_0_0"}, {"texts": ["The girl is combing a black horse on the muddy surface."], "durations": null, "exact_frames_per_prompt": [66], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-5_ouPNg-x0.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 14, "npz_gt_video_start_frame": 14, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 14, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1658_0_1"}, {"texts": ["A woman wearing maroon cloth is standing and holding the rope of a horse."], "durations": null, "exact_frames_per_prompt": [10], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-5_ouPNg-x0.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 14, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1659_0_0"}, {"texts": ["The woman is combing the hairs of a horse."], "durations": null, "exact_frames_per_prompt": [66], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-5_ouPNg-x0.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 14, "npz_gt_video_start_frame": 14, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 14, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1659_0_1"}, {"texts": ["A man wearing a black t-shirt is standing on the brown surface, feeding the group of birds while throwing the food in the air."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-5VuqHbo5nE.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1661_0_0"}, {"texts": ["A woman wearing a red apron is standing, and picking a glass bowl from the counter top while a man on the left is mixing food with the spatula."], "durations": null, "exact_frames_per_prompt": [11], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/48XaKbtNDGs.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 15, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1662_0_0"}, {"texts": ["A man wearing black clothes is standing and cooking vegetables with a wooden spatula while the woman is taking the bowl"], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/48XaKbtNDGs.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1662_1_0"}, {"texts": ["A man wearing a red and black shirt is tightening up the tire nozzle."], "durations": null, "exact_frames_per_prompt": [21], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/0VUklVS4Qf8.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 25, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1663_0_0"}, {"texts": ["The man wearing a red and black shirt is touching the wheel of the black car."], "durations": null, "exact_frames_per_prompt": [54], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/0VUklVS4Qf8.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 25, "npz_gt_video_start_frame": 25, "npz_gt_video_end_frame": 79, "skip_frames_after_generation": 25, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1663_0_1"}, {"texts": ["A person wearing a black top is putting a fuel hose in a fuel station."], "durations": null, "exact_frames_per_prompt": [39], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/0hr3KZf0r-k.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 43, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1664_0_0"}, {"texts": ["The person is moving."], "durations": null, "exact_frames_per_prompt": [22], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/0hr3KZf0r-k.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 43, "npz_gt_video_start_frame": 43, "npz_gt_video_end_frame": 65, "skip_frames_after_generation": 43, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1664_0_1"}, {"texts": ["A brown-white horse is standing on the grey surface and getting its eye and nostril wiped by the first man."], "durations": null, "exact_frames_per_prompt": [46], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/3lqRo6vLYRU.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 50, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1667_2_0"}, {"texts": ["The brown-white horse is getting its tail held by the first man."], "durations": null, "exact_frames_per_prompt": [30], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/3lqRo6vLYRU.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 50, "npz_gt_video_start_frame": 50, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 50, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1667_2_1"}, {"texts": ["A child wearing a black t-shirt and covering his body with a white cloth is eating a watermelon while group of people are sitting, some of them are walking at the backside."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/31JNlhNBf7c.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1670_1_0"}, {"texts": ["A woman wearing a purple-black top and black pants is holding some papers and walking here and there while in front the boy eating watermelon without using hands and turn his head right side looks at her."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/31JNlhNBf7c.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1670_2_0"}, {"texts": ["A man wearing a blue t-shirt is standing, holding a glass of beer in his right hand."], "durations": null, "exact_frames_per_prompt": [39], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/4BEpcH2XyaY.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 43, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1671_0_0"}, {"texts": ["The man drinks the beer in one sip."], "durations": null, "exact_frames_per_prompt": [31], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/4BEpcH2XyaY.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 43, "npz_gt_video_start_frame": 43, "npz_gt_video_end_frame": 74, "skip_frames_after_generation": 43, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1671_0_1"}, {"texts": ["A man wearing a blue t-shirt is standing, holding a glass of beer in his right hand."], "durations": null, "exact_frames_per_prompt": [41], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/4BEpcH2XyaY.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 45, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1672_0_0"}, {"texts": ["The man is drinking the beer."], "durations": null, "exact_frames_per_prompt": [29], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/4BEpcH2XyaY.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 45, "npz_gt_video_start_frame": 45, "npz_gt_video_end_frame": 74, "skip_frames_after_generation": 45, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1672_0_1"}, {"texts": ["A person whose only hands are visible is holding and pouring motor oil into a car's engine."], "durations": null, "exact_frames_per_prompt": [58], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/09n25FNmN-0.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 22, "npz_gt_video_start_frame": 22, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 22, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_167_0_ms_0"}, {"texts": ["A girl in pink and white clothing is folding the blanket kept on the bed."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/hYmPxuWZJSY.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1681_0_0"}, {"texts": ["A man wearing a black shirt is standing."], "durations": null, "exact_frames_per_prompt": [24], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-d4yuGuednI.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 28, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1690_0_0"}, {"texts": ["The man puts a tie on."], "durations": null, "exact_frames_per_prompt": [52], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-d4yuGuednI.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 28, "npz_gt_video_start_frame": 28, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 28, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1690_0_1"}, {"texts": ["A cyclist wearing a green helmet is at first standing on the left side while getting the cycle repaired while a group of cyclists and a group of vehicles are moving on the road."], "durations": null, "exact_frames_per_prompt": [54], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/2C92mxWhc04.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 58, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1691_1_0"}, {"texts": ["The cyclist starts cycling on the path while being pushed from behind while a man in blue jeans pushes him and a group of people are standing."], "durations": null, "exact_frames_per_prompt": [22], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/2C92mxWhc04.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 58, "npz_gt_video_start_frame": 58, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 58, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1691_1_1"}, {"texts": ["A man wearing a white helmet is sitting behind the man riding a motor cycle while in a group of people, some people are riding bicycles, and the rest are standing on the left side and looking at the group of cars going in forward directions."], "durations": null, "exact_frames_per_prompt": [40], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/2C92mxWhc04.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 40, "npz_gt_video_start_frame": 40, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 40, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1691_6_0"}, {"texts": ["A man wearing green clothes is at first repairing the cycle a man wearing red clothes is at last riding cycle behind the car"], "durations": null, "exact_frames_per_prompt": [67], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/2C92mxWhc04.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 71, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1691_7_0"}, {"texts": ["The man wearing green clothes is then running behind the cyclist while pushing the cycle."], "durations": null, "exact_frames_per_prompt": [9], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/2C92mxWhc04.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 71, "npz_gt_video_start_frame": 71, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 71, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1691_7_1"}, {"texts": ["A man wearing a red shirt and black pants is making a bed."], "durations": null, "exact_frames_per_prompt": [50], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/Ith8Lw-yeis.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 54, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1692_0_0"}, {"texts": ["The man walks towards the right."], "durations": null, "exact_frames_per_prompt": [11], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/Ith8Lw-yeis.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 54, "npz_gt_video_start_frame": 54, "npz_gt_video_end_frame": 65, "skip_frames_after_generation": 54, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1692_0_1"}, {"texts": ["The man takes a blanket and puts it on the bed."], "durations": null, "exact_frames_per_prompt": [15], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/Ith8Lw-yeis.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 65, "npz_gt_video_start_frame": 65, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 65, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1692_0_2"}, {"texts": ["A man wearing a yellow cap is blowing air in the inflating animal design balloon through his mouth."], "durations": null, "exact_frames_per_prompt": [28], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/2FiQtUVHJvE.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 32, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1693_0_0"}, {"texts": ["A person whom hands are only visible is taking a potato from the water. "], "durations": null, "exact_frames_per_prompt": [11], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/45cAm-ss4xM.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 15, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1694_0_0"}, {"texts": ["The person starts peeling it."], "durations": null, "exact_frames_per_prompt": [30], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/45cAm-ss4xM.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 15, "npz_gt_video_start_frame": 15, "npz_gt_video_end_frame": 45, "skip_frames_after_generation": 15, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1694_0_1"}, {"texts": ["A man wearing a gray t-shirt is sitting on the right side of the woman, watching her and laughing while a woman is drinking something from the glass then wipes her mouth, and a group of people are sitting at the back."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/3jhoSl18LDI.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1695_1_0"}, {"texts": ["A baby sitting on a white baby chair is at first eating bread."], "durations": null, "exact_frames_per_prompt": [20], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/14AZeSMkm-M.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 24, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1696_0_0"}, {"texts": ["The baby puts the bread on the tray."], "durations": null, "exact_frames_per_prompt": [28], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/14AZeSMkm-M.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 24, "npz_gt_video_start_frame": 24, "npz_gt_video_end_frame": 53, "skip_frames_after_generation": 24, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1696_0_1"}, {"texts": ["The baby is pointing the bread with his finger while looking on the right side."], "durations": null, "exact_frames_per_prompt": [26], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/14AZeSMkm-M.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 53, "npz_gt_video_start_frame": 53, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 53, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1696_0_2"}, {"texts": ["A woman wearing a light green tank-top and grey pants is standing while holding a fry pan."], "durations": null, "exact_frames_per_prompt": [10], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/2jhBB7aXIDI.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 14, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_169_0_0"}, {"texts": ["The woman is tossing the chapati."], "durations": null, "exact_frames_per_prompt": [43], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/2jhBB7aXIDI.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 14, "npz_gt_video_start_frame": 14, "npz_gt_video_end_frame": 57, "skip_frames_after_generation": 14, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_169_0_1"}, {"texts": ["Then the chapati falls on the surface."], "durations": null, "exact_frames_per_prompt": [4], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/2jhBB7aXIDI.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 57, "npz_gt_video_start_frame": 57, "npz_gt_video_end_frame": 61, "skip_frames_after_generation": 57, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_169_0_2"}, {"texts": ["The woman is tilting her body to pick the chapati from the surface."], "durations": null, "exact_frames_per_prompt": [19], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/2jhBB7aXIDI.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 61, "npz_gt_video_start_frame": 61, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 61, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_169_0_3"}, {"texts": ["A baby boy wearing a blue vest is standing, watching the goats and speaking while a man wearing a red t-shirt is standing and feeding the goat with his hand."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/0cu3HHsVCGE.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_16_0_0"}, {"texts": ["A man wearing a red t-shirt and gray shorts is standing and feeding the brown goat while a kid wearing a blue vest is standing and pointing out in the direction of the goats in the cage, and other goats are moving at the back of the cage."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/0cu3HHsVCGE.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_16_1_0"}, {"texts": ["A brown goat is standing in the cage and licking a man's hand while boy pointing towards the goat."], "durations": null, "exact_frames_per_prompt": [62], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/0cu3HHsVCGE.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 18, "npz_gt_video_start_frame": 18, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 18, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_16_2_0"}, {"texts": ["A small boy wearing black clothes is sitting on a bed and is crying while holding a shoe while a person whose hands are visible is tearing papers in front of the kid."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/0t2ou6rPPus.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1700_0_0"}, {"texts": ["A person whose only hands are visible is holding a paper and showing the boy."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/0t2ou6rPPus.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1700_1_0"}, {"texts": ["A man wearing a white shirt and black pants is standing, putting something on the right side of the counter."], "durations": null, "exact_frames_per_prompt": [42], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/3xm8feXbrAQ.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 46, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1702_0_0"}, {"texts": ["The man is touching the food."], "durations": null, "exact_frames_per_prompt": [34], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/3xm8feXbrAQ.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 46, "npz_gt_video_start_frame": 46, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 46, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1702_0_1"}, {"texts": ["A man on the right side is wearing white gloves and tattooing on a person's wrist with a tattoo machine."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/1-GHskPiScc.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1703_0_0"}, {"texts": ["A person on the left side whose hand is visible is getting a tattoo on his wrist while a man wearing blue t-shirt and gloves is making tattoo on the person's hand and talking."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/1-GHskPiScc.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1703_1_0"}, {"texts": ["A man wearing a blue-white checked shirt is tying a bow tie and talking."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/4JPE_G0h0zA.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1704_0_0"}, {"texts": ["A person whose fingers are visible is pouring beaten eggs from a glass bowl into a pan."], "durations": null, "exact_frames_per_prompt": [15], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/2GK6dKmLxeI.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 32, "npz_gt_video_start_frame": 32, "npz_gt_video_end_frame": 47, "skip_frames_after_generation": 32, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1707_0_0"}, {"texts": ["A woman wearing a dark gray sweatshirt and gray jeans is sitting on an orange chair and speaking to the dog."], "durations": null, "exact_frames_per_prompt": [47], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/1UukK6zwgpY.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 33, "npz_gt_video_start_frame": 33, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 33, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1709_0_ms_0"}, {"texts": ["A white-black dog is sitting on the wooden floor and looking in front."], "durations": null, "exact_frames_per_prompt": [26], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/1UukK6zwgpY.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 33, "npz_gt_video_start_frame": 33, "npz_gt_video_end_frame": 59, "skip_frames_after_generation": 33, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1709_1_0"}, {"texts": ["The dog is looking down and spreading the legs on the wooden floor."], "durations": null, "exact_frames_per_prompt": [21], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/1UukK6zwgpY.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 59, "npz_gt_video_start_frame": 59, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 59, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1709_1_1"}, {"texts": ["A woman wearing a grey hoodie is sitting on an orange chair and looking at the dog."], "durations": null, "exact_frames_per_prompt": [47], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/1UukK6zwgpY.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 33, "npz_gt_video_start_frame": 33, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 33, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1710_0_0"}, {"texts": ["A grey-black dog is sitting on the wooden floor while a woman wearing grey and blue outfit is sitting on a chair and looking towards the dog and saying something."], "durations": null, "exact_frames_per_prompt": [31], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/1UukK6zwgpY.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 33, "npz_gt_video_start_frame": 33, "npz_gt_video_end_frame": 64, "skip_frames_after_generation": 33, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1710_1_0"}, {"texts": ["The dog starts lying on the floor."], "durations": null, "exact_frames_per_prompt": [16], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/1UukK6zwgpY.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 64, "npz_gt_video_start_frame": 64, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 64, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1710_1_1"}, {"texts": ["A man wearing a black shirt is sitting while another man wearing black clothes is sitting on the left side and drinking from a can."], "durations": null, "exact_frames_per_prompt": [10], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/3YeNgtAmPx8.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 14, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1716_0_0"}, {"texts": ["The man is doing the first bump while another man wearing black clothes is also doing the fist bump."], "durations": null, "exact_frames_per_prompt": [46], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/3YeNgtAmPx8.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 14, "npz_gt_video_start_frame": 14, "npz_gt_video_end_frame": 60, "skip_frames_after_generation": 14, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1716_0_1"}, {"texts": ["A man on the left side wearing black clothes is sitting, drinking something from a can"], "durations": null, "exact_frames_per_prompt": [30], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/3YeNgtAmPx8.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 34, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1716_1_0"}, {"texts": ["The man on the left side wearing black clothes is doing a fist bump while the other person reacts into it ."], "durations": null, "exact_frames_per_prompt": [26], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/3YeNgtAmPx8.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 34, "npz_gt_video_start_frame": 34, "npz_gt_video_end_frame": 60, "skip_frames_after_generation": 34, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1716_1_1"}, {"texts": ["A man wearing a black shirt is sitting."], "durations": null, "exact_frames_per_prompt": [13], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/3YeNgtAmPx8.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 17, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1717_0_0"}, {"texts": ["The man doing the fist bump with the other man."], "durations": null, "exact_frames_per_prompt": [43], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/3YeNgtAmPx8.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 17, "npz_gt_video_start_frame": 17, "npz_gt_video_end_frame": 60, "skip_frames_after_generation": 17, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1717_0_1"}, {"texts": ["A man on the left side, wearing black cloth, is sitting, drinking a can."], "durations": null, "exact_frames_per_prompt": [28], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/3YeNgtAmPx8.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 32, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1717_1_0"}, {"texts": ["The man is doing a fist bump with the other man."], "durations": null, "exact_frames_per_prompt": [28], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/3YeNgtAmPx8.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 32, "npz_gt_video_start_frame": 32, "npz_gt_video_end_frame": 60, "skip_frames_after_generation": 32, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1717_1_1"}, {"texts": ["A person wearing a black diving suit is swimming underwater and untying a rope."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/0gQEjQ3WVeg.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1719_0_0"}, {"texts": ["A boy wearing a check shirt is walking on the grassy surface of the orchid while looking here and there while another boy wearing yellow outfit is walking behind first boy and a person wearing white and black outfit is standing in front of first boy and he is plucking something."], "durations": null, "exact_frames_per_prompt": [27], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/0hP9sqHtSMo.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 31, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_171_0_0"}, {"texts": ["A man wearing an off-white shirt is standing a head of the boy in check shirt and touching the fruits on the fruit tree"], "durations": null, "exact_frames_per_prompt": [58], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/0hP9sqHtSMo.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 23, "npz_gt_video_start_frame": 23, "npz_gt_video_end_frame": 81, "skip_frames_after_generation": 23, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_171_2_0"}, {"texts": ["A person wearing a black wet-suit, black snorkel mask is sitting on his knees in the river. "], "durations": null, "exact_frames_per_prompt": [26], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/0gQEjQ3WVeg.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 30, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1720_0_0"}, {"texts": ["The person wearing a black wet-suit is looking on the right side while untying the white rope."], "durations": null, "exact_frames_per_prompt": [50], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/0gQEjQ3WVeg.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 30, "npz_gt_video_start_frame": 30, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 30, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1720_0_1"}, {"texts": ["A man wearing a grey-white t-shirt is standing on the green golf course and playing golf while a man wearing a blue t-shirt is standing on the left and looking at the first man."], "durations": null, "exact_frames_per_prompt": [20], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/0I79X1juaE0.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 24, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1721_0_0"}, {"texts": ["The man moves his hand while the man wearing a blue t-shirt is walking towards the right."], "durations": null, "exact_frames_per_prompt": [20], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/0I79X1juaE0.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 24, "npz_gt_video_start_frame": 24, "npz_gt_video_end_frame": 44, "skip_frames_after_generation": 24, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1721_0_1"}, {"texts": ["The man starts walking to the right side while the other man starts hitting the golf ball with his golf stick."], "durations": null, "exact_frames_per_prompt": [12], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/0I79X1juaE0.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 44, "npz_gt_video_start_frame": 44, "npz_gt_video_end_frame": 56, "skip_frames_after_generation": 44, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1721_0_2"}, {"texts": ["The man starts playing golf."], "durations": null, "exact_frames_per_prompt": [24], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/0I79X1juaE0.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 56, "npz_gt_video_start_frame": 56, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 56, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1721_0_3"}, {"texts": ["A man wearing a blue t-shirt is standing on the left side, holding a golf stick while a man wearing white shorts is holding a golf stick and standing in a position to hit the ball, and then hits the ball."], "durations": null, "exact_frames_per_prompt": [23], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/0I79X1juaE0.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 27, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1721_1_0"}, {"texts": ["The man starts walking on the green golf course."], "durations": null, "exact_frames_per_prompt": [12], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/0I79X1juaE0.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 27, "npz_gt_video_start_frame": 27, "npz_gt_video_end_frame": 39, "skip_frames_after_generation": 27, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1721_1_1"}, {"texts": ["The man is moving golf balls with the golf stick then moving his hand while a man wearing white shorts moves around and then bends in a position to hit the ball."], "durations": null, "exact_frames_per_prompt": [41], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/0I79X1juaE0.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 39, "npz_gt_video_start_frame": 39, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 39, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1721_1_2"}, {"texts": ["A man wearing white t-shirt and gray lower is standing on the brown floor and peeling a potato."], "durations": null, "exact_frames_per_prompt": [40], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/2kmLJ0CqwPU.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 44, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1723_0_0"}, {"texts": ["The man takes out the potato from the peeler."], "durations": null, "exact_frames_per_prompt": [36], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/2kmLJ0CqwPU.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 44, "npz_gt_video_start_frame": 44, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 44, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1723_0_1"}, {"texts": ["A woman wearing a brown-white striped top is standing, holding a kettle and pouring some liquid into a white teapot."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/3w7Pa5CEhbI.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1724_0_0"}, {"texts": ["A girl sitting in the middle also wearing a white t-shirt is crying fake while a girl in white top sits in front of her and looks at her back  ."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/0MEgCamKdTE.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1725_1_0"}, {"texts": ["A girl sitting on the right also wearing a white t-shirt is crying fake while a girl wearing white top sitting in the middle and keeping her hands on her face and crying fake, a girl wearing white top sitting in the left side is crying while her head down."], "durations": null, "exact_frames_per_prompt": [31], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/0MEgCamKdTE.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 35, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1725_2_0"}, {"texts": ["The girl starts coughing while a girl sitting on the right side turns her head back, looks at the other girl then turns back in front."], "durations": null, "exact_frames_per_prompt": [45], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/0MEgCamKdTE.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 35, "npz_gt_video_start_frame": 35, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 35, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1725_2_1"}, {"texts": ["A girl wearing a white t-shirt is sitting on the left side and crying fake while a girl wearing white shirt is sitting and then turns back and then looking at the front."], "durations": null, "exact_frames_per_prompt": [44], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/0MEgCamKdTE.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 36, "npz_gt_video_start_frame": 36, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 36, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1726_0_0"}, {"texts": ["A girl wearing a white t-shirt is sitting in the middle, also crying fake.\n while a girl on left is sitting and crying fake and another girl on right is also sitting and crying fake."], "durations": null, "exact_frames_per_prompt": [31], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/0MEgCamKdTE.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 35, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1726_1_0"}, {"texts": ["A girl wearing a white t-shirt is sitting on the right side, crying fake and coughing along with some other girls, and another girl is sitting in the front and turning her head to look at them."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/0MEgCamKdTE.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1726_2_0"}, {"texts": ["A man on the right side wearing a red t-shirt is filling the petrol in the car while a woman wearing a blue top is standing and looking at the fuel hose and another woman on the right is looking in the front."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/0Ey1aw2FWV0.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1727_0_0"}, {"texts": ["A girl is taking a video in selfie mode while a boy in a red t-shirt is standing and filling the car's fuel tank and a girl in a blue t-shirt is standing."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/0Ey1aw2FWV0.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1727_2_0"}, {"texts": ["A boy wearing a red shirt is standing on the gray road and fueling the car while a woman wearing blue hoodie is standing on the left, and another woman is standing near the woman is recording herself."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/0Ey1aw2FWV0.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1728_0_0"}, {"texts": ["A girl wearing a blue shirt is standing on the right side of another girl, looking here and there while a boy wearing a red t-shirt is filling the vehicle with fuel and another girl on the left is showing her face."], "durations": null, "exact_frames_per_prompt": [60], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/0Ey1aw2FWV0.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 20, "npz_gt_video_start_frame": 20, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 20, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1728_1_0"}, {"texts": ["A boy wearing denim shorts is standing on the grey road, holding a blue leash of the dog, and pointing his finger towards the left."], "durations": null, "exact_frames_per_prompt": [19], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/07SD274Dems.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 23, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1729_0_0"}, {"texts": ["The boy is leaving the leash and starts running towards the left."], "durations": null, "exact_frames_per_prompt": [30], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/07SD274Dems.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 23, "npz_gt_video_start_frame": 23, "npz_gt_video_end_frame": 53, "skip_frames_after_generation": 23, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1729_0_1"}, {"texts": ["A man wearing a white shirt is standing and plucking fruits from a tree."], "durations": null, "exact_frames_per_prompt": [59], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/0hP9sqHtSMo.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 22, "npz_gt_video_start_frame": 22, "npz_gt_video_end_frame": 81, "skip_frames_after_generation": 22, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_172_2_0"}, {"texts": ["A man wearing black trousers is standing."], "durations": null, "exact_frames_per_prompt": [32], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/3f-4cQcwRx4.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 36, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1731_0_0"}, {"texts": ["The man wearing black trousers starts ironing cloth on the ironing board."], "durations": null, "exact_frames_per_prompt": [44], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/3f-4cQcwRx4.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 36, "npz_gt_video_start_frame": 36, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 36, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1731_0_1"}, {"texts": ["A person whose only one hand is visible is holding an oil filter in their hand."], "durations": null, "exact_frames_per_prompt": [26], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/2E5f3IZD7XM.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 30, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1732_0_0"}, {"texts": ["A woman wearing a black jacket and a grey top underneath is reading news."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/4U2UWj8SG5w.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1734_0_0"}, {"texts": ["A girl wearing blue clothes is holding a cushion, is sitting and speaking and starts crying while a person whose hand is visible is showing a toy to the girl."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-u2nP9SDE38.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1735_0_0"}, {"texts": ["A person whose hand is visible is holding a toy and giving it to the girl."], "durations": null, "exact_frames_per_prompt": [23], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-u2nP9SDE38.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 55, "npz_gt_video_start_frame": 55, "npz_gt_video_end_frame": 78, "skip_frames_after_generation": 55, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1735_1_0"}, {"texts": ["A baby boy wearing blue pants is crying while sitting on a toy car ride machine while a person wearing black clothes is caressing the baby and some people are walking at the back."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/0t7w8in5oeA.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1737_0_0"}, {"texts": ["A person standing on a patterned surface in a navy blue jacket is moving her hand on a boy's head while a boy started crying and riding the kids car."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/0t7w8in5oeA.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1737_2_0"}, {"texts": ["A person whom half body is visible is wearing a pink t-shirt is sitting."], "durations": null, "exact_frames_per_prompt": [24], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/20NoP2irHZE.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 28, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1739_0_0"}, {"texts": ["The person is peeling an apple with a knife."], "durations": null, "exact_frames_per_prompt": [52], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/20NoP2irHZE.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 28, "npz_gt_video_start_frame": 28, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 28, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1739_0_1"}, {"texts": ["A man wearing black suit is standing and he is talking."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/0KRkkOapZEs.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_173_0_0"}, {"texts": ["A person whose hands are visible only visible is tearing a paper."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/11vm6DICgWo.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1741_0_0"}, {"texts": ["A man wearing a black suit is standing. "], "durations": null, "exact_frames_per_prompt": [28], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-G-5CJ0JkKY.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 32, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1742_0_0"}, {"texts": ["The man is moving on the right side and then putting his hand on the display."], "durations": null, "exact_frames_per_prompt": [48], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-G-5CJ0JkKY.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 32, "npz_gt_video_start_frame": 32, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 32, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1742_0_1"}, {"texts": ["A blindfolded girl wearing a pink top is sitting and is being fed by a man."], "durations": null, "exact_frames_per_prompt": [39], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/20ckloWunwI.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 43, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1743_0_0"}, {"texts": ["The girl gets up and runs away towards the left."], "durations": null, "exact_frames_per_prompt": [13], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/20ckloWunwI.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 43, "npz_gt_video_start_frame": 43, "npz_gt_video_end_frame": 57, "skip_frames_after_generation": 43, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1743_0_1"}, {"texts": ["A girl wearing a pink t-shirt, blindfoldedly consuming something from a spoon while a man wearing a gray shirt is standing and putting the spoon inside the mouth of the girl."], "durations": null, "exact_frames_per_prompt": [37], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/20ckloWunwI.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 41, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1744_1_0"}, {"texts": ["A person whose only hands are visible is holding a paper."], "durations": null, "exact_frames_per_prompt": [22], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/1vcoD5DIN4w.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 26, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1745_0_0"}, {"texts": ["The person flips it and turns it."], "durations": null, "exact_frames_per_prompt": [54], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/1vcoD5DIN4w.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 26, "npz_gt_video_start_frame": 26, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 26, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1745_0_1"}, {"texts": ["A boy wearing a white-blue striped t-shirt is playing golf."], "durations": null, "exact_frames_per_prompt": [61], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/15OA4S0Xi60.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 65, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1746_0_0"}, {"texts": ["A woman wearing a black blazer is standing on the right side."], "durations": null, "exact_frames_per_prompt": [36], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/0gUUB1T4cVs.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 40, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1747_0_0"}, {"texts": ["The woman wearing a black blazer is showing the weather forecast."], "durations": null, "exact_frames_per_prompt": [40], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/0gUUB1T4cVs.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 40, "npz_gt_video_start_frame": 40, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 40, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1747_0_1"}, {"texts": ["A woman wearing a black blazer is standing on the right side while holding her hands together."], "durations": null, "exact_frames_per_prompt": [29], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/0gUUB1T4cVs.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 33, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1748_0_ms_0"}, {"texts": ["The woman walks to the right side."], "durations": null, "exact_frames_per_prompt": [21], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/0gUUB1T4cVs.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 33, "npz_gt_video_start_frame": 33, "npz_gt_video_end_frame": 54, "skip_frames_after_generation": 33, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1748_0_ms_1"}, {"texts": ["A man wearing a black suit is standing and reading the weather report."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/0KRkkOapZEs.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_174_0_0"}, {"texts": ["A man wearing a red t-shirt and a red cap is holding a mobile phone and taking a selfie while another man wearing a white and blue t-shirt is eating and holding a mobile phone."], "durations": null, "exact_frames_per_prompt": [40], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/1gGYr5ojTgA.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 44, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1750_0_0"}, {"texts": ["The man is eating the food while a group of people are standing and moving at the back."], "durations": null, "exact_frames_per_prompt": [36], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/1gGYr5ojTgA.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 44, "npz_gt_video_start_frame": 44, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 44, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1750_0_1"}, {"texts": ["A man wearing a multi colored t-shirt is sitting on the right side and taking a selfie while a man wearing red t-shirt is sitting on left and he is taking selfie."], "durations": null, "exact_frames_per_prompt": [60], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/1gGYr5ojTgA.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 64, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1750_1_0"}, {"texts": ["The man is eating the food.  while people are standing and they are doing different activity."], "durations": null, "exact_frames_per_prompt": [16], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/1gGYr5ojTgA.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 64, "npz_gt_video_start_frame": 64, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 64, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1750_1_1"}, {"texts": ["A person whose only hands are visible, wearing a white full-sleeve t-shirt, holding a green paper in the left hand and then setting up the green paper on the white paper."], "durations": null, "exact_frames_per_prompt": [33], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-43sC7j9Qa0.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 37, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1753_0_0"}, {"texts": ["The person is cutting the green paper with the scissor, and then pasting the green paper on the white paper."], "durations": null, "exact_frames_per_prompt": [31], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-43sC7j9Qa0.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 37, "npz_gt_video_start_frame": 37, "npz_gt_video_end_frame": 68, "skip_frames_after_generation": 37, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1753_0_1"}, {"texts": ["The person is pasting the green paper on the white paper."], "durations": null, "exact_frames_per_prompt": [12], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-43sC7j9Qa0.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 68, "npz_gt_video_start_frame": 68, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 68, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1753_0_2"}, {"texts": ["A person whose only hands are visible is holding a knife, then cuts the watermelon into halves,."], "durations": null, "exact_frames_per_prompt": [34], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/9y4yWVUYet4.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 38, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1754_0_0"}, {"texts": ["The person puts one piece aside."], "durations": null, "exact_frames_per_prompt": [8], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/9y4yWVUYet4.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 38, "npz_gt_video_start_frame": 38, "npz_gt_video_end_frame": 46, "skip_frames_after_generation": 38, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1754_0_1"}, {"texts": ["The person starts cutting the other piece into halves."], "durations": null, "exact_frames_per_prompt": [34], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/9y4yWVUYet4.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 46, "npz_gt_video_start_frame": 46, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 46, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1754_0_2"}, {"texts": ["A man wearing a steel blue color vest and red shorts is standing on the left side of the boy and talking to the boy."], "durations": null, "exact_frames_per_prompt": [31], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/5ePGarYjxcM.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 35, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1757_0_0"}, {"texts": ["The man is peeling a potato."], "durations": null, "exact_frames_per_prompt": [45], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/5ePGarYjxcM.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 35, "npz_gt_video_start_frame": 35, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 35, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1757_0_1"}, {"texts": ["A boy wearing a black shorts is standing on the right side of the man and looking at him."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/5ePGarYjxcM.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1757_1_0"}, {"texts": ["A man wearing a dark brown jacket is standing on the right side of the first man, holding a mic in one hand and pointing at the first dog using the other while a man wearing a black outfit is standing, holding a dog's leash, and patting the dog, a black dog is coming from the right side towards the man wearing a black outfit."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/1YSsfes0uJY.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1758_1_0"}, {"texts": ["A man wearing a black dress is standing on the right side of the first dog, holding the leash of the dog from one hand and caressing from the other while a person wearing a brown jacket is standing in front of the man and looking at him while moving its hands, and another black dog is coming from the right side and standing near the first dog."], "durations": null, "exact_frames_per_prompt": [65], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/1YSsfes0uJY.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 15, "npz_gt_video_start_frame": 15, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 15, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1758_2_0"}, {"texts": ["A black dog wearing an orange cloth is sitting on the gray surface, tied with a leash and being caressed by the third man while another man is coming with another dog from the right side."], "durations": null, "exact_frames_per_prompt": [64], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/1YSsfes0uJY.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 16, "npz_gt_video_start_frame": 16, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 16, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1758_4_0"}, {"texts": ["A man wearing a brown jacket is standing and holding a microphone in his left hand and talking with others."], "durations": null, "exact_frames_per_prompt": [24], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/1YSsfes0uJY.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 28, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1759_1_0"}, {"texts": ["A third man wearing a black jacket is standing and holding a dog leash and then starts rubbing the dog's head while a person is also coming from the right and holding another dog's leash, and another man wearing a brown blazer is standing on the left side holding a mic towards another person."], "durations": null, "exact_frames_per_prompt": [70], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/1YSsfes0uJY.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 10, "npz_gt_video_start_frame": 10, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 10, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1759_2_0"}, {"texts": ["A black dog wearing an orange jacket is tied with a leash and sitting on the gray floor and getting its head rubbed by a third man while a man standing in the front is moving his hand, another black dog comes from the right towards the first dog and a person whose legs are visible is holding the second dog's leash."], "durations": null, "exact_frames_per_prompt": [65], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/1YSsfes0uJY.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 15, "npz_gt_video_start_frame": 15, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 15, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1759_4_0"}, {"texts": ["A black dog wearing an orange jacket is tied with a leash is standing on the gray floor and watching the first dog while a man wearing a black jeans is lean down and rubbing the head of the first dog."], "durations": null, "exact_frames_per_prompt": [45], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/1YSsfes0uJY.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 35, "npz_gt_video_start_frame": 35, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 35, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1759_5_0"}, {"texts": ["A man wearing a black suit with a grey tie is giving a weather report."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/0KRkkOapZEs.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_175_0_0"}, {"texts": ["A man wearing red clothes is holding a golf stick."], "durations": null, "exact_frames_per_prompt": [37], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/0LCMaS5Q45Y.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 41, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1764_0_0"}, {"texts": ["The man is putting a golf ball."], "durations": null, "exact_frames_per_prompt": [38], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/0LCMaS5Q45Y.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 41, "npz_gt_video_start_frame": 41, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 41, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1764_0_1"}, {"texts": ["A black puppy is at first moving towards the food kept in a white utensil."], "durations": null, "exact_frames_per_prompt": [31], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-lu-uk6lhWY.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 35, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1766_0_0"}, {"texts": ["The puppy snatches the food and moves backward direction, and starts eating the food."], "durations": null, "exact_frames_per_prompt": [45], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-lu-uk6lhWY.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 35, "npz_gt_video_start_frame": 35, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 35, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1766_0_1"}, {"texts": ["A man wearing a gray blazer is standing on the left side, holding a white rope and knotting it."], "durations": null, "exact_frames_per_prompt": [41], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-g1NC5XFWgE.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 45, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1769_0_0"}, {"texts": ["The man is showing it to the woman."], "durations": null, "exact_frames_per_prompt": [35], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-g1NC5XFWgE.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 45, "npz_gt_video_start_frame": 45, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 45, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1769_0_1"}, {"texts": ["A woman wearing a green shirt is standing on the right side, while putting her hands on the countertop and watching the man."], "durations": null, "exact_frames_per_prompt": [26], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-g1NC5XFWgE.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 48, "npz_gt_video_start_frame": 48, "npz_gt_video_end_frame": 74, "skip_frames_after_generation": 48, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1769_1_0"}, {"texts": ["A man is standing and trying to eat a piece of bread."], "durations": null, "exact_frames_per_prompt": [58], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/3YbuslVAai8.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 22, "npz_gt_video_start_frame": 22, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 22, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1770_3_0"}, {"texts": ["A woman wearing a white apron is standing on the left side and talking to the first woman."], "durations": null, "exact_frames_per_prompt": [37], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/3Us0ldleUfY.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 43, "npz_gt_video_start_frame": 43, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 43, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1771_1_0"}, {"texts": ["A woman wearing a black t-shirt is standing, and peeling a sweet potato."], "durations": null, "exact_frames_per_prompt": [71], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/5HhefhTJqxE.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 75, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1775_0_0"}, {"texts": ["The woman is putting the sweet potato into a white bowl."], "durations": null, "exact_frames_per_prompt": [5], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/5HhefhTJqxE.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 75, "npz_gt_video_start_frame": 75, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 75, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1775_0_1"}, {"texts": ["A girl wearing a printed frock is standing on the wooden floor, and eating a piece of cake, while looking in the right side."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/67BRTo7ao5U.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1776_0_0"}, {"texts": ["A man wearing a black t-shirt is standing, holding a glass and a bottle while a man in a black t-shirt is sitting and a man in spectacles is standing and talking."], "durations": null, "exact_frames_per_prompt": [29], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/5nHUJEAxLeE.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 33, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1777_0_0"}, {"texts": ["The man is pouring the drink into a glass and the glass is being held by the man in spectacles."], "durations": null, "exact_frames_per_prompt": [32], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/5nHUJEAxLeE.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 33, "npz_gt_video_start_frame": 33, "npz_gt_video_end_frame": 65, "skip_frames_after_generation": 33, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1777_0_1"}, {"texts": ["A man wearing a black t-shirt is speaking while a man wearing a black t-shirt is sitting near a wall and another man in a black t-shirt with a white design is pouring a drink from a bottle into a green glass."], "durations": null, "exact_frames_per_prompt": [38], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/5nHUJEAxLeE.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 19, "npz_gt_video_start_frame": 19, "npz_gt_video_end_frame": 57, "skip_frames_after_generation": 19, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1777_2_0"}, {"texts": ["The man holding a glass.  when a man in a black t-shirt with a white design gives the drink glass to him."], "durations": null, "exact_frames_per_prompt": [16], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/5nHUJEAxLeE.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 57, "npz_gt_video_start_frame": 57, "npz_gt_video_end_frame": 73, "skip_frames_after_generation": 57, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1777_2_1"}, {"texts": ["The man starts drinking."], "durations": null, "exact_frames_per_prompt": [7], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/5nHUJEAxLeE.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 73, "npz_gt_video_start_frame": 73, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 73, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1777_2_2"}, {"texts": ["A man wearing a brown hat is sitting on a white-black horse while a group of people ride horses, a dog approaches from the right."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-SvoZu_9_nU.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_177_0_0"}, {"texts": ["A woman wearing a white shirt is riding a brown-white horse then stops on the left side while another woman wearing black and blue outfit is riding on a brown horse and the brown horse is moving towards the left side and a brown dog is moving behind the horse and a person wearing yellow and blue outfit is sitting on a black and white horse and black and white horse is standing on the ground."], "durations": null, "exact_frames_per_prompt": [64], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-SvoZu_9_nU.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 5, "npz_gt_video_start_frame": 5, "npz_gt_video_end_frame": 69, "skip_frames_after_generation": 5, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_177_1_0"}, {"texts": ["A woman wearing a black t-shirt is riding a brown horse while a woman wearing a white shirt is riding a brown horse towards the left, and a brown dog is walking behind the horses."], "durations": null, "exact_frames_per_prompt": [51], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-SvoZu_9_nU.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 8, "npz_gt_video_start_frame": 8, "npz_gt_video_end_frame": 59, "skip_frames_after_generation": 8, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_177_2_0"}, {"texts": ["The woman stops behind the first woman while a man wearing a cream shirt is sitting on the black-white horse."], "durations": null, "exact_frames_per_prompt": [21], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-SvoZu_9_nU.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 59, "npz_gt_video_start_frame": 59, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 59, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_177_2_1"}, {"texts": ["A brown horse is walking behind the second horse while two people are riding both the horses and a brown dog is walking behind."], "durations": null, "exact_frames_per_prompt": [41], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-SvoZu_9_nU.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 12, "npz_gt_video_start_frame": 12, "npz_gt_video_end_frame": 53, "skip_frames_after_generation": 12, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_177_5_0"}, {"texts": ["The horse stops behind the second horse while a man in the front is riding a black and white horse, and another horse whose head is visible is entering from the front."], "durations": null, "exact_frames_per_prompt": [27], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-SvoZu_9_nU.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 53, "npz_gt_video_start_frame": 53, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 53, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_177_5_1"}, {"texts": ["A brown dog is walking behind the third horse while two people are riding the horses towards the right."], "durations": null, "exact_frames_per_prompt": [47], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-SvoZu_9_nU.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 15, "npz_gt_video_start_frame": 15, "npz_gt_video_end_frame": 62, "skip_frames_after_generation": 15, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_177_7_0"}, {"texts": ["A woman whose hands are visible wearing a light-purple sweater is unfolding a red napkin."], "durations": null, "exact_frames_per_prompt": [43], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/1p7Jxouh5C4.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 37, "npz_gt_video_start_frame": 37, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 37, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1780_0_0"}, {"texts": ["A person wearing blue jeans is making the bed."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/hZS4ACUuXdA.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1781_0_0"}, {"texts": ["A man wearing a grey sweatshirt is making the bed."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/hZS4ACUuXdA.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1782_0_0"}, {"texts": ["A man wearing a red t-shirt is riding on a camel on the muddy surface while another man wearing a grey jacket is holding the camel's leash and a group of people are standing and moving at the back."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-NMbe5RMZzU.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1783_0_0"}, {"texts": ["A man wearing a jacket is holding the halter of the camel and walking along with the camel in the left direction while a man wearing a red t-shirt is sitting on the camel and speaking, and a group of people in which some are sitting and some are walking."], "durations": null, "exact_frames_per_prompt": [22], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-NMbe5RMZzU.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 26, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1783_1_0"}, {"texts": ["The man wearing a jacket then takes a U-turn."], "durations": null, "exact_frames_per_prompt": [54], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-NMbe5RMZzU.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 26, "npz_gt_video_start_frame": 26, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 26, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1783_1_1"}, {"texts": ["A camel is walking in the left direction while a man wearing red t-shirt is riding and sitting on the camel's back and the man wearing black jacket is holding the camel's chain"], "durations": null, "exact_frames_per_prompt": [19], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-NMbe5RMZzU.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 23, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1783_2_0"}, {"texts": ["The camel takes a U-turn while giving a ride to the man sitting on its back."], "durations": null, "exact_frames_per_prompt": [57], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-NMbe5RMZzU.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 23, "npz_gt_video_start_frame": 23, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 23, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1783_2_1"}, {"texts": ["A girl whose upper half-body is visible, wearing a black t-shirt, is sitting on the left side and is feeding something while putting something in the other girl's mouth with her right hand."], "durations": null, "exact_frames_per_prompt": [20], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/1uT-rbAkdSU.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 24, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1785_0_0"}, {"texts": ["The girl whose upper half-body is visible, wearing a black t-shirt starts speaking and the girl sitting on the right side is eating something and removing the cloth from her face."], "durations": null, "exact_frames_per_prompt": [56], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/1uT-rbAkdSU.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 24, "npz_gt_video_start_frame": 24, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 24, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1785_0_1"}, {"texts": ["A girl whose upper half-body is visible, wearing a white t-shirt and her eyes are covered with a red cloth, is sitting on the right side and is eating something given by the first girl."], "durations": null, "exact_frames_per_prompt": [63], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/1uT-rbAkdSU.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 67, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1785_1_0"}, {"texts": ["The girl is speaking while removing the red cloth from her eyes."], "durations": null, "exact_frames_per_prompt": [13], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/1uT-rbAkdSU.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 67, "npz_gt_video_start_frame": 67, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 67, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1785_1_1"}, {"texts": ["A man wearing a blue t-shirt is standing, holding a rope which is tightened with an iron handle then he starts pulling the rope from one side."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/0IGqNcUyIVI.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1786_0_0"}, {"texts": ["A man wearing a white t-shirt is sitting and is folding a blue paper."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/31tH8NjslHU.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1787_0_0"}, {"texts": ["A person wearing a white green striped t-shirt is standing holding a paper."], "durations": null, "exact_frames_per_prompt": [15], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/0Y8sTxewumo.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 19, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1788_0_0"}, {"texts": ["The person is shredding it in a shredding machine."], "durations": null, "exact_frames_per_prompt": [60], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/0Y8sTxewumo.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 19, "npz_gt_video_start_frame": 19, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 19, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1788_0_1"}, {"texts": ["A person wearing white green striped t-shirt is standing."], "durations": null, "exact_frames_per_prompt": [12], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/0Y8sTxewumo.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 16, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1789_0_0"}, {"texts": ["The person is shredding papers in a shredding machine."], "durations": null, "exact_frames_per_prompt": [64], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/0Y8sTxewumo.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 16, "npz_gt_video_start_frame": 16, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 16, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1789_0_1"}, {"texts": ["A man wearing an orange t-shirt is sitting and getting his arm wrapped by a woman."], "durations": null, "exact_frames_per_prompt": [41], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/1jcCX8RALKU.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 45, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_178_0_0"}, {"texts": ["A woman wearing black trousers is standing and wrapping a crepe bandage on a man's arm."], "durations": null, "exact_frames_per_prompt": [39], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/1jcCX8RALKU.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 43, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_178_1_0"}, {"texts": ["A man wearing white shirt is walking on the soil surface while some people are sitting on the elephant."], "durations": null, "exact_frames_per_prompt": [8], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/0sC7Ovz0h20.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 12, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1790_1_0"}, {"texts": ["The man wearing white shirt goes to the backside and puts his hand on the elephant while a man wearing blue t-shirt is walking towards the elephant."], "durations": null, "exact_frames_per_prompt": [68], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/0sC7Ovz0h20.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 12, "npz_gt_video_start_frame": 12, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 12, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1790_1_1"}, {"texts": ["A man wearing a blue t-shirt is walking on the soil surface while another man wearing brown pants is walking on the soil surface to the left side and touches the elephant and few people are sitting on the elephant."], "durations": null, "exact_frames_per_prompt": [56], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/0sC7Ovz0h20.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 60, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1790_2_0"}, {"texts": ["The man is trying to climb the elephant."], "durations": null, "exact_frames_per_prompt": [20], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/0sC7Ovz0h20.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 60, "npz_gt_video_start_frame": 60, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 60, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1790_2_1"}, {"texts": ["A grey elephant is standing on the soil surface while carrying people on its back, eating green leaves while a man wearing a white shirt is moving towards the elephant, and another man wearing a blue t-shirt is standing on the brown surface."], "durations": null, "exact_frames_per_prompt": [11], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/0sC7Ovz0h20.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 15, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1790_3_0"}, {"texts": ["The grey elephant bent its back leg while the man in the white shirt holds the elephant, another person moves towards it and tries to climb it."], "durations": null, "exact_frames_per_prompt": [65], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/0sC7Ovz0h20.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 15, "npz_gt_video_start_frame": 15, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 15, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1790_3_1"}, {"texts": ["A woman wearing a blue top is at first inserts her hand into her pocket while the dog follows the woman."], "durations": null, "exact_frames_per_prompt": [11], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/0hggXp7zXwI.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 15, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1792_0_0"}, {"texts": ["The woman wearing a blue top picks up the dog food from it."], "durations": null, "exact_frames_per_prompt": [28], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/0hggXp7zXwI.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 15, "npz_gt_video_start_frame": 15, "npz_gt_video_end_frame": 43, "skip_frames_after_generation": 15, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1792_0_1"}, {"texts": ["The woman wearing a blue top drops the food on the floor while the dog try to catch the food."], "durations": null, "exact_frames_per_prompt": [5], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/0hggXp7zXwI.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 43, "npz_gt_video_start_frame": 43, "npz_gt_video_end_frame": 48, "skip_frames_after_generation": 43, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1792_0_2"}, {"texts": ["The woman wearing a blue top picks it up from the floor."], "durations": null, "exact_frames_per_prompt": [13], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/0hggXp7zXwI.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 48, "npz_gt_video_start_frame": 48, "npz_gt_video_end_frame": 61, "skip_frames_after_generation": 48, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1792_0_3"}, {"texts": ["The woman wearing a blue top moves in the left direction near the chair and put the dog food under the chair while holding the dog collar belt from one hand while the dog start eating the food."], "durations": null, "exact_frames_per_prompt": [19], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/0hggXp7zXwI.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 61, "npz_gt_video_start_frame": 61, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 61, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1792_0_4"}, {"texts": ["A black dog is moving along with the woman in a blue top for the food which is in her hands while a woman bends down and keeps the foods on the earth surface"], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/0hggXp7zXwI.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1792_1_0"}, {"texts": ["A woman wearing a purple t-shirt and green pajamas is standing while holding a dog leash a black dog standing on the ground"], "durations": null, "exact_frames_per_prompt": [12], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/0hggXp7zXwI.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 16, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1793_0_0"}, {"texts": ["The woman is taking something out of her pocket while a dog moved towards the woman"], "durations": null, "exact_frames_per_prompt": [32], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/0hggXp7zXwI.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 16, "npz_gt_video_start_frame": 16, "npz_gt_video_end_frame": 48, "skip_frames_after_generation": 16, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1793_0_1"}, {"texts": ["The woman is scattering something on the floor a dog goes for eat something"], "durations": null, "exact_frames_per_prompt": [32], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/0hggXp7zXwI.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 48, "npz_gt_video_start_frame": 48, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 48, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1793_0_2"}, {"texts": ["A black dog is standing while a woman is holding the dog's leash and then putting something on the floor."], "durations": null, "exact_frames_per_prompt": [66], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/0hggXp7zXwI.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 70, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1793_1_0"}, {"texts": ["The dog is sniffing on the floor."], "durations": null, "exact_frames_per_prompt": [10], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/0hggXp7zXwI.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 70, "npz_gt_video_start_frame": 70, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 70, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1793_1_1"}, {"texts": ["A girl wearing a yellow top and a white feeding bib on her neck is sitting on a baby feeding chair and is eating noodles from the bowl while holding a fork in her right hand and then looking on the left side while eating noodles."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/0QEc0kJWUaU.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1794_0_0"}, {"texts": ["A man wearing a black t-shirt with written words is sitting on a white chair and drinking beer with a jug."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/3_H7RgPUQxM.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1796_0_0"}, {"texts": ["A lady wearing a white dress is standing and holding a steel bowl with her right hand, mixing yellow liquid with a whisk."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/3IVAPCUxZ_s.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1797_0_0"}, {"texts": ["A woman wearing a white shirt is standing, mixing the batter in the bowl with the help of a whisker."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/3IVAPCUxZ_s.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1798_0_0"}, {"texts": ["A woman is applying makeup below her left eyebrow with a makeup brush."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-L6XUFw1n6o.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1799_0_0"}, {"texts": ["A man wearing a black hoodie pushing clothes into a washing machine."], "durations": null, "exact_frames_per_prompt": [12], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/2lW6skTENSw.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 16, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_179_0_0"}, {"texts": ["The man is closing the door of the washing machine."], "durations": null, "exact_frames_per_prompt": [13], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/2lW6skTENSw.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 16, "npz_gt_video_start_frame": 16, "npz_gt_video_end_frame": 29, "skip_frames_after_generation": 16, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_179_0_1"}, {"texts": ["The man is searching his pants pocket for something."], "durations": null, "exact_frames_per_prompt": [51], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/2lW6skTENSw.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 29, "npz_gt_video_start_frame": 29, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 29, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_179_0_2"}, {"texts": ["A boy wearing a blue vest is standing on the floor, pointing a finger towards the goat cage, and talking."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/0cu3HHsVCGE.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_17_0_0"}, {"texts": ["A person wearing a red t-shirt and gray shorts is standing on the floor while a kid wearing a blue t-shirt is standing and pointing his finger toward the goat and speaking."], "durations": null, "exact_frames_per_prompt": [15], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/0cu3HHsVCGE.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 19, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_17_1_0"}, {"texts": ["The person wearing a red t-shirt and gray shorts is offering a hand to a goat for licking."], "durations": null, "exact_frames_per_prompt": [61], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/0cu3HHsVCGE.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 19, "npz_gt_video_start_frame": 19, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 19, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_17_1_1"}, {"texts": ["A brown goat is licking the hand of the person while a kid is standing on the left side and group of goat is in the cage."], "durations": null, "exact_frames_per_prompt": [29], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/0cu3HHsVCGE.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 18, "npz_gt_video_start_frame": 18, "npz_gt_video_end_frame": 47, "skip_frames_after_generation": 18, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_17_2_0"}, {"texts": ["A person whose hand is visible is peeling the watermelon and putting it on the plate.\n"], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/98q1tYCEVoQ.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1800_0_0"}, {"texts": ["A person whose hands are only visible is cutting watermelon pieces from its ring through a knife."], "durations": null, "exact_frames_per_prompt": [32], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/98q1tYCEVoQ.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 36, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1801_0_0"}, {"texts": ["A person whose hand is visible is writing on paper with a pen."], "durations": null, "exact_frames_per_prompt": [66], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-uR3unjeRWg.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 14, "npz_gt_video_start_frame": 14, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 14, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1804_0_0"}, {"texts": ["A boy wearing gray clothes is sitting and eating."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/04Q8f9lEzoA.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1805_0_0"}, {"texts": ["A girl wearing a black and purple dress is at first looking to the horse."], "durations": null, "exact_frames_per_prompt": [16], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-2OIwKdLW_Q.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 20, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1807_0_0"}, {"texts": ["The girl touches it and then turns in the right direction and walks away."], "durations": null, "exact_frames_per_prompt": [39], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-2OIwKdLW_Q.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 20, "npz_gt_video_start_frame": 20, "npz_gt_video_end_frame": 59, "skip_frames_after_generation": 20, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1807_0_1"}, {"texts": ["A girl wearing a black top is standing on the right side."], "durations": null, "exact_frames_per_prompt": [13], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-2OIwKdLW_Q.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 17, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1808_1_0"}, {"texts": ["The girl is caressing the horse."], "durations": null, "exact_frames_per_prompt": [16], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-2OIwKdLW_Q.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 17, "npz_gt_video_start_frame": 17, "npz_gt_video_end_frame": 33, "skip_frames_after_generation": 17, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1808_1_1"}, {"texts": ["A person whose hands are visible is mixing eggs in a bowl."], "durations": null, "exact_frames_per_prompt": [14], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/2wYJ7ktqcbQ.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 18, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1809_0_0"}, {"texts": ["The person is putting some stuff and basil on the plate"], "durations": null, "exact_frames_per_prompt": [49], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/2wYJ7ktqcbQ.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 18, "npz_gt_video_start_frame": 18, "npz_gt_video_end_frame": 67, "skip_frames_after_generation": 18, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1809_0_1"}, {"texts": ["The person is cutting the cheese slice."], "durations": null, "exact_frames_per_prompt": [13], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/2wYJ7ktqcbQ.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 67, "npz_gt_video_start_frame": 67, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 67, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1809_0_2"}, {"texts": ["A person whose half body is visible is standing and folding a handkerchief from the corner."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/0RaB5Is8lqE.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_180_0_0"}, {"texts": ["A child wearing a white cloth is lying on the baby seat and crying while a person whose hands are visible is touching the baby's face and holding its hand."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/13FlkeBsdP0.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1810_0_0"}, {"texts": ["A person whose only hand is visible is putting his hand on the child's mouth."], "durations": null, "exact_frames_per_prompt": [44], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/13FlkeBsdP0.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 48, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1810_1_0"}, {"texts": ["The person is holding the child's hand."], "durations": null, "exact_frames_per_prompt": [31], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/13FlkeBsdP0.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 48, "npz_gt_video_start_frame": 48, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 48, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1810_1_1"}, {"texts": ["A man wearing a black shirt is standing and moving his hands in sign language."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/6jKTR82hEAw.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1811_0_0"}, {"texts": ["A person whose only legs are visible is turning the burned wood with a round shovel."], "durations": null, "exact_frames_per_prompt": [48], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/1B0UEFlj1KE.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 52, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1820_0_0"}, {"texts": ["A person whose legs are visible, is standing and collecting coal with a shovel."], "durations": null, "exact_frames_per_prompt": [51], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/1B0UEFlj1KE.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 55, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1821_0_0"}, {"texts": ["A woman wearing white-brown clothes is sitting and speaking."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/1WKxo53nUFk.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1822_0_0"}, {"texts": ["A person, whose only hands and legs are visible, wearing black gloves, black jeans, and black boots, and is holding a shovel and removing the snow with a shovel."], "durations": null, "exact_frames_per_prompt": [32], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/06b4WyjU4k4.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 28, "npz_gt_video_start_frame": 28, "npz_gt_video_end_frame": 60, "skip_frames_after_generation": 28, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1825_1_0"}, {"texts": ["A boy wearing a yellow t-shirt is sitting and taking out the toys with his hands."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-7ID845G8-Q.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1826_0_0"}, {"texts": ["A boy wearing a yellow t-shirt is sitting."], "durations": null, "exact_frames_per_prompt": [24], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-7ID845G8-Q.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 28, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1827_0_0"}, {"texts": ["The boy wearing a yellow t-shirt is taking out a toy with his hands."], "durations": null, "exact_frames_per_prompt": [52], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-7ID845G8-Q.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 28, "npz_gt_video_start_frame": 28, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 28, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1827_0_1"}, {"texts": ["A boy wearing a black hoodie is standing and holding something in his hands."], "durations": null, "exact_frames_per_prompt": [20], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/1JiuP_HYIDg.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 51, "npz_gt_video_start_frame": 51, "npz_gt_video_end_frame": 71, "skip_frames_after_generation": 51, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1828_1_0"}, {"texts": ["A man wearing a shirt and pants is setting fire."], "durations": null, "exact_frames_per_prompt": [43], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/7Lc4GRMjphM.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 47, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_182_0_0"}, {"texts": ["The man is breaking a wood stick."], "durations": null, "exact_frames_per_prompt": [22], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/7Lc4GRMjphM.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 47, "npz_gt_video_start_frame": 47, "npz_gt_video_end_frame": 69, "skip_frames_after_generation": 47, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_182_0_1"}, {"texts": ["The man is cooking sausage."], "durations": null, "exact_frames_per_prompt": [10], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/7Lc4GRMjphM.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 69, "npz_gt_video_start_frame": 69, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 69, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_182_0_2"}, {"texts": ["A woman wearing a red t-shirt is standing and decorating a cake with a piping bag."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/4VNMQxewqe8.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1831_0_0"}, {"texts": ["A person whose hand is visible, is cutting the hairs of white dog."], "durations": null, "exact_frames_per_prompt": [36], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/0Dhj_d6PDRI.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 40, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1832_0_ms_0"}, {"texts": ["A white dog is standing and getting cut on its hair."], "durations": null, "exact_frames_per_prompt": [34], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/0Dhj_d6PDRI.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 6, "npz_gt_video_start_frame": 6, "npz_gt_video_end_frame": 40, "skip_frames_after_generation": 6, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1832_1_ms_0"}, {"texts": ["A man wearing a dark blue jacket is holding a baby and is helping the baby in plucking the apples."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/1gIVwBdd4nE.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1833_0_0"}, {"texts": ["A baby wearing light blue clothes is being held by the man and the baby is plucking the apples from the green tree."], "durations": null, "exact_frames_per_prompt": [13], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/1gIVwBdd4nE.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 17, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1833_1_0"}, {"texts": ["A man wearing a dark blue jacket and a red cap is standing while holding a boy and then touches the green apple with his left hand."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/1gIVwBdd4nE.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1834_0_0"}, {"texts": ["A boy wearing a sky blue hoodie and blue jeans is being held by the man and is plucking the green apple from the tree with his right hand and holding the apple with both hands."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/1gIVwBdd4nE.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1834_1_0"}, {"texts": ["A man on the left side wearing white gloves is tattooing on a person's back with a tattoo machine."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/1AMAaZTN3tk.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1837_0_0"}, {"texts": ["A person on the right side wearing a metal chain is getting a tattoo on his back while the other person wearing black t-shirt is applies tattoo on the first person's back"], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/1AMAaZTN3tk.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1837_1_0"}, {"texts": ["A person wearing black clothes is standing on the floor and tying the knot on the rope."], "durations": null, "exact_frames_per_prompt": [77], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/01Q2INe8JuY.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 81, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1838_0_0"}, {"texts": ["A boy, wearing a graphic red t-shirt and black pants is standing on the brown surface, holding a half torn white paper in his hands, then the video starts to reverse and the paper is back to the floor before it is torn by the boy."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/2K-dquqiFz4.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_183_0_0"}, {"texts": ["A man wearing white clothes is sitting on green chair and he is making tea while another man wearing a blue shirt is standing and looking here and there while touching the first man."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/3B-aQKJuDxM.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1840_0_0"}, {"texts": ["A man wearing light blue shirt is standing on the right and he is talking while a man in white T-shirt sits on the chair and making something and laughing."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/3B-aQKJuDxM.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1840_1_0"}, {"texts": ["A boy wearing a gray-black sweater is sitting on a chair is cutting a gift wrapping paper from the paper roll."], "durations": null, "exact_frames_per_prompt": [31], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/0Zce-xGFFqM.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 35, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1842_0_0"}, {"texts": ["The boy throws the paper roll on the bed and starts wrapping."], "durations": null, "exact_frames_per_prompt": [44], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/0Zce-xGFFqM.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 35, "npz_gt_video_start_frame": 35, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 35, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1842_0_1"}, {"texts": ["A woman wearing a white-black striped shirt is standing, touching her bow tie, smiling while a man wearing a white shirt is standing on the right side and touching the woman's bow tie."], "durations": null, "exact_frames_per_prompt": [16], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/0hUNeCQJCZA.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 20, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1843_0_0"}, {"texts": ["The woman wearing a white-black striped shirt is getting her bow tie opened by the man."], "durations": null, "exact_frames_per_prompt": [42], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/0hUNeCQJCZA.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 20, "npz_gt_video_start_frame": 20, "npz_gt_video_end_frame": 63, "skip_frames_after_generation": 20, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1843_0_1"}, {"texts": ["The woman wearing a white-black striped shirt is talking and walking away while a man wearing a white shirt is walking towards the left while holding a bow tie in his hands."], "durations": null, "exact_frames_per_prompt": [11], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/0hUNeCQJCZA.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 63, "npz_gt_video_start_frame": 63, "npz_gt_video_end_frame": 74, "skip_frames_after_generation": 63, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1843_0_2"}, {"texts": ["A man wearing a white shirt is standing.  while a woman wearing a striped shirt is standing on the left side and opening her bow tie."], "durations": null, "exact_frames_per_prompt": [11], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/0hUNeCQJCZA.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 15, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1843_1_0"}, {"texts": ["The man is opening a bow tie of the woman."], "durations": null, "exact_frames_per_prompt": [48], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/0hUNeCQJCZA.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 15, "npz_gt_video_start_frame": 15, "npz_gt_video_end_frame": 63, "skip_frames_after_generation": 15, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1843_1_1"}, {"texts": ["The man is moving and talking and the woman is walking to the right."], "durations": null, "exact_frames_per_prompt": [17], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/0hUNeCQJCZA.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 63, "npz_gt_video_start_frame": 63, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 63, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1843_1_2"}, {"texts": ["A woman wearing a black-white lining shirt is standing and touching the black bow tie, then the man is tying the bow tie."], "durations": null, "exact_frames_per_prompt": [63], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/0hUNeCQJCZA.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 67, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1844_0_0"}, {"texts": ["The woman is walking towards the right side while a man wearing a white shirt is walking towards the left side, holding a bow tie in his hand."], "durations": null, "exact_frames_per_prompt": [13], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/0hUNeCQJCZA.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 67, "npz_gt_video_start_frame": 67, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 67, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1844_0_1"}, {"texts": ["A man wearing a white shirt is standing."], "durations": null, "exact_frames_per_prompt": [15], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/0hUNeCQJCZA.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 19, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1844_1_0"}, {"texts": ["The man is untying the bow tie of the woman."], "durations": null, "exact_frames_per_prompt": [39], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/0hUNeCQJCZA.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 19, "npz_gt_video_start_frame": 19, "npz_gt_video_end_frame": 58, "skip_frames_after_generation": 19, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1844_1_1"}, {"texts": ["The man is holding the bow tie in his hand."], "durations": null, "exact_frames_per_prompt": [22], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/0hUNeCQJCZA.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 58, "npz_gt_video_start_frame": 58, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 58, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1844_1_2"}, {"texts": ["A man whose hand is visible is getting a tattoo on his finger then getting his finger wiped by the other man."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/2orR-NkIVJo.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1845_0_0"}, {"texts": ["A man wearing glasses is making tattoo on the finger of the first person with a tattoo making machine then starts wiping the finger with a white cloth."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/2orR-NkIVJo.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1845_1_0"}, {"texts": ["A person whose hand is visible wearing a black cloth is putting some yellow stuff on bread slices through a silicone spatula."], "durations": null, "exact_frames_per_prompt": [48], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/0jLiuH1GrEQ.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 32, "npz_gt_video_start_frame": 32, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 32, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1846_0_0"}, {"texts": ["A baby lying on the white bed sheet on the floor is looking at the person's hand and is laughing."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/2gsYyurCQao.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1849_0_0"}, {"texts": ["A person whose only hands are visible is tearing the papers while a baby is lying on the white cloth and watching the person."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/2gsYyurCQao.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1849_1_0"}, {"texts": ["A woman wearing a blue top is showing hand signals on the stage and then starts walking while people are sitting in the front and watching the woman."], "durations": null, "exact_frames_per_prompt": [27], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/4xrmCu8eCHg.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 31, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_184_0_0"}, {"texts": ["The woman starts walking and then people start clapping."], "durations": null, "exact_frames_per_prompt": [23], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/4xrmCu8eCHg.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 31, "npz_gt_video_start_frame": 31, "npz_gt_video_end_frame": 54, "skip_frames_after_generation": 31, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_184_0_1"}, {"texts": ["A man wearing a blue t-shirt is sitting in a chair in the front while a girl in blue top is performing on the stage and some people are watching her and then starts clapping."], "durations": null, "exact_frames_per_prompt": [30], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/4xrmCu8eCHg.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 34, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_184_1_0"}, {"texts": ["The man wearing a blue t-shirt starts cheering."], "durations": null, "exact_frames_per_prompt": [20], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/4xrmCu8eCHg.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 34, "npz_gt_video_start_frame": 34, "npz_gt_video_end_frame": 54, "skip_frames_after_generation": 34, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_184_1_1"}, {"texts": ["A woman wearing a yellow top is sitting in the back while another woman wearing a blue top is standing on the stage and moving her hands then makes a pose, a person whose head is visible is sitting and watching another woman."], "durations": null, "exact_frames_per_prompt": [25], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/4xrmCu8eCHg.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 29, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_184_2_0"}, {"texts": ["The woman wearing a yellow top starts clapping above her head while other people also starts clapping."], "durations": null, "exact_frames_per_prompt": [25], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/4xrmCu8eCHg.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 29, "npz_gt_video_start_frame": 29, "npz_gt_video_end_frame": 54, "skip_frames_after_generation": 29, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_184_2_1"}, {"texts": ["A baby is lying on the bed, laughing and watching the person one."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/2gsYyurCQao.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1850_2_0"}, {"texts": ["A man wearing a blue shirt is speaking."], "durations": null, "exact_frames_per_prompt": [47], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/2wG3dRYonNA.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 51, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1851_0_0"}, {"texts": ["A lady wearing white top and jeans is sitting on the grey surface and cleaning the tears of a white cattle while another person is talking with her and laughing."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/0RnxRZ744Ds.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1852_0_0"}, {"texts": ["A white cattle is standing on dry grass while a woman in blue jeans is sitting and cleaning cattle's udders with a cloth and giving it to a person."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/0RnxRZ744Ds.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1852_2_0"}, {"texts": ["A man in red and blue clothing is kneeling on the floor near the car while holding a gauge in his hand."], "durations": null, "exact_frames_per_prompt": [30], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-zXe3CFCbww.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 34, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1857_0_0"}, {"texts": ["The man loosens the valve of the car's wheel."], "durations": null, "exact_frames_per_prompt": [46], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-zXe3CFCbww.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 34, "npz_gt_video_start_frame": 34, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 34, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1857_0_1"}, {"texts": ["A man wearing red clothes is standing and holding golf sticks and speaking."], "durations": null, "exact_frames_per_prompt": [68], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-kwBcs0-DdQ.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 72, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1858_0_0"}, {"texts": ["The man is hitting a golf ball with his golf stick."], "durations": null, "exact_frames_per_prompt": [8], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-kwBcs0-DdQ.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 72, "npz_gt_video_start_frame": 72, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 72, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1858_0_1"}, {"texts": ["A man wearing a red t-shirt is standing and holding the golf sticks on his both hands."], "durations": null, "exact_frames_per_prompt": [69], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-kwBcs0-DdQ.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 73, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1859_0_0"}, {"texts": ["The man wearing a red t-shirt is taking a shot from the left hand."], "durations": null, "exact_frames_per_prompt": [6], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-kwBcs0-DdQ.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 73, "npz_gt_video_start_frame": 73, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 73, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1859_0_1"}, {"texts": ["A man wearing blue trousers is at first kneeling on the pavement while holding the collar belt of the dog."], "durations": null, "exact_frames_per_prompt": [9], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/3p17Ro4ckUg.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 13, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_185_0_0"}, {"texts": ["The man wearing blue trousers gets up and starts walking in the straight direction along with the dog."], "durations": null, "exact_frames_per_prompt": [32], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/3p17Ro4ckUg.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 13, "npz_gt_video_start_frame": 13, "npz_gt_video_end_frame": 45, "skip_frames_after_generation": 13, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_185_0_1"}, {"texts": ["A white colored dog is at first sitting on the pavement while a man wearing a black jacket is attaching the leash to the dog and then standing up."], "durations": null, "exact_frames_per_prompt": [26], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/3p17Ro4ckUg.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 30, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_185_1_0"}, {"texts": ["The white colored dog starts walking along with the man in blue trousers."], "durations": null, "exact_frames_per_prompt": [42], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/3p17Ro4ckUg.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 30, "npz_gt_video_start_frame": 30, "npz_gt_video_end_frame": 72, "skip_frames_after_generation": 30, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_185_1_1"}, {"texts": ["A man wearing a gray vest and black shorts is standing on a bench near the trees while a man wearing a cap is standing on the tree on the right side."], "durations": null, "exact_frames_per_prompt": [22], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/1hhfZG8ModU.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 26, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1861_0_0"}, {"texts": ["The man gets down from the bench while a man wearing a cap gets unbalanced and falls."], "durations": null, "exact_frames_per_prompt": [20], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/1hhfZG8ModU.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 26, "npz_gt_video_start_frame": 26, "npz_gt_video_end_frame": 46, "skip_frames_after_generation": 26, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1861_0_1"}, {"texts": ["The man walks and holds the other man."], "durations": null, "exact_frames_per_prompt": [34], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/1hhfZG8ModU.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 46, "npz_gt_video_start_frame": 46, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 46, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1861_0_2"}, {"texts": ["A man wearing a red t-shirt and shorts is standing on the corner of the bench holding the tree branch.  while another man wearing a grey t-shirt and black shorts is standing on the bench and then steps down."], "durations": null, "exact_frames_per_prompt": [42], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/1hhfZG8ModU.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 46, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1861_1_0"}, {"texts": ["The man falls down on the ground.\n while another man wearing a grey t-shirt and black shorts is moving towards the man."], "durations": null, "exact_frames_per_prompt": [19], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/1hhfZG8ModU.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 46, "npz_gt_video_start_frame": 46, "npz_gt_video_end_frame": 65, "skip_frames_after_generation": 46, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1861_1_1"}, {"texts": ["The man starts walking while another man wearing a grey t-shirt and black shorts is standing and looking at him."], "durations": null, "exact_frames_per_prompt": [15], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/1hhfZG8ModU.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 65, "npz_gt_video_start_frame": 65, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 65, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1861_1_2"}, {"texts": ["A person wearing a white vest is standing on a bench and eating fruits from the second person."], "durations": null, "exact_frames_per_prompt": [22], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/1hhfZG8ModU.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 26, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1862_0_0"}, {"texts": ["The person wearing a white vest gets down from the bench and then the first person falls on the ground."], "durations": null, "exact_frames_per_prompt": [54], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/1hhfZG8ModU.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 26, "npz_gt_video_start_frame": 26, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 26, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1862_0_1"}, {"texts": ["A person wearing a red t-shirt is standing on the top of a bench and plucking fruits from a tree, giving them to the first person."], "durations": null, "exact_frames_per_prompt": [28], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/1hhfZG8ModU.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 32, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1862_1_0"}, {"texts": ["The person is falling while a person wearing a grey vest is runs towards the another man."], "durations": null, "exact_frames_per_prompt": [21], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/1hhfZG8ModU.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 32, "npz_gt_video_start_frame": 32, "npz_gt_video_end_frame": 53, "skip_frames_after_generation": 32, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1862_1_1"}, {"texts": ["A man wearing a grey vest and black shorts is standing on the bench near the tree.  while another man in grey shorts is standing on the bench picking something from the tree."], "durations": null, "exact_frames_per_prompt": [25], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/1hhfZG8ModU.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 29, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1863_0_0"}, {"texts": ["The man gets down from the bench and then walks and holds the other man."], "durations": null, "exact_frames_per_prompt": [41], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/1hhfZG8ModU.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 29, "npz_gt_video_start_frame": 29, "npz_gt_video_end_frame": 70, "skip_frames_after_generation": 29, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1863_0_1"}, {"texts": ["A man wearing a red t-shirt and shorts is standing on the corner of the bench holding the tree branch with a man wearing grey t-shirt standing on the middle of the bench"], "durations": null, "exact_frames_per_prompt": [36], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/1hhfZG8ModU.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 40, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1863_1_0"}, {"texts": ["The man falls down on the ground and the second man moves to left to  help the first man"], "durations": null, "exact_frames_per_prompt": [27], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/1hhfZG8ModU.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 40, "npz_gt_video_start_frame": 40, "npz_gt_video_end_frame": 68, "skip_frames_after_generation": 40, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1863_1_1"}, {"texts": ["The man starts walking."], "durations": null, "exact_frames_per_prompt": [12], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/1hhfZG8ModU.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 68, "npz_gt_video_start_frame": 68, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 68, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1863_1_2"}, {"texts": ["A boy wearing a dark blue t-shirt is drinking a drink from the black can and is speaking."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/3-5u_EheUkY.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1864_0_0"}, {"texts": ["A girl wearing red-black clothes is sitting and holding a white spoon."], "durations": null, "exact_frames_per_prompt": [20], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/2AFstwPeq6c.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 24, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1868_0_0"}, {"texts": ["The girl wearing red-black clothes is mixing the vegetables in a box on the blue table."], "durations": null, "exact_frames_per_prompt": [56], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/2AFstwPeq6c.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 24, "npz_gt_video_start_frame": 24, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 24, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1868_0_1"}, {"texts": ["A person wearing black clothes whose hand is visible is making a bread sandwich."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/0PGydvxk00Q.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1869_0_0"}, {"texts": ["A boy wearing a blue- black t-shirt is standing and holding the red metal stick with sausages."], "durations": null, "exact_frames_per_prompt": [34], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/6dXOjPfksa8.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 10, "npz_gt_video_start_frame": 10, "npz_gt_video_end_frame": 44, "skip_frames_after_generation": 10, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1873_0_0"}, {"texts": ["The boy puts the sausages on the barbeque."], "durations": null, "exact_frames_per_prompt": [36], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/6dXOjPfksa8.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 44, "npz_gt_video_start_frame": 44, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 44, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1873_0_1"}, {"texts": ["A woman wearing a black top is getting a tattoo drawn up on her back while a man behind the girl is making a tattoo on a woman's back."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/2GXs2qZ-47g.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1874_0_0"}, {"texts": ["A man wearing a black t-shirt is drawing a tattoo on the back of a woman."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/2GXs2qZ-47g.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1874_1_0"}, {"texts": ["A man wearing a white shirt is sitting while putting his hands on the table as an old man wearing a white kurta is sitting and moving his hands while speaking."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/7scGpjeIzk8.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1875_0_0"}, {"texts": ["A man wearing a white cap is sitting on a black chair, moving his hand and speaking while another man on the right side wearing a white shirt is sitting and looking down at the book and looks at the first man."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/7scGpjeIzk8.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1875_1_0"}, {"texts": ["A second man whose upper half-body is visible, wearing a green t-shirt and a camouflage printed hat."], "durations": null, "exact_frames_per_prompt": [23], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/0UgyqJtni6g.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 57, "npz_gt_video_start_frame": 57, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 57, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1876_4_0"}, {"texts": ["A woman wearing a white dress is sitting on the back of an elephant and taking a ride while looking in the right direction while the man is sitting on the head of the elephant"], "durations": null, "exact_frames_per_prompt": [25], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-ke3Rv7nUTI.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 37, "npz_gt_video_start_frame": 37, "npz_gt_video_end_frame": 62, "skip_frames_after_generation": 37, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1878_0_0"}, {"texts": ["A man in greyish clothing is sitting on the head of an elephant on which a woman is taking a ride."], "durations": null, "exact_frames_per_prompt": [25], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-ke3Rv7nUTI.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 37, "npz_gt_video_start_frame": 37, "npz_gt_video_end_frame": 62, "skip_frames_after_generation": 37, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1878_1_0"}, {"texts": ["An elephant is giving a ride to the man and the woman sitting on its back."], "durations": null, "exact_frames_per_prompt": [25], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-ke3Rv7nUTI.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 37, "npz_gt_video_start_frame": 37, "npz_gt_video_end_frame": 62, "skip_frames_after_generation": 37, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1878_2_0"}, {"texts": ["A person wearing light green trousers is standing and holding a golf stick."], "durations": null, "exact_frames_per_prompt": [24], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-0Yb1tD3HUo.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 28, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_187_0_0"}, {"texts": ["The person wearing light green trousers is hitting a golf ball on a green surface."], "durations": null, "exact_frames_per_prompt": [52], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-0Yb1tD3HUo.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 28, "npz_gt_video_start_frame": 28, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 28, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_187_0_1"}, {"texts": ["A man whose only hands are visible is tying the knot of a white rope against a black scissor."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/1ay4OcifxI0.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1880_0_0"}, {"texts": ["A man whose hands are visible is holding a scissor and rope, knotting the rope by pulling it."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/1ay4OcifxI0.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1881_0_0"}, {"texts": ["A man whose hands are visible is tying a knot in the white rope against a black scissor."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/1ay4OcifxI0.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1882_0_0"}, {"texts": ["A woman wearing a light green cloth whose hand is visible is taking oil in a spoon."], "durations": null, "exact_frames_per_prompt": [26], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/3Jisdo1qg1Y.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 30, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1883_0_0"}, {"texts": ["The woman starts mixing the vegetables from chopsticks."], "durations": null, "exact_frames_per_prompt": [50], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/3Jisdo1qg1Y.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 30, "npz_gt_video_start_frame": 30, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 30, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1883_0_1"}, {"texts": ["A girl wearing a blue t-shirt, sky blue jeans and brown boots is sitting on the first brown horse and is riding in the jungle."], "durations": null, "exact_frames_per_prompt": [55], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/14f747vIr70.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 59, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1884_0_0"}, {"texts": ["A girl wearing a blue t-shirt, sky blue jeans and brown boots is looking at the backside."], "durations": null, "exact_frames_per_prompt": [14], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/14f747vIr70.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 59, "npz_gt_video_start_frame": 59, "npz_gt_video_end_frame": 74, "skip_frames_after_generation": 59, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1884_0_1"}, {"texts": ["A white-brown horse is walking in the river in the jungle while a woman wearing a blue top is sitting on the horse, another brown horse is also moving behind the white-brown horse."], "durations": null, "exact_frames_per_prompt": [70], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/14f747vIr70.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 74, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1884_1_0"}, {"texts": ["A girl wearing a grey top and white shorts is standing on the grey surface, holding a red bottle in one hand and a knife in the other."], "durations": null, "exact_frames_per_prompt": [23], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/rJ7PZcjOm-8.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 27, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1885_0_0"}, {"texts": ["The girl is trying to open the cap of the bottle."], "durations": null, "exact_frames_per_prompt": [53], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/rJ7PZcjOm-8.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 27, "npz_gt_video_start_frame": 27, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 27, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1885_0_1"}, {"texts": ["A boy wearing a white t-shirt is sitting and eating food and drinking."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-uNlu8LzEeg.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1886_0_0"}, {"texts": ["A person whose hand is visible is getting a knot tied on the white cloth wrapped around his hand by another person in a blue t-shirt."], "durations": null, "exact_frames_per_prompt": [64], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/1Y_GqG1Kg04.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 68, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1887_1_0"}, {"texts": ["A woman wearing a blue shirt is cutting something."], "durations": null, "exact_frames_per_prompt": [24], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/0okGQMbIe8I.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 28, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1889_0_0"}, {"texts": ["The woman putting some black ingredient on the bread sandwich."], "durations": null, "exact_frames_per_prompt": [52], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/0okGQMbIe8I.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 28, "npz_gt_video_start_frame": 28, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 28, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1889_0_1"}, {"texts": ["A man wearing a white sweatshirt and grey pants is standing."], "durations": null, "exact_frames_per_prompt": [22], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/3-LqeQoaJgg.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 26, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_188_0_0"}, {"texts": ["The man then picks up a gas pump hose."], "durations": null, "exact_frames_per_prompt": [20], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/3-LqeQoaJgg.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 26, "npz_gt_video_start_frame": 26, "npz_gt_video_end_frame": 46, "skip_frames_after_generation": 26, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_188_0_1"}, {"texts": ["A person, of whom hand is visible only, is caressing the first fish while an orange-silver fish is swimming in the aquarium."], "durations": null, "exact_frames_per_prompt": [46], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/1nNs5vNTYXo.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 50, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1893_0_0"}, {"texts": ["The person, of whom hand is visible only is then touching the second fish in the aquarium."], "durations": null, "exact_frames_per_prompt": [30], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/1nNs5vNTYXo.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 50, "npz_gt_video_start_frame": 50, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 50, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1893_0_1"}, {"texts": ["A golden yellow fish is swimming in the aquarium and being caressed by the person and another white and red fish swims in the aquarium, being stroked by the person."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/1nNs5vNTYXo.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1893_1_0"}, {"texts": ["A gray-orange fish is swimming in the aquarium from right to left."], "durations": null, "exact_frames_per_prompt": [15], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/1nNs5vNTYXo.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 40, "npz_gt_video_start_frame": 40, "npz_gt_video_end_frame": 55, "skip_frames_after_generation": 40, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1893_2_0"}, {"texts": ["The fish is being touched by the person."], "durations": null, "exact_frames_per_prompt": [25], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/1nNs5vNTYXo.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 55, "npz_gt_video_start_frame": 55, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 55, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1893_2_1"}, {"texts": ["A girl wearing a purple dress is standing near a woman, holding a gray fork."], "durations": null, "exact_frames_per_prompt": [12], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-WKXzXGaKzE.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 16, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1899_0_0"}, {"texts": ["The girl wearing a purple dress is picking food from the gray pan and the woman is helping her to pick up the food."], "durations": null, "exact_frames_per_prompt": [52], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-WKXzXGaKzE.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 16, "npz_gt_video_start_frame": 16, "npz_gt_video_end_frame": 68, "skip_frames_after_generation": 16, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1899_0_1"}, {"texts": ["The girl wearing a purple dress leaves the fork then the woman starts picking the food herself."], "durations": null, "exact_frames_per_prompt": [12], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-WKXzXGaKzE.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 68, "npz_gt_video_start_frame": 68, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 68, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1899_0_2"}, {"texts": ["A man wearing a white full-sleeve t-shirt and gray jeans is standing and talking to the other man."], "durations": null, "exact_frames_per_prompt": [20], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/3-LqeQoaJgg.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 24, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_189_0_0"}, {"texts": ["The man is picking up the digital fuel nozzle with his right hand from the fuel dispenser machine."], "durations": null, "exact_frames_per_prompt": [22], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/3-LqeQoaJgg.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 24, "npz_gt_video_start_frame": 24, "npz_gt_video_end_frame": 46, "skip_frames_after_generation": 24, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_189_0_1"}, {"texts": ["A man whose only upper half-body is visible wearing a blue hoodie, is standing while putting his own hands inside the jacket's pocket and is talking to the first man."], "durations": null, "exact_frames_per_prompt": [42], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/3-LqeQoaJgg.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 46, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_189_1_0"}, {"texts": ["A man wearing a red t-shirt, gray shorts and black slippers is standing on the left side while a boy in the blue vest is standing and talking."], "durations": null, "exact_frames_per_prompt": [26], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/0cu3HHsVCGE.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 30, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_18_0_0"}, {"texts": ["The man wearing a red t-shirt, gray shorts and black slippers is feeding the seeds to the brown goat while two lambs are standing at the back in the barn."], "durations": null, "exact_frames_per_prompt": [50], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/0cu3HHsVCGE.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 30, "npz_gt_video_start_frame": 30, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 30, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_18_0_1"}, {"texts": ["A boy whose upper half-body is visible wearing a dark blue t-shirt is standing on the left side of the gray surface and is speaking while pointing his left hand finger at the goat while the man wearing white shorts is feeding a goat with brown fur, and the two small goats are looking at the boy."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/0cu3HHsVCGE.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_18_1_0"}, {"texts": ["A brown goat is standing on the right side of the light brown grass surface in the cage and is eating the seeds given by the man and the kid wearing a blue t-shirt is standing and pointing towards the goat, and some goats are standing at the back."], "durations": null, "exact_frames_per_prompt": [55], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/0cu3HHsVCGE.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 19, "npz_gt_video_start_frame": 19, "npz_gt_video_end_frame": 74, "skip_frames_after_generation": 19, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_18_2_0"}, {"texts": ["A girl wearing a black hoodie is standing and singing while doing hand gestures."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/1e99jW8aUVc.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1901_0_0"}, {"texts": ["A boy wearing a blue t-shirt is standing, holding a fishing rod and hanging a fish on the hook of the fishing rod."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/29-_P0vNz0w.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1904_0_0"}, {"texts": ["A fish is hanging on the hook of the fishing rod while a boy in a printed blue t-shirt is holding the fishing rod in his left hand."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/29-_P0vNz0w.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1904_1_0"}, {"texts": ["A man whose only hands are visible, wearing a watch in his left hand and  putting the white rope on the brown pair of ropes while knitting the rope."], "durations": null, "exact_frames_per_prompt": [38], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/03dHaEVHpKA.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 42, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1905_0_0"}, {"texts": ["The man pulling the brown and white ropes with both hands."], "durations": null, "exact_frames_per_prompt": [28], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/03dHaEVHpKA.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 42, "npz_gt_video_start_frame": 42, "npz_gt_video_end_frame": 70, "skip_frames_after_generation": 42, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1905_0_1"}, {"texts": ["A person wearing blue jeans is playing golf."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/1dvcVDJ8dMk.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1906_0_0"}, {"texts": ["A man wearing a grey t-shirt and pink shorts is standing, holding a fishing rod."], "durations": null, "exact_frames_per_prompt": [11], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/2rdfX9Y_9jo.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 15, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1907_0_0"}, {"texts": ["The man leans forward."], "durations": null, "exact_frames_per_prompt": [14], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/2rdfX9Y_9jo.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 15, "npz_gt_video_start_frame": 15, "npz_gt_video_end_frame": 29, "skip_frames_after_generation": 15, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1907_0_1"}, {"texts": ["The man gets the fish out of the fishing net."], "durations": null, "exact_frames_per_prompt": [51], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/2rdfX9Y_9jo.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 29, "npz_gt_video_start_frame": 29, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 29, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1907_0_2"}, {"texts": ["A man wearing a white t-shirt, gray pants, blue goggles and a gray cap is sitting on a wooden stool and milking the udder in a bucket."], "durations": null, "exact_frames_per_prompt": [32], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-jRvvpzCh7w.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 36, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1908_0_0"}, {"texts": ["A woman, whose hand is visible only, is milking the udder in a bucket.\n while a person whose legs are visible is standing on the brown surface."], "durations": null, "exact_frames_per_prompt": [32], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-jRvvpzCh7w.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 48, "npz_gt_video_start_frame": 48, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 48, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1908_5_0"}, {"texts": ["A light brown cow is standing on the wooden ramp and letting the man milking."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-jRvvpzCh7w.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1908_6_0"}, {"texts": ["A man wearing an off-white t-shirt and light brown trousers is sitting and cleaning the udder of the cow while a man wearing blue jeans is standing near the cow."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-jRvvpzCh7w.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1909_0_0"}, {"texts": ["A person whose hand is visible is pressing cow's udder and milking the cow while another person wearing wearing blue jeans is standing at the back."], "durations": null, "exact_frames_per_prompt": [32], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-jRvvpzCh7w.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 48, "npz_gt_video_start_frame": 48, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 48, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1909_3_0"}, {"texts": ["A brown-white cow is standing and getting milked while the man takes the milk from the cow."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-jRvvpzCh7w.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1909_4_0"}, {"texts": ["A man wearing a white t-shirt is standing on the back while the man in blue top is speaking something"], "durations": null, "exact_frames_per_prompt": [22], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/3-LqeQoaJgg.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 26, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_190_0_0"}, {"texts": ["The man takes the fuel dispenser in his hand."], "durations": null, "exact_frames_per_prompt": [20], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/3-LqeQoaJgg.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 26, "npz_gt_video_start_frame": 26, "npz_gt_video_end_frame": 46, "skip_frames_after_generation": 26, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_190_0_1"}, {"texts": ["A man wearing a blue hoodie is standing on the left side while a man wearing white T-shirt is talking with him and holding a bow."], "durations": null, "exact_frames_per_prompt": [42], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/3-LqeQoaJgg.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 46, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_190_1_0"}, {"texts": ["A man wearing a black t-shirt is moving an object on the wooden rack while a girl wearing a pink dress is feeding the goat with her hand and then she steps back."], "durations": null, "exact_frames_per_prompt": [48], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/1CneleBQSZs.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 32, "npz_gt_video_start_frame": 32, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 32, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1910_1_0"}, {"texts": ["A man wearing a jacket is sitting on a chair and moving his hands."], "durations": null, "exact_frames_per_prompt": [49], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/9F5eHKqWq2o.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 31, "npz_gt_video_start_frame": 31, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 31, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1911_0_0"}, {"texts": ["A person whose arm is visible is on the left side.\n"], "durations": null, "exact_frames_per_prompt": [49], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/9F5eHKqWq2o.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 31, "npz_gt_video_start_frame": 31, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 31, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1911_1_ms_0"}, {"texts": ["A man wearing a black t-shirt is sitting and eating chips."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/1oJZGZ4whRA.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1913_0_0"}, {"texts": ["A person whose hands are visible is folding a pack of Canadian dollars."], "durations": null, "exact_frames_per_prompt": [43], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/0u_WQyfsUa4.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 47, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1914_0_0"}, {"texts": ["The person is counting the notes by flipping them."], "durations": null, "exact_frames_per_prompt": [33], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/0u_WQyfsUa4.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 47, "npz_gt_video_start_frame": 47, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 47, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1914_0_1"}, {"texts": ["A boy wearing a black printed t-shirt is at first speaking while showing his hand to the horse."], "durations": null, "exact_frames_per_prompt": [34], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/0bLE1iAXn2Q.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 38, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1915_0_0"}, {"texts": ["The boy is rubbing his hands."], "durations": null, "exact_frames_per_prompt": [21], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/0bLE1iAXn2Q.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 38, "npz_gt_video_start_frame": 38, "npz_gt_video_end_frame": 59, "skip_frames_after_generation": 38, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1915_0_1"}, {"texts": ["The boy is applying cream on the horse."], "durations": null, "exact_frames_per_prompt": [21], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/0bLE1iAXn2Q.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 59, "npz_gt_video_start_frame": 59, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 59, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1915_0_2"}, {"texts": ["A brown-colored horse is standing on the muddy surface under the shade and getting a cream applied on it."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/0bLE1iAXn2Q.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1915_1_0"}, {"texts": ["A boy wearing a black t-shirt and gray pants is standing, his hands are lifted up with a horse on right"], "durations": null, "exact_frames_per_prompt": [34], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/0bLE1iAXn2Q.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 38, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1916_0_0"}, {"texts": ["The boy starts rubbing his hands together."], "durations": null, "exact_frames_per_prompt": [21], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/0bLE1iAXn2Q.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 38, "npz_gt_video_start_frame": 38, "npz_gt_video_end_frame": 59, "skip_frames_after_generation": 38, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1916_0_1"}, {"texts": ["The boy touches the horse's face."], "durations": null, "exact_frames_per_prompt": [21], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/0bLE1iAXn2Q.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 59, "npz_gt_video_start_frame": 59, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 59, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1916_0_2"}, {"texts": ["A dark brown horse is standing near the railing while the boy explains something"], "durations": null, "exact_frames_per_prompt": [51], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/0bLE1iAXn2Q.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 55, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1916_1_0"}, {"texts": ["The horse is getting touched by the boy."], "durations": null, "exact_frames_per_prompt": [25], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/0bLE1iAXn2Q.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 55, "npz_gt_video_start_frame": 55, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 55, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1916_1_1"}, {"texts": ["A boy wearing a red t-shirt is touching the nose of a black-white goat."], "durations": null, "exact_frames_per_prompt": [15], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/1hCeTrNDlnA.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 29, "npz_gt_video_start_frame": 29, "npz_gt_video_end_frame": 44, "skip_frames_after_generation": 29, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1917_3_0"}, {"texts": ["The boy starts walking and pointing his finger towards the sheep."], "durations": null, "exact_frames_per_prompt": [36], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/1hCeTrNDlnA.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 44, "npz_gt_video_start_frame": 44, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 44, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1917_3_1"}, {"texts": ["A brown-white goat is standing on a wooden table and getting caressed by the woman while a group of people in which some people are walking and some people are standing and two animals are standing on the brown surface."], "durations": null, "exact_frames_per_prompt": [20], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/1hCeTrNDlnA.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 24, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1917_4_0"}, {"texts": ["A person wearing black gloves is holding a hive frame, then takes a brush in their hand."], "durations": null, "exact_frames_per_prompt": [63], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-fGAlRGg-xU.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 67, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1918_0_0"}, {"texts": ["A man wearing a black t-shirt detaches the peeled apple from the dark blue drill."], "durations": null, "exact_frames_per_prompt": [47], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/1bKsC3HY2MA.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 51, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1919_0_0"}, {"texts": ["The man wearing a black t-shirt picks up another apple from the packet and attaches it to the drill."], "durations": null, "exact_frames_per_prompt": [29], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/1bKsC3HY2MA.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 51, "npz_gt_video_start_frame": 51, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 51, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1919_0_1"}, {"texts": ["A man wearing a green t-shirt is mixing hot yellow stuff in a white bowl with his hands."], "durations": null, "exact_frames_per_prompt": [71], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-R5LWN8NUws.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 9, "npz_gt_video_start_frame": 9, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 9, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_191_0_0"}, {"texts": ["A man whose only hand is visible is sauteing food with a spatula in a pan."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/0O4ozn9-jGk.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1921_0_0"}, {"texts": ["A man wearing a gray t-shirt, and black pants is standing on the left side and holding a black spatula in his right hand and holding a pan handle with his left hand and saut\u00e9ing the yellow food in the pan."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/0O4ozn9-jGk.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1922_0_0"}, {"texts": ["A white dog is being held by a man while a woman in black top watching them."], "durations": null, "exact_frames_per_prompt": [36], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/2kv48vBWTqs.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 40, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1923_1_0"}, {"texts": ["The white dog moves in and out of the cage, and later moving into the cage."], "durations": null, "exact_frames_per_prompt": [40], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/2kv48vBWTqs.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 40, "npz_gt_video_start_frame": 40, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 40, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1923_1_1"}, {"texts": ["A man wearing a white t-shirt and blue jeans is standing, holding a dog, sitting.  while a woman wearing black and white outfit is first standing on the left side of the man then she sits down."], "durations": null, "exact_frames_per_prompt": [36], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/2kv48vBWTqs.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 40, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1923_2_0"}, {"texts": ["The man is moving the dog in and out of the cage."], "durations": null, "exact_frames_per_prompt": [22], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/2kv48vBWTqs.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 40, "npz_gt_video_start_frame": 40, "npz_gt_video_end_frame": 62, "skip_frames_after_generation": 40, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1923_2_1"}, {"texts": ["The man is putting the dog on the ground."], "durations": null, "exact_frames_per_prompt": [18], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/2kv48vBWTqs.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 62, "npz_gt_video_start_frame": 62, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 62, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1923_2_2"}, {"texts": ["A man wearing a black court dress is sitting on a black chair."], "durations": null, "exact_frames_per_prompt": [44], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/35OdEjZ7738.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 48, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1926_0_0"}, {"texts": ["A man wearing a black t-shirt with written words is mixing chicken meat with his hands in an aluminium foil container."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-N69Jdzk6B4.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1927_0_0"}, {"texts": ["A girl wearing a green top is ironing a purple cloth on a small pink ironing table."], "durations": null, "exact_frames_per_prompt": [62], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/2i_HYrPDaoY.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 66, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1928_0_0"}, {"texts": ["A girl wearing a sky blue t-shirt is standing on a black floor, holding an iron in her right hand and ironing a purple cloth on a pink ironing board."], "durations": null, "exact_frames_per_prompt": [62], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/2i_HYrPDaoY.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 66, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1929_0_0"}, {"texts": ["A woman wearing a top is sitting on a black chair and doing hand gestures."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/4Qc70Olmktk.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1933_0_0"}, {"texts": ["A woman whose only hands are visible is rubbing her hand with the finger."], "durations": null, "exact_frames_per_prompt": [21], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-PBcrI7pXUM.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 25, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1936_0_0"}, {"texts": ["The woman is holding an eye cream above the hand."], "durations": null, "exact_frames_per_prompt": [37], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-PBcrI7pXUM.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 25, "npz_gt_video_start_frame": 25, "npz_gt_video_end_frame": 63, "skip_frames_after_generation": 25, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1936_0_1"}, {"texts": ["A man wearing black t-shirt is sitting on the chair while a man in a black t-shirt and a man in a white t-shirt are sitting opposite each other and eating noodles, the man in the white t-shirt stands up."], "durations": null, "exact_frames_per_prompt": [13], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/0uay8Sbzn8E.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 17, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1937_1_0"}, {"texts": ["The man is eating noodles while the man in the white t-shirt picks up the noodle bowl in his hand."], "durations": null, "exact_frames_per_prompt": [63], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/0uay8Sbzn8E.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 17, "npz_gt_video_start_frame": 17, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 17, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1937_1_1"}, {"texts": ["A man wearing a black cloth is sitting on the left side is eating noodles while two other men are sitting and eating noodles."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/0uay8Sbzn8E.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1937_2_0"}, {"texts": ["A person wearing black gloves is walking in a straight direction while holding a glass of water and the doughnut box while in a group of people, some are sitting, some are moving, and some are eating food."], "durations": null, "exact_frames_per_prompt": [23], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/3y0UMK5DoQU.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 27, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1939_0_0"}, {"texts": ["The person puts the doughnut box and the glass on the ground."], "durations": null, "exact_frames_per_prompt": [37], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/3y0UMK5DoQU.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 27, "npz_gt_video_start_frame": 27, "npz_gt_video_end_frame": 64, "skip_frames_after_generation": 27, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1939_0_1"}, {"texts": ["The person takes doughnuts from the doughnut box."], "durations": null, "exact_frames_per_prompt": [16], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/3y0UMK5DoQU.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 64, "npz_gt_video_start_frame": 64, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 64, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1939_0_2"}, {"texts": ["A man wearing blue and red clothing is sitting on the ground and eating doughnuts while a person wearing a white t-shirt holds a box of donuts and then keeps the glass and box of donuts on the road and picks up donuts from the box and his sunglasses hang, a group of people, some of whom are standing and some of whom are sitting, are eating and drinking."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/3y0UMK5DoQU.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1939_1_0"}, {"texts": ["A man wearing a red t-shirt and dark gray pants is shearing a sheep with a shearing machine on the wooden surface."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-Awg3SHiG9g.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1940_0_0"}, {"texts": ["A white sheep is getting sheared by the man."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-Awg3SHiG9g.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1940_1_0"}, {"texts": ["A small boy whose only upper body is visible wearing a gray t-shirt is standing."], "durations": null, "exact_frames_per_prompt": [20], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/4JnyCzDj37Y.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 24, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1941_0_0"}, {"texts": ["The small boy is holding a cream bottle in his right hand and then opens the book with his left hand and pours the cream on the book."], "durations": null, "exact_frames_per_prompt": [22], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/4JnyCzDj37Y.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 24, "npz_gt_video_start_frame": 24, "npz_gt_video_end_frame": 46, "skip_frames_after_generation": 24, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1941_0_1"}, {"texts": ["A man wearing a black suit is holding a mike and standing on the floor."], "durations": null, "exact_frames_per_prompt": [20], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/2BIFWXEcbtc.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 24, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1942_0_0"}, {"texts": ["The man wearing a black suit is moving in the left direction while doing a hand gesture."], "durations": null, "exact_frames_per_prompt": [56], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/2BIFWXEcbtc.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 24, "npz_gt_video_start_frame": 24, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 24, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1942_0_1"}, {"texts": ["A man wearing a red t-shirt and black trousers is standing while a brown cat is moving on the brown surface and a kid is sitting on a chair."], "durations": null, "exact_frames_per_prompt": [37], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/0JLz81Wvk_k.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 41, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1944_1_0"}, {"texts": ["The man is moving the kid's chair."], "durations": null, "exact_frames_per_prompt": [13], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/0JLz81Wvk_k.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 41, "npz_gt_video_start_frame": 41, "npz_gt_video_end_frame": 54, "skip_frames_after_generation": 41, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1944_1_1"}, {"texts": ["A kid wearing a green t-shirt and white shorts is sitting on a chair and moving his head while the man wearing red t-shirt moves the chair to the right"], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/0JLz81Wvk_k.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1944_2_0"}, {"texts": ["A man wearing a black t-shirt is sitting on a black chair and eating watermelon with its shell."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-_xARnNGrcc.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1947_0_0"}, {"texts": ["A man whose hand is visible is putting a wine glass on a wooden table."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/2_PlnTA-oms.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1950_0_0"}, {"texts": ["A man wearing a black shirt and blue jeans is standing on the white floor and hanging his black-red jacket."], "durations": null, "exact_frames_per_prompt": [37], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/6-UBRxJv6QM.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 41, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1951_0_0"}, {"texts": ["A big black goat is sitting on a grey surface while carrying a baby black goat on its back while the deer passes by"], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/--lrRHlpK68.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1954_1_0"}, {"texts": ["A man wearing a red t-shirt and jeans is sitting on the first horse while the woman in white hoodie sitting on the back horse follow the first horse with the help of the man who hold the horse rope."], "durations": null, "exact_frames_per_prompt": [35], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/6R7D2huuZJM.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 30, "npz_gt_video_start_frame": 30, "npz_gt_video_end_frame": 65, "skip_frames_after_generation": 30, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1955_2_0"}, {"texts": ["A woman wearing a white hoodie and black pants is sitting on the second horse while a man wearing an orange t-shirt is holding a horse leash and moving, the first horse is walking while carrying a person on its back, and another person wearing white clothes is moving in the front."], "durations": null, "exact_frames_per_prompt": [19], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/6R7D2huuZJM.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 46, "npz_gt_video_start_frame": 46, "npz_gt_video_end_frame": 65, "skip_frames_after_generation": 46, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1955_3_0"}, {"texts": ["A man wearing an orange cloth is walking in front of the second horse as a woman wearing a white hoodie is sitting on the back of the second horse."], "durations": null, "exact_frames_per_prompt": [32], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/6R7D2huuZJM.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 33, "npz_gt_video_start_frame": 33, "npz_gt_video_end_frame": 65, "skip_frames_after_generation": 33, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1955_5_0"}, {"texts": ["A black cat is sitting on the red wooden bench and touched and rubbed by a person."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/2xXAiarmD6o.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1958_0_0"}, {"texts": ["A person whose only hand is visible is touching and rubbing the cat head."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/2xXAiarmD6o.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1958_1_0"}, {"texts": ["A woman wearing a black vest is standing in the front, takes a bottle of spice, opens the cap of the bottle then starts sprinkling the spice"], "durations": null, "exact_frames_per_prompt": [44], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/4p6AJcPOg5I.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 48, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1959_0_0"}, {"texts": ["The woman is mixing food in the bowl with a spatula."], "durations": null, "exact_frames_per_prompt": [32], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/4p6AJcPOg5I.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 48, "npz_gt_video_start_frame": 48, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 48, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1959_0_1"}, {"texts": ["A man wearing a black t-shirt and spectacles is sitting behind the table, picking up a fork of brownie and moving it forward to the right side, then eating it himself."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/5V75OcSj-9A.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_195_0_0"}, {"texts": ["A woman of whom hands are visible only, is folding a multi-color t-shirt on the gray surface."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/2a7QdRSjNKQ.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1962_0_0"}, {"texts": ["A person whose only hands are visible is folding a colorful t-shirt."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/2a7QdRSjNKQ.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1963_0_0"}, {"texts": ["A person wearing black clothes is standing and holding a golf stick."], "durations": null, "exact_frames_per_prompt": [11], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-1brKJdL-iM.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 15, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1964_0_0"}, {"texts": ["The person is hitting a ball."], "durations": null, "exact_frames_per_prompt": [15], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-1brKJdL-iM.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 15, "npz_gt_video_start_frame": 15, "npz_gt_video_end_frame": 30, "skip_frames_after_generation": 15, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1964_0_1"}, {"texts": ["A person wearing a black jacket is pumping a samovar with a shoe."], "durations": null, "exact_frames_per_prompt": [45], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/0Oqfu-guuKA.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 49, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1969_0_0"}, {"texts": ["A group of people is sitting and a few are standing on the backside of the man."], "durations": null, "exact_frames_per_prompt": [12], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-3TTZ9RtlwM.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 35, "npz_gt_video_start_frame": 35, "npz_gt_video_end_frame": 47, "skip_frames_after_generation": 35, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1970_1_ms_0"}, {"texts": ["A man whose only upper body is visible wearing a black t-shirt is standing on the left side of the counter and drinking a shot while a man wearing white vest is standing on the left side of the counter and drinking a shot, a man wearing olive-green t-shirt standing right side of the counter and drinking a shot and putting ice cube on his eyes, a man wearing black t-shirt standing on the right side of the counter is drinking a shot."], "durations": null, "exact_frames_per_prompt": [15], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/1so9-8alZwA.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 19, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1971_0_0"}, {"texts": ["The man is putting the ice cube on his eyes while a man wearing white vest is  putting ice cube on his eyes and screaming keeping his hands on his eyes and moving up and down, other two men are also putting ice cube on his eyes and laughing."], "durations": null, "exact_frames_per_prompt": [61], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/1so9-8alZwA.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 19, "npz_gt_video_start_frame": 19, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 19, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1971_0_1"}, {"texts": ["A man, whose only upper body is visible wearing a white t-shirt, is standing on the left side of the counter and drinking a shot a man wearing black t-shirt standing in left side drinking wine in wine glass"], "durations": null, "exact_frames_per_prompt": [16], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/1so9-8alZwA.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 20, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1971_1_0"}, {"texts": ["The man whose only upper body is visible wearing a white t-shirt is putting the ice cube in his eyes a wearing a man wearing black top starts laughs"], "durations": null, "exact_frames_per_prompt": [23], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/1so9-8alZwA.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 20, "npz_gt_video_start_frame": 20, "npz_gt_video_end_frame": 44, "skip_frames_after_generation": 20, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1971_1_1"}, {"texts": ["A man whose upper body is visible wearing black t-shirt, is standing on the right side behind the man wearing olive-green t-shirt and drinking a shot with a man man wearing black t-shirt and behind him there is another man wearing white t-shirt on the left side and drinking shot"], "durations": null, "exact_frames_per_prompt": [44], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/1so9-8alZwA.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 48, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1971_3_0"}, {"texts": ["The man is putting the ice cube on his eyes while others are laughing"], "durations": null, "exact_frames_per_prompt": [32], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/1so9-8alZwA.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 48, "npz_gt_video_start_frame": 48, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 48, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1971_3_1"}, {"texts": ["A girl wearing white and red cloth is standing on the left of a baby while a baby is sitting and holding an object and watching it."], "durations": null, "exact_frames_per_prompt": [30], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-drhuBP6vps.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 34, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1972_0_0"}, {"texts": ["The girl went to the right side and watching on a mirror while the baby starts eating the object."], "durations": null, "exact_frames_per_prompt": [46], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-drhuBP6vps.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 34, "npz_gt_video_start_frame": 34, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 34, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1972_0_1"}, {"texts": ["A baby wearing yellow clothes is sitting while a girl wearing red and white clothes moves back and walks towards the right side."], "durations": null, "exact_frames_per_prompt": [41], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-drhuBP6vps.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 45, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1972_1_0"}, {"texts": ["The baby is eating food while the girl wearing red and white clothes is standing and looking in the mirror."], "durations": null, "exact_frames_per_prompt": [35], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-drhuBP6vps.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 45, "npz_gt_video_start_frame": 45, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 45, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1972_1_1"}, {"texts": ["A man is wearing a black t-shirt is sitting."], "durations": null, "exact_frames_per_prompt": [34], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-JKOYkHiTaU.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 38, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1974_0_0"}, {"texts": ["The man is drinking from a bottle."], "durations": null, "exact_frames_per_prompt": [42], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-JKOYkHiTaU.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 38, "npz_gt_video_start_frame": 38, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 38, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1974_0_1"}, {"texts": ["A man wearing a sky-blue shirt is touching the white dog"], "durations": null, "exact_frames_per_prompt": [16], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/1EkFwrGO3uU.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 20, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1978_0_ms_0"}, {"texts": ["The man is walking on the grass surface and the white dog is walking towards the man."], "durations": null, "exact_frames_per_prompt": [17], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/1EkFwrGO3uU.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 20, "npz_gt_video_start_frame": 20, "npz_gt_video_end_frame": 37, "skip_frames_after_generation": 20, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1978_0_ms_1"}, {"texts": ["A white dog is walking on the grass surface and getting touched by a man."], "durations": null, "exact_frames_per_prompt": [33], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/1EkFwrGO3uU.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 37, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1978_1_0"}, {"texts": ["A white-brown dog wearing a red cap is sitting near the Christmas tree."], "durations": null, "exact_frames_per_prompt": [17], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/1EkFwrGO3uU.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 37, "npz_gt_video_start_frame": 37, "npz_gt_video_end_frame": 54, "skip_frames_after_generation": 37, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1978_2_ms_0"}, {"texts": ["A man wearing a black t-shirt is sitting behind the table and moving his hand in the right direction and starts eating brown food."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/5V75OcSj-9A.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_197_0_0"}, {"texts": ["A man wearing a dark blue printed shirt is standing and cleaning a dog's paw."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/0oYuaHvmibU.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1980_0_0"}, {"texts": ["A beige dog is standing in a metal tub and getting its paw cleaned.\n while a man wearing blue and white outfit is standing near the dog and cleaning the paw of a beige color dog."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/0oYuaHvmibU.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1980_1_0"}, {"texts": ["A man with the white long hair is sitting on a wheelchair. He is at first dragging the blue box in the backward direction and a dog jumps on the box while a group of people are standing on a grey  object."], "durations": null, "exact_frames_per_prompt": [45], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-QVqh513RxQ.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 49, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1982_0_0"}, {"texts": ["The man pats on the dog's head."], "durations": null, "exact_frames_per_prompt": [20], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-QVqh513RxQ.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 49, "npz_gt_video_start_frame": 49, "npz_gt_video_end_frame": 69, "skip_frames_after_generation": 49, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1982_0_1"}, {"texts": ["A orange-brown colored dog is at first jumping on the blue box while it is being dragged by the person."], "durations": null, "exact_frames_per_prompt": [46], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-QVqh513RxQ.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 50, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1982_1_0"}, {"texts": ["The dog is being patted on the head by a man on a wheel chair while a group of people is standing at the back and watching the man and the dog."], "durations": null, "exact_frames_per_prompt": [19], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-QVqh513RxQ.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 50, "npz_gt_video_start_frame": 50, "npz_gt_video_end_frame": 69, "skip_frames_after_generation": 50, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1982_1_1"}, {"texts": ["A baby wearing babywear is sitting on a baby chair and eating a chocolate cake with his hands."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/48ezk8aL8oU.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1985_0_0"}, {"texts": ["A person wearing a cap is standing on the left side and holding a fishing rod of which its thread is stuck downwards as another person wearing a brown jacket is also standing and pulling up the thread of fishing rod."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/2xwEGCtb8cY.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1987_0_0"}, {"texts": ["A person in beige-colored clothing is bending downwards and pulling the fishing thread while another person wearing a black and blue jacket is standing and holding the fishing rod."], "durations": null, "exact_frames_per_prompt": [72], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/2xwEGCtb8cY.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 76, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1987_1_0"}, {"texts": ["The person gets up."], "durations": null, "exact_frames_per_prompt": [4], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/2xwEGCtb8cY.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 76, "npz_gt_video_start_frame": 76, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 76, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1987_1_1"}, {"texts": ["A boy wearing green shorts is standing and eating a watermelon.\n"], "durations": null, "exact_frames_per_prompt": [59], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/3S20Sqh1Bmg.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 63, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1988_0_0"}, {"texts": ["A boy on the left wearing a purple black spandex is standing and ironing an orange shirt on an ironing board while a boy on the right side wearing a red-pink spandex is standing and ironing an orange shirt on an ironing board."], "durations": null, "exact_frames_per_prompt": [59], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/2cBGeBKn9mU.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 63, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1989_0_0"}, {"texts": ["A boy on the right wearing a red spandex is standing and ironing an orange shirt on an ironing board while a boy on left is also standing and ironing an orange shirt on an ironing board."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/2cBGeBKn9mU.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1989_1_0"}, {"texts": ["A person on the left side wearing red clothes is standing and holding an award with another person on the right side."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/0r4xQs25W1k.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1991_0_0"}, {"texts": ["A person on the right side wearing black clothes is standing and holding an award with the person on the left side while a woman in a white dress is standing on the right side and clapping."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/0r4xQs25W1k.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1991_1_0"}, {"texts": ["A child wearing a blue t-shirt is standing on the grey floor and moving hands inside a washing machine while a child wearing a black t-shirt is standing on the grey floor and moving hands inside a washing machine"], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/7d1sB-ZEqKk.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1992_0_0"}, {"texts": ["A boy wearing a black t-shirt is standing on the grey floor and moving another washing machine drum while a kid on the left side in pink shorts is moving the washing machine's drum."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/7d1sB-ZEqKk.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1992_1_0"}, {"texts": ["A man wearing a black suit is standing in the front and holding a microphone and a tablet."], "durations": null, "exact_frames_per_prompt": [17], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/369UIM8O_7U.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 63, "npz_gt_video_start_frame": 63, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 63, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1993_3_ms_0"}, {"texts": ["A person whose only hands are visible is at first picking up the papers."], "durations": null, "exact_frames_per_prompt": [36], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-hUz_lQZsg0.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 40, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1994_0_0"}, {"texts": ["The person is stapling the papers with a grey stapler while others reacts to it."], "durations": null, "exact_frames_per_prompt": [40], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-hUz_lQZsg0.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 40, "npz_gt_video_start_frame": 40, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 40, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1994_0_1"}, {"texts": ["The video of a man in black shirt is streaming side by side. The man is watching the person picking up the papers."], "durations": null, "exact_frames_per_prompt": [38], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-hUz_lQZsg0.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 42, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1994_1_0"}, {"texts": ["The video of a woman is streaming side by side. The woman is watching the person stapling the papers."], "durations": null, "exact_frames_per_prompt": [14], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-hUz_lQZsg0.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 66, "npz_gt_video_start_frame": 66, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 66, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1994_2_ms_0"}, {"texts": ["A person whose only hands are visible is collecting postcards."], "durations": null, "exact_frames_per_prompt": [35], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-hUz_lQZsg0.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 39, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1995_0_0"}, {"texts": ["The person whose only hands are visible is stapling white papers."], "durations": null, "exact_frames_per_prompt": [41], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-hUz_lQZsg0.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 39, "npz_gt_video_start_frame": 39, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 39, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1995_0_1"}, {"texts": ["A man wearing purple clothes, visible on a small screen is speaking while a person is picking up papers and stapling them."], "durations": null, "exact_frames_per_prompt": [61], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-hUz_lQZsg0.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 65, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1995_1_ms_0"}, {"texts": ["A woman, visible on the same small screen is also speaking while a person pressed the stapler and peel a sheet of paper."], "durations": null, "exact_frames_per_prompt": [15], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-hUz_lQZsg0.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 65, "npz_gt_video_start_frame": 65, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 65, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1995_2_ms_0"}, {"texts": ["A woman wearing a blue t-shirt and beige colored pants is playing golf."], "durations": null, "exact_frames_per_prompt": [21], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/07wiAPphbv4.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 25, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_199_0_0"}, {"texts": ["A person wearing a blue t-shirt is bending and fastening a nut with a cordless screwdriver."], "durations": null, "exact_frames_per_prompt": [42], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/1p6HEyC1hHk.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 46, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_19_0_0"}, {"texts": ["A man wearing a half-sleeve green shirt and blue jeans is standing on the left side and is picking the meat from the tub with his left hand, putting the meat into the meat mincer machine, and looking at the meat mincer machine."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/2GVrl7hxsVA.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_1_0_0"}, {"texts": ["A man wearing a black blazer is standing on the left side on the right side in grey suit is speaking"], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/30Uur42btTI.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2002_0_0"}, {"texts": ["A man wearing a brown blazer is standing on the right side, speaking and moving his hand while a man wearing black suit is standing on left and a man whom head is visible is standing infront."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/30Uur42btTI.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2002_1_0"}, {"texts": ["A baby wearing a white t-shirt is sitting in front of the woman."], "durations": null, "exact_frames_per_prompt": [16], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/24EKXd4_QN8.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 20, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2003_1_0"}, {"texts": ["The baby is sucking a white straw."], "durations": null, "exact_frames_per_prompt": [60], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/24EKXd4_QN8.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 20, "npz_gt_video_start_frame": 20, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 20, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2003_1_1"}, {"texts": ["A girl wearing a pink and white frock is standing on the brown chair and is pouring a yellow paste into a black pan from a red bowl."], "durations": null, "exact_frames_per_prompt": [49], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/1RbSLP2TKzg.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 53, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2004_0_0"}, {"texts": ["The girl picks up a brown spatula."], "durations": null, "exact_frames_per_prompt": [27], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/1RbSLP2TKzg.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 53, "npz_gt_video_start_frame": 53, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 53, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2004_0_1"}, {"texts": ["A girl wearing a designer frock is standing on a chair. then putting a whisked egg into a pan from the bowl."], "durations": null, "exact_frames_per_prompt": [25], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/1RbSLP2TKzg.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 29, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2005_0_0"}, {"texts": ["The girl is then putting the bowl aside."], "durations": null, "exact_frames_per_prompt": [24], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/1RbSLP2TKzg.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 29, "npz_gt_video_start_frame": 29, "npz_gt_video_end_frame": 53, "skip_frames_after_generation": 29, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2005_0_1"}, {"texts": ["The girl is then picking up the wooden spatula and making an omelet."], "durations": null, "exact_frames_per_prompt": [27], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/1RbSLP2TKzg.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 53, "npz_gt_video_start_frame": 53, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 53, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2005_0_2"}, {"texts": ["A man wearing a gray shirt is standing and holding a gray tie from both hands, which is wrapped around his neck, and speaking."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/3IEJaQFNazA.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2007_0_0"}, {"texts": ["A woman wearing a green t-shirt is holding a snake in her hand. She is standing and talking while another woman on the right side starts taking the snake in her hands."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/3lDHsrgmZt4.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2008_0_0"}, {"texts": ["A brown and black snake is lying in the woman's hand a woman wearing green top holding a snake"], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/3lDHsrgmZt4.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2008_2_0"}, {"texts": ["A woman wearing a pink t-shirt is standing holding a black-red box."], "durations": null, "exact_frames_per_prompt": [55], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-4ybF8b7JdA.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 59, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2009_0_0"}, {"texts": ["The woman wearing a pink t-shirt starts opening it."], "durations": null, "exact_frames_per_prompt": [20], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-4ybF8b7JdA.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 59, "npz_gt_video_start_frame": 59, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 59, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2009_0_1"}, {"texts": ["A man wearing grey shorts is sitting on a box and touching the udder of the cow."], "durations": null, "exact_frames_per_prompt": [32], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-Olw5bPU_RE.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 36, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_200_0_0"}, {"texts": ["The man starts milking the cow."], "durations": null, "exact_frames_per_prompt": [35], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-Olw5bPU_RE.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 36, "npz_gt_video_start_frame": 36, "npz_gt_video_end_frame": 71, "skip_frames_after_generation": 36, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_200_0_1"}, {"texts": ["The man takes a steel container."], "durations": null, "exact_frames_per_prompt": [9], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-Olw5bPU_RE.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 71, "npz_gt_video_start_frame": 71, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 71, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_200_0_2"}, {"texts": ["A brown-white cow is standing on the soil surface, getting its udder touched and milked by the man."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-Olw5bPU_RE.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_200_1_0"}, {"texts": ["A woman wearing baby pink clothes is washing clothes in a blue tub with her hands while another woman wearing striped top is sitting and is putting clothes in the red tub."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-mWhX_yXOS8.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2010_0_0"}, {"texts": ["A woman wearing black and white clothes is transferring clothes from the red basket to the red tub while another woman in a pink dress is sitting and washing clothes."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-mWhX_yXOS8.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2010_1_0"}, {"texts": ["A woman wearing a pink dress is sitting on a stool and washing clothes with her hands in the tub while another woman wearing a striped dress is putting clothes into the red tub from the red basket."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-mWhX_yXOS8.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2011_0_0"}, {"texts": ["A woman wearing a white-black dress is sitting and taking out clothes from the bucket and putting them into a red tub while a woman wearing pink sando is sitting and rubbing the clothes in the blue bucket."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-mWhX_yXOS8.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2011_1_0"}, {"texts": ["A woman wearing a pink dress is sitting and washing clothes with her hands while another woman wearing black and white outfit is sitting on the left side of first woman and she is putting the clothes together."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-mWhX_yXOS8.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2012_0_0"}, {"texts": ["A woman wearing a white-black dress is also sitting and washing clothes with her hands while a woman in a pink dress is sitting and washing clothes with her hands."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-mWhX_yXOS8.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2012_1_0"}, {"texts": ["A man wearing a black t-shirt is sitting and opening the bolt with a wrench of the wheel."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-6JrhxygJEY.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2018_0_0"}, {"texts": ["A person wearing a t-shirt is eating food with his hand while sitting on a chair."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/40VTPMXHfYk.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2019_0_0"}, {"texts": ["A man wearing a black jacket and black jeans is doing sheep shearing with a shearing clipper machine while standing on the wooden surface while a person whose lower body is visible, wearing black pants is standing on the right side."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/0M1hNtjS5P0.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2020_0_0"}, {"texts": ["A man wearing a brown shirt and blue jeans, is standing on the surface and there is a sheep who is getting trimmed by first person"], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/0M1hNtjS5P0.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2020_1_0"}, {"texts": ["A sheep is on the wooden surface and is screaming when the first man is sharing the sheep's wool."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/0M1hNtjS5P0.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2020_2_0"}, {"texts": ["A woman wearing a light blue t-shirt is standing and she is holding a bird feed tray while a group of birds flies and sits on the tray and starts eating and a group of people are standing and some are walking on the backside."], "durations": null, "exact_frames_per_prompt": [66], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/--PyMoD3_eg.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 70, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2021_0_0"}, {"texts": ["A woman is standing on the left and she is also holding a bird feed tray while a woman wearing a cap is standing on the right side, holding a bird feed tray, a man wearing a blue t-shirt is standing and taking pictures with a phone, a person whose hand is visible is taking the bird feed tray from the woman, and a group of people are standing and moving at the back."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/--PyMoD3_eg.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2021_1_0"}, {"texts": ["A man wearing a blue t-shirt is standing and he is clicking pictures of the second woman while a woman wearing a cap is standing on the right side, holding a bird feed tray, a person whose hand is visible is holding the bird's feed tray along with the woman, and a group of people are standing and moving at the back."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/--PyMoD3_eg.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2021_2_0"}, {"texts": ["A man wearing a cap is standing, holding a glass. "], "durations": null, "exact_frames_per_prompt": [49], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/4ZviMrYmEQM.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 21, "npz_gt_video_start_frame": 21, "npz_gt_video_end_frame": 70, "skip_frames_after_generation": 21, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2022_1_0"}, {"texts": ["The man wearing a cap is drinking from it."], "durations": null, "exact_frames_per_prompt": [10], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/4ZviMrYmEQM.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 70, "npz_gt_video_start_frame": 70, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 70, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2022_1_1"}, {"texts": ["A man wearing a white shirt and black trousers is sitting."], "durations": null, "exact_frames_per_prompt": [17], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/1f8mMdNYVbY.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 21, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2023_0_0"}, {"texts": ["The man is tightening a tire nozzle."], "durations": null, "exact_frames_per_prompt": [59], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/1f8mMdNYVbY.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 21, "npz_gt_video_start_frame": 21, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 21, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2023_0_1"}, {"texts": ["A man wearing a white t-shirt and a violet garland is standing in front and speaking."], "durations": null, "exact_frames_per_prompt": [23], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/3bwm8jerAmY.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 27, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2024_0_0"}, {"texts": ["A woman wearing a pink-green dress is standing behind the first man and looking here and there as a man in a white t-shirt is speaking while standing in the front."], "durations": null, "exact_frames_per_prompt": [24], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/3bwm8jerAmY.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 28, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2024_1_0"}, {"texts": ["A man wearing a white t-shirt is bending over and eating watermelon while some people near the man are also eating watermelon and a group of people at the back are standing."], "durations": null, "exact_frames_per_prompt": [18], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/3bwm8jerAmY.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 62, "npz_gt_video_start_frame": 62, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 62, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2024_3_0"}, {"texts": ["A man whose head is visible only, wearing a black goggles is eating watermelon."], "durations": null, "exact_frames_per_prompt": [18], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/3bwm8jerAmY.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 62, "npz_gt_video_start_frame": 62, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 62, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2024_4_ms_0"}, {"texts": ["A woman wearing pink clothes is standing behind the countertop and chopping the pineapple."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/1C4wtWdaYRE.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2025_0_0"}, {"texts": ["A person whose half body is visible wearing an orange t-shirt is flipping the grill sticks with his hand and roasting the meat."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/19ZQ1IG5r_k.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2026_0_0"}, {"texts": ["A woman wearing a dark green t-shirt with white stripes is standing, holding a sauce jar."], "durations": null, "exact_frames_per_prompt": [8], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/0DssRvFYL4E.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 12, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2028_0_0"}, {"texts": ["The woman is applying the sauce."], "durations": null, "exact_frames_per_prompt": [50], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/0DssRvFYL4E.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 12, "npz_gt_video_start_frame": 12, "npz_gt_video_end_frame": 62, "skip_frames_after_generation": 12, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2028_0_1"}, {"texts": ["The woman is taking something out of the packet and talking."], "durations": null, "exact_frames_per_prompt": [18], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/0DssRvFYL4E.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 62, "npz_gt_video_start_frame": 62, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 62, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2028_0_2"}, {"texts": ["A girl wearing a brown sweater is standing near the kitchen counter and making a sandwich."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/2uoTS5SiImc.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2029_0_0"}, {"texts": ["A brown dog is sitting on a sand surface and watching the second person and the dog food."], "durations": null, "exact_frames_per_prompt": [56], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/39DA6bzPfl8.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 24, "npz_gt_video_start_frame": 24, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 24, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2030_1_0"}, {"texts": ["A person whose hand is visible is caressing the dog."], "durations": null, "exact_frames_per_prompt": [15], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/39DA6bzPfl8.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 6, "npz_gt_video_start_frame": 6, "npz_gt_video_end_frame": 21, "skip_frames_after_generation": 6, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2030_2_0"}, {"texts": ["A man wearing a white vest is sitting holding a newspaper and moving his other hand while a kid is sitting on a table, taking support from a man's arm, and looking at the newspaper,"], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/33J6Q-mStVY.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2031_0_0"}, {"texts": ["A baby wearing a white romper is lying on the man's arm."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/33J6Q-mStVY.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2031_1_0"}, {"texts": ["A man wearing a white vest is reading a newspaper with a baby sitting on his arm."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/33J6Q-mStVY.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2032_0_0"}, {"texts": ["A baby wearing a white onesie is sitting beside the first man."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/33J6Q-mStVY.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2032_1_0"}, {"texts": ["A man whose only lower body is visible is tying a hand wrap around his hand."], "durations": null, "exact_frames_per_prompt": [77], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-kgVnNAHLLM.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 81, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2034_0_0"}, {"texts": ["A man wearing a red t-shirt is standing and holding and cutting the pineapple with a knife."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-qUYrOaqWOg.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2036_0_0"}, {"texts": ["A person wearing red clothes is standing and holding a pineapple and knife and peeling the pineapple."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-qUYrOaqWOg.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2037_0_0"}, {"texts": ["A man wearing a printed sweater is standing and holding a microphone."], "durations": null, "exact_frames_per_prompt": [41], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/4iZgVdDK3r8.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 45, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2038_0_0"}, {"texts": ["A man wearing black pants is shearing a grey sheep with a shearing machine."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/168f9ODKMPg.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2039_0_0"}, {"texts": ["A grey sheep is lying on a brown surface while a man wearing black pants is shearing a grey sheep with a shearing machine."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/168f9ODKMPg.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2039_1_0"}, {"texts": ["A boy wearing a black-red t-shirt is sitting with a man."], "durations": null, "exact_frames_per_prompt": [16], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/1-YO_7yN9pQ.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 20, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_203_0_0"}, {"texts": ["The boy is eating food while a man wearing red white t-shirt is playing with the mouth and laughing when the boy is trying to copy."], "durations": null, "exact_frames_per_prompt": [60], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/1-YO_7yN9pQ.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 20, "npz_gt_video_start_frame": 20, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 20, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_203_0_1"}, {"texts": ["A man wearing a red-white-black t-shirt is sitting with the boy, holding a book."], "durations": null, "exact_frames_per_prompt": [36], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/1-YO_7yN9pQ.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 40, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_203_1_0"}, {"texts": ["The man starts shaking his head and looking at the boy."], "durations": null, "exact_frames_per_prompt": [40], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/1-YO_7yN9pQ.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 40, "npz_gt_video_start_frame": 40, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 40, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_203_1_1"}, {"texts": ["A woman whose only hands are visible, is cutting a red vegetable."], "durations": null, "exact_frames_per_prompt": [13], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/0ThjElOLUMQ.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 17, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2043_0_0"}, {"texts": ["The woman whose only hands are visible is mixing all the vegetables with a spoon."], "durations": null, "exact_frames_per_prompt": [38], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/0ThjElOLUMQ.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 17, "npz_gt_video_start_frame": 17, "npz_gt_video_end_frame": 55, "skip_frames_after_generation": 17, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2043_0_1"}, {"texts": ["A woman wearing black clothes is standing behind the lectern and speaking while a man in a gray suit is standing behind her holding an award, and a group of people is sitting and watching her."], "durations": null, "exact_frames_per_prompt": [63], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-flfP3yiji8.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 67, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2044_0_0"}, {"texts": ["The woman starts doing a handshake with the first man."], "durations": null, "exact_frames_per_prompt": [13], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-flfP3yiji8.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 67, "npz_gt_video_start_frame": 67, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 67, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2044_0_1"}, {"texts": ["A man wearing black clothes is walking towards the woman a man wearing a suit standing holding something in hand"], "durations": null, "exact_frames_per_prompt": [22], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-flfP3yiji8.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 15, "npz_gt_video_start_frame": 15, "npz_gt_video_end_frame": 37, "skip_frames_after_generation": 15, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2044_1_0"}, {"texts": ["The man is doing a handshake with the second man a man wearing suit shaking his hand to the first person"], "durations": null, "exact_frames_per_prompt": [30], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-flfP3yiji8.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 37, "npz_gt_video_start_frame": 37, "npz_gt_video_end_frame": 67, "skip_frames_after_generation": 37, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2044_1_1"}, {"texts": ["The man is doing handshake with the woman while a man wearing suit watching"], "durations": null, "exact_frames_per_prompt": [13], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-flfP3yiji8.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 67, "npz_gt_video_start_frame": 67, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 67, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2044_1_2"}, {"texts": ["A man wearing dark green clothes is standing on the right side of the woman while another man wearing a black suit comes from the right side, and a group of people is sitting on the chairs."], "durations": null, "exact_frames_per_prompt": [41], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-flfP3yiji8.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 45, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2044_2_0"}, {"texts": ["The man is doing handshake with the first man while a woman in a black suit shakes her hand with the man in a black suit."], "durations": null, "exact_frames_per_prompt": [35], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-flfP3yiji8.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 45, "npz_gt_video_start_frame": 45, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 45, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2044_2_1"}, {"texts": ["A man wearing a black and gray jacket is drilling into the snow surface with an ice drill.\n"], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/2blFwlIWiKA.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2046_0_0"}, {"texts": ["A man wearing a green top is sitting on his knee."], "durations": null, "exact_frames_per_prompt": [30], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/4H1XxUr-b20.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 34, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2047_0_0"}, {"texts": ["The man puts the red and white bottle aside."], "durations": null, "exact_frames_per_prompt": [6], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/4H1XxUr-b20.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 34, "npz_gt_video_start_frame": 34, "npz_gt_video_end_frame": 40, "skip_frames_after_generation": 34, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2047_0_1"}, {"texts": ["The man picks some stuff from the tyre and puts them on the floor while another man is fixing the tire on the vehicle."], "durations": null, "exact_frames_per_prompt": [40], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/4H1XxUr-b20.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 40, "npz_gt_video_start_frame": 40, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 40, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2047_0_2"}, {"texts": ["A boy wearing blue clothes is standing and reading a newspaper while girls are moving here and there."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/05-73___szY.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2049_0_0"}, {"texts": ["A girl wearing an orange cloth is standing while the boy wearing a hat reads the newspaper."], "durations": null, "exact_frames_per_prompt": [69], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/05-73___szY.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 73, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2049_1_0"}, {"texts": ["The girl is holding the microphone in her right hand while the girl in yellow cloth passes by."], "durations": null, "exact_frames_per_prompt": [7], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/05-73___szY.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 73, "npz_gt_video_start_frame": 73, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 73, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2049_1_1"}, {"texts": ["A girl wearing a blue-yellow cloth is walking while the other two childrens stands in front of the microphone."], "durations": null, "exact_frames_per_prompt": [26], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/05-73___szY.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 30, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2049_2_0"}, {"texts": ["The girl goes behind the first and second boy and girl."], "durations": null, "exact_frames_per_prompt": [50], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/05-73___szY.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 30, "npz_gt_video_start_frame": 30, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 30, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2049_2_1"}, {"texts": ["A girl wearing a blue-green cloth is walking and goes behind the second girl while a boy wearing black clothes is standing holding a paper in his hand, and another girl wearing a blue-yellow jacket is walking and goes behind him."], "durations": null, "exact_frames_per_prompt": [32], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/05-73___szY.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 25, "npz_gt_video_start_frame": 25, "npz_gt_video_end_frame": 57, "skip_frames_after_generation": 25, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2049_3_0"}, {"texts": ["A person whose only hand is visible wearing black clothes is scraping the eggs with a scraper."], "durations": null, "exact_frames_per_prompt": [43], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/0AP-Md16Gfk.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 37, "npz_gt_video_start_frame": 37, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 37, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2051_0_0"}, {"texts": ["A person wearing green clothes is holding a plate and eating food."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/4MxZMWptmeg.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2054_0_0"}, {"texts": ["A person whose only hands are visible is holding a knife."], "durations": null, "exact_frames_per_prompt": [17], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/aG2CjGczfh8.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 21, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2057_0_0"}, {"texts": ["The person cuts the watermelon into halves."], "durations": null, "exact_frames_per_prompt": [23], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/aG2CjGczfh8.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 21, "npz_gt_video_start_frame": 21, "npz_gt_video_end_frame": 44, "skip_frames_after_generation": 21, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2057_0_1"}, {"texts": ["The person starts chopping the watermelon into small pieces."], "durations": null, "exact_frames_per_prompt": [36], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/aG2CjGczfh8.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 44, "npz_gt_video_start_frame": 44, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 44, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2057_0_2"}, {"texts": ["A man wearing white clothes is holding a fry pan and tossing the vegetables."], "durations": null, "exact_frames_per_prompt": [26], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/07GR9f0TFp0.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 30, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2059_0_0"}, {"texts": ["The man puts the fry pan on the gas stove."], "durations": null, "exact_frames_per_prompt": [50], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/07GR9f0TFp0.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 30, "npz_gt_video_start_frame": 30, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 30, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2059_0_1"}, {"texts": ["A woman on the left side wearing a white t-shirt is standing then a woman wearing black shirt walks beside her and help on her."], "durations": null, "exact_frames_per_prompt": [22], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/j9o3Bsgs5E8.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 26, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2060_0_0"}, {"texts": ["The woman wearing white t-shirt is putting a pillow into a cover and the woman wearing black shirt stops helping her and help her again."], "durations": null, "exact_frames_per_prompt": [54], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/j9o3Bsgs5E8.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 26, "npz_gt_video_start_frame": 26, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 26, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2060_0_1"}, {"texts": ["A woman on the right side wearing a black t-shirt is standing, pulling her pants up while a woman on the left side is putting a pillow into the pink cover."], "durations": null, "exact_frames_per_prompt": [64], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/j9o3Bsgs5E8.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 68, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2060_1_0"}, {"texts": ["The woman is putting a pillow into the pink cover while a woman wearing black t-shirt is trying to putting a pillow into the pink cover."], "durations": null, "exact_frames_per_prompt": [11], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/j9o3Bsgs5E8.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 68, "npz_gt_video_start_frame": 68, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 68, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2060_1_1"}, {"texts": ["A person whose hands are visible is touching a glass cup"], "durations": null, "exact_frames_per_prompt": [43], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-6zh0xWFWT4.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 47, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2061_0_0"}, {"texts": ["A person whose hands are visible is adjusting the oven knob."], "durations": null, "exact_frames_per_prompt": [23], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-6zh0xWFWT4.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 47, "npz_gt_video_start_frame": 47, "npz_gt_video_end_frame": 70, "skip_frames_after_generation": 47, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2061_0_1"}, {"texts": ["A man whose only hand is visible is touching a mug of water."], "durations": null, "exact_frames_per_prompt": [36], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-6zh0xWFWT4.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 10, "npz_gt_video_start_frame": 10, "npz_gt_video_end_frame": 46, "skip_frames_after_generation": 10, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2062_0_0"}, {"texts": ["The man whose only hand is visible is touching the oven knob."], "durations": null, "exact_frames_per_prompt": [34], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-6zh0xWFWT4.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 46, "npz_gt_video_start_frame": 46, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 46, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2062_0_1"}, {"texts": ["A woman wearing a black sweater is standing in the back and leaning forward.\n while a woman on the left side in a black jacket is sitting, a boy in a blue t-shirt is standing, and a girl in a purple top is showing paper money, she then starts going down the stairs."], "durations": null, "exact_frames_per_prompt": [17], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/06ON1-8oHTc.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 21, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2063_0_0"}, {"texts": ["A girl wearing a purple-white top is showing money and starts walking while other people are standing on the platform and watching here and there."], "durations": null, "exact_frames_per_prompt": [20], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/06ON1-8oHTc.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 24, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2063_3_ms_0"}, {"texts": ["A woman wearing a dark blue sweater and black trousers is riding an elephant while a person on the left side wearing blue denim is walking on the brown soil surface."], "durations": null, "exact_frames_per_prompt": [55], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/06ON1-8oHTc.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 25, "npz_gt_video_start_frame": 25, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 25, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2063_4_0"}, {"texts": ["A man wearing a brown vest and blue jeans is walking in front of the elephant."], "durations": null, "exact_frames_per_prompt": [53], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/06ON1-8oHTc.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 27, "npz_gt_video_start_frame": 27, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 27, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2063_5_ms_0"}, {"texts": ["A black elephant is walking and ridden by the third woman."], "durations": null, "exact_frames_per_prompt": [56], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/06ON1-8oHTc.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 24, "npz_gt_video_start_frame": 24, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 24, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2063_6_ms_0"}, {"texts": ["A girl wearing purple-white cloth is holding a bank note and going down from the platform while a woman wearing black outfit is standing in bending position beside the girl and she is holding a kid in her hand and a kid wearing blue outfit is being held by the first woman and another woman wearing black outfit is standing on the left side of first woman and she is holding a metal frame."], "durations": null, "exact_frames_per_prompt": [22], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/06ON1-8oHTc.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 26, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2064_3_0"}, {"texts": ["An elephant is walking and carrying a woman.\n while a woman wearing blue jeans is standing in front of the elephant."], "durations": null, "exact_frames_per_prompt": [54], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/06ON1-8oHTc.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 26, "npz_gt_video_start_frame": 26, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 26, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2064_4_0"}, {"texts": ["A man wearing blue cloth is walking in front of the elephant while a person is sitting on the elephant and a camel is sitting on the backside."], "durations": null, "exact_frames_per_prompt": [52], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/06ON1-8oHTc.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 28, "npz_gt_video_start_frame": 28, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 28, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2064_5_0"}, {"texts": ["A woman wearing blue cloth is sitting and riding over the elephant."], "durations": null, "exact_frames_per_prompt": [55], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/06ON1-8oHTc.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 25, "npz_gt_video_start_frame": 25, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 25, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2064_6_ms_0"}, {"texts": ["A girl wearing a black hoodie is sitting on a wooden chair, looking at a paper on her lap and moving her hands in sign language.\n"], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/2iLJQsIbcyg.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2065_0_0"}, {"texts": ["A baby wearing a pink cap is sitting, taking a cake slice from the second woman and the first woman is unfolding the a white wipe."], "durations": null, "exact_frames_per_prompt": [33], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/0Jpl4du8F8o.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 37, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2067_0_0"}, {"texts": ["The baby is eating it and then the second woman wiping the baby's hand."], "durations": null, "exact_frames_per_prompt": [43], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/0Jpl4du8F8o.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 37, "npz_gt_video_start_frame": 37, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 37, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2067_0_1"}, {"texts": ["A woman on the right wearing a black top is sitting, holding a tissue while the other woman tries to feed the baby."], "durations": null, "exact_frames_per_prompt": [40], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/0Jpl4du8F8o.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 44, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2067_1_0"}, {"texts": ["The woman is giving the tissue to the other woman."], "durations": null, "exact_frames_per_prompt": [36], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/0Jpl4du8F8o.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 44, "npz_gt_video_start_frame": 44, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 44, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2067_1_1"}, {"texts": ["A woman on the left side wearing a green top is sitting and giving a cake slice to the baby while a woman wearing a black outfit is sitting on the right side and giving a tissue to another woman, and a group of people and vehicles are moving in the back"], "durations": null, "exact_frames_per_prompt": [36], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/0Jpl4du8F8o.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 40, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2067_2_0"}, {"texts": ["The woman is taking a tissue from the first woman while a baby wearing a cap is sitting and eating a cake slice."], "durations": null, "exact_frames_per_prompt": [26], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/0Jpl4du8F8o.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 40, "npz_gt_video_start_frame": 40, "npz_gt_video_end_frame": 66, "skip_frames_after_generation": 40, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2067_2_1"}, {"texts": ["The woman is clearing the baby's hand."], "durations": null, "exact_frames_per_prompt": [14], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/0Jpl4du8F8o.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 66, "npz_gt_video_start_frame": 66, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 66, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2067_2_2"}, {"texts": ["A baby wearing a pink cap is sitting while a woman on the left is giving the baby a piece of cake and another woman on the right is holding tissue paper and wiping the baby's hand."], "durations": null, "exact_frames_per_prompt": [35], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/0Jpl4du8F8o.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 39, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2068_0_0"}, {"texts": ["The baby is eating food while a group of people and vehicles are moving at the back."], "durations": null, "exact_frames_per_prompt": [41], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/0Jpl4du8F8o.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 39, "npz_gt_video_start_frame": 39, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 39, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2068_0_1"}, {"texts": ["A woman on the right wearing black cloth is sitting while holding a tissue while the other woman gives the food to the baby"], "durations": null, "exact_frames_per_prompt": [26], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/0Jpl4du8F8o.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 30, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2068_1_0"}, {"texts": ["The woman on the right wearing black cloth is giving the tissue to the other woman while the baby is eating the food"], "durations": null, "exact_frames_per_prompt": [50], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/0Jpl4du8F8o.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 30, "npz_gt_video_start_frame": 30, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 30, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2068_1_1"}, {"texts": ["A woman on the left side wearing a green cloth is sitting while a baby wearing a pink cap is sitting on the chair in the middle, the first woman wearing black clothes is sitting on the right and unfolding a tissue and passing it."], "durations": null, "exact_frames_per_prompt": [35], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/0Jpl4du8F8o.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 39, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2068_2_0"}, {"texts": ["The woman is taking a tissue from the first woman while the baby wearing a pink cap is sitting on the chair in the middle and holding an object in his hand near his mouth."], "durations": null, "exact_frames_per_prompt": [27], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/0Jpl4du8F8o.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 39, "npz_gt_video_start_frame": 39, "npz_gt_video_end_frame": 66, "skip_frames_after_generation": 39, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2068_2_1"}, {"texts": ["The woman is clearing the baby hand while the first woman sitting on the right is looking at them."], "durations": null, "exact_frames_per_prompt": [14], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/0Jpl4du8F8o.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 66, "npz_gt_video_start_frame": 66, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 66, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2068_2_2"}, {"texts": ["A woman wearing a designer dress is standing and mixing salad in a green bowl with a spatula while speaking."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/3TjNsvBDb7k.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2069_0_0"}, {"texts": ["A baby wearing a white cloth with multicolour dots is sitting and moving his right hand.\n"], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-chFCJcgA4k.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2070_0_0"}, {"texts": ["A baby wearing a white cloth is sitting and moving."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-chFCJcgA4k.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2071_0_0"}, {"texts": ["A person wearing a dark colored t-shirt is walking towards the right."], "durations": null, "exact_frames_per_prompt": [32], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/0fItLr9CtVg.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 48, "npz_gt_video_start_frame": 48, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 48, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2073_4_0"}, {"texts": ["A woman with brown hair is drinking water from the bottle while another woman wearing a white vest is drinking water from the glass."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/1fnWPB5I2gY.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2075_0_0"}, {"texts": ["A woman wearing a white top is drinking water from the glass."], "durations": null, "exact_frames_per_prompt": [34], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/1fnWPB5I2gY.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 46, "npz_gt_video_start_frame": 46, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 46, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2075_1_0"}, {"texts": ["A woman wearing a white top is serving beer in a glass and then standing with the man."], "durations": null, "exact_frames_per_prompt": [57], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/0k1rhe0iJro.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 23, "npz_gt_video_start_frame": 23, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 23, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2076_1_ms_0"}, {"texts": ["A man wearing a gray t-shirt is standing next to the woman holding a bottle."], "durations": null, "exact_frames_per_prompt": [13], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/0k1rhe0iJro.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 67, "npz_gt_video_start_frame": 67, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 67, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2076_2_0"}, {"texts": ["A white brown dog is walking from the left to right side behind the third dog and a black dog is also walking behind them."], "durations": null, "exact_frames_per_prompt": [54], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/0lp9mBNDAQM.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 14, "npz_gt_video_start_frame": 14, "npz_gt_video_end_frame": 68, "skip_frames_after_generation": 14, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2080_2_0"}, {"texts": ["A dog is walking in front of the first and second dog."], "durations": null, "exact_frames_per_prompt": [63], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/0lp9mBNDAQM.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 67, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2080_3_0"}, {"texts": ["A person whose hand are visible only is doing a face decoration of the cake."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-UIei6eyzuk.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2081_0_0"}, {"texts": ["A person wearing a white checked shirt whose only hands are visible is putting the paper on the paper shredder machine."], "durations": null, "exact_frames_per_prompt": [36], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/6m1FpSlGE7U.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 40, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2089_0_0"}, {"texts": ["The person presses the button and then starts pointing an finger at the machine microphone"], "durations": null, "exact_frames_per_prompt": [40], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/6m1FpSlGE7U.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 40, "npz_gt_video_start_frame": 40, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 40, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2089_0_1"}, {"texts": ["A man wearing a gray t-shirt and black jeans is standing, holding a drilling machine and peeling a red potato attached to the machine with a peeler."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/39VFZoJaFdE.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_208_0_0"}, {"texts": ["A man wearing camouflage pants is sitting on a metal chair and holding the string of a fishing rod."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/0yrOv3hhHDk.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2090_0_0"}, {"texts": ["A person whom hand is visible is holding a knife and pineapple and cutting it."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/1xO9gNY9tJs.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2092_0_0"}, {"texts": ["A person wearing a red t-shirt is cutting a pineapple."], "durations": null, "exact_frames_per_prompt": [67], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/1xO9gNY9tJs.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 71, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2093_0_0"}, {"texts": ["The person is throwing a pineapple piece in a sink."], "durations": null, "exact_frames_per_prompt": [9], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/1xO9gNY9tJs.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 71, "npz_gt_video_start_frame": 71, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 71, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2093_0_1"}, {"texts": ["A man whose half body is visible wearing a black t-shirt is standing and holding a paper shredding machine while another person is holding a paper and putting the paper inside the paper shredding machine."], "durations": null, "exact_frames_per_prompt": [71], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/7M_B3KPAPe0.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 75, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2094_0_0"}, {"texts": ["A person whose only hand is visible is holding a piece of paper.  while a person wearing a black t-shirt is holding the paper shredding machine."], "durations": null, "exact_frames_per_prompt": [22], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/7M_B3KPAPe0.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 26, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2094_1_0"}, {"texts": ["A person whose only hand is visible putting the piece of paper into the paper shredding machine."], "durations": null, "exact_frames_per_prompt": [49], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/7M_B3KPAPe0.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 26, "npz_gt_video_start_frame": 26, "npz_gt_video_end_frame": 75, "skip_frames_after_generation": 26, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2094_1_1"}, {"texts": ["A person whose hand is visible only is putting a paper in a shredding machine.\n while a person wearing a black t-shirt is standing and holding a shredding machine in his hands."], "durations": null, "exact_frames_per_prompt": [71], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/7M_B3KPAPe0.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 75, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2095_0_0"}, {"texts": ["A man wearing black clothes is standing and holding a shredding machine in both hands while a man whose only hand is visible wearing grey clothes is dropping a piece of paper in the shredding machine."], "durations": null, "exact_frames_per_prompt": [71], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/7M_B3KPAPe0.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 75, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2095_1_0"}, {"texts": ["A girl wearing white cloth is standing on the right near the white counter while a woman wearing a white top is sitting on the chair, two people are spreading the white bed-sheet and a person is lying at the back."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/j3siMgcjJlc.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2096_0_0"}, {"texts": ["A girl wearing white cloth is holding a bed sheet and making a bed while the boy helps from the other end."], "durations": null, "exact_frames_per_prompt": [46], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/j3siMgcjJlc.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 50, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2096_1_0"}, {"texts": ["The girl is walking towards the first girl while the boy moves away."], "durations": null, "exact_frames_per_prompt": [29], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/j3siMgcjJlc.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 50, "npz_gt_video_start_frame": 50, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 50, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2096_1_1"}, {"texts": ["A boy wearing white clothes is standing and holding a white bed sheet and helping the second girl in the making of bed while a girl is sitting and other girl is standing on the wall."], "durations": null, "exact_frames_per_prompt": [71], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/j3siMgcjJlc.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 75, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2096_2_0"}, {"texts": ["A boy wearing white cloth is sitting near the first girl while two people are spreading the bed sheet and one person is lying to the right side."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/j3siMgcjJlc.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2096_3_0"}, {"texts": ["A girl wearing a brown vest and black t-shirt is sitting and speaking while a group of people are sitting around and looking at the girl."], "durations": null, "exact_frames_per_prompt": [40], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-FoTOgyrttQ.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 44, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_209_0_0"}, {"texts": ["A woman sitting on the right wearing a black suit is watching the girl while a group of girl is sitting in the back side and looking in front."], "durations": null, "exact_frames_per_prompt": [29], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-FoTOgyrttQ.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 33, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_209_1_0"}, {"texts": ["The woman turns her head, and smiles."], "durations": null, "exact_frames_per_prompt": [11], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-FoTOgyrttQ.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 33, "npz_gt_video_start_frame": 33, "npz_gt_video_end_frame": 44, "skip_frames_after_generation": 33, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_209_1_1"}, {"texts": ["A man wearing a blue t-shirt is standing and leaning towards the car tire and unscrewing a tire bolt with the help of an electric wrench."], "durations": null, "exact_frames_per_prompt": [42], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/1p6HEyC1hHk.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 46, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_20_0_0"}, {"texts": ["A man wearing a green t-shirt is sitting on a couch and holding a baby while a girl wearing a pink dress is snuggling with the baby."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/1fwl6wRp_Tg.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2100_0_0"}, {"texts": ["A baby wearing a red designer t-shirt is lying on the men."], "durations": null, "exact_frames_per_prompt": [23], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/1fwl6wRp_Tg.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 27, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2100_1_0"}, {"texts": ["The baby is tickled by a girl and laughing."], "durations": null, "exact_frames_per_prompt": [53], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/1fwl6wRp_Tg.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 27, "npz_gt_video_start_frame": 27, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 27, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2100_1_1"}, {"texts": ["A girl wearing a pink dress is standing while a man wearing a green t-shirt is sitting on a sofa while holding a baby in red clothes."], "durations": null, "exact_frames_per_prompt": [26], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/1fwl6wRp_Tg.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 30, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2100_2_0"}, {"texts": ["The girl starts tickling the baby while the man in the green t-shirt watches and smiles."], "durations": null, "exact_frames_per_prompt": [50], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/1fwl6wRp_Tg.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 30, "npz_gt_video_start_frame": 30, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 30, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2100_2_1"}, {"texts": ["A man wearing a black t-shirt is standing and drinking beer.\n while a man wearing a white t-shirt is standing and drinking from a glass, and makes faces and then pours the liquid from the bottle."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/3mdDqxRnG3E.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2102_0_0"}, {"texts": ["A man wearing a white t-shirt is drinking beer while another man wearing a black t-shirt is standing on the left and drinking beer from the glass."], "durations": null, "exact_frames_per_prompt": [48], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/3mdDqxRnG3E.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 52, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2102_1_0"}, {"texts": ["The man picks up the bottle taking beer in the glass."], "durations": null, "exact_frames_per_prompt": [28], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/3mdDqxRnG3E.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 52, "npz_gt_video_start_frame": 52, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 52, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2102_1_1"}, {"texts": ["A baby wearing blue clothes is held while a black and brown dog is moving on the floor."], "durations": null, "exact_frames_per_prompt": [41], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/48XkS3Hkmws.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 45, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2103_2_0"}, {"texts": ["The baby is fed by the first person."], "durations": null, "exact_frames_per_prompt": [35], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/48XkS3Hkmws.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 45, "npz_gt_video_start_frame": 45, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 45, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2103_2_1"}, {"texts": ["A person wearing a white shirt and brown pants is sitting on the right side of the cow and milking the udder in a bucket while a girl is sitting on the left."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/1G52dKoPPvE.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2104_0_0"}, {"texts": ["A girl wearing a white-blue striped top and white pants is sitting on a brown chair and watching the process of milking while a man wearing white shirt is milking."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/1G52dKoPPvE.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2104_1_0"}, {"texts": ["A black cow is standing on the left side of both people and letting that person milking from its udder."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/1G52dKoPPvE.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2104_2_0"}, {"texts": ["A person placing sandwich layers on a metal sandwich maker. "], "durations": null, "exact_frames_per_prompt": [33], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-yGhq9FaowU.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 37, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2106_0_0"}, {"texts": ["The person putting the sandwich maker in a burning fire."], "durations": null, "exact_frames_per_prompt": [42], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-yGhq9FaowU.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 37, "npz_gt_video_start_frame": 37, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 37, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2106_0_1"}, {"texts": ["A man on the right is standing and is setting up the green machine while a man on the left side is looking at what the man on the right side is doing."], "durations": null, "exact_frames_per_prompt": [34], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/27Vw4slHE_E.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 46, "npz_gt_video_start_frame": 46, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 46, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_210_0_0"}, {"texts": ["A man wearing a green t-shirt on the left is also standing and is watching person one."], "durations": null, "exact_frames_per_prompt": [35], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/27Vw4slHE_E.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 45, "npz_gt_video_start_frame": 45, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 45, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_210_1_0"}, {"texts": ["A person whose only hands are visible is sticking a tape on a cube shaped sheet."], "durations": null, "exact_frames_per_prompt": [39], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/0UuzBKzUQw8.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 43, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2111_0_0"}, {"texts": ["The person is removing the sticker of the tape and sticking the tape on the other side of the sheet."], "durations": null, "exact_frames_per_prompt": [37], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/0UuzBKzUQw8.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 43, "npz_gt_video_start_frame": 43, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 43, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2111_0_1"}, {"texts": ["A person whose hands are visible is folding a yellow cloth on a brown wooden table."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-XOM5PjRYns.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2112_0_0"}, {"texts": ["A person whose only hands are visible is folding a cloth."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-XOM5PjRYns.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2113_0_0"}, {"texts": ["A man wearing black white clothes is standing and speaking."], "durations": null, "exact_frames_per_prompt": [38], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-Qzp1teT7j0.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 42, "npz_gt_video_start_frame": 42, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 42, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2114_0_0"}, {"texts": ["A person whose hand is visible is holding makizushi."], "durations": null, "exact_frames_per_prompt": [60], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-rdGzf3MnNA.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 64, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2118_0_0"}, {"texts": ["The person puts the makizushi on a green surface."], "durations": null, "exact_frames_per_prompt": [16], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-rdGzf3MnNA.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 64, "npz_gt_video_start_frame": 64, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 64, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2118_0_1"}, {"texts": ["A man wearing a black t-shirt is sitting, holding a box and showing an object."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/17FG2B3x9iA.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2121_0_0"}, {"texts": ["A man wearing a black t-shirt is standing."], "durations": null, "exact_frames_per_prompt": [15], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/2cu1-EjEW0U.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 19, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2122_0_0"}, {"texts": ["The man is ironing a white cloth on the ironing table."], "durations": null, "exact_frames_per_prompt": [60], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/2cu1-EjEW0U.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 19, "npz_gt_video_start_frame": 19, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 19, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2122_0_1"}, {"texts": ["A boy wearing a dark pink t-shirt, black shorts, and a dark blue cap is standing while holding a snake around his neck and then smiling while touching the snake with both hands while a person wearing blue and white outfit first moves towards the boy then he moves back."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/5AsLIAtGmuw.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2123_0_0"}, {"texts": ["A brown snake is on the boy's neck and is creeping on the boy's body."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/5AsLIAtGmuw.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2123_1_0"}, {"texts": ["A man wearing a dark blue t-shirt, gray shorts and white shoes, is walking towards the boy."], "durations": null, "exact_frames_per_prompt": [14], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/5AsLIAtGmuw.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 33, "npz_gt_video_start_frame": 33, "npz_gt_video_end_frame": 47, "skip_frames_after_generation": 33, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2123_2_0"}, {"texts": ["The man then walks on the backside."], "durations": null, "exact_frames_per_prompt": [7], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/5AsLIAtGmuw.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 47, "npz_gt_video_start_frame": 47, "npz_gt_video_end_frame": 54, "skip_frames_after_generation": 47, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2123_2_1"}, {"texts": ["A person wearing a black t-shirt and black jeans is standing on the left side of the gray table, holding a book and putting it into a paper binding machine."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/17_43haGOf8.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2124_0_0"}, {"texts": ["A man wearing a t-shirt is gargling."], "durations": null, "exact_frames_per_prompt": [23], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/6GGT6poH3yE.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 27, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2127_0_0"}, {"texts": ["The man wearing a t-shirt is splitting in a glass."], "durations": null, "exact_frames_per_prompt": [32], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/6GGT6poH3yE.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 27, "npz_gt_video_start_frame": 27, "npz_gt_video_end_frame": 59, "skip_frames_after_generation": 27, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2127_0_1"}, {"texts": ["The man wearing a t-shirt starts drinking."], "durations": null, "exact_frames_per_prompt": [21], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/6GGT6poH3yE.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 59, "npz_gt_video_start_frame": 59, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 59, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2127_0_2"}, {"texts": ["A boy who is nude is holding a glass and drinking from it."], "durations": null, "exact_frames_per_prompt": [33], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/0P_LkJddMLg.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 10, "npz_gt_video_start_frame": 10, "npz_gt_video_end_frame": 43, "skip_frames_after_generation": 10, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2129_1_ms_0"}, {"texts": ["A woman wearing a lab coat is holding an object and tapping it from her other hand.\n"], "durations": null, "exact_frames_per_prompt": [23], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/0P_LkJddMLg.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 46, "npz_gt_video_start_frame": 46, "npz_gt_video_end_frame": 69, "skip_frames_after_generation": 46, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2129_3_ms_0"}, {"texts": ["A man wearing a dark-grey t-shirt with a white design is standing on the brown floor and putting grated cheese on the rolled dough."], "durations": null, "exact_frames_per_prompt": [46], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/0JTvkJnwdFE.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 50, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_212_0_0"}, {"texts": ["The man then starts rolling the edges of the rolled dough."], "durations": null, "exact_frames_per_prompt": [30], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/0JTvkJnwdFE.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 50, "npz_gt_video_start_frame": 50, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 50, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_212_0_1"}, {"texts": ["A man wearing a blue t-shirt is standing and moving."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/2RxICVcd9Rs.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2130_1_0"}, {"texts": ["A big girl wearing a pink t-shirt, black goggles, and blue jeans is sitting on the left side while holding a cow's udder with her left hand and speaking something to the small girl while a baby in black black pants is ubber  with the right hand moving away from there."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-MYJtLnxUqY.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2132_1_0"}, {"texts": ["A small girl wearing a gray-pink t-shirt and dark blue jeans is walking towards the cow and then standing while a girl wearing pink t-shirt holding the cow's udder."], "durations": null, "exact_frames_per_prompt": [26], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-MYJtLnxUqY.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 30, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2132_2_0"}, {"texts": ["The small girl wearing a gray-pink t-shirt and dark blue jeans is pressing the cow's udder."], "durations": null, "exact_frames_per_prompt": [22], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-MYJtLnxUqY.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 30, "npz_gt_video_start_frame": 30, "npz_gt_video_end_frame": 53, "skip_frames_after_generation": 30, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2132_2_1"}, {"texts": ["The small girl wearing a gray-pink t-shirt and dark blue jeans is walking on the backside while a girl wearing pink t-shirt pressing the cow's udder and laughing."], "durations": null, "exact_frames_per_prompt": [11], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-MYJtLnxUqY.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 53, "npz_gt_video_start_frame": 53, "npz_gt_video_end_frame": 64, "skip_frames_after_generation": 53, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2132_2_2"}, {"texts": ["A person wearing a blue t-shirt, a black cap, is sitting on the left side while holding a blue bucket under the udder of the cow with their right hand while a small girl wearing a gray-pink t-shirt and dark blue jeans is pressing the cow's udder and walking on the backside and other girl wearing pink t-shirt holding the cow's udder."], "durations": null, "exact_frames_per_prompt": [38], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-MYJtLnxUqY.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 16, "npz_gt_video_start_frame": 16, "npz_gt_video_end_frame": 54, "skip_frames_after_generation": 16, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2132_3_0"}, {"texts": ["A white-black cow is standing on the black surface and giving milk in the blue bucket while a girl wearing pink t-shirt holding and pressing the cow's udder and then laughing and a kid wearing pink-gray t-shirt is walking towards the cow then pressing the cow's udder and after that walking backside and other group of people sitting around the cow."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-MYJtLnxUqY.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2132_5_0"}, {"texts": ["A person whose hand is visible is touching a fly."], "durations": null, "exact_frames_per_prompt": [19], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/0IAlB8EBZkc.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 23, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2133_0_0"}, {"texts": ["A black fly is sitting on a green leaf.\n while a person is touching the black fly."], "durations": null, "exact_frames_per_prompt": [57], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/0IAlB8EBZkc.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 61, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2133_1_0"}, {"texts": ["A girl is standing near the wooden fence while a boy wearing green t-shirt walks to the left."], "durations": null, "exact_frames_per_prompt": [17], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/0jJdBQAjbs8.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 11, "npz_gt_video_start_frame": 11, "npz_gt_video_end_frame": 28, "skip_frames_after_generation": 11, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2134_1_0"}, {"texts": ["The girl at first throws the food inside the fence while goats are standing near the wooden fence on the other side."], "durations": null, "exact_frames_per_prompt": [6], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/0jJdBQAjbs8.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 28, "npz_gt_video_start_frame": 28, "npz_gt_video_end_frame": 34, "skip_frames_after_generation": 28, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2134_1_1"}, {"texts": ["The girl moves backward."], "durations": null, "exact_frames_per_prompt": [13], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/0jJdBQAjbs8.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 34, "npz_gt_video_start_frame": 34, "npz_gt_video_end_frame": 47, "skip_frames_after_generation": 34, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2134_1_2"}, {"texts": ["A man wearing a pink shirt is sitting on the white surface and crying."], "durations": null, "exact_frames_per_prompt": [26], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/108dmK0DCfw.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 30, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2136_0_ms_0"}, {"texts": ["A person whose only hand is visible is frying vegetables in a pan."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/4PubjLF2-TE.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2141_0_0"}, {"texts": ["A person whose hand is visible is stirring the vegetables with a wooden spatula in the pan. "], "durations": null, "exact_frames_per_prompt": [41], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/4PubjLF2-TE.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 39, "npz_gt_video_start_frame": 39, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 39, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2142_0_0"}, {"texts": ["A woman wearing grey pants is sitting on a camel's back while the man feeds the camel"], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-gv2GpdS7EI.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2147_0_0"}, {"texts": ["A brown camel, carrying a woman on its back, is sitting on a brown surface while a man wearing a black jacket and holding a bottle in his hand is coming towards the camel's mouth."], "durations": null, "exact_frames_per_prompt": [25], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-gv2GpdS7EI.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 29, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2147_1_0"}, {"texts": ["The camel is drinking a black soft drink from a bottle."], "durations": null, "exact_frames_per_prompt": [51], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-gv2GpdS7EI.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 29, "npz_gt_video_start_frame": 29, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 29, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2147_1_1"}, {"texts": ["A man wearing a black jacket is standing while the brown camel is sitting on the floor and a girl is sitting on it."], "durations": null, "exact_frames_per_prompt": [18], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-gv2GpdS7EI.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 13, "npz_gt_video_start_frame": 13, "npz_gt_video_end_frame": 31, "skip_frames_after_generation": 13, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2147_2_0"}, {"texts": ["The man is giving a black soft drink to a brown camel."], "durations": null, "exact_frames_per_prompt": [49], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-gv2GpdS7EI.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 31, "npz_gt_video_start_frame": 31, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 31, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2147_2_1"}, {"texts": ["A girl wearing a white hoodie is sitting in the car and crying while speaking."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-Sd-j0rKeKw.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2148_0_0"}, {"texts": ["A woman is brushing her eyebrows with a black brush."], "durations": null, "exact_frames_per_prompt": [37], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/0eAbf3x4foY.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 41, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2149_0_0"}, {"texts": ["The woman is defining her eyebrows with a black eyebrow pencil."], "durations": null, "exact_frames_per_prompt": [39], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/0eAbf3x4foY.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 41, "npz_gt_video_start_frame": 41, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 41, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2149_0_1"}, {"texts": ["A man whose only half body is visible is wearing a denim shirt and jeans is putting banknotes into a note counting machine while a person moving his hands near a note counting machine."], "durations": null, "exact_frames_per_prompt": [23], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/2j8_HQk-pW8.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 27, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_214_0_0"}, {"texts": ["The man picks up the notes."], "durations": null, "exact_frames_per_prompt": [5], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/2j8_HQk-pW8.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 27, "npz_gt_video_start_frame": 27, "npz_gt_video_end_frame": 32, "skip_frames_after_generation": 27, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_214_0_1"}, {"texts": ["A person whose only hands are visible is putting his hands in front of the machine with an open palm."], "durations": null, "exact_frames_per_prompt": [26], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/2j8_HQk-pW8.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 6, "npz_gt_video_start_frame": 6, "npz_gt_video_end_frame": 32, "skip_frames_after_generation": 6, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_214_1_0"}, {"texts": ["A boy wearing a red t-shirt is holding an ice-cream cone and eating an ice-cream and smiling."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-8dDh2JSy_E.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2150_0_0"}, {"texts": ["A person whose hands are visible is folding the cloth on the bed."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/JmF1Cz8CD0I.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2151_1_0"}, {"texts": ["A girl wearing a maroon top is speaking."], "durations": null, "exact_frames_per_prompt": [21], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/1oI9CmYAdKs.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 25, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2152_0_0"}, {"texts": ["The girl is eating a chocolate."], "durations": null, "exact_frames_per_prompt": [34], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/1oI9CmYAdKs.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 25, "npz_gt_video_start_frame": 25, "npz_gt_video_end_frame": 59, "skip_frames_after_generation": 25, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2152_0_1"}, {"texts": ["The girl is smiling."], "durations": null, "exact_frames_per_prompt": [21], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/1oI9CmYAdKs.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 59, "npz_gt_video_start_frame": 59, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 59, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2152_0_2"}, {"texts": ["A man wearing a multicolour shirt is standing, holding food in his hand, and feeding the birds.\n"], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-MwPLE_xWz0.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2153_0_0"}, {"texts": ["A man wearing swimming goggles is swimming underwater and feeding the group of grey fishes while holding a white object."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/0c-YOkQRe9I.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2154_0_0"}, {"texts": ["A white fish is swimming in the sea in the front while a man swimming inside a river and feeding the group of fish."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/0c-YOkQRe9I.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2155_1_0"}, {"texts": ["A person whose hand is visible is swimming behind the white fish in the sea."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/0c-YOkQRe9I.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2155_2_0"}, {"texts": ["A man wearing a black t-shirt is sitting, holding a food packet."], "durations": null, "exact_frames_per_prompt": [50], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/0H7QF6YNj7k.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 54, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2156_0_0"}, {"texts": ["The man starts eating the food."], "durations": null, "exact_frames_per_prompt": [26], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/0H7QF6YNj7k.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 54, "npz_gt_video_start_frame": 54, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 54, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2156_0_1"}, {"texts": ["A girl wearing a colorful striped top and pink slippers is sitting on the baby car seat in the car on the right side while holding a teddy toy and is laughing while kicking her right leg on the boy's face."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/1nyzGiAv4DU.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2158_0_0"}, {"texts": ["A boy wearing a gray-white checkered shirt and a black cap is sitting in the car on the left side and he moves his face back and forth when the girl kicks him."], "durations": null, "exact_frames_per_prompt": [40], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/1nyzGiAv4DU.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 44, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2158_1_0"}, {"texts": ["The boy hides his face with his hands while the girl beside laughs"], "durations": null, "exact_frames_per_prompt": [36], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/1nyzGiAv4DU.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 44, "npz_gt_video_start_frame": 44, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 44, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2158_1_1"}, {"texts": ["A woman wearing red lower is sitting on the floor and tearing a white paper while a baby wearing a white sweatshirt is sitting on a blue toy chair and is laughing when the woman is tearing the paper."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-QFrYnizm74.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_215_0_0"}, {"texts": ["A baby wearing white clothes, sitting on a baby chair is looking at the woman and is laughing."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-QFrYnizm74.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_215_1_0"}, {"texts": ["A man wearing khaki green clothes and black slippers is sitting and tightening the pulley"], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/14w2XPnkR5Y.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2160_0_0"}, {"texts": ["A group of six horses are standing on the brown surface."], "durations": null, "exact_frames_per_prompt": [38], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/26f5X-kDaT8.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 42, "npz_gt_video_start_frame": 42, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 42, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2161_0_ms_0"}, {"texts": ["A woman wearing a maroon jacket, black jeans, and black shoes is combing the horse's hair with the horse hair brush while standing on the left side of the brown surface."], "durations": null, "exact_frames_per_prompt": [34], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/26f5X-kDaT8.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 38, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2161_1_ms_0"}, {"texts": ["A white-black horse is getting hair combed by the woman while standing on the brown surface."], "durations": null, "exact_frames_per_prompt": [32], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/26f5X-kDaT8.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 5, "npz_gt_video_start_frame": 5, "npz_gt_video_end_frame": 37, "skip_frames_after_generation": 5, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2161_2_ms_0"}, {"texts": ["A brown cow is standing on a green grass surface and getting milked by the first man.\n while a person is standing on the backside."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/0rEq__LZZmc.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2164_2_0"}, {"texts": ["A woman wearing a green jacket and blue jeans is riding a horse while other two person is also riding a horse."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/3qNOjGNjKFk.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2167_0_0"}, {"texts": ["A person wearing a red and black jacket is sitting on a horse."], "durations": null, "exact_frames_per_prompt": [23], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/3qNOjGNjKFk.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 39, "npz_gt_video_start_frame": 39, "npz_gt_video_end_frame": 62, "skip_frames_after_generation": 39, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2167_1_ms_0"}, {"texts": ["A man wearing a blue sweatshirt and blue pants is riding a horse.\n while a man wearing a black-red jacket is sitting on the back of a horse."], "durations": null, "exact_frames_per_prompt": [16], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/3qNOjGNjKFk.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 64, "npz_gt_video_start_frame": 64, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 64, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2167_2_0"}, {"texts": ["A brown horse is walking around the ground with a woman sitting on its back."], "durations": null, "exact_frames_per_prompt": [37], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/3qNOjGNjKFk.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 41, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2167_4_0"}, {"texts": ["A horse is standing with a person sitting on its back while a man with blue jacket enjoying horse riding"], "durations": null, "exact_frames_per_prompt": [41], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/3qNOjGNjKFk.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 39, "npz_gt_video_start_frame": 39, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 39, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2167_5_0"}, {"texts": ["A horse is walking around on the ground with two men sitting on its back."], "durations": null, "exact_frames_per_prompt": [13], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/3qNOjGNjKFk.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 67, "npz_gt_video_start_frame": 67, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 67, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2167_6_ms_0"}, {"texts": ["A person wearing protective gear is sitting and spraying liquid with a sprayer.\n while other person wearing protective gear and gray jeans is walking towards him and sitting beside him."], "durations": null, "exact_frames_per_prompt": [72], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/1WOhUb-Gsb4.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 76, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2168_0_0"}, {"texts": ["A person also wearing protective gear is standing near the boundary while the first person wearing protective gear is sitting on the brown floor on his knees."], "durations": null, "exact_frames_per_prompt": [20], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/1WOhUb-Gsb4.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 24, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2168_1_0"}, {"texts": ["The man starts walking."], "durations": null, "exact_frames_per_prompt": [25], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/1WOhUb-Gsb4.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 24, "npz_gt_video_start_frame": 24, "npz_gt_video_end_frame": 49, "skip_frames_after_generation": 24, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2168_1_1"}, {"texts": ["The man sits down near the first person."], "durations": null, "exact_frames_per_prompt": [31], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/1WOhUb-Gsb4.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 49, "npz_gt_video_start_frame": 49, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 49, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2168_1_2"}, {"texts": ["A woman wearing a white shirt is standing in the front and rubbing her hands while the man behind moves his hand back and front and turns his head his right"], "durations": null, "exact_frames_per_prompt": [12], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/5zwidJ_3Zzc.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 16, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2169_0_0"}, {"texts": ["The woman puts her hand in a wooden container and the man looks down and puts his hand the container"], "durations": null, "exact_frames_per_prompt": [29], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/5zwidJ_3Zzc.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 16, "npz_gt_video_start_frame": 16, "npz_gt_video_end_frame": 45, "skip_frames_after_generation": 16, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2169_0_1"}, {"texts": ["A man wearing a white shirt is standing on the backside, moving his hand while a woman also wearing a white shirt is standing on the right, moving his hand."], "durations": null, "exact_frames_per_prompt": [34], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/5zwidJ_3Zzc.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 38, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2169_1_0"}, {"texts": ["The man puts his hand in the wooden container while woman also puts her hand in the wooden container."], "durations": null, "exact_frames_per_prompt": [5], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/5zwidJ_3Zzc.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 38, "npz_gt_video_start_frame": 38, "npz_gt_video_end_frame": 43, "skip_frames_after_generation": 38, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2169_1_1"}, {"texts": ["A baby wearing a white t-shirt is sitting in a bumbo baby chair and laughing while a woman wearing red lower is sitting on the floor and tearing a white paper."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-QFrYnizm74.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_216_0_0"}, {"texts": ["A person wearing red joggers is sitting and tearing papers while the baby laughs."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-QFrYnizm74.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_216_1_0"}, {"texts": ["A person whose hands are visible is folding a piece of paper and making something with it."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/2mZ4EWfCC00.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2174_0_0"}, {"texts": ["A man wearing a black jacket, black jeans, and black shoes is standing while holding a fry pan with the chapati."], "durations": null, "exact_frames_per_prompt": [9], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-Ji8q5l4Q0I.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 13, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2177_0_0"}, {"texts": ["The fry pan breaks and falls on the floor when he starts tossing the chapati."], "durations": null, "exact_frames_per_prompt": [40], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-Ji8q5l4Q0I.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 13, "npz_gt_video_start_frame": 13, "npz_gt_video_end_frame": 53, "skip_frames_after_generation": 13, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2177_0_1"}, {"texts": ["The man keeps the fry pan handle on the counter-top."], "durations": null, "exact_frames_per_prompt": [5], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-Ji8q5l4Q0I.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 53, "npz_gt_video_start_frame": 53, "npz_gt_video_end_frame": 58, "skip_frames_after_generation": 53, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2177_0_2"}, {"texts": ["The man tilts his body to pick up the chapati with his right hand from the floor."], "durations": null, "exact_frames_per_prompt": [22], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-Ji8q5l4Q0I.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 58, "npz_gt_video_start_frame": 58, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 58, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2177_0_3"}, {"texts": ["A girl wearing a purple t-shirt and blue pajamas with black dots is sitting on the floor and unwrapping her gift."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-cerMH7BTOc.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2178_0_0"}, {"texts": ["A woman wearing a red t-shirt and blue jeans is holding a transparent bag with apples, and laughing while a person wearing a black hat is holding something in both hands and laughing."], "durations": null, "exact_frames_per_prompt": [12], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-iFTaj2NjwI.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 16, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2179_0_0"}, {"texts": ["The woman is plucking the apple from the tree while the person moves to the left side."], "durations": null, "exact_frames_per_prompt": [10], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-iFTaj2NjwI.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 16, "npz_gt_video_start_frame": 16, "npz_gt_video_end_frame": 26, "skip_frames_after_generation": 16, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2179_0_1"}, {"texts": ["The woman is throwing the apple."], "durations": null, "exact_frames_per_prompt": [7], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-iFTaj2NjwI.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 26, "npz_gt_video_start_frame": 26, "npz_gt_video_end_frame": 33, "skip_frames_after_generation": 26, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2179_0_2"}, {"texts": ["A woman wearing a white jacket, black jeans, and a black hat is standing while holding apples in her hands and then is laughing while walking on the green grass surface while a woman wearing a red top is holding a carry bag in her right hand and plucking a apple from her left hand and throwing it in front then turns and walks to the left side and laughing."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-iFTaj2NjwI.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2179_1_0"}, {"texts": ["A person wearing a green cloth and a watch is binding the papers with a binding machine."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-CMo6AJhtZo.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2180_0_0"}, {"texts": ["A boy wearing a white t-shirt is standing on a green grass surface."], "durations": null, "exact_frames_per_prompt": [31], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/1-ziZ2AakwY.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 35, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2182_0_0"}, {"texts": ["The boy walks towards a tree branch."], "durations": null, "exact_frames_per_prompt": [11], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/1-ziZ2AakwY.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 35, "npz_gt_video_start_frame": 35, "npz_gt_video_end_frame": 46, "skip_frames_after_generation": 35, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2182_0_1"}, {"texts": ["The boy plucks an apple from the tree branch."], "durations": null, "exact_frames_per_prompt": [34], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/1-ziZ2AakwY.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 46, "npz_gt_video_start_frame": 46, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 46, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2182_0_2"}, {"texts": ["A person wearing shorts is at first screwing the nut with a drilling machine on the wooden frame."], "durations": null, "exact_frames_per_prompt": [11], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/47M1MLCC2Uk.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 15, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2183_0_0"}, {"texts": ["The man moves to the other side."], "durations": null, "exact_frames_per_prompt": [12], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/47M1MLCC2Uk.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 15, "npz_gt_video_start_frame": 15, "npz_gt_video_end_frame": 27, "skip_frames_after_generation": 15, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2183_0_1"}, {"texts": ["The man puts a nut into the other side."], "durations": null, "exact_frames_per_prompt": [53], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/47M1MLCC2Uk.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 27, "npz_gt_video_start_frame": 27, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 27, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2183_0_2"}, {"texts": ["A girl wearing a white printed top is standing on the muddy surface while picking the food in her hands from her clothes."], "durations": null, "exact_frames_per_prompt": [42], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/1b696frJiK8.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 46, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2184_0_0"}, {"texts": ["The girl turns in the left direction. "], "durations": null, "exact_frames_per_prompt": [12], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/1b696frJiK8.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 46, "npz_gt_video_start_frame": 46, "npz_gt_video_end_frame": 58, "skip_frames_after_generation": 46, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2184_0_1"}, {"texts": ["A girl wearing yellow clothes is lying on the surface and eating food."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-3gv9XDUOFA.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2187_0_0"}, {"texts": ["A girl wearing yellow cloth is lying on the surface and eating food."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-3gv9XDUOFA.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2188_0_0"}, {"texts": ["A child wearing a sky blue t-shirt and white shorts is sitting on a chair on the left side and is turning the pages of the magazine with their left hand while holding the magazine with their right hand on the table.\n"], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/5jpXtwT7Enk.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2189_0_0"}, {"texts": ["A person wearing white clothes is holding a watermelon."], "durations": null, "exact_frames_per_prompt": [24], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/9JkW76UWAUs.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 28, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_218_0_0"}, {"texts": ["The person wearing white clothes put it on a whiteboard."], "durations": null, "exact_frames_per_prompt": [20], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/9JkW76UWAUs.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 28, "npz_gt_video_start_frame": 28, "npz_gt_video_end_frame": 48, "skip_frames_after_generation": 28, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_218_0_1"}, {"texts": ["The person wearing white clothes starts cutting with a knife."], "durations": null, "exact_frames_per_prompt": [32], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/9JkW76UWAUs.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 48, "npz_gt_video_start_frame": 48, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 48, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_218_0_2"}, {"texts": ["A man wearing a black t-shirt is sitting on a chair and getting a plaster in his hand while another man wearing a blue shirt is bandaging the man's hand and speaking, and a group of people is standing around the man."], "durations": null, "exact_frames_per_prompt": [77], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/15-45XF4S5A.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 81, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2190_0_0"}, {"texts": ["A man wearing a white black top is standing and wrapping a plaster in the first person's hand while a group of people are standing around them and watching."], "durations": null, "exact_frames_per_prompt": [77], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/15-45XF4S5A.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 81, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2190_1_0"}, {"texts": ["A man wearing a black t-shirt is checking the depth of a Tyre with a coin."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/38ooOGWvMIw.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2192_0_0"}, {"texts": ["A man wearing light green cloth is standing and holding a coca cola bottle and pouring in a glass."], "durations": null, "exact_frames_per_prompt": [34], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/rcKVpSTOWZE.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 38, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2193_0_0"}, {"texts": ["The man is closing the cap of the coca cola bottle."], "durations": null, "exact_frames_per_prompt": [43], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/rcKVpSTOWZE.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 38, "npz_gt_video_start_frame": 38, "npz_gt_video_end_frame": 81, "skip_frames_after_generation": 38, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2193_0_1"}, {"texts": ["A woman wearing white gloves is sitting and milking a cow."], "durations": null, "exact_frames_per_prompt": [33], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/1gGVdnXlW5o.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 37, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2194_0_0"}, {"texts": ["The woman wearing white gloves is stands up."], "durations": null, "exact_frames_per_prompt": [43], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/1gGVdnXlW5o.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 37, "npz_gt_video_start_frame": 37, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 37, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2194_0_1"}, {"texts": ["A woman wearing a light brown top is peeling the watermelon with help of the knife.\n"], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/Amxs2Uun458.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2196_0_0"}, {"texts": ["A person whose hands are visible is tying a knot using two orange ropes."], "durations": null, "exact_frames_per_prompt": [59], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/0PBtUnoPwvU.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 21, "npz_gt_video_start_frame": 21, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 21, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_219_0_0"}, {"texts": ["A woman wearing blue clothes is sitting while a kid wearing a purple outfit is sitting on the woman's lap and eating something with a fork."], "durations": null, "exact_frames_per_prompt": [39], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/1G0RbpsS_aE.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 43, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2200_0_0"}, {"texts": ["A baby girl wearing purple clothes sitting on the lap of the first woman is eating food."], "durations": null, "exact_frames_per_prompt": [39], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/1G0RbpsS_aE.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 43, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2200_1_0"}, {"texts": ["A baby wearing blue clothes is sitting on the lap of a woman, holding a fork."], "durations": null, "exact_frames_per_prompt": [24], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/1G0RbpsS_aE.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 28, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2201_0_0"}, {"texts": ["The baby is eating food."], "durations": null, "exact_frames_per_prompt": [15], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/1G0RbpsS_aE.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 28, "npz_gt_video_start_frame": 28, "npz_gt_video_end_frame": 43, "skip_frames_after_generation": 28, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2201_0_1"}, {"texts": ["A woman wearing a white t-shirt is sitting on the bed and holding the black cloth."], "durations": null, "exact_frames_per_prompt": [27], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/0xigrP4k5-c.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 31, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2203_0_0"}, {"texts": ["The woman throws the cloth."], "durations": null, "exact_frames_per_prompt": [10], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/0xigrP4k5-c.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 31, "npz_gt_video_start_frame": 31, "npz_gt_video_end_frame": 41, "skip_frames_after_generation": 31, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2203_0_1"}, {"texts": ["The woman is picking-up another cloth."], "durations": null, "exact_frames_per_prompt": [39], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/0xigrP4k5-c.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 41, "npz_gt_video_start_frame": 41, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 41, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2203_0_2"}, {"texts": ["A girl wearing a purple top is sitting and chewing something in her mouth."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/4P1kp5oqA8I.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2204_0_0"}, {"texts": ["A girl wearing denim jeans is sitting on the floor, collecting waste while a kid wearing a red t-shirt is sitting on the brown floor and touches the box."], "durations": null, "exact_frames_per_prompt": [37], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/01rdh-3CnsE.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 41, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2205_0_0"}, {"texts": ["The girl is holding a blue bag."], "durations": null, "exact_frames_per_prompt": [33], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/01rdh-3CnsE.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 41, "npz_gt_video_start_frame": 41, "npz_gt_video_end_frame": 74, "skip_frames_after_generation": 41, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2205_0_1"}, {"texts": ["A baby boy wearing a red t-shirt is sitting on the floor and touching the green boxes while a person sitting on the wooden floor and carrying a blue bag is keeping the gift wrap on the right side, and another person whose hands are visible is holding a white t-shirt."], "durations": null, "exact_frames_per_prompt": [70], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/01rdh-3CnsE.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 74, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2205_1_0"}, {"texts": ["An orange fish is swimming inside an aquarium along with other fishes."], "durations": null, "exact_frames_per_prompt": [67], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/14GQGPU0k9Q.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 71, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2206_1_0"}, {"texts": ["The fish starts eating food given by a woman."], "durations": null, "exact_frames_per_prompt": [9], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/14GQGPU0k9Q.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 71, "npz_gt_video_start_frame": 71, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 71, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2206_1_1"}, {"texts": ["A brown dog is sitting on an aggregate surface while a person wearing blue jeans is brushing the body of the dog."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-nzBW4S8UqA.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2207_0_0"}, {"texts": ["A person is brushing the hair of the dog with a brush."], "durations": null, "exact_frames_per_prompt": [33], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-nzBW4S8UqA.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 37, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2207_1_0"}, {"texts": ["The person is removing the hair from the brush."], "durations": null, "exact_frames_per_prompt": [43], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-nzBW4S8UqA.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 37, "npz_gt_video_start_frame": 37, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 37, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2207_1_1"}, {"texts": ["A girl wearing a yellow striped printed black jacket and white spotted black leggings is sitting on the brown camel's back ahead of the other girl and riding on the road while the man wearing blue jeans is walking on the road while holding another camel's rope and moving ahead of the brown camel, which is carrying girls on its back."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/0A87paHKUec.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2210_0_0"}, {"texts": ["A girl wearing a blue printed white t-shirt and black jeans is sitting on the brown camel's back behind the first girl and is riding a camel on the road while other man wearing white shirt with another camel walking along them."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/0A87paHKUec.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2210_1_0"}, {"texts": ["A brown camel is tied with the other camel by a rope and is walking behind the other camel while carrying girls on his back while the man holding the camel rope and walks in front of camels."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/0A87paHKUec.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2210_2_0"}, {"texts": ["A man wearing a white shirt and black pants is holding the second camel by a rope while a person and a girl are riding on the back of the first camel."], "durations": null, "exact_frames_per_prompt": [48], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/0A87paHKUec.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 32, "npz_gt_video_start_frame": 32, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 32, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2210_3_0"}, {"texts": ["A dark brown camel is being held by the man through a rope and is walking behind the man while the kids are on the back of the camel."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/0A87paHKUec.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2210_4_0"}, {"texts": ["A baby wearing a pink dress is sitting in the lap of a woman and laughing while watching a brown dog."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-v_iMUMMX6g.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2211_0_0"}, {"texts": ["A brown dog facing the backside is sitting while other woman wearing gray printed shirt is talking to him and laughing and a baby wearing pink and white dress sitting on her lap and is also laughing."], "durations": null, "exact_frames_per_prompt": [61], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-v_iMUMMX6g.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 65, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2211_1_0"}, {"texts": ["The dog is barking at a baby and a woman."], "durations": null, "exact_frames_per_prompt": [15], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-v_iMUMMX6g.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 65, "npz_gt_video_start_frame": 65, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 65, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2211_1_1"}, {"texts": ["A woman wearing blue jeans is sitting on a green sofa while holding the baby in her lap while a brown dog barking in front of the woman"], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-v_iMUMMX6g.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2211_2_0"}, {"texts": ["A man wearing a black cloth is sitting on a chair on the stage.  while a woman is sitting with a baby and a group of people in which some people are walking and some people are standing on the stage."], "durations": null, "exact_frames_per_prompt": [59], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/0GMwoizbG58.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 63, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2212_2_0"}, {"texts": ["The man is kissing the baby's hand."], "durations": null, "exact_frames_per_prompt": [17], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/0GMwoizbG58.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 63, "npz_gt_video_start_frame": 63, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 63, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2212_2_1"}, {"texts": ["A woman wearing a gray-blue cloth is sitting on a brown chair on the stage while holding the baby in her hands.\n while the man with white and black headgear kissing the baby's hand"], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/0GMwoizbG58.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2212_3_0"}, {"texts": ["A baby wearing a white cloth is lying on the woman's lap while being held by the woman while a man wearing black clothes is sitting on the right and other people are doing different activity."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/0GMwoizbG58.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2212_4_0"}, {"texts": ["A person whose only hands are visible is holding a rope."], "durations": null, "exact_frames_per_prompt": [45], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/0_jaO1sfF2Y.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 49, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2213_0_0"}, {"texts": ["The person is keeping both the ends in a criss-cross form."], "durations": null, "exact_frames_per_prompt": [31], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/0_jaO1sfF2Y.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 49, "npz_gt_video_start_frame": 49, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 49, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2213_0_1"}, {"texts": ["A person whose only hands are visible is holding a rope."], "durations": null, "exact_frames_per_prompt": [15], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/0_jaO1sfF2Y.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 19, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2214_0_0"}, {"texts": ["The person is putting the rope on the brown surface. "], "durations": null, "exact_frames_per_prompt": [23], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/0_jaO1sfF2Y.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 19, "npz_gt_video_start_frame": 19, "npz_gt_video_end_frame": 42, "skip_frames_after_generation": 19, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2214_0_1"}, {"texts": ["The person puts one side of the rope on top of the other."], "durations": null, "exact_frames_per_prompt": [38], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/0_jaO1sfF2Y.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 42, "npz_gt_video_start_frame": 42, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 42, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2214_0_2"}, {"texts": ["A person wearing chef dress is standing and holding a frosting tube while talking."], "durations": null, "exact_frames_per_prompt": [36], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/2iFfYyS7IWc.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 44, "npz_gt_video_start_frame": 44, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 44, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2215_3_0"}, {"texts": ["A boy wearing a grey t-shirt is sitting on a chair and eating food using his hands."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-jK-YcxW6A0.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2217_0_0"}, {"texts": ["A baby is sitting on a plastic chair and eating noodles and a person is moving noodles on the tray towards the baby."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-0R6wpipD-c.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2219_0_0"}, {"texts": ["A person is sliding the noodles with his finger in front of the baby."], "durations": null, "exact_frames_per_prompt": [20], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-0R6wpipD-c.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 38, "npz_gt_video_start_frame": 38, "npz_gt_video_end_frame": 58, "skip_frames_after_generation": 38, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2219_1_0"}, {"texts": ["A woman wearing a black top, a white apparel and spectacles is standing with a spatula in her pocket."], "durations": null, "exact_frames_per_prompt": [25], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-Z-KvgyNArU.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 29, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2221_0_0"}, {"texts": ["The woman is looking down and smiling."], "durations": null, "exact_frames_per_prompt": [28], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-Z-KvgyNArU.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 29, "npz_gt_video_start_frame": 29, "npz_gt_video_end_frame": 57, "skip_frames_after_generation": 29, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2221_0_1"}, {"texts": ["A woman wearing a black top, a white apparel, and a flower hair band is holding a pan and flipping a pancake in the pan."], "durations": null, "exact_frames_per_prompt": [22], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-Z-KvgyNArU.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 58, "npz_gt_video_start_frame": 58, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 58, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2221_2_ms_0"}, {"texts": ["A man wearing a white t-shirt is riding on the back of the camel in a right direction while other person wearing blue t-shirt is also riding on the back of the another camel and other person wearing brown shirt with two more camels walking along them."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/19lg_bl0HzY.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2226_2_0"}, {"texts": ["A man is walking on the sand surface along with the camels in a right direction while others sit on the camels."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/19lg_bl0HzY.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2226_3_0"}, {"texts": ["A boy wearing a blue t-shirt is standing. "], "durations": null, "exact_frames_per_prompt": [19], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/13TjWqod8s4.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 23, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2228_1_0"}, {"texts": ["The boy wearing a blue t-shirt is holding the fuel dispenser knob in his hand and speaking."], "durations": null, "exact_frames_per_prompt": [57], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/13TjWqod8s4.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 23, "npz_gt_video_start_frame": 23, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 23, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2228_1_1"}, {"texts": ["A man wearing a white shirt is standing."], "durations": null, "exact_frames_per_prompt": [22], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/4Eop4Q333d4.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 26, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2229_0_0"}, {"texts": ["The man sits."], "durations": null, "exact_frames_per_prompt": [28], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/4Eop4Q333d4.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 26, "npz_gt_video_start_frame": 26, "npz_gt_video_end_frame": 54, "skip_frames_after_generation": 26, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2229_0_1"}, {"texts": ["The man stands and puts the glass on the table."], "durations": null, "exact_frames_per_prompt": [26], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/4Eop4Q333d4.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 54, "npz_gt_video_start_frame": 54, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 54, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2229_0_2"}, {"texts": ["A person wearing a white shirt is standing."], "durations": null, "exact_frames_per_prompt": [17], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/4Eop4Q333d4.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 21, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2230_0_0"}, {"texts": ["The person wearing a white shirt sits."], "durations": null, "exact_frames_per_prompt": [32], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/4Eop4Q333d4.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 21, "npz_gt_video_start_frame": 21, "npz_gt_video_end_frame": 54, "skip_frames_after_generation": 21, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2230_0_1"}, {"texts": ["The person wearing a white shirt again stands."], "durations": null, "exact_frames_per_prompt": [8], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/4Eop4Q333d4.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 54, "npz_gt_video_start_frame": 54, "npz_gt_video_end_frame": 62, "skip_frames_after_generation": 54, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2230_0_2"}, {"texts": ["The person wearing a white shirt puts the glass on the table."], "durations": null, "exact_frames_per_prompt": [18], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/4Eop4Q333d4.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 62, "npz_gt_video_start_frame": 62, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 62, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2230_0_3"}, {"texts": ["A person wearing maroon cloth is standing and mixing some food on a white plate."], "durations": null, "exact_frames_per_prompt": [56], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-G5l8SuVPac.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 60, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2231_0_0"}, {"texts": ["The person is pouring something from a small bowl."], "durations": null, "exact_frames_per_prompt": [19], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-G5l8SuVPac.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 60, "npz_gt_video_start_frame": 60, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 60, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2231_0_1"}, {"texts": ["A person  wearing a maroon chef coat is mixing breadcrumbs on a plate."], "durations": null, "exact_frames_per_prompt": [66], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-G5l8SuVPac.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 70, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2232_0_0"}, {"texts": ["The person is sprinkling something from a bowl on the white flour."], "durations": null, "exact_frames_per_prompt": [10], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-G5l8SuVPac.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 70, "npz_gt_video_start_frame": 70, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 70, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2232_0_1"}, {"texts": ["A woman wearing peach-colored clothes is applying the cream under her eyes and on her cheekbones."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/3p0DeP6eZjg.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2233_0_0"}, {"texts": ["A girl, whose hand is only visible, is wearing a blue sweatshirt and opening a section of a maroon box."], "durations": null, "exact_frames_per_prompt": [33], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-gRVGgSAjwk.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 37, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2235_0_0"}, {"texts": ["The girl, whose hand is only visible, is wearing a blue sweatshirt and holding an eyelash curler in her hand."], "durations": null, "exact_frames_per_prompt": [23], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-gRVGgSAjwk.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 37, "npz_gt_video_start_frame": 37, "npz_gt_video_end_frame": 60, "skip_frames_after_generation": 37, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2235_0_1"}, {"texts": ["The girl wearing a blue sweatshirt is sitting on a grey carpet, opening a gift wrap and starts cutting the gift wrap with a pair of scissors."], "durations": null, "exact_frames_per_prompt": [20], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-gRVGgSAjwk.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 60, "npz_gt_video_start_frame": 60, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 60, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2235_0_2"}, {"texts": ["A person wearing a white-blue check shirt, black pants and a box on his head is walking on the green surface and moving his hands up and down."], "durations": null, "exact_frames_per_prompt": [36], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/25-z4KitoAo.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 44, "npz_gt_video_start_frame": 44, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 44, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2236_0_0"}, {"texts": ["A person whose hand is visible is holding a pan and a spatula and mixing egg in the pan."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/15svAxreiro.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2237_0_0"}, {"texts": ["A man whose finger is visible is steering the yellow food in a frying pan with the help of a black spatula."], "durations": null, "exact_frames_per_prompt": [51], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/15svAxreiro.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 13, "npz_gt_video_start_frame": 13, "npz_gt_video_end_frame": 64, "skip_frames_after_generation": 13, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2238_0_0"}, {"texts": ["A man wearing a white t-shirt is sitting on his knees, holding a green-black drilling machine then attaches a screw on the drilling machine."], "durations": null, "exact_frames_per_prompt": [19], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/jv--6BZC2T0.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 23, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2243_0_0"}, {"texts": ["The man wearing a white t-shirt starts screwing the wooden frame."], "durations": null, "exact_frames_per_prompt": [51], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/jv--6BZC2T0.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 23, "npz_gt_video_start_frame": 23, "npz_gt_video_end_frame": 74, "skip_frames_after_generation": 23, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2243_0_1"}, {"texts": ["The man wearing a white t-shirt starts sliding a part of the wooden frame."], "durations": null, "exact_frames_per_prompt": [6], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/jv--6BZC2T0.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 74, "npz_gt_video_start_frame": 74, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 74, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2243_0_2"}, {"texts": ["A woman wearing a pink-black cloth is standing and holding a cloth, and folding it on a gray counter-top."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/1ArSEgPksog.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2244_0_0"}, {"texts": ["A woman wearing a pink suit is standing and folding a piece of printed cloth on the table."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/1ArSEgPksog.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2245_0_0"}, {"texts": ["A man in a cap is standing with a black goat and feeding it from a bottle."], "durations": null, "exact_frames_per_prompt": [56], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-lOSt51AJeM.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 60, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2246_0_0"}, {"texts": ["A black goat is drinking from the bottle while the man holds the bottle."], "durations": null, "exact_frames_per_prompt": [56], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-lOSt51AJeM.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 60, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2246_4_0"}, {"texts": ["A man in a cap is standing with a black goat and feeding it from the bottle."], "durations": null, "exact_frames_per_prompt": [56], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-lOSt51AJeM.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 60, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2247_0_0"}, {"texts": ["A black goat is drinking from the bottle while a man is holding the bottle and some people are standing on the backside."], "durations": null, "exact_frames_per_prompt": [56], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-lOSt51AJeM.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 60, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2247_4_0"}, {"texts": ["A kid wearing white t-shirt is standing on the wooden surface."], "durations": null, "exact_frames_per_prompt": [8], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-uw6ynoFYog.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 12, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2248_0_0"}, {"texts": ["The kid wearing white t-shirt is throwing the fish seed."], "durations": null, "exact_frames_per_prompt": [68], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-uw6ynoFYog.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 12, "npz_gt_video_start_frame": 12, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 12, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2248_0_1"}, {"texts": ["A boy wearing a white printed t-shirt is sitting near the table and crying and He shouts for the plate while a person whose hand is visible is holding a plate."], "durations": null, "exact_frames_per_prompt": [49], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/0Cvt71QfP6k.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 53, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2249_0_0"}, {"texts": ["The boy picks up the cloth and throws it in the left direction."], "durations": null, "exact_frames_per_prompt": [26], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/0Cvt71QfP6k.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 53, "npz_gt_video_start_frame": 53, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 53, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2249_0_1"}, {"texts": ["A person whose hands are visible is tightening a knot by pulling orange ropes."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/049Oz5Mk9YE.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2250_0_0"}, {"texts": ["A girl wearing a pink floral printed top is sitting on a wooden chair, eating peanut butter with a spoon while the woman behind her does some kind of action."], "durations": null, "exact_frames_per_prompt": [60], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/1knVO7tRcYc.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 64, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2251_0_0"}, {"texts": ["The girl is standing on the left and putting chips in the mouth of another girl."], "durations": null, "exact_frames_per_prompt": [16], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/1knVO7tRcYc.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 64, "npz_gt_video_start_frame": 64, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 64, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2251_0_1"}, {"texts": ["A girl wearing a black t-shirt with written words is standing behind the first girl, holding bottles in her hand while a girl wearing a purple sweatshirt is sitting and eating peanut butter on the spoon."], "durations": null, "exact_frames_per_prompt": [31], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/1knVO7tRcYc.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 35, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2251_1_0"}, {"texts": ["The girl is putting a bottle on the table and putting her hand on her chin."], "durations": null, "exact_frames_per_prompt": [29], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/1knVO7tRcYc.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 35, "npz_gt_video_start_frame": 35, "npz_gt_video_end_frame": 64, "skip_frames_after_generation": 35, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2251_1_1"}, {"texts": ["The girl is sitting on the right side with her one hand in the front and opening her mouth while a girl wearing a purple sweatshirt is standing and putting food in the mouth of a girl wearing a black shirt."], "durations": null, "exact_frames_per_prompt": [16], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/1knVO7tRcYc.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 64, "npz_gt_video_start_frame": 64, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 64, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2251_1_2"}, {"texts": ["A baby is sitting in a pink baby walker and eating something."], "durations": null, "exact_frames_per_prompt": [45], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/1viCCaNGFC0.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 49, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2253_0_0"}, {"texts": ["A woman wearing a black dress is standing near the fuel dispenser machine and inserts the nozzle in the car's fuel tank."], "durations": null, "exact_frames_per_prompt": [16], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/3hlX_Dz6BMk.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 20, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2256_1_0"}, {"texts": ["The woman presses the keys on the dispenser machine."], "durations": null, "exact_frames_per_prompt": [60], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/3hlX_Dz6BMk.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 20, "npz_gt_video_start_frame": 20, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 20, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2256_1_1"}, {"texts": ["A girl at the back is filling fuel in the black SUV."], "durations": null, "exact_frames_per_prompt": [14], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/3hlX_Dz6BMk.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 18, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2257_1_0"}, {"texts": ["The girl is pressing the yellow button on the fuel station."], "durations": null, "exact_frames_per_prompt": [25], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/3hlX_Dz6BMk.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 18, "npz_gt_video_start_frame": 18, "npz_gt_video_end_frame": 44, "skip_frames_after_generation": 18, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2257_1_1"}, {"texts": ["The girl is entering the amount in the keypad."], "durations": null, "exact_frames_per_prompt": [35], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/3hlX_Dz6BMk.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 44, "npz_gt_video_start_frame": 44, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 44, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2257_1_2"}, {"texts": ["A man wearing a blue t-shirt is standing on the pavement and tying the knot on a rope."], "durations": null, "exact_frames_per_prompt": [31], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/0886v4d06mk.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 35, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2259_0_0"}, {"texts": ["The man is stretching the knot."], "durations": null, "exact_frames_per_prompt": [18], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/0886v4d06mk.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 35, "npz_gt_video_start_frame": 35, "npz_gt_video_end_frame": 53, "skip_frames_after_generation": 35, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2259_0_1"}, {"texts": ["A golden retriever dog tied with a red leash is walking on the grey surface."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/0P35GPu_hik.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_225_0_0"}, {"texts": ["A girl in black clothing is holding a wooden basket."], "durations": null, "exact_frames_per_prompt": [24], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/0luQfCHdTUE.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 28, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2263_0_0"}, {"texts": ["The basket is taken away from her by the person."], "durations": null, "exact_frames_per_prompt": [52], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/0luQfCHdTUE.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 28, "npz_gt_video_start_frame": 28, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 28, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2263_0_1"}, {"texts": ["A person in black clothing takes away a wooden basket from the girl in black clothing while the man is trying to open the basket."], "durations": null, "exact_frames_per_prompt": [73], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/0luQfCHdTUE.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 7, "npz_gt_video_start_frame": 7, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 7, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2263_1_0"}, {"texts": ["A woman wearing a green cloth is cutting a pineapple inside a sink."], "durations": null, "exact_frames_per_prompt": [41], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/0916lKaPolM.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 45, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2265_0_0"}, {"texts": ["The woman starts smelling it."], "durations": null, "exact_frames_per_prompt": [30], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/0916lKaPolM.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 45, "npz_gt_video_start_frame": 45, "npz_gt_video_end_frame": 76, "skip_frames_after_generation": 45, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2265_0_1"}, {"texts": ["A man wearing a gray t-shirt and jeans is swinging on an ironing board which is attached with a silver chain."], "durations": null, "exact_frames_per_prompt": [39], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-2pbmcFUlqg.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 43, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2266_0_0"}, {"texts": ["A boy wearing a black t-shirt is sitting on a ride and filming the man's activity."], "durations": null, "exact_frames_per_prompt": [20], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-2pbmcFUlqg.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 23, "npz_gt_video_start_frame": 23, "npz_gt_video_end_frame": 43, "skip_frames_after_generation": 23, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2266_2_0"}, {"texts": ["A man wearing a gray shirt and black jacket is standing in the middle and talking while the woman beside him is laughing"], "durations": null, "exact_frames_per_prompt": [37], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/1Ze35l3p4rs.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 41, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2267_1_0"}, {"texts": ["A woman wearing a black jacket is standing on the right and smiling as a man wearing a black jacket is standing on the left and a group of people is moving here and there on the backside."], "durations": null, "exact_frames_per_prompt": [37], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/1Ze35l3p4rs.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 41, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2267_2_0"}, {"texts": ["A man wearing a grey shirt is speaking and standing in between a woman and the first man."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/1Ze35l3p4rs.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2268_1_0"}, {"texts": ["A brown camel is walking and ridden by a woman and a kid.\n while a woman wearing black clothes is holding the camel's leash and walking, and a group of people are standing, sitting and moving around."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/0o8cWaxV78s.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2270_0_0"}, {"texts": ["A woman wearing a black hoodie and black trousers is riding a camel.\n a woman is walking on the  surface along with the camels in a right direction."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/0o8cWaxV78s.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2270_1_0"}, {"texts": ["A woman wearing a black t-shirt and black trousers is walking with the camel, holding the leash while people are sitting on the camel and some people are walking and some are sitting in the audience sitting area."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/0o8cWaxV78s.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2270_3_0"}, {"texts": ["A girl wearing a white t-shirt is shredding a paper with the manual paper shredding machine."], "durations": null, "exact_frames_per_prompt": [23], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/5J6f8Utidg4.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 27, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2273_0_0"}, {"texts": ["A girl wearing a white t-shirt puts the shredding machine on her shoulder and starts rotating the shredding knob."], "durations": null, "exact_frames_per_prompt": [52], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/5J6f8Utidg4.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 27, "npz_gt_video_start_frame": 27, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 27, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2273_0_1"}, {"texts": ["A man wearing a red shirt is sitting on a chair while a woman is sitting next to him and watching him."], "durations": null, "exact_frames_per_prompt": [19], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-7v78zoqTrU.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 23, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2278_0_0"}, {"texts": ["The man wearing a red shirt is taking out a maroon book from a box."], "durations": null, "exact_frames_per_prompt": [44], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-7v78zoqTrU.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 23, "npz_gt_video_start_frame": 23, "npz_gt_video_end_frame": 67, "skip_frames_after_generation": 23, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2278_0_1"}, {"texts": ["The man wearing a red shirt starts removing plastic from the book."], "durations": null, "exact_frames_per_prompt": [13], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-7v78zoqTrU.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 67, "npz_gt_video_start_frame": 67, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 67, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2278_0_2"}, {"texts": ["A girl wearing a black-red printed dress is sitting on the left side while a man wearing a red shirt is sitting on the right side and taking a red object out of the box."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-7v78zoqTrU.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2278_1_0"}, {"texts": ["Vegetables are frying in the black frying pan."], "durations": null, "exact_frames_per_prompt": [67], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/51BOpaiVB-w.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 13, "npz_gt_video_start_frame": 13, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 13, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_227_0_ms_0"}, {"texts": ["A woman wearing a black hoodie is standing on the left and taking a pair of scissors from a brown table."], "durations": null, "exact_frames_per_prompt": [28], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/1B4e7GvtTzc.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 32, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2281_0_0"}, {"texts": ["The woman takes the end part of a ribbon on the gift box then starts trimming the end part of the ribbon."], "durations": null, "exact_frames_per_prompt": [48], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/1B4e7GvtTzc.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 32, "npz_gt_video_start_frame": 32, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 32, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2281_0_1"}, {"texts": ["A boy standing in the front is playing with water in an aquarium using a silver bowl.\n while a group of people and a girl are standing and moving."], "durations": null, "exact_frames_per_prompt": [53], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/0rMk6fXr7Xk.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 57, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2284_0_0"}, {"texts": ["A small girl wearing a blue dress is looking at the aquarium."], "durations": null, "exact_frames_per_prompt": [20], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/0rMk6fXr7Xk.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 24, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2284_1_0"}, {"texts": ["The small girl doing hand gestures while other group of people standing and watching."], "durations": null, "exact_frames_per_prompt": [33], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/0rMk6fXr7Xk.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 24, "npz_gt_video_start_frame": 24, "npz_gt_video_end_frame": 57, "skip_frames_after_generation": 24, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2284_1_1"}, {"texts": ["A man wearing a white vest and black shorts is standing, holding money, and throwing the money on the floor while moving his body."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-WkLm8vfvZk.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2286_0_0"}, {"texts": ["A man wearing a white tank t-shirt is standing and throwing dollar notes from his hands to a brown surface."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-WkLm8vfvZk.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2287_0_0"}, {"texts": ["A woman wearing brown clothes is sitting and checking a white-blue cloth while a man wearing a brown t-shirt is sitting on the right side and folding a red piece of paper."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-kfK-xHGs2A.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2289_0_0"}, {"texts": ["A man wearing brown clothes is sitting and folding a red plastic while a woman in a brown sweater is sitting and folding clothes then catches something and again starts folding clothes."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-kfK-xHGs2A.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2289_1_0"}, {"texts": ["A person whose hands are visible is holding a pen in his right hand and writing on the paper."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/07pCpaFU-mU.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2291_0_0"}, {"texts": ["A person whose hand is visible, petting the rabbits."], "durations": null, "exact_frames_per_prompt": [21], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/1mQwb1lL9Nk.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 37, "npz_gt_video_start_frame": 37, "npz_gt_video_end_frame": 58, "skip_frames_after_generation": 37, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2294_0_0"}, {"texts": ["A gray rabbit is sitting along with white rabbit."], "durations": null, "exact_frames_per_prompt": [36], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/1mQwb1lL9Nk.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 40, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2294_1_0"}, {"texts": ["The grey rabbit is being petted by a person along with white rabbit."], "durations": null, "exact_frames_per_prompt": [18], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/1mQwb1lL9Nk.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 40, "npz_gt_video_start_frame": 40, "npz_gt_video_end_frame": 58, "skip_frames_after_generation": 40, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2294_1_1"}, {"texts": ["A white black rabbit is sitting with grey rabbit on his left"], "durations": null, "exact_frames_per_prompt": [35], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/1mQwb1lL9Nk.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 39, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2294_2_0"}, {"texts": ["The rabbit is being petted by a person including the grey one"], "durations": null, "exact_frames_per_prompt": [19], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/1mQwb1lL9Nk.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 39, "npz_gt_video_start_frame": 39, "npz_gt_video_end_frame": 58, "skip_frames_after_generation": 39, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2294_2_1"}, {"texts": ["A white rabbit is sitting on the left side on the brown cloth.\n with a grey rabbit which is sitting on the right side."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/1mQwb1lL9Nk.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2295_0_0"}, {"texts": ["A gray rabbit is sitting on the right side of the brown cloth while another man touches the  rabbit body."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/1mQwb1lL9Nk.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2295_1_0"}, {"texts": ["A white dog is walking towards the boy."], "durations": null, "exact_frames_per_prompt": [28], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/2FTRR7WD9ls.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 32, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2299_0_0"}, {"texts": ["The dog sits after the boy instructions."], "durations": null, "exact_frames_per_prompt": [48], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/2FTRR7WD9ls.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 32, "npz_gt_video_start_frame": 32, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 32, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2299_0_1"}, {"texts": ["A boy is sitting on the floor and instructing the dog and the dog follows his instructions"], "durations": null, "exact_frames_per_prompt": [65], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/2FTRR7WD9ls.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 69, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2299_1_0"}, {"texts": ["The boy is touching the dog while the dog crawls towards the boy"], "durations": null, "exact_frames_per_prompt": [11], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/2FTRR7WD9ls.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 69, "npz_gt_video_start_frame": 69, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 69, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2299_1_1"}, {"texts": ["A brown-black goat is standing on the brown surface and getting milked by the second man while a person in a blue t-shirt is holding the goat and another man in a black t-shirt is watching."], "durations": null, "exact_frames_per_prompt": [36], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/0XyEeNJp5_g.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 44, "npz_gt_video_start_frame": 44, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 44, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_229_10_0"}, {"texts": ["A woman wearing a black t-shirt is milking a cow while a man wearing a white shirt is standing on the left side and caressing the cow, a group of people are standing on the backside, and some are standing on the soil surface."], "durations": null, "exact_frames_per_prompt": [17], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/0XyEeNJp5_g.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 32, "npz_gt_video_start_frame": 32, "npz_gt_video_end_frame": 49, "skip_frames_after_generation": 32, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_229_2_0"}, {"texts": ["A man wearing a white checked shirt is standing on the brown surface and caressing a cow."], "durations": null, "exact_frames_per_prompt": [26], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/0XyEeNJp5_g.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 21, "npz_gt_video_start_frame": 21, "npz_gt_video_end_frame": 47, "skip_frames_after_generation": 21, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_229_3_ms_0"}, {"texts": ["A man wearing an orange t-shirt is milking a goat while another person wearing a blue t-shirt is holding the goat and a third person wearing a black outfit is standing."], "durations": null, "exact_frames_per_prompt": [35], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/0XyEeNJp5_g.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 43, "npz_gt_video_start_frame": 43, "npz_gt_video_end_frame": 78, "skip_frames_after_generation": 43, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_229_4_0"}, {"texts": ["A group of people are sitting in the audience sitting area."], "durations": null, "exact_frames_per_prompt": [14], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/0XyEeNJp5_g.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 7, "npz_gt_video_start_frame": 7, "npz_gt_video_end_frame": 21, "skip_frames_after_generation": 7, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_229_7_ms_0"}, {"texts": ["A girl wearing a blue t-shirt is holding an umbrella and standing on the grassy surface and is at first watching the dog."], "durations": null, "exact_frames_per_prompt": [14], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-pUiDc8X-To.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 18, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_22_0_0"}, {"texts": ["The girl is walking towards the water and the dog is looking at the girl."], "durations": null, "exact_frames_per_prompt": [10], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-pUiDc8X-To.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 18, "npz_gt_video_start_frame": 18, "npz_gt_video_end_frame": 28, "skip_frames_after_generation": 18, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_22_0_1"}, {"texts": ["The girl starts moving her umbrella in the water while the dog is jumping and then walking to the left in the water."], "durations": null, "exact_frames_per_prompt": [52], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-pUiDc8X-To.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 28, "npz_gt_video_start_frame": 28, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 28, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_22_0_2"}, {"texts": ["A beige-colored dog is at first standing on the water and drinking it while a kid wearing blue top is holding an umbrella and standing then moves forward."], "durations": null, "exact_frames_per_prompt": [24], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-pUiDc8X-To.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 28, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_22_1_0"}, {"texts": ["The dog is moving here and there while the girl puts the umbrella in the water starts moving it."], "durations": null, "exact_frames_per_prompt": [52], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-pUiDc8X-To.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 28, "npz_gt_video_start_frame": 28, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 28, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_22_1_1"}, {"texts": ["A kid wearing a blue hoodie is sitting on the silver chair and is eating a red carrot."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/3ELUJhfK1ZE.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2302_0_0"}, {"texts": ["A man wearing a white shirt is standing near the wall and speaking."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/5LB1i6ybekY.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2306_0_0"}, {"texts": ["A man wearing a white t-shirt and gas mask is sitting and drinking beer from the glass with a black pipe attached with a mask while a person wearing a black t-shirt is holding the straw and a group of people are standing and sitting at the back."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/5eVRpnHYpnI.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2307_0_0"}, {"texts": ["Another man whose hands are visible is holding the black pipe while a man wearing a gas mask is drinking from the straw and a man wearing a cap is sitting on the backside and laughing."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/5eVRpnHYpnI.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2307_1_0"}, {"texts": ["A man wearing a black t-shirt is sitting on a bench and watching the first while a man in a gas mask is sitting and drinking a brown liquid through a pipe and a person is holding that pipe."], "durations": null, "exact_frames_per_prompt": [54], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/5eVRpnHYpnI.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 58, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2307_2_0"}, {"texts": ["The man starts smoking."], "durations": null, "exact_frames_per_prompt": [22], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/5eVRpnHYpnI.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 58, "npz_gt_video_start_frame": 58, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 58, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2307_2_1"}, {"texts": ["A woman wearing a red t-shirt is moving behind the window while a person wearing a white t-shirt is sitting, wearing a mask and drinking beer from the glass through a straw, another man wearing a black t-shirt is sitting on the right and holding the straw, and another man wearing a black cap is sitting at the back on a bench, in front of the person who is wearing a white shirt."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/5eVRpnHYpnI.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2307_4_0"}, {"texts": ["A boy wearing a maroon hoodie is sitting on a chair while the other boy is sitting on the left, holding a bottle and starts tapping the orange slime on the table."], "durations": null, "exact_frames_per_prompt": [38], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/3aio4WRmjaM.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 42, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2309_1_0"}, {"texts": ["The boy takes a water bottle from the table, drinks water."], "durations": null, "exact_frames_per_prompt": [17], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/3aio4WRmjaM.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 42, "npz_gt_video_start_frame": 42, "npz_gt_video_end_frame": 59, "skip_frames_after_generation": 42, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2309_1_1"}, {"texts": ["The boy puts the bottle on the table while the other boy keeps the bottle in his mouth."], "durations": null, "exact_frames_per_prompt": [21], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/3aio4WRmjaM.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 59, "npz_gt_video_start_frame": 59, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 59, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2309_1_2"}, {"texts": ["A boy wearing a red jacket is sitting on the chair.  while another boy on the left side wearing a red t-shirt is sitting while holding a bottle and playing with a bouncy thing."], "durations": null, "exact_frames_per_prompt": [40], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/3aio4WRmjaM.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 44, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2310_0_0"}, {"texts": ["The boy is holding a plastic bottle."], "durations": null, "exact_frames_per_prompt": [4], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/3aio4WRmjaM.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 44, "npz_gt_video_start_frame": 44, "npz_gt_video_end_frame": 48, "skip_frames_after_generation": 44, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2310_0_1"}, {"texts": ["The boy is moving a sky-blue object, and starts drinking."], "durations": null, "exact_frames_per_prompt": [15], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/3aio4WRmjaM.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 48, "npz_gt_video_start_frame": 48, "npz_gt_video_end_frame": 63, "skip_frames_after_generation": 48, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2310_0_2"}, {"texts": ["A boy wearing a red t-shirt is sitting on the chair, holding a plastic bottle."], "durations": null, "exact_frames_per_prompt": [10], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/3aio4WRmjaM.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 14, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2310_1_0"}, {"texts": ["The boy is playing with a toy while the boy in the red hoodie holds the plastic and drinks it."], "durations": null, "exact_frames_per_prompt": [50], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/3aio4WRmjaM.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 14, "npz_gt_video_start_frame": 14, "npz_gt_video_end_frame": 64, "skip_frames_after_generation": 14, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2310_1_1"}, {"texts": ["The boy starts drinking."], "durations": null, "exact_frames_per_prompt": [16], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/3aio4WRmjaM.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 64, "npz_gt_video_start_frame": 64, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 64, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2310_1_2"}, {"texts": ["A boy wearing a maroon t-shirt with written words is sitting on a chair and holding a water bottle while a boy wearing a red hoodie is sitting on the right is holding a blue object in his hand and speaking."], "durations": null, "exact_frames_per_prompt": [9], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/3aio4WRmjaM.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 13, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2311_0_0"}, {"texts": ["The boy is playing with a gel exercise ball while a boy is drinking water from the bottle."], "durations": null, "exact_frames_per_prompt": [47], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/3aio4WRmjaM.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 13, "npz_gt_video_start_frame": 13, "npz_gt_video_end_frame": 60, "skip_frames_after_generation": 13, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2311_0_1"}, {"texts": ["The boy is putting the water bottle in his mouth while a boy putting a bottle on the table and starts speaking."], "durations": null, "exact_frames_per_prompt": [20], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/3aio4WRmjaM.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 60, "npz_gt_video_start_frame": 60, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 60, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2311_0_2"}, {"texts": ["A boy wearing a maroon hoodie is sitting on a chair while another boy wearing a red t-shirt is sitting on the left side and tapping a yellow balloon on the table."], "durations": null, "exact_frames_per_prompt": [41], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/3aio4WRmjaM.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 45, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2311_1_0"}, {"texts": ["The boy wearing a maroon hoodie takes a water bottle from the table, starts drinking water"], "durations": null, "exact_frames_per_prompt": [18], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/3aio4WRmjaM.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 45, "npz_gt_video_start_frame": 45, "npz_gt_video_end_frame": 64, "skip_frames_after_generation": 45, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2311_1_1"}, {"texts": ["The boy wearing a maroon hoodie puts the water bottle on the table."], "durations": null, "exact_frames_per_prompt": [15], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/3aio4WRmjaM.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 64, "npz_gt_video_start_frame": 64, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 64, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2311_1_2"}, {"texts": ["A baby wearing sky blue clothes is sitting and eating and laughing."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/2_jLu17BSoE.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2314_0_0"}, {"texts": ["A baby wearing a blue dress is sitting and eating an orange coloured food."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/2_jLu17BSoE.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2315_0_0"}, {"texts": ["A girl wearing a white designer t-shirt is standing with a snake in her hand and watching the snake, then touching the snake neck."], "durations": null, "exact_frames_per_prompt": [40], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/5VZkiNzditU.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 44, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2316_0_0"}, {"texts": ["A green-white python snake is held by a girl in her hand."], "durations": null, "exact_frames_per_prompt": [40], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/5VZkiNzditU.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 44, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2316_1_0"}, {"texts": ["A person whose hands are visible is folding a white paper on a blue printed chopping board."], "durations": null, "exact_frames_per_prompt": [60], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-S7fL3iwekg.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 20, "npz_gt_video_start_frame": 20, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 20, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2317_0_0"}, {"texts": ["A person wearing a black dress is standing and stretching his hand forward, which is being held by a man, getting a gauze roll on his hand and being pressed by the man."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/1vnjCOq4rGQ.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2319_0_0"}, {"texts": ["A man wearing a dark cream shirt and black pants is standing and holding the wrapped hand with a gauze roll of a person and pressing it."], "durations": null, "exact_frames_per_prompt": [20], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/1vnjCOq4rGQ.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 24, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2319_1_0"}, {"texts": ["The man starts moving his hand."], "durations": null, "exact_frames_per_prompt": [56], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/1vnjCOq4rGQ.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 24, "npz_gt_video_start_frame": 24, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 24, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2319_1_1"}, {"texts": ["A girl whose hands are visible is holding a packet of cheese in her hand and taking cheese from the packet then putting the cheese on a pizza dough."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/17i4zRLoHqA.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_231_0_0"}, {"texts": ["A woman wearing a purple top is standing on the right side and looking at the white-red printed gift wrapped box while a man wearing a black jacket is standing on the left side and wrapping the box."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/1-587fJFDcE.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2321_0_0"}, {"texts": ["A man wearing a black jacket is standing on the left side and sticking the white-red printed gift wrap on the box and the woman wearing purple top is watching him and speaking"], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/1-587fJFDcE.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2321_1_0"}, {"texts": ["A person in black clothing is at first holding the pieces of an apple."], "durations": null, "exact_frames_per_prompt": [40], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/1Rhy8gl_l4M.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 44, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2322_0_0"}, {"texts": ["The person in black clothing is inserting an apple in a drilling machine."], "durations": null, "exact_frames_per_prompt": [36], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/1Rhy8gl_l4M.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 44, "npz_gt_video_start_frame": 44, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 44, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2322_0_1"}, {"texts": ["A man whose hands are visible is holding apple slices in his hands."], "durations": null, "exact_frames_per_prompt": [41], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/1Rhy8gl_l4M.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 45, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2323_0_0"}, {"texts": ["The man is attaching an apple to a red and black drilling machine."], "durations": null, "exact_frames_per_prompt": [35], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/1Rhy8gl_l4M.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 45, "npz_gt_video_start_frame": 45, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 45, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2323_0_1"}, {"texts": ["A man wearing a gray shirt is sitting and caressing a white black rabbit while two black rabbits are sitting in the container, a white and brown rabbit is being caressed by the woman sitting on the right and a kid wearing a green t-shirt, a woman wearing a shirt is sitting in the middle and a person is standing at the back."], "durations": null, "exact_frames_per_prompt": [18], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-As6MdOsTns.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 22, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2325_0_0"}, {"texts": ["A woman wearing a black top is sitting holding a white rabbit while three persons besides her doing their activities"], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-As6MdOsTns.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2325_2_0"}, {"texts": ["A kid wearing a green t-shirt is standing and kissing a white rabbit."], "durations": null, "exact_frames_per_prompt": [30], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-As6MdOsTns.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 18, "npz_gt_video_start_frame": 18, "npz_gt_video_end_frame": 48, "skip_frames_after_generation": 18, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2325_3_0"}, {"texts": ["A white black rabbit is sitting on a brown surface while some people and a boy are sitting on chairs and another rabbit is also sitting on the brown surface."], "durations": null, "exact_frames_per_prompt": [67], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-As6MdOsTns.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 13, "npz_gt_video_start_frame": 13, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 13, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2325_5_0"}, {"texts": ["A white rabbit is sitting on a brown surface a boy wearing a green t-shirt is standing on the right side of the table and caressing the rabbit; a woman wearing a black shirt is sitting on the left side of the table and holding a rabbit; and a woman wearing a grey shirt is sitting and watching the boy, a person wearing greenish t-shirt also sitting in the left side."], "durations": null, "exact_frames_per_prompt": [68], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-As6MdOsTns.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 12, "npz_gt_video_start_frame": 12, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 12, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2325_6_0"}, {"texts": ["A black rabbit is sitting in a plastic container on the right side while a man in a gray shirt is sitting and petting the black white rabbit, a woman in black top is sitting in the middle, another woman also wearing a black top is sitting holding a white rabbit and that rabbit is being touched by the kid wearing a green t-shirt, and a girl wearing a green t-shirt is standing at the back."], "durations": null, "exact_frames_per_prompt": [75], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-As6MdOsTns.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 5, "npz_gt_video_start_frame": 5, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 5, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2325_7_0"}, {"texts": ["A boy wearing a shirt is sitting on the chair and watching the first dog while a woman is standing near a boy and using a spatula, she shapes the dough with a dough cutter."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/09yxWl2Ek1M.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2326_0_0"}, {"texts": ["A woman wearing a black t-shirt is standing while a boy wearing a checked shirt is sitting on the chair and watching towards the dog."], "durations": null, "exact_frames_per_prompt": [35], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/09yxWl2Ek1M.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 39, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2326_1_0"}, {"texts": ["The woman lifts the cookie cutter while a boy is trying to hold the cookie cutter and a dog is standing on the brown surface."], "durations": null, "exact_frames_per_prompt": [31], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/09yxWl2Ek1M.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 39, "npz_gt_video_start_frame": 39, "npz_gt_video_end_frame": 70, "skip_frames_after_generation": 39, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2326_1_1"}, {"texts": ["The woman puts the cutter on the food."], "durations": null, "exact_frames_per_prompt": [10], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/09yxWl2Ek1M.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 70, "npz_gt_video_start_frame": 70, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 70, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2326_1_2"}, {"texts": ["A person whose hands are visible is opening a box and taking out a black object."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-nQODSSv-Ss.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2327_0_0"}, {"texts": ["A man wearing specs is pouring beer into a glass from a beer bottle."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/3gZnc9jlUws.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2329_0_0"}, {"texts": ["A man wearing a graphic orange shirt and spectacles is sitting and eating chips while looking in front."], "durations": null, "exact_frames_per_prompt": [57], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/2D9g6QcSnKc.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 61, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_232_0_0"}, {"texts": ["A man wearing a teal green t-shirt is sitting and taking beer out from the bottle to a glass."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/3gZnc9jlUws.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2330_0_0"}, {"texts": ["A man wearing a white-purple-blue hoodie is standing on a white boat, holding a fishing landing net and a person wearing a camouflage jacket holding a fishing rod and rotating reel handles."], "durations": null, "exact_frames_per_prompt": [51], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-bzdUfeyq_0.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 55, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2333_0_0"}, {"texts": ["The man catches a grey fish in the fishing landing net."], "durations": null, "exact_frames_per_prompt": [12], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-bzdUfeyq_0.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 55, "npz_gt_video_start_frame": 55, "npz_gt_video_end_frame": 67, "skip_frames_after_generation": 55, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2333_0_1"}, {"texts": ["A man wearing a camouflage hoodie is standing on the white boat and fishing with a fishing rod while the another man is holding a fishing net."], "durations": null, "exact_frames_per_prompt": [9], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-bzdUfeyq_0.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 13, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2333_1_0"}, {"texts": ["The man catches a grey fish with the fishing rod and another man holds that fish with the fishing net."], "durations": null, "exact_frames_per_prompt": [67], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-bzdUfeyq_0.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 13, "npz_gt_video_start_frame": 13, "npz_gt_video_end_frame": 81, "skip_frames_after_generation": 13, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2333_1_1"}, {"texts": ["A man wearing a black-blue hoodie is fishing with a fishing rod while another person wearing a green camouflage dress, holding a fishing rod in his hand is standing on the right side of the man and a fish gets caught in the net."], "durations": null, "exact_frames_per_prompt": [41], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-bzdUfeyq_0.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 23, "npz_gt_video_start_frame": 23, "npz_gt_video_end_frame": 64, "skip_frames_after_generation": 23, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2333_2_0"}, {"texts": ["A man wearing a hoodie is sitting, holding and touching a box with green wrapping paper."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/06t5SARFE2I.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2334_0_0"}, {"texts": ["A man wearing a black t-shirt is standing holding a mic and speaking while other man wearing man wearing gray checked shirt is sitting holding a mic and giving expressions."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/2Wkf7GiT37c.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2336_0_0"}, {"texts": ["A man wearing a checkered shirt is sitting on a chair holding a mic and speaking while a man in black T-shirt speaks in front of the people and they start shouting and clapping."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/2Wkf7GiT37c.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2336_1_0"}, {"texts": ["A man wearing a yellow t-shirt is sitting and milking the cow's udder."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/0k45Xa4OrFc.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2337_0_0"}, {"texts": ["A cow is standing and getting milked by the man."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/0k45Xa4OrFc.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2337_1_0"}, {"texts": ["A man on the right side wearing an orange tank t-shirt is shearing a sheep beside a man wearing gray sando and shearing a sheep while a group of people is standing and watching them."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/0f3AvGrqhCY.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2338_1_0"}, {"texts": ["The man wearing a green tank t-shirt is shearing another sheep while some other people are also shearing the sheep, and some people are standing and looking at them."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/0f3AvGrqhCY.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2338_2_0"}, {"texts": ["A gray bird is eating food from the hand feeding syringe."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/0Th7JdzVdSo.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2339_0_0"}, {"texts": ["A gray baby bird is eating food from a hand-feeding syringe."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/0Th7JdzVdSo.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2340_0_0"}, {"texts": ["A person on the left whose lower body is visible, takes the first bun and keeps both the parts separately."], "durations": null, "exact_frames_per_prompt": [37], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/4wB4dYmZE-w.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 43, "npz_gt_video_start_frame": 43, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 43, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2341_1_0"}, {"texts": ["A woman is standing under the trees while a girl wearing a pink top is standing on the right side and looking at the woman, another girl wearing a printed frock is giving an apple to the woman."], "durations": null, "exact_frames_per_prompt": [17], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/0hJeAZrxfKA.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 21, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2345_0_0"}, {"texts": ["The woman is taking a photo of a fruit with her smartphone while both girls walk towards the left on the grass surface."], "durations": null, "exact_frames_per_prompt": [13], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/0hJeAZrxfKA.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 21, "npz_gt_video_start_frame": 21, "npz_gt_video_end_frame": 34, "skip_frames_after_generation": 21, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2345_0_1"}, {"texts": ["A child wearing a pink top and sea green pants is walking on the right side of the other child while holding a plastic bag while a woman wearing blue shorts is standing on the left, holding an apple, and clicking pictures of it with a mobile phone."], "durations": null, "exact_frames_per_prompt": [34], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/0hJeAZrxfKA.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 38, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2345_1_0"}, {"texts": ["A child wearing a white-blue frock is standing on the right side of the woman and holding some fruits, then showing a fruit to the woman while the girl with pink top walking front and back"], "durations": null, "exact_frames_per_prompt": [23], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/0hJeAZrxfKA.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 27, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2345_2_0"}, {"texts": ["The child walks to the right side and shows it in the right while the girl with pink top walk towards the left"], "durations": null, "exact_frames_per_prompt": [11], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/0hJeAZrxfKA.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 27, "npz_gt_video_start_frame": 27, "npz_gt_video_end_frame": 38, "skip_frames_after_generation": 27, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2345_2_1"}, {"texts": ["A person whose hands are visible is holding a piece of paper, putting it on the table and then starting to fold it."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/14_WPQE88rM.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2348_0_0"}, {"texts": ["A person whose hand are visible, is holding a paper and starts rolling it.\n"], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/14_WPQE88rM.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2349_0_0"}, {"texts": ["A man wearing a black t-shirt and black gloves is sitting on the right side and wiping the leg of the other man with a white napkin."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/0pYKoluVWLE.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2351_0_0"}, {"texts": ["A person wearing black clothes is rubbing the leg of a person one."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/0pYKoluVWLE.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2352_1_0"}, {"texts": ["A woman wearing a grey dress is holding the dog back while a woman wearing black pants is spraying the dog and and move beside the man that kneeling on the floor."], "durations": null, "exact_frames_per_prompt": [38], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/2PC9ZOWXklI.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 42, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2355_0_0"}, {"texts": ["The woman is walking."], "durations": null, "exact_frames_per_prompt": [38], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/2PC9ZOWXklI.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 42, "npz_gt_video_start_frame": 42, "npz_gt_video_end_frame": 81, "skip_frames_after_generation": 42, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2355_0_1"}, {"texts": ["A woman wearing a pink top is standing and spraying a spray on the dog while the woman in skirt touches the dog tail and walks towards table and the other people are sitting on their knees to trims the dog hair."], "durations": null, "exact_frames_per_prompt": [77], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/2PC9ZOWXklI.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 81, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2355_1_0"}, {"texts": ["A man wearing a grey hoodie is sitting on his knees on the grey surface and touches the dog's neck a woman wearing a pink top is standing and spraying a spray on the dog."], "durations": null, "exact_frames_per_prompt": [77], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/2PC9ZOWXklI.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 81, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2355_2_0"}, {"texts": ["A man wearing a black hoodie is sitting on his knees and combing the dog leg hair while the other people are doing some kind of activity."], "durations": null, "exact_frames_per_prompt": [77], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/2PC9ZOWXklI.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 81, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2355_3_0"}, {"texts": ["A black dog is standing and groomed by a group of people and another group of people are standing and walking in different directions."], "durations": null, "exact_frames_per_prompt": [77], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/2PC9ZOWXklI.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 81, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2355_4_0"}, {"texts": ["A boy wearing a red printed t-shirt holding a white paper first bends down and then gets up."], "durations": null, "exact_frames_per_prompt": [25], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/0DMS72_ngM0.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 29, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2357_0_0"}, {"texts": ["The boy starts tearing the white paper."], "durations": null, "exact_frames_per_prompt": [51], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/0DMS72_ngM0.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 29, "npz_gt_video_start_frame": 29, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 29, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2357_0_1"}, {"texts": ["A man whose upper half-body is visible wearing a black t-shirt is speaking while putting his hand on the rock slab."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/4HWg_GDHDME.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_235_0_0"}, {"texts": ["A woman wearing black clothes is lying on the black surface and eating the donut."], "durations": null, "exact_frames_per_prompt": [66], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/3TpxYOKV1Qc.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 70, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2360_1_0"}, {"texts": ["A man wearing a white shirt is holding a printed beige coloured bow tie in his hands."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/a1L-XAcoOrU.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2361_0_0"}, {"texts": ["A person whose hand is visible is holding a pen and writing."], "durations": null, "exact_frames_per_prompt": [68], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/0EKyR5nelok.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 72, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2364_0_0"}, {"texts": ["A person whose hand is visible is holding a silver rod."], "durations": null, "exact_frames_per_prompt": [10], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/4AyLLgC24mY.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 14, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2367_0_0"}, {"texts": ["The person is baking pizza while a man wearing a black t-shirt is walking in front and then sitting on a rock."], "durations": null, "exact_frames_per_prompt": [66], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/4AyLLgC24mY.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 14, "npz_gt_video_start_frame": 14, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 14, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2367_0_1"}, {"texts": ["A brown cow is standing in the shed and giving milk while a man wearing purple and black clothes is sitting on his knee and milking a cow, the man turns around in the front and speaks, and spills the cow's milk."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/0wQbexQlq4M.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2368_0_0"}, {"texts": ["A man wearing a blue shirt and black pants is milking a cow in the silver bucket."], "durations": null, "exact_frames_per_prompt": [39], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/0wQbexQlq4M.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 43, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2368_1_0"}, {"texts": ["The man is spilling the milk from the cow toward the left."], "durations": null, "exact_frames_per_prompt": [37], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/0wQbexQlq4M.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 43, "npz_gt_video_start_frame": 43, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 43, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2368_1_1"}, {"texts": ["A man wearing white pants is standing on the left side taking a yellow object from his pocket while a girl in blue jeans putting something down."], "durations": null, "exact_frames_per_prompt": [29], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-W9Iu49chDY.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 33, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2369_0_0"}, {"texts": ["The man wearing white pants is then opening a beehive box with it and the girl in blue jeans stands up."], "durations": null, "exact_frames_per_prompt": [47], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-W9Iu49chDY.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 33, "npz_gt_video_start_frame": 33, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 33, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2369_0_1"}, {"texts": ["A woman wearing blue jeans is standing on the right side, holding an object while a person wearing a beekeeping suit is standing beside her."], "durations": null, "exact_frames_per_prompt": [15], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-W9Iu49chDY.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 19, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2369_1_0"}, {"texts": ["The woman sits and puts it down."], "durations": null, "exact_frames_per_prompt": [31], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-W9Iu49chDY.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 19, "npz_gt_video_start_frame": 19, "npz_gt_video_end_frame": 50, "skip_frames_after_generation": 19, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2369_1_1"}, {"texts": ["A woman wearing a black-and-white striped top is sitting on a chair and speaking."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/1skLVYWLozY.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2370_0_0"}, {"texts": ["A man wearing a black blazer and white pants is standing behind the people, holding a small diary and writing on it with a pen while a man on the left side in a black t-shirt, blindfolded, is sitting, a boy in a grey t-shirt is sitting and starts drinking something from a glass, and a woman in a white blazer is sitting."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-KViqaTb4nM.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2371_0_0"}, {"texts": ["A man wearing a black t-shirt is sitting on a chair on the left side with his eyes are covered with a white cloth, and is keeping the juice glass on the table while a woman wearing a red vest and white coat is sitting on the right and a person wearing white pants is standing behind the woman."], "durations": null, "exact_frames_per_prompt": [9], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-KViqaTb4nM.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 13, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2371_1_0"}, {"texts": ["The man starts cracking his fingers while the person wearing white pants is writing something."], "durations": null, "exact_frames_per_prompt": [16], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-KViqaTb4nM.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 13, "npz_gt_video_start_frame": 13, "npz_gt_video_end_frame": 29, "skip_frames_after_generation": 13, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2371_1_1"}, {"texts": ["A man wearing a gray t-shirt is sitting on a chair on the right side with his eyes covered with a white cloth and is drinking juice from the glass while holding a glass with his right hand and holding chips in his left hand and two person are standing and sitting on the left side."], "durations": null, "exact_frames_per_prompt": [50], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-KViqaTb4nM.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 30, "npz_gt_video_start_frame": 30, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 30, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2371_2_0"}, {"texts": ["A boy wearing a white t-shirt standing on a stage is juggling bottles while a group of people are standing, looking at the boy, and cheering him."], "durations": null, "exact_frames_per_prompt": [77], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/2JMB3GW6P9g.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 81, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2373_0_0"}, {"texts": ["A boy also wearing a white t-shirt standing on the right is watching the performance while a boy in white T-shirt started magic."], "durations": null, "exact_frames_per_prompt": [17], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/2JMB3GW6P9g.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 21, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2373_1_0"}, {"texts": ["The boy starts moving his hand while others started shouting."], "durations": null, "exact_frames_per_prompt": [60], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/2JMB3GW6P9g.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 21, "npz_gt_video_start_frame": 21, "npz_gt_video_end_frame": 81, "skip_frames_after_generation": 21, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2373_1_1"}, {"texts": ["A man wearing black clothes is walking and looking outside."], "durations": null, "exact_frames_per_prompt": [21], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/2EGEUdrgnzw.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 25, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2374_1_0"}, {"texts": ["The man is standing."], "durations": null, "exact_frames_per_prompt": [31], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/2EGEUdrgnzw.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 25, "npz_gt_video_start_frame": 25, "npz_gt_video_end_frame": 56, "skip_frames_after_generation": 25, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2374_1_1"}, {"texts": ["A man wearing a white shirt is standing and opening a bottle with a spatula."], "durations": null, "exact_frames_per_prompt": [58], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/RusQhRqbMSU.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 62, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2377_0_0"}, {"texts": ["The man is drinking from it."], "durations": null, "exact_frames_per_prompt": [18], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/RusQhRqbMSU.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 62, "npz_gt_video_start_frame": 62, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 62, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2377_0_1"}, {"texts": ["A baby with long hair is writing with a pencil on a white paper."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/2a868r9VXt4.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2380_0_0"}, {"texts": ["A kid wearing a black graphic t-shirt and black trousers is standing and flipping something in the pan while a woman wearing a grey t-shirt is holding the pan and helping the kid flip the omelette."], "durations": null, "exact_frames_per_prompt": [77], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-9oHBPWsXqg.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 81, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2381_0_0"}, {"texts": ["A woman wearing a black striped t-shirt and black trousers is helping the kid while the kid holding the pan and laughing"], "durations": null, "exact_frames_per_prompt": [77], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-9oHBPWsXqg.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 81, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2381_1_0"}, {"texts": ["A girl wearing a white t-shirt is standing on the left side holding the yellow cloth then starts folding the yellow cloth."], "durations": null, "exact_frames_per_prompt": [27], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/jhIuHluFWgQ.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 54, "npz_gt_video_start_frame": 54, "npz_gt_video_end_frame": 81, "skip_frames_after_generation": 54, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2383_2_0"}, {"texts": ["A woman wearing a white-black design top is sitting on a chair, lip-syncing and moving her hands in sign language.\n"], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/5J5uwiRZUSk.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2388_0_0"}, {"texts": ["A girl wearing a white and multicolor t-shirt is standing in the back while a man in white shirt started jumping and took the serving tray comes forward."], "durations": null, "exact_frames_per_prompt": [21], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-Fs-zEnMYgc.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 25, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2389_1_0"}, {"texts": ["The girl wearing a white and multicolor t-shirt is moving while a man in white shirt is showing meat in front of the camera and goes back and keeps the tray on the table then  tries to cut the meat."], "durations": null, "exact_frames_per_prompt": [55], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-Fs-zEnMYgc.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 25, "npz_gt_video_start_frame": 25, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 25, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2389_1_1"}, {"texts": ["A person wearing a purple t-shirt is adding mint leaves inside a pan and holding a wooden spatula."], "durations": null, "exact_frames_per_prompt": [70], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-07Ke73N4zI.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 10, "npz_gt_video_start_frame": 10, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 10, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2391_0_0"}, {"texts": ["A man wearing a white t-shirt is standing in the left side on the grassy surface and speaking while moving his hands while a wearing a white shirt is holding a golf stick and standing."], "durations": null, "exact_frames_per_prompt": [70], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/10R_e6SZQ8c.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 74, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2392_0_0"}, {"texts": ["A man wearing black trousers is standing on the right side and holding a golf stick and trying to hit a golf ball while another man on the left side wearing grey trousers is standing on the grass surface and speaking while holding a golf ball."], "durations": null, "exact_frames_per_prompt": [46], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/10R_e6SZQ8c.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 28, "npz_gt_video_start_frame": 28, "npz_gt_video_end_frame": 74, "skip_frames_after_generation": 28, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2392_1_0"}, {"texts": ["A man wearing a red sweatshirt is standing and holding the glass bowl in his right hand behind the counter-top."], "durations": null, "exact_frames_per_prompt": [40], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/0Gy_u05iEfQ.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 44, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2394_0_0"}, {"texts": ["The man is putting the chocolate on the cookies."], "durations": null, "exact_frames_per_prompt": [36], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/0Gy_u05iEfQ.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 44, "npz_gt_video_start_frame": 44, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 44, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2394_0_1"}, {"texts": ["A man wearing a red t-shirt is walking while holding a lease and playing with a white dog."], "durations": null, "exact_frames_per_prompt": [18], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-l4vDp6OZ30.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 22, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2395_0_ms_0"}, {"texts": ["A man wearing denim jeans is walking while holding a lease on the gray surface."], "durations": null, "exact_frames_per_prompt": [59], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-l4vDp6OZ30.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 21, "npz_gt_video_start_frame": 21, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 21, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2395_1_ms_0"}, {"texts": ["A white dog is moving around and then starts playing with a man one."], "durations": null, "exact_frames_per_prompt": [15], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-l4vDp6OZ30.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 19, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2395_2_0"}, {"texts": ["A white brown dog is walking on the gray surface while a man wearing green and blue outfit is walking along with the dog and he is holding a rope that is attached to the dog."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-l4vDp6OZ30.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2395_3_0"}, {"texts": ["A girl wearing a blue t-shirt is standing."], "durations": null, "exact_frames_per_prompt": [12], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/0aiiFHqQTLU.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 16, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2398_0_0"}, {"texts": ["The girl wearing a blue t-shirt is showing dance moves in sign language with her hands."], "durations": null, "exact_frames_per_prompt": [64], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/0aiiFHqQTLU.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 16, "npz_gt_video_start_frame": 16, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 16, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2398_0_1"}, {"texts": ["A man wearing a pink checked shirt is standing, talking, and cooking sausages."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/4HXkM2akSAw.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2399_0_0"}, {"texts": ["A person whose hands are only visible is touching some objects on a brown surface."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/1m6rxXas1ZA.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_239_0_0"}, {"texts": ["A woman wearing a black blazer is sitting on a chair while other two women is also sitting on the chair."], "durations": null, "exact_frames_per_prompt": [24], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/459CvSXVUbs.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 13, "npz_gt_video_start_frame": 13, "npz_gt_video_end_frame": 37, "skip_frames_after_generation": 13, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2401_3_ms_0"}, {"texts": ["The woman turns and starts looking at the other woman while other woman wearing red blazer also turns and looking at the another woman who is wearing pink blazer."], "durations": null, "exact_frames_per_prompt": [15], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/459CvSXVUbs.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 37, "npz_gt_video_start_frame": 37, "npz_gt_video_end_frame": 52, "skip_frames_after_generation": 37, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2401_3_ms_1"}, {"texts": ["A woman wearing a red blazer is sitting in the middle.  while a woman wearing a white blazer is looking at the woman in a red blazer and a woman wearing a black blazer is turning to the right."], "durations": null, "exact_frames_per_prompt": [26], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/459CvSXVUbs.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 14, "npz_gt_video_start_frame": 14, "npz_gt_video_end_frame": 40, "skip_frames_after_generation": 14, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2401_4_ms_0"}, {"texts": ["The woman turns towards the other woman while a woman wearing a black blazer is sitting and speaking while looking at the front."], "durations": null, "exact_frames_per_prompt": [41], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/459CvSXVUbs.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 40, "npz_gt_video_start_frame": 40, "npz_gt_video_end_frame": 81, "skip_frames_after_generation": 40, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2401_4_ms_1"}, {"texts": ["A woman wearing a light-pink blazer is sitting on the left side while others reading the news and looking towards the left side."], "durations": null, "exact_frames_per_prompt": [67], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/459CvSXVUbs.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 14, "npz_gt_video_start_frame": 14, "npz_gt_video_end_frame": 81, "skip_frames_after_generation": 14, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2401_5_ms_0"}, {"texts": ["A person with only their hands visible is folding a yellow paper and making a plane."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/0Osiwalr9-g.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2402_0_0"}, {"texts": ["A person whose hand is visible is crafting an airplane with yellow paper."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/0Osiwalr9-g.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2403_0_0"}, {"texts": ["A person wearing a grey pyjama is sitting and opening a blue box with his hands."], "durations": null, "exact_frames_per_prompt": [23], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-n5SQx38lno.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 27, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2405_0_0"}, {"texts": ["The person is using a key."], "durations": null, "exact_frames_per_prompt": [33], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-n5SQx38lno.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 27, "npz_gt_video_start_frame": 27, "npz_gt_video_end_frame": 60, "skip_frames_after_generation": 27, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2405_0_1"}, {"texts": ["A man wearing a printed blue shirt is at first opens a fish food sack."], "durations": null, "exact_frames_per_prompt": [30], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-vWHTPm5Jc0.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 34, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2406_0_0"}, {"texts": ["The man drops the food in the water."], "durations": null, "exact_frames_per_prompt": [46], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-vWHTPm5Jc0.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 34, "npz_gt_video_start_frame": 34, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 34, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2406_0_1"}, {"texts": ["A woman wearing a white top is smiling while cooking."], "durations": null, "exact_frames_per_prompt": [50], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-kVHRsmmbOE.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 15, "npz_gt_video_start_frame": 15, "npz_gt_video_end_frame": 65, "skip_frames_after_generation": 15, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2409_0_0"}, {"texts": ["The woman turns into the left direction and walking."], "durations": null, "exact_frames_per_prompt": [15], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-kVHRsmmbOE.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 65, "npz_gt_video_start_frame": 65, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 65, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2409_0_1"}, {"texts": ["A person wearing a blue shirt is standing behind the counter-top, and mixing bean sprout salad."], "durations": null, "exact_frames_per_prompt": [62], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/4MG0_uie2_M.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 66, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_240_0_0"}, {"texts": ["The person is sprinkling some white material."], "durations": null, "exact_frames_per_prompt": [14], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/4MG0_uie2_M.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 66, "npz_gt_video_start_frame": 66, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 66, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_240_0_1"}, {"texts": ["A person wearing a striped t-shirt is standing on the left side of the fish tank and dropping fish inside the fish tank."], "durations": null, "exact_frames_per_prompt": [60], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/0N8eKEPgrPk.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 64, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2411_0_0"}, {"texts": ["A man whom hands are visible is putting an orange in the squeezer."], "durations": null, "exact_frames_per_prompt": [51], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/0i_YpIINipc.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 55, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2412_0_0"}, {"texts": ["The man is pressing it, inserting the juice above the vegetables."], "durations": null, "exact_frames_per_prompt": [25], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/0i_YpIINipc.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 55, "npz_gt_video_start_frame": 55, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 55, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2412_0_1"}, {"texts": ["A baby wearing a blue designer t-shirt is sitting on a black chair and  laughing while a person wearing orange outfit, holding an ice cream and a spoon in her hand, is sitting on the right side of the baby and she is feeding her."], "durations": null, "exact_frames_per_prompt": [66], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/0HDeeGOp6E0.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 70, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2413_0_0"}, {"texts": ["The baby wearing a blue designer t-shirt is eating."], "durations": null, "exact_frames_per_prompt": [10], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/0HDeeGOp6E0.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 70, "npz_gt_video_start_frame": 70, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 70, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2413_0_1"}, {"texts": ["A person whose hand is visible, holding a bowl and putting vegetables in a fry pan.\n"], "durations": null, "exact_frames_per_prompt": [22], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/4Ft4akRN7F0.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 36, "npz_gt_video_start_frame": 36, "npz_gt_video_end_frame": 58, "skip_frames_after_generation": 36, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2415_0_0"}, {"texts": ["A baby wearing white clothes is smiling and lying on the red-sky blue surface."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/1Hb-3gipffU.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2417_0_0"}, {"texts": ["A man wearing a black t-shirt is standing in front of a lectern and speaking into a microphone while others are clapping for him"], "durations": null, "exact_frames_per_prompt": [20], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/5FlI-v5349w.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 24, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2418_0_0"}, {"texts": ["The man is then drinking."], "durations": null, "exact_frames_per_prompt": [26], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/5FlI-v5349w.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 24, "npz_gt_video_start_frame": 24, "npz_gt_video_end_frame": 50, "skip_frames_after_generation": 24, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2418_0_1"}, {"texts": ["A woman wearing a black t-shirt is standing and then starts walking.  while others are clapping."], "durations": null, "exact_frames_per_prompt": [39], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/5FlI-v5349w.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 43, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2418_4_0"}, {"texts": ["A person wearing black clothes is riding on a camel while other person wearing black clothes is walking along with him."], "durations": null, "exact_frames_per_prompt": [74], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/0L3RDU3j3h4.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 6, "npz_gt_video_start_frame": 6, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 6, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2421_0_0"}, {"texts": ["A camel on the right is also walking on the brown surface and is carrying person three while a brown camel and a man wearing a black t-shirt are walking with the camel, and a group of people are going with camels."], "durations": null, "exact_frames_per_prompt": [70], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/0L3RDU3j3h4.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 10, "npz_gt_video_start_frame": 10, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 10, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2421_5_0"}, {"texts": ["A man on the right side, wearing a black cloth is riding a camel."], "durations": null, "exact_frames_per_prompt": [55], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/0L3RDU3j3h4.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 6, "npz_gt_video_start_frame": 6, "npz_gt_video_end_frame": 61, "skip_frames_after_generation": 6, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2422_0_0"}, {"texts": ["A woman wearing a pink top is standing, picking the clothes and putting them in the washing machine."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/5osvBYChUg8.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2424_0_0"}, {"texts": ["A man wearing a blue lining shirt is sitting on the chair and speaking and another man wearing a black suit is sitting on the chair while putting his hands on the table and looking towards the left."], "durations": null, "exact_frames_per_prompt": [49], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/3w2sR7V_DII.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 53, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2426_0_0"}, {"texts": ["The man wearing a blue lining shirt is putting his hand on his cheek."], "durations": null, "exact_frames_per_prompt": [27], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/3w2sR7V_DII.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 53, "npz_gt_video_start_frame": 53, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 53, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2426_0_1"}, {"texts": ["A man wearing a black suit is sitting on the chair, keeping his hand on the table and holding a pen."], "durations": null, "exact_frames_per_prompt": [15], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/3w2sR7V_DII.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 38, "npz_gt_video_start_frame": 38, "npz_gt_video_end_frame": 53, "skip_frames_after_generation": 38, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2426_1_0"}, {"texts": ["A person wearing black cloth is standing and holding ground beef from the outlet of the meat grinder."], "durations": null, "exact_frames_per_prompt": [31], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/3gN4HyHfb-Q.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 49, "npz_gt_video_start_frame": 49, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 49, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2427_0_0"}, {"texts": ["A man wearing a dress is standing on the green surface, holding a golf stick."], "durations": null, "exact_frames_per_prompt": [16], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-i_87yGRnvM.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 20, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2428_0_0"}, {"texts": ["The man wearing a dress is hitting a golf ball, then standing steady."], "durations": null, "exact_frames_per_prompt": [59], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-i_87yGRnvM.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 20, "npz_gt_video_start_frame": 20, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 20, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2428_0_1"}, {"texts": ["A girl wearing a blue cloth is putting her right hand inside the cage while a group of goats are behind the fencing."], "durations": null, "exact_frames_per_prompt": [13], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/0StAORjZzdE.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 17, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2429_0_0"}, {"texts": ["The girl puts her left hand inside and starts feeding green food to a white-brown goat."], "durations": null, "exact_frames_per_prompt": [63], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/0StAORjZzdE.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 17, "npz_gt_video_start_frame": 17, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 17, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2429_0_1"}, {"texts": ["A white-brown goat on the right is standing towards the cage while a girl in blue dress is trying to feed the goat."], "durations": null, "exact_frames_per_prompt": [24], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/0StAORjZzdE.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 28, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2429_1_0"}, {"texts": ["The white-brown goat is eating food given by the girl."], "durations": null, "exact_frames_per_prompt": [52], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/0StAORjZzdE.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 28, "npz_gt_video_start_frame": 28, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 28, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2429_1_1"}, {"texts": ["A white-brown goat on the left is standing on the soil surface and moving its head while a person whose hand is visible is standing in the front and feeding the white goat behind the fence and a group of white goats is sitting at the back."], "durations": null, "exact_frames_per_prompt": [70], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/0StAORjZzdE.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 74, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2429_2_0"}, {"texts": ["A person wearing a white shirt is at first inserts his finger into the folded napkin."], "durations": null, "exact_frames_per_prompt": [49], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/2jr4_vtUlPs.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 53, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_242_0_0"}, {"texts": ["The person puts the knife and the fork inside the napkin"], "durations": null, "exact_frames_per_prompt": [27], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/2jr4_vtUlPs.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 53, "npz_gt_video_start_frame": 53, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 53, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_242_0_1"}, {"texts": ["A man wearing a gray shirt and black trousers is standing, talking."], "durations": null, "exact_frames_per_prompt": [25], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/5J1VA-S1vL4.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 29, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2434_0_0"}, {"texts": ["The man is walking."], "durations": null, "exact_frames_per_prompt": [51], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/5J1VA-S1vL4.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 29, "npz_gt_video_start_frame": 29, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 29, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2434_0_1"}, {"texts": ["A person wearing white clothes is sitting and repairing a car wheel."], "durations": null, "exact_frames_per_prompt": [44], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/0WvEWBsVEc4.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 48, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2438_0_0"}, {"texts": ["The person is standing."], "durations": null, "exact_frames_per_prompt": [8], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/0WvEWBsVEc4.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 48, "npz_gt_video_start_frame": 48, "npz_gt_video_end_frame": 56, "skip_frames_after_generation": 48, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2438_0_1"}, {"texts": ["The person is going on the road."], "durations": null, "exact_frames_per_prompt": [24], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/0WvEWBsVEc4.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 56, "npz_gt_video_start_frame": 56, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 56, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2438_0_2"}, {"texts": ["A person whose hands are visible wearing a purple sweater is turning the newspaper page."], "durations": null, "exact_frames_per_prompt": [49], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/10lpbBJKh0U.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 31, "npz_gt_video_start_frame": 31, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 31, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2439_0_0"}, {"texts": ["A kid wearing a blue printed t-shirt is standing and holding something edible."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/0lOQGjgwzGU.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_243_0_0"}, {"texts": ["A man wearing a gray shirt is tying the blue tie with his hands."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/8jhFbBFohkA.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2440_0_0"}, {"texts": ["A man wearing a light-blue shirt is tying a black-yellow design bow tie with his hands."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/8jhFbBFohkA.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2441_0_0"}, {"texts": ["A baby wearing blue-white clothes is sitting and eating food while a woman wearing blue top is sitting beside him."], "durations": null, "exact_frames_per_prompt": [32], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-bYMchht1KU.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 36, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2442_0_0"}, {"texts": ["A boy wearing a black t-shirt is sitting on a chair and eating noodles."], "durations": null, "exact_frames_per_prompt": [32], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-bYMchht1KU.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 36, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2443_0_0"}, {"texts": ["A baby wearing a multicolored dress is sitting on a baby chair."], "durations": null, "exact_frames_per_prompt": [34], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/0VUSj-SAcUs.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 38, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2446_0_0"}, {"texts": ["The baby wearing a multicolored dress is eating food with a spoon while a person whose hand is visible is holding the spoon."], "durations": null, "exact_frames_per_prompt": [42], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/0VUSj-SAcUs.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 38, "npz_gt_video_start_frame": 38, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 38, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2446_0_1"}, {"texts": ["A man wearing a blue shirt is tying a brown checkered tie with his hands.\n"], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/0EO13FILDb4.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2449_0_0"}, {"texts": ["A man wearing an orange t-shirt and brown trousers is performing a golf shot"], "durations": null, "exact_frames_per_prompt": [54], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-4ggQ2PfUOc.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 58, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_244_0_0"}, {"texts": ["The man stands on a green grass field."], "durations": null, "exact_frames_per_prompt": [22], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-4ggQ2PfUOc.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 58, "npz_gt_video_start_frame": 58, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 58, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_244_0_1"}, {"texts": ["A person wearing a black cloth is sitting on the left side and is playing a musical drum."], "durations": null, "exact_frames_per_prompt": [14], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/0ql2oEqTuMk.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 54, "npz_gt_video_start_frame": 54, "npz_gt_video_end_frame": 68, "skip_frames_after_generation": 54, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2450_2_0"}, {"texts": ["A girl wearing a red top is spitting water in the sink and the girl wearing pink top is standing on the right holding a glass"], "durations": null, "exact_frames_per_prompt": [19], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-QuSYXee6Pc.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 23, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2454_0_0"}, {"texts": ["The girl is wiping her mouth and the second girl tries to drink water"], "durations": null, "exact_frames_per_prompt": [32], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-QuSYXee6Pc.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 23, "npz_gt_video_start_frame": 23, "npz_gt_video_end_frame": 58, "skip_frames_after_generation": 23, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2454_0_1"}, {"texts": ["The girl is laughing and the second girl drinks the water"], "durations": null, "exact_frames_per_prompt": [22], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-QuSYXee6Pc.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 58, "npz_gt_video_start_frame": 58, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 58, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2454_0_2"}, {"texts": ["A girl wearing a pink top is speaking, laughing while a girl in a red t-shirt is rinsing off the water, she returns by placing a towel over her mouth and looking at the girl."], "durations": null, "exact_frames_per_prompt": [66], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-QuSYXee6Pc.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 70, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2454_1_0"}, {"texts": ["The girl wearing a pink top is drinking water from the pink cup while a girl wearing a red t-shirt is standing and laughing."], "durations": null, "exact_frames_per_prompt": [10], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-QuSYXee6Pc.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 70, "npz_gt_video_start_frame": 70, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 70, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2454_1_1"}, {"texts": ["A person wearing a blue shirt and dark blue jeans is riding on the back of the gray elephant while in back side the woman sits and record the video."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-50M_ypyDWw.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2456_0_0"}, {"texts": ["A woman wearing a blue t-shirt, cream pants is sitting behind the person and riding on the back of the elephant."], "durations": null, "exact_frames_per_prompt": [67], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-50M_ypyDWw.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 13, "npz_gt_video_start_frame": 13, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 13, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2456_1_0"}, {"texts": ["A grey elephant is walking and carrying both the people on his back."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-50M_ypyDWw.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2456_2_0"}, {"texts": ["A man wearing a blue shirt is riding on a gray elephant while a woman is sitting on a elephant and an elephant is standing on the left side."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-50M_ypyDWw.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2457_0_0"}, {"texts": ["A gray elephant is walking and is carrying both the people."], "durations": null, "exact_frames_per_prompt": [41], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-50M_ypyDWw.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 39, "npz_gt_video_start_frame": 39, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 39, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2457_2_0"}, {"texts": ["A baby wearing a white-pink dress is sitting and getting fed by the woman."], "durations": null, "exact_frames_per_prompt": [30], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/4H_vqh4ZjD4.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 34, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2458_0_0"}, {"texts": ["A baby wearing a white-pink dress starts making faces."], "durations": null, "exact_frames_per_prompt": [45], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/4H_vqh4ZjD4.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 34, "npz_gt_video_start_frame": 34, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 34, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2458_0_1"}, {"texts": ["A woman on the right side, wearing a blue t-shirt is sitting and feeding the baby."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/4H_vqh4ZjD4.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2458_1_0"}, {"texts": ["A girl whose upper half-body is visible, wearing a black t-shirt is sitting on the right side while holding the carrot in her left hand while other girl wearing a cap on the left is talking."], "durations": null, "exact_frames_per_prompt": [14], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-drAc7uvFBc.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 18, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2459_0_0"}, {"texts": ["The girl is eating the carrot while the other girl wearing a cap is holding the carrot and glasses in her right hand."], "durations": null, "exact_frames_per_prompt": [25], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-drAc7uvFBc.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 18, "npz_gt_video_start_frame": 18, "npz_gt_video_end_frame": 43, "skip_frames_after_generation": 18, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2459_0_1"}, {"texts": ["A girl, whose upper half-body is visible wearing a black t-shirt is sitting on the left side while another girl is sitting on the right side."], "durations": null, "exact_frames_per_prompt": [21], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-drAc7uvFBc.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 25, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2459_1_0"}, {"texts": ["The girl, whose upper half-body is visible wearing a black t-shirt is holding the carrot and specs in her right hand.  and the other girl sitting on the right side is eating a carrot."], "durations": null, "exact_frames_per_prompt": [29], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-drAc7uvFBc.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 25, "npz_gt_video_start_frame": 25, "npz_gt_video_end_frame": 54, "skip_frames_after_generation": 25, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2459_1_1"}, {"texts": ["The girl, whose upper half-body is visible wearing a black t-shirt is then wearing the specs with her hand over her own eyes while speaking."], "durations": null, "exact_frames_per_prompt": [26], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-drAc7uvFBc.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 54, "npz_gt_video_start_frame": 54, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 54, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2459_1_2"}, {"texts": ["A golden-brown colored cat is sitting on the bed and looking here and there while being touched by, the girl in a sweatshirt."], "durations": null, "exact_frames_per_prompt": [37], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/2O7CGhAvH38.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 41, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2462_1_0"}, {"texts": ["A boy wearing a white t-shirt is standing a black bucket and flushing the udder of a cow with a teat dip cup."], "durations": null, "exact_frames_per_prompt": [32], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-CbkApe0H-g.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 36, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2464_0_0"}, {"texts": ["The boy wearing a white t-shirt pushes the bucket to the back side."], "durations": null, "exact_frames_per_prompt": [22], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-CbkApe0H-g.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 36, "npz_gt_video_start_frame": 36, "npz_gt_video_end_frame": 58, "skip_frames_after_generation": 36, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2464_0_1"}, {"texts": ["The boy wearing a white t-shirt stands on the bucket and again starts flushing the udder of another cow."], "durations": null, "exact_frames_per_prompt": [22], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-CbkApe0H-g.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 58, "npz_gt_video_start_frame": 58, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 58, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2464_0_2"}, {"texts": ["A man whose hands are visible only, is holding a plane of paper, folding and unfolding it.\n"], "durations": null, "exact_frames_per_prompt": [50], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/1zDC25VKkG4.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 54, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2465_0_0"}, {"texts": ["A woman wearing a green t-shirt is standing and ironing the red cloth on the grey table."], "durations": null, "exact_frames_per_prompt": [34], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/1on-3mYBnPg.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 38, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2466_0_0"}, {"texts": ["A man wearing a white shirt is tying a white bow tie on the collar with his hands."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/592eg79lJTU.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_246_0_0"}, {"texts": ["A woman wearing a dark blue t-shirt is attaching an apple to the peeler."], "durations": null, "exact_frames_per_prompt": [32], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-XzEhqbdlA0.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 13, "npz_gt_video_start_frame": 13, "npz_gt_video_end_frame": 45, "skip_frames_after_generation": 13, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2470_1_0"}, {"texts": ["The woman is peeling the apple."], "durations": null, "exact_frames_per_prompt": [35], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-XzEhqbdlA0.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 45, "npz_gt_video_start_frame": 45, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 45, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2470_1_1"}, {"texts": ["A man wearing a gray shirt is chopping something on a chopping board with a knife as a woman in a blue t-shirt put an apple in a peeler machine and starts rotating it."], "durations": null, "exact_frames_per_prompt": [70], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-XzEhqbdlA0.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 10, "npz_gt_video_start_frame": 10, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 10, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2470_2_0"}, {"texts": ["A man beside person three is also chopping something with a knife while a woman wearing blue t-shirt is peeling an apple."], "durations": null, "exact_frames_per_prompt": [71], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-XzEhqbdlA0.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 9, "npz_gt_video_start_frame": 9, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 9, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2470_3_0"}, {"texts": ["A woman wearing a blue t-shirt is standing and attaching an apple to a peeler machine while another person wearing a grey shirt is standing on the right side."], "durations": null, "exact_frames_per_prompt": [32], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-XzEhqbdlA0.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 13, "npz_gt_video_start_frame": 13, "npz_gt_video_end_frame": 45, "skip_frames_after_generation": 13, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2471_1_0"}, {"texts": ["The woman wearing a blue t-shirt is rotating the handle of the peeler machine while the third person, wearing a grey sweatshirt, is also standing on the right side and picking up something."], "durations": null, "exact_frames_per_prompt": [35], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-XzEhqbdlA0.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 45, "npz_gt_video_start_frame": 45, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 45, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2471_1_1"}, {"texts": ["A person whose only hands are visible is holding a knife inserted in the watermelon and raising one hand from the watermelon."], "durations": null, "exact_frames_per_prompt": [43], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/9RMEkvyy9B4.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 47, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2472_0_0"}, {"texts": ["A woman wearing a blue jacket and black jeans is standing on the left side while tilting her body and holding a black stick in her hand and the horse by a rope."], "durations": null, "exact_frames_per_prompt": [26], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/0JgXOs8Ap0w.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 54, "npz_gt_video_start_frame": 54, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 54, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2473_0_ms_0"}, {"texts": ["A light brown horse is walking on the dry grass surface and is standing."], "durations": null, "exact_frames_per_prompt": [49], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/0JgXOs8Ap0w.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 53, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2473_1_0"}, {"texts": ["The woman holds him by a rope."], "durations": null, "exact_frames_per_prompt": [25], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/0JgXOs8Ap0w.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 53, "npz_gt_video_start_frame": 53, "npz_gt_video_end_frame": 78, "skip_frames_after_generation": 53, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2473_1_1"}, {"texts": ["The horse starts sitting on the soil surface."], "durations": null, "exact_frames_per_prompt": [2], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/0JgXOs8Ap0w.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 78, "npz_gt_video_start_frame": 78, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 78, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2473_1_2"}, {"texts": ["A man wearing a white t-shirt is standing and playing golf on the grass surface."], "durations": null, "exact_frames_per_prompt": [36], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-bPfpYQ1CQI.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 40, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2474_1_0"}, {"texts": ["A woman wearing a shirt is standing, drinking from the bottle, then putting the bottle near the black bottles."], "durations": null, "exact_frames_per_prompt": [28], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/7_ZwejMIFWU.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 32, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2476_0_0"}, {"texts": ["The woman at last picking up all four bottles and going back to the side."], "durations": null, "exact_frames_per_prompt": [21], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/7_ZwejMIFWU.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 32, "npz_gt_video_start_frame": 32, "npz_gt_video_end_frame": 53, "skip_frames_after_generation": 32, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2476_0_1"}, {"texts": ["A woman wearing a green t-shirt is brushing the tail of the horse with a brush while a mice walks from left to the right."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-jDvSfh74Zk.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2477_0_0"}, {"texts": ["A brown horse is standing on the light-grey surface and getting its tail brushed by the woman."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-jDvSfh74Zk.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2477_1_0"}, {"texts": ["A man wearing a violet graphic t-shirt is standing and drinking from a bottle then starts speaking while a boy wearing a blue t-shirt stands behind the man, drinking from the glass, and looks at the man."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-zbHop1XLyM.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2478_0_0"}, {"texts": ["A boy wearing a blue t-shirt is sitting and drinking."], "durations": null, "exact_frames_per_prompt": [20], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-zbHop1XLyM.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 24, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2479_1_0"}, {"texts": ["The boy is watching the man."], "durations": null, "exact_frames_per_prompt": [32], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-zbHop1XLyM.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 24, "npz_gt_video_start_frame": 24, "npz_gt_video_end_frame": 56, "skip_frames_after_generation": 24, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2479_1_1"}, {"texts": ["A woman wearing a green shirt is walking ahead while holding a cow leash while other woman wearing a black top is also walking ahead while holding a cow leash and another woman wearing pink top and black pant is sitting on a chair."], "durations": null, "exact_frames_per_prompt": [47], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-6u7lnRGS58.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 33, "npz_gt_video_start_frame": 33, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 33, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_247_1_0"}, {"texts": ["A brown cow with white spot is walking along with the first woman on the red carpet surface while a brown cow is walking on a red carpet surface with the woman wearing white pants, a person wearing black pants is sitting on a chair, and a man wearing a blue outfit is standing at the back."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-6u7lnRGS58.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_247_4_0"}, {"texts": ["A brown cow is walking along with second woman on the red carpet surface while first woman wearing black and white outfit is walking along with another brown cow and another brown cow is moving towards right side and a person wearing blue and black suit is standing on the backside of first woman and third woman wearing pink and black suit, holding a book in her hand is sitting on a chair on the backside."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-6u7lnRGS58.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_247_5_0"}, {"texts": ["A person whose only hand is visible is picking meat pieces with a tong and putting them into a meat grinder."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/5uM5Y4nxQCM.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2482_0_0"}, {"texts": ["A man wearing a purple hoodie is standing."], "durations": null, "exact_frames_per_prompt": [12], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/0iNc1DSb2Lc.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 16, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2483_0_0"}, {"texts": ["The man wearing a purple hoodie is eating rolled food and speaking."], "durations": null, "exact_frames_per_prompt": [18], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/0iNc1DSb2Lc.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 16, "npz_gt_video_start_frame": 16, "npz_gt_video_end_frame": 34, "skip_frames_after_generation": 16, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2483_0_1"}, {"texts": ["The man wearing a purple hoodie is speaking."], "durations": null, "exact_frames_per_prompt": [46], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/0iNc1DSb2Lc.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 34, "npz_gt_video_start_frame": 34, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 34, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2483_0_2"}, {"texts": ["A person wearing a purple hoodie is standing holding food."], "durations": null, "exact_frames_per_prompt": [12], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/0iNc1DSb2Lc.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 16, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2484_0_0"}, {"texts": ["The person starts eating the food."], "durations": null, "exact_frames_per_prompt": [59], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/0iNc1DSb2Lc.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 16, "npz_gt_video_start_frame": 16, "npz_gt_video_end_frame": 75, "skip_frames_after_generation": 16, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2484_0_1"}, {"texts": ["A person wearing red clothes is putting the sushi roll pieces in the foil box."], "durations": null, "exact_frames_per_prompt": [33], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/34q7p7UrpWA.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 37, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2486_0_0"}, {"texts": ["A man wearing a black-white striped t-shirt is practising golf on the grey surface."], "durations": null, "exact_frames_per_prompt": [51], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/1cRLb1VsO-g.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 55, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2490_0_0"}, {"texts": ["A man wearing a white shirt is practicing golf on the grey surface."], "durations": null, "exact_frames_per_prompt": [25], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/1cRLb1VsO-g.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 55, "npz_gt_video_start_frame": 55, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 55, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2490_1_0"}, {"texts": ["A person wearing white pants is standing on the golf course and playing golf."], "durations": null, "exact_frames_per_prompt": [75], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/0Rhv50mhWhU.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 79, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2491_0_0"}, {"texts": ["A kid wearing a yellow printed t-shirt is sitting on the chair, eating a carrot and spitting on the brown table."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/4O5A-S6R0b4.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2492_0_0"}, {"texts": ["A woman wearing a black jacket is sitting in the middle and drinking beer while other two person is sitting with her."], "durations": null, "exact_frames_per_prompt": [45], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/1R1wxagNCco.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 49, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2494_0_0"}, {"texts": ["The woman is using her tablet inside the metro."], "durations": null, "exact_frames_per_prompt": [31], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/1R1wxagNCco.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 49, "npz_gt_video_start_frame": 49, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 49, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2494_0_1"}, {"texts": ["A woman wearing a dark brown jacket and blue jeans is sitting on the right side of the first woman in the metro."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/1R1wxagNCco.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2494_2_0"}, {"texts": ["A man wearing a blue shirt is standing and adjusting and pulling up his bow tie."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/A8WxqlB_yUs.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2495_0_0"}, {"texts": ["A woman wearing white coat is taking the trophy from a man, behind the podium while other group of people is standing behind the podium."], "durations": null, "exact_frames_per_prompt": [34], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/0sHU2Ee8LOg.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 46, "npz_gt_video_start_frame": 46, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 46, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2496_1_0"}, {"texts": ["A man wearing a shiny blazer is taking the trophy from the first woman while the woman in white dress looks disappointed."], "durations": null, "exact_frames_per_prompt": [24], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/0sHU2Ee8LOg.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 28, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2496_2_0"}, {"texts": ["The man is giving it to the second woman while others are reacting to it."], "durations": null, "exact_frames_per_prompt": [52], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/0sHU2Ee8LOg.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 28, "npz_gt_video_start_frame": 28, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 28, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2496_2_1"}, {"texts": ["A person whose hand is visible is holding a pan and shaking it."], "durations": null, "exact_frames_per_prompt": [20], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/0jDhXM7j9F8.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 60, "npz_gt_video_start_frame": 60, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 60, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_249_0_0"}, {"texts": ["A man wearing a cream-black cloth is sitting on a blue chair and touching the second dog while a woman wearing black gray jacket is walking ahead holding the leash of a white dog and another woman wearing black clothes is sitting on a blue chair."], "durations": null, "exact_frames_per_prompt": [28], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/3pnM1Ya67I0.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 42, "npz_gt_video_start_frame": 42, "npz_gt_video_end_frame": 70, "skip_frames_after_generation": 42, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2501_2_0"}, {"texts": ["A white dog is walking with the first woman."], "durations": null, "exact_frames_per_prompt": [20], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/3pnM1Ya67I0.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 24, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2501_3_0"}, {"texts": ["The dog sits when the first woman stops."], "durations": null, "exact_frames_per_prompt": [11], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/3pnM1Ya67I0.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 24, "npz_gt_video_start_frame": 24, "npz_gt_video_end_frame": 35, "skip_frames_after_generation": 24, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2501_3_1"}, {"texts": ["The dog starts walking when the first woman starts walking again. Another woman and a man wearing black clothes are sitting on chairs near the wall."], "durations": null, "exact_frames_per_prompt": [45], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/3pnM1Ya67I0.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 35, "npz_gt_video_start_frame": 35, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 35, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2501_3_2"}, {"texts": ["A white dog is walking with the first woman."], "durations": null, "exact_frames_per_prompt": [19], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/3pnM1Ya67I0.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 23, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2502_3_0"}, {"texts": ["The dog sits."], "durations": null, "exact_frames_per_prompt": [11], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/3pnM1Ya67I0.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 23, "npz_gt_video_start_frame": 23, "npz_gt_video_end_frame": 34, "skip_frames_after_generation": 23, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2502_3_1"}, {"texts": ["The dog starts walking along with woman"], "durations": null, "exact_frames_per_prompt": [46], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/3pnM1Ya67I0.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 34, "npz_gt_video_start_frame": 34, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 34, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2502_3_2"}, {"texts": ["A woman wearing black cloth is holding the collar of a dog and walking."], "durations": null, "exact_frames_per_prompt": [13], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/3pnM1Ya67I0.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 17, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2503_0_0"}, {"texts": ["The woman wearing black cloth instructed the dog to sit."], "durations": null, "exact_frames_per_prompt": [14], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/3pnM1Ya67I0.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 17, "npz_gt_video_start_frame": 17, "npz_gt_video_end_frame": 32, "skip_frames_after_generation": 17, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2503_0_1"}, {"texts": ["The woman wearing black cloth starts walking while the white dog also starts walking, a woman is sitting at the back along with a dog and a man is caressing another dog."], "durations": null, "exact_frames_per_prompt": [47], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/3pnM1Ya67I0.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 32, "npz_gt_video_start_frame": 32, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 32, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2503_0_2"}, {"texts": ["A white dog is walking behind the woman."], "durations": null, "exact_frames_per_prompt": [12], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/3pnM1Ya67I0.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 16, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2503_1_0"}, {"texts": ["The white dog sits down."], "durations": null, "exact_frames_per_prompt": [19], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/3pnM1Ya67I0.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 16, "npz_gt_video_start_frame": 16, "npz_gt_video_end_frame": 35, "skip_frames_after_generation": 16, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2503_1_1"}, {"texts": ["The white dog again starts walking behind the woman."], "durations": null, "exact_frames_per_prompt": [45], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/3pnM1Ya67I0.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 35, "npz_gt_video_start_frame": 35, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 35, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2503_1_2"}, {"texts": ["A man wearing a pink shirt is demonstrating how to tie a bow tie."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/9RTVnUDU6BQ.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2504_0_0"}, {"texts": ["A dark brown puppy dog is walking while barking on the paper shredder machine while a person wearing a red t-shirt is sitting on the left side and putting a piece of paper inside the machine."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/18IsTNYYQG4.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2505_1_0"}, {"texts": ["A white-brown cat is lying on the person's lap."], "durations": null, "exact_frames_per_prompt": [24], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/23uD5oqmFR0.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 28, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2507_0_0"}, {"texts": ["The cat is looking in the front with its head up."], "durations": null, "exact_frames_per_prompt": [52], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/23uD5oqmFR0.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 28, "npz_gt_video_start_frame": 28, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 28, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2507_0_1"}, {"texts": ["A man wearing a black t-shirt is standing on the right side of the cooking stove.\n"], "durations": null, "exact_frames_per_prompt": [20], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/2djtbcbuhkI.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 58, "npz_gt_video_start_frame": 58, "npz_gt_video_end_frame": 78, "skip_frames_after_generation": 58, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2508_0_0"}, {"texts": ["A woman wearing black clothes is standing and cutting a dog's hair with scissor while a man on the right wearing white long sleeve polo is brushing the hair of a dog and a group of people is standing on the front."], "durations": null, "exact_frames_per_prompt": [20], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/1BFdH2wYiog.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 24, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2509_0_0"}, {"texts": ["A woman wearing dark blue clothes is standing and trimming a dog's hair with a trimmer."], "durations": null, "exact_frames_per_prompt": [55], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/1BFdH2wYiog.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 25, "npz_gt_video_start_frame": 25, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 25, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2509_1_0"}, {"texts": ["A man on the right side wearing a white shirt is standing and combing a dog's hair while some other people are cutting dogs' hair and a group of people are looking at them."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/1BFdH2wYiog.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2509_2_0"}, {"texts": ["A dark brown dog is standing on a pet grooming table in front of the second woman.\n"], "durations": null, "exact_frames_per_prompt": [55], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/1BFdH2wYiog.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 25, "npz_gt_video_start_frame": 25, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 25, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2509_4_ms_0"}, {"texts": ["A light brown dog on the right side, is standing on a pet grooming table in front of the man while a man wearing white cloth is grooming him, another dark brown dog is standing on a pet grooming table and groomed by a woman wearing black cloth and a group of people standing around them."], "durations": null, "exact_frames_per_prompt": [20], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/1BFdH2wYiog.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 24, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2509_5_ms_0"}, {"texts": ["A girl wearing a red t-shirt is standing and a person whose hands are visible is giving a red mixing blender and then starts mixing the food better into a bowl with another mixing blender."], "durations": null, "exact_frames_per_prompt": [37], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/4COLHcNGWzM.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 41, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2510_0_0"}, {"texts": ["The girl wearing a red t-shirt is holding a red mixing blender."], "durations": null, "exact_frames_per_prompt": [18], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/4COLHcNGWzM.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 41, "npz_gt_video_start_frame": 41, "npz_gt_video_end_frame": 59, "skip_frames_after_generation": 41, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2510_0_1"}, {"texts": ["The girl wearing a red t-shirt is put into the white food batter."], "durations": null, "exact_frames_per_prompt": [21], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/4COLHcNGWzM.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 59, "npz_gt_video_start_frame": 59, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 59, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2510_0_2"}, {"texts": ["A man wearing a white striped dark gray shirt, blue jeans, a white hat, and brown shoes is sitting on the boat and holding the fish with a string while another person is sitting on the left on the boat."], "durations": null, "exact_frames_per_prompt": [57], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/1PvKX0jbaNI.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 61, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2511_0_0"}, {"texts": ["A gray fish is being held with a string by the man."], "durations": null, "exact_frames_per_prompt": [57], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/1PvKX0jbaNI.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 61, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2511_1_0"}, {"texts": ["A man whose half body is visible wearing a red vest and pajama is standing, rubbing his elbow."], "durations": null, "exact_frames_per_prompt": [59], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/1DPHExN1xQU.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 63, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2512_0_0"}, {"texts": ["The man is taking out cream from the spoon with his finger."], "durations": null, "exact_frames_per_prompt": [16], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/1DPHExN1xQU.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 63, "npz_gt_video_start_frame": 63, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 63, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2512_0_1"}, {"texts": ["A man wearing a white chef dress and specs is standing behind the table and holding a wooden spatula."], "durations": null, "exact_frames_per_prompt": [15], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/35It-NgBD98.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 5, "npz_gt_video_start_frame": 5, "npz_gt_video_end_frame": 20, "skip_frames_after_generation": 5, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2513_0_0"}, {"texts": ["The man is saut\u00e9ing spaghetti and hot dogs with a wooden spatula in a fry pan.\n"], "durations": null, "exact_frames_per_prompt": [60], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/35It-NgBD98.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 20, "npz_gt_video_start_frame": 20, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 20, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2513_0_1"}, {"texts": ["A brown horse falls backward on a grey road with a man while others watch it."], "durations": null, "exact_frames_per_prompt": [72], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/0snJBTZ8M6M.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 8, "npz_gt_video_start_frame": 8, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 8, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2515_1_0"}, {"texts": ["A group of people are watching the falling horse and man."], "durations": null, "exact_frames_per_prompt": [62], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/0snJBTZ8M6M.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 8, "npz_gt_video_start_frame": 8, "npz_gt_video_end_frame": 70, "skip_frames_after_generation": 8, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2515_3_ms_0"}, {"texts": ["A man wearing brown clothes is bending on a white table and eating a burger."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/12-7o2QML9w.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2517_0_0"}, {"texts": ["A woman wearing a black dress is standing behind the counter, pouring a few drops of vanilla into the bottle."], "durations": null, "exact_frames_per_prompt": [33], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/0K_2x5-FmFc.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 37, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2519_0_0"}, {"texts": ["The woman is pouring a dash of cinnamon into the bottle."], "durations": null, "exact_frames_per_prompt": [12], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/0K_2x5-FmFc.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 37, "npz_gt_video_start_frame": 37, "npz_gt_video_end_frame": 49, "skip_frames_after_generation": 37, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2519_0_1"}, {"texts": ["The woman is putting the glass in the bottle and starts shaking the liquid in the bottle. "], "durations": null, "exact_frames_per_prompt": [27], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/0K_2x5-FmFc.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 49, "npz_gt_video_start_frame": 49, "npz_gt_video_end_frame": 76, "skip_frames_after_generation": 49, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2519_0_2"}, {"texts": ["The woman is pouring the liquid into the martini glass from the bottle."], "durations": null, "exact_frames_per_prompt": [4], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/0K_2x5-FmFc.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 76, "npz_gt_video_start_frame": 76, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 76, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2519_0_3"}, {"texts": ["A girl wearing a multicoloured dress is sitting holding a book and reading it."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/0Wn3zce40Mw.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2520_0_0"}, {"texts": ["A boy wearing black clothes is standing on the left while a man wearing a black jacket is standing on the right side and pulling something from the sea."], "durations": null, "exact_frames_per_prompt": [12], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/1GC52gQjSJQ.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 16, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2522_1_0"}, {"texts": ["The boy walks towards the railing and starts looking into the sea."], "durations": null, "exact_frames_per_prompt": [63], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/1GC52gQjSJQ.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 16, "npz_gt_video_start_frame": 16, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 16, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2522_1_1"}, {"texts": ["A man wearing black clothes is standing, holding and rotating the fish rod and looking into the sea.\n while a boy in a black jacket is holding the boat railing and looking into the sea."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/1GC52gQjSJQ.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2522_2_0"}, {"texts": ["A man wearing a white vest is sitting in the car, holding a burger in his hands."], "durations": null, "exact_frames_per_prompt": [16], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/1-fccTBU0FM.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 20, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2525_0_0"}, {"texts": ["The man is eating the burger."], "durations": null, "exact_frames_per_prompt": [60], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/1-fccTBU0FM.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 20, "npz_gt_video_start_frame": 20, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 20, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2525_0_1"}, {"texts": ["A man wearing glasses is moving his head."], "durations": null, "exact_frames_per_prompt": [20], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/0_WZHFQOD64.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 57, "npz_gt_video_start_frame": 57, "npz_gt_video_end_frame": 77, "skip_frames_after_generation": 57, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2527_0_0"}, {"texts": ["A man wearing green pants is standing on the ice surface."], "durations": null, "exact_frames_per_prompt": [25], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/06Yvm4X81WI.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 29, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2528_0_0"}, {"texts": ["The man is breaking the ice surface with an ice scraper."], "durations": null, "exact_frames_per_prompt": [51], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/06Yvm4X81WI.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 29, "npz_gt_video_start_frame": 29, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 29, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2528_0_1"}, {"texts": ["A man wearing a grey t-shirt is sitting and moving his hand while crying.\n a girl wearing a white top is standing just behind the man and moving her hand then touches her face."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/0-6Tn0Ie-AQ.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2529_0_0"}, {"texts": ["A girl wearing a white top is standing just behind the man and moving her hand then touches her face a man wearing a grey t-shirt is sitting and moving his hand while crying."], "durations": null, "exact_frames_per_prompt": [45], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/0-6Tn0Ie-AQ.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 49, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2529_1_0"}, {"texts": ["A man wearing a white t-shirt is filling the glass from the bottle while a man wearing gray t-shirt is standing and looking towards the right side, and group of people are standing and looking here and there."], "durations": null, "exact_frames_per_prompt": [22], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/8mbN0ubh0q8.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 26, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_252_0_0"}, {"texts": ["The man is drinking it, again drinking from the glass by the first man."], "durations": null, "exact_frames_per_prompt": [54], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/8mbN0ubh0q8.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 26, "npz_gt_video_start_frame": 26, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 26, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_252_0_1"}, {"texts": ["A man wearing a grey t-shirt is holding a glass while another man on the right is pouring a drink into the glass and drinking it."], "durations": null, "exact_frames_per_prompt": [28], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/8mbN0ubh0q8.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 32, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_252_1_0"}, {"texts": ["The man is giving a drink to the first man while a group of people are sitting and standing at the back."], "durations": null, "exact_frames_per_prompt": [48], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/8mbN0ubh0q8.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 32, "npz_gt_video_start_frame": 32, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 32, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_252_1_1"}, {"texts": ["A person whose only hand is visible is wearing a grey glove and opening an engine cap."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/04akMVAa4tE.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2530_0_0"}, {"texts": ["A man wearing black-gray cloth is standing and making a white object."], "durations": null, "exact_frames_per_prompt": [67], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-TJ5RM7Lm04.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 71, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2531_0_0"}, {"texts": ["The man is measuring it by a scale."], "durations": null, "exact_frames_per_prompt": [9], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-TJ5RM7Lm04.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 71, "npz_gt_video_start_frame": 71, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 71, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2531_0_1"}, {"texts": ["A man takes some shredded meat."], "durations": null, "exact_frames_per_prompt": [21], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/2OeyaqN2FFk.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 25, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2537_0_0"}, {"texts": ["The man puts it on a yellow tray then picks up the tray in his hands."], "durations": null, "exact_frames_per_prompt": [42], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/2OeyaqN2FFk.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 25, "npz_gt_video_start_frame": 25, "npz_gt_video_end_frame": 67, "skip_frames_after_generation": 25, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2537_0_1"}, {"texts": ["The man starts moving."], "durations": null, "exact_frames_per_prompt": [13], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/2OeyaqN2FFk.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 67, "npz_gt_video_start_frame": 67, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 67, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2537_0_2"}, {"texts": ["A woman wearing a blue dress is standing."], "durations": null, "exact_frames_per_prompt": [30], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/0KUo8zx5wJo.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 34, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2538_0_0"}, {"texts": ["The woman pouring a white paste over the chicken in a cauldron."], "durations": null, "exact_frames_per_prompt": [13], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/0KUo8zx5wJo.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 34, "npz_gt_video_start_frame": 34, "npz_gt_video_end_frame": 47, "skip_frames_after_generation": 34, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2538_0_1"}, {"texts": ["The woman is mixing it with tong."], "durations": null, "exact_frames_per_prompt": [33], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/0KUo8zx5wJo.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 47, "npz_gt_video_start_frame": 47, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 47, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2538_0_2"}, {"texts": ["A woman wearing a blue dress is standing and pouring a white paste over the chicken in a cauldron."], "durations": null, "exact_frames_per_prompt": [13], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/0KUo8zx5wJo.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 33, "npz_gt_video_start_frame": 33, "npz_gt_video_end_frame": 46, "skip_frames_after_generation": 33, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2539_0_0"}, {"texts": ["The woman is mixing it with tongs."], "durations": null, "exact_frames_per_prompt": [27], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/0KUo8zx5wJo.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 46, "npz_gt_video_start_frame": 46, "npz_gt_video_end_frame": 74, "skip_frames_after_generation": 46, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2539_0_1"}, {"texts": ["A person wearing a honey bee protection suit is standing and opening the beehive from the top."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/3hdEyGHPHmE.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_253_0_0"}, {"texts": ["A person whose only hand is visible is wearing red gloves and is putting white heated stones onto a black lid."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/1P0zfkMi6jc.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2542_0_0"}, {"texts": ["A woman wearing blue pants is walking and going out of the door with a black and white dog."], "durations": null, "exact_frames_per_prompt": [51], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/1C1GFAUxiKg.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 55, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2544_0_0"}, {"texts": ["A black-white dog is walking on the carpet with the first woman."], "durations": null, "exact_frames_per_prompt": [49], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/1C1GFAUxiKg.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 53, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2544_4_0"}, {"texts": ["A black dog is standing near the second woman on the black counter."], "durations": null, "exact_frames_per_prompt": [20], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/1C1GFAUxiKg.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 60, "npz_gt_video_start_frame": 60, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 60, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2544_5_0"}, {"texts": ["A woman wearing black outfit, standing in the kitchen is speaking."], "durations": null, "exact_frames_per_prompt": [27], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/1Nk5D2V4Kqs.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 31, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2545_0_0"}, {"texts": ["The woman wearing black outfit is pouring salt in the dish."], "durations": null, "exact_frames_per_prompt": [49], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/1Nk5D2V4Kqs.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 31, "npz_gt_video_start_frame": 31, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 31, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2545_0_1"}, {"texts": ["A woman wearing a black-purple top is standing and speaking"], "durations": null, "exact_frames_per_prompt": [30], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/1Nk5D2V4Kqs.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 34, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2546_0_0"}, {"texts": ["A woman wearing a black-purple top is adding salt to the yellow food."], "durations": null, "exact_frames_per_prompt": [46], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/1Nk5D2V4Kqs.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 34, "npz_gt_video_start_frame": 34, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 34, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2546_0_1"}, {"texts": ["A woman wearing black clothes is pouring salt and pepper in the food."], "durations": null, "exact_frames_per_prompt": [53], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/1Nk5D2V4Kqs.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 27, "npz_gt_video_start_frame": 27, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 27, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2547_0_0"}, {"texts": ["A man wearing a white jacket, white gloves, and a bee veil hat is standing on the left side and speaking while holding the box cover in his hand."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/3hdEyGHPHmE.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_254_1_0"}, {"texts": ["A boy wearing a white shirt is standing,  moving his hand towards the right side."], "durations": null, "exact_frames_per_prompt": [45], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/37QXT0GIJto.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 49, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2552_0_0"}, {"texts": ["The boy wearing a white shirt is licking the white batter from the spoon and  moving his hand towards the right side."], "durations": null, "exact_frames_per_prompt": [31], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/37QXT0GIJto.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 49, "npz_gt_video_start_frame": 49, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 49, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2552_0_1"}, {"texts": ["A man wearing red-white clothes is standing and holding a spatula and mixing the vegetables in a frying pan on the stove."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/3gMgNoCntEk.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2553_0_0"}, {"texts": ["A person whose hand is visible is mixing the food in a pan with a wooden spatula."], "durations": null, "exact_frames_per_prompt": [17], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/0Y9hXGNwfkA.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 21, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2556_0_0"}, {"texts": ["A person wearing a printed blue shirt is trying to open a glass bottle cap with his hands."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/s1T2iY0JnZo.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2560_0_0"}, {"texts": ["A person wearing a bee protective helmet is standing on a green surface, holding a box of bees and shaking it."], "durations": null, "exact_frames_per_prompt": [26], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/4_nPVREmJzs.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 26, "npz_gt_video_start_frame": 26, "npz_gt_video_end_frame": 52, "skip_frames_after_generation": 26, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2563_0_0"}, {"texts": ["The person wearing a bee protective helmet picks a plant and shakes it."], "durations": null, "exact_frames_per_prompt": [27], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/4_nPVREmJzs.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 52, "npz_gt_video_start_frame": 52, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 52, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2563_0_1"}, {"texts": ["A person whose hands are visible is ironing a checkered shirt on a white ironing board."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/0nvEZQbl7Vw.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2564_0_0"}, {"texts": ["A baby on the right side wearing gray-black clothes is lying on a yellow bed and holding hands of another baby and laughing."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-xd3aIXUJuE.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2565_0_0"}, {"texts": ["A baby on the left side wearing white-black cloth is lying on yellow bed with a baby on the right side."], "durations": null, "exact_frames_per_prompt": [47], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-xd3aIXUJuE.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 51, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2565_1_0"}, {"texts": ["The baby on the left side wearing white-black cloth is holding hands of the first baby and laughing."], "durations": null, "exact_frames_per_prompt": [29], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-xd3aIXUJuE.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 51, "npz_gt_video_start_frame": 51, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 51, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2565_1_1"}, {"texts": ["A kid wearing a vertical lining t-shirt is lying on the right side of the bed, touching the hands of another kid."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-xd3aIXUJuE.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2566_0_0"}, {"texts": ["A kid wearing a horizontal lining t-shirt is also lying on the left side of the bed, sucking his finger and touching the hand of the first kid while the right side kid laughing by moves hands."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-xd3aIXUJuE.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2566_1_0"}, {"texts": ["A baby wearing a white vest and white pants is sitting on the bed and holding food with a fork."], "durations": null, "exact_frames_per_prompt": [52], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-RG6SvbKKk4.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 56, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2567_0_0"}, {"texts": ["The baby is eating while dancing."], "durations": null, "exact_frames_per_prompt": [24], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-RG6SvbKKk4.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 56, "npz_gt_video_start_frame": 56, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 56, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2567_0_1"}, {"texts": ["A person whose hand is visible is mixing chicken and yogurt in a black pan with a wooden spatula."], "durations": null, "exact_frames_per_prompt": [55], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/0Hz2h1CLfj4.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 59, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2568_0_0"}, {"texts": ["The person whose hand is visible is then stirring it."], "durations": null, "exact_frames_per_prompt": [21], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/0Hz2h1CLfj4.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 59, "npz_gt_video_start_frame": 59, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 59, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2568_0_1"}, {"texts": ["A baby wearing a red t-shirt is sitting on the brown mat and laughing."], "durations": null, "exact_frames_per_prompt": [20], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-mmu5VJRY1Y.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 24, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_256_0_0"}, {"texts": ["The baby is putting their hands into their mouth."], "durations": null, "exact_frames_per_prompt": [56], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-mmu5VJRY1Y.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 24, "npz_gt_video_start_frame": 24, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 24, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_256_0_1"}, {"texts": ["A girl wearing a purple hijab is plucking yellow fruits from a tree"], "durations": null, "exact_frames_per_prompt": [31], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/1IE8VkavVoI.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 12, "npz_gt_video_start_frame": 12, "npz_gt_video_end_frame": 43, "skip_frames_after_generation": 12, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2574_0_0"}, {"texts": ["The girl is getting her hand held by the other girl."], "durations": null, "exact_frames_per_prompt": [37], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/1IE8VkavVoI.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 43, "npz_gt_video_start_frame": 43, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 43, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2574_0_1"}, {"texts": ["A baby on the right side wearing a pink bib is laughing and sitting on the black baby chair while another baby wearing pink cloth is sitting on a baby chair on the left side and laughing."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/0bYUAffLy0c.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2575_0_0"}, {"texts": ["A baby on the left side wearing a pink bib is also laughing and sitting on the black baby chair with another baby on the right side wearing a pink bib is laughing and sitting on the black baby chair."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/0bYUAffLy0c.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2575_1_0"}, {"texts": ["A baby wearing pink cloth is sitting on a baby chair on the right side and laughing another baby wearing pink cloth is sitting on a baby chair on the left side, also laughing."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/0bYUAffLy0c.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2576_0_0"}, {"texts": ["A baby wearing pink cloth is sitting on a baby chair on the left side and laughing while a baby on the right side wearing a pink bib is laughing and sitting on the black baby chair."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/0bYUAffLy0c.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2576_1_0"}, {"texts": ["A man wearing white clothes is holding a knife and cutting makizushi and putting it in a tray."], "durations": null, "exact_frames_per_prompt": [57], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/6TDcZN08_YE.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 61, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2577_0_0"}, {"texts": ["A man wearing a black suit is standing on the right side holding a glass of alcohol while a woman wearing grey t-shirt is on the left and she is looking at the bottle."], "durations": null, "exact_frames_per_prompt": [49], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/0JdK6RgwlK4.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 53, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2578_0_0"}, {"texts": ["The man starts drinking."], "durations": null, "exact_frames_per_prompt": [15], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/0JdK6RgwlK4.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 53, "npz_gt_video_start_frame": 53, "npz_gt_video_end_frame": 68, "skip_frames_after_generation": 53, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2578_0_1"}, {"texts": ["A woman wearing a grey t-shirt is leaning towards the counter, holding a bottle and looking at the bottle while a man wearing a black coat is standing and drinking a glass of liquor."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/0JdK6RgwlK4.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2578_1_0"}, {"texts": ["A boy wearing red clothes is sitting."], "durations": null, "exact_frames_per_prompt": [8], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/05zoDSBn6ck.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 12, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_257_0_0"}, {"texts": ["The boy is speaking.\n\n"], "durations": null, "exact_frames_per_prompt": [7], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/05zoDSBn6ck.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 12, "npz_gt_video_start_frame": 12, "npz_gt_video_end_frame": 19, "skip_frames_after_generation": 12, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_257_0_1"}, {"texts": ["The boy is showing Oreo packet"], "durations": null, "exact_frames_per_prompt": [61], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/05zoDSBn6ck.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 19, "npz_gt_video_start_frame": 19, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 19, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_257_0_2"}, {"texts": ["A boy wearing a pink t-shirt is standing holding a spoon and applying sauce on pizza dough while the girl is also doing the same activity."], "durations": null, "exact_frames_per_prompt": [56], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/0rGWpMgjlVU.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 24, "npz_gt_video_start_frame": 24, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 24, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2580_0_0"}, {"texts": ["A girl wearing a pink t-shirt is standing holding a spoon and applying sauce on a pizza dough while a boy in pink t-shirt also applying sauce on a pizza dough and a woman in white jacket is standing to the right side of the girl."], "durations": null, "exact_frames_per_prompt": [59], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/0rGWpMgjlVU.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 21, "npz_gt_video_start_frame": 21, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 21, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2580_1_0"}, {"texts": ["A girl wearing a gray t-shirt is standing on a chair holding a spoon and poking the pizza dough while a person wearing an apron stands on the left, pointing at the table, and a kid wearing a red t-shirt is garnishing red sauce on the bread slices."], "durations": null, "exact_frames_per_prompt": [19], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/0rGWpMgjlVU.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 23, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2580_2_0"}, {"texts": ["A woman wearing a white top is standing and pointing hand towards the pizza while the girl place spoon on the dough."], "durations": null, "exact_frames_per_prompt": [23], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/0rGWpMgjlVU.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 27, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2580_3_0"}, {"texts": ["A woman wearing a white top picks up a spoon while the 2kids from left side spreads the sauce on  the dough."], "durations": null, "exact_frames_per_prompt": [52], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/0rGWpMgjlVU.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 27, "npz_gt_video_start_frame": 27, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 27, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2580_3_1"}, {"texts": ["A gray elephant is standing on the soil surface and carries a group of people on his back and another group of people is moving here and there on the back."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-EBIDrH1sVE.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2581_5_0"}, {"texts": ["A man wearing greenish clothes is riding on an elephant back on the soil surface while some people are sitting on an elephant and a group of people are moving at the back."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-EBIDrH1sVE.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2581_7_0"}, {"texts": ["A boy wearing a grey t-shirt is sitting on his knees, taking a bottle of juice from the refrigerator."], "durations": null, "exact_frames_per_prompt": [21], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-xqh0BXezLI.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 25, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2584_0_0"}, {"texts": ["The boy starts drinking the juice."], "durations": null, "exact_frames_per_prompt": [17], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-xqh0BXezLI.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 25, "npz_gt_video_start_frame": 25, "npz_gt_video_end_frame": 42, "skip_frames_after_generation": 25, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2584_0_1"}, {"texts": ["The boy puts the bottle on the counter."], "durations": null, "exact_frames_per_prompt": [12], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-xqh0BXezLI.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 42, "npz_gt_video_start_frame": 42, "npz_gt_video_end_frame": 54, "skip_frames_after_generation": 42, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2584_0_2"}, {"texts": ["The boy starts walking towards the right and wipes his nose."], "durations": null, "exact_frames_per_prompt": [26], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-xqh0BXezLI.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 54, "npz_gt_video_start_frame": 54, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 54, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2584_0_3"}, {"texts": ["A woman whose only half body is visible wearing a white top is standing and folding an orange shirt."], "durations": null, "exact_frames_per_prompt": [36], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/17VmzhDfYTY.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 40, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2585_0_0"}, {"texts": ["The woman puts the shirt on the ironing board."], "durations": null, "exact_frames_per_prompt": [40], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/17VmzhDfYTY.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 40, "npz_gt_video_start_frame": 40, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 40, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2585_0_1"}, {"texts": ["A woman wearing blue pants is sitting on a light brown floor, folding a paper."], "durations": null, "exact_frames_per_prompt": [17], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/1fLI-0NsMgg.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 21, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2587_0_0"}, {"texts": ["The woman wearing blue pants is picking the book covers, placing the paper in it."], "durations": null, "exact_frames_per_prompt": [34], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/1fLI-0NsMgg.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 21, "npz_gt_video_start_frame": 21, "npz_gt_video_end_frame": 55, "skip_frames_after_generation": 21, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2587_0_1"}, {"texts": ["The woman wearing blue pants is then showing it."], "durations": null, "exact_frames_per_prompt": [25], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/1fLI-0NsMgg.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 55, "npz_gt_video_start_frame": 55, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 55, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2587_0_2"}, {"texts": ["A woman wearing blue pants is sitting on the floor, folding paper."], "durations": null, "exact_frames_per_prompt": [21], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/1fLI-0NsMgg.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 25, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2588_0_0"}, {"texts": ["The woman is picking book covers, placing the paper in the book covers."], "durations": null, "exact_frames_per_prompt": [28], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/1fLI-0NsMgg.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 25, "npz_gt_video_start_frame": 25, "npz_gt_video_end_frame": 53, "skip_frames_after_generation": 25, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2588_0_1"}, {"texts": ["The woman is showing it."], "durations": null, "exact_frames_per_prompt": [27], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/1fLI-0NsMgg.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 53, "npz_gt_video_start_frame": 53, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 53, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2588_0_2"}, {"texts": ["A man on the left side is drinking something from a glass with the first person."], "durations": null, "exact_frames_per_prompt": [40], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/5DZ89cq0Css.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 12, "npz_gt_video_start_frame": 12, "npz_gt_video_end_frame": 52, "skip_frames_after_generation": 12, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2589_1_0"}, {"texts": ["A man on the extreme left is counting and standing with the first and second person while a group of people are sitting and standing behind them in the club."], "durations": null, "exact_frames_per_prompt": [48], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/5DZ89cq0Css.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 52, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2589_2_0"}, {"texts": ["A man wearing a light blue shirt, light blue pants, and a white hat is sitting on the dark brown elephant's head and is giving a ride to other people."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/0mffd8TMtHg.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2590_1_0"}, {"texts": ["A man wearing a black t-shirt is sitting on the dark brown elephant's back behind the first man and is riding an elephant."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/0mffd8TMtHg.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2590_2_0"}, {"texts": ["A dark brown elephant is walking from right to left while carrying people on its back."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/0mffd8TMtHg.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2590_4_0"}, {"texts": ["An elephant is standing on the left side of the green grass surface while a man wearing blue shirt sitting on his head"], "durations": null, "exact_frames_per_prompt": [15], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/0mffd8TMtHg.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 31, "npz_gt_video_start_frame": 31, "npz_gt_video_end_frame": 46, "skip_frames_after_generation": 31, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2590_8_0"}, {"texts": ["A girl wearing a black-white printed top is sitting on the right, eating food from the white bowl with chopsticks."], "durations": null, "exact_frames_per_prompt": [51], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/0wCcqh5ZHs8.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 55, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2592_0_0"}, {"texts": ["A girl wearing a multicolored dress is standing and sauteing food in a pan and flipping it with a spatula."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/1h9aVL3c1lU.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2593_0_0"}, {"texts": ["A man whose hands are only visible, is holding a shrimp and coating it with flour"], "durations": null, "exact_frames_per_prompt": [18], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/19J04I1K2rM.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 22, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2598_0_0"}, {"texts": ["The man whose hands are only visible putting the shrimp on the plate."], "durations": null, "exact_frames_per_prompt": [42], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/19J04I1K2rM.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 22, "npz_gt_video_start_frame": 22, "npz_gt_video_end_frame": 64, "skip_frames_after_generation": 22, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2598_0_1"}, {"texts": ["A baby wearing a blue bin is taking a white and green dish into his mouth."], "durations": null, "exact_frames_per_prompt": [30], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/0TfuoJOcAUE.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 34, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2600_0_0"}, {"texts": ["A baby boy wearing white-black clothes is sitting in the baby's high chair, holding and then starts licking the white green plate."], "durations": null, "exact_frames_per_prompt": [30], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/0TfuoJOcAUE.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 34, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2601_0_0"}, {"texts": ["A man is wearing a white shirt and black waistcoat stand, pouring something from the bottle into a cup and talks."], "durations": null, "exact_frames_per_prompt": [32], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/0m_UI3CoDtM.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 33, "npz_gt_video_start_frame": 33, "npz_gt_video_end_frame": 65, "skip_frames_after_generation": 33, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2602_2_0"}, {"texts": ["A man wearing black shirt is standing behind a counter and pouring drink from a yellow bottle in the glasses."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/18En8rBDAcU.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2603_0_0"}, {"texts": ["A white kid is standing behind the fence and being fed by the person while another brown and white goat kid came from the right side, stood on the fence, and then walked away."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-tYO4PkKD7I.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2605_1_0"}, {"texts": ["A man wearing a full-sleeve white t-shirt and a black cap is sitting on a chair and drinking juice from the bottle while holding the bottle with his left hand."], "durations": null, "exact_frames_per_prompt": [24], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-chadEau4KM.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 28, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2612_1_ms_0"}, {"texts": ["The man starts holding the fork with his right hand others start clapping."], "durations": null, "exact_frames_per_prompt": [15], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-chadEau4KM.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 28, "npz_gt_video_start_frame": 28, "npz_gt_video_end_frame": 43, "skip_frames_after_generation": 28, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2612_1_ms_1"}, {"texts": ["The man is picking food with the fork."], "durations": null, "exact_frames_per_prompt": [31], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-chadEau4KM.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 43, "npz_gt_video_start_frame": 43, "npz_gt_video_end_frame": 74, "skip_frames_after_generation": 43, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2612_1_ms_2"}, {"texts": ["A white animal is standing on the green table and getting its hair cut by the person."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-CPHycRUJfA.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2613_1_0"}, {"texts": ["A man wearing a blue t-shirt is putting a brown piece of baked food on the spatula while a boy on the left side and another boy on the right side of the man are watching the man."], "durations": null, "exact_frames_per_prompt": [13], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/0Ata9VACjv8.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 17, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2614_0_0"}, {"texts": ["The man wearing a blue t-shirt is showing the food."], "durations": null, "exact_frames_per_prompt": [27], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/0Ata9VACjv8.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 17, "npz_gt_video_start_frame": 17, "npz_gt_video_end_frame": 44, "skip_frames_after_generation": 17, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2614_0_1"}, {"texts": ["The man wearing a blue t-shirt puts the food on the table."], "durations": null, "exact_frames_per_prompt": [16], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/0Ata9VACjv8.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 44, "npz_gt_video_start_frame": 44, "npz_gt_video_end_frame": 60, "skip_frames_after_generation": 44, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2614_0_2"}, {"texts": ["The man wearing a blue t-shirt starts taking another piece of food from the tray with the spatula."], "durations": null, "exact_frames_per_prompt": [20], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/0Ata9VACjv8.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 60, "npz_gt_video_start_frame": 60, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 60, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2614_0_3"}, {"texts": ["A boy wearing a black t-shirt is on the left side, looking at the food and putting his hands on the table while a man wearing a blue t-shirt picks up the food from the spatula and keeps it to the right, a boy wearing a white t-shirt stands near the food table ."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/0Ata9VACjv8.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2614_1_0"}, {"texts": ["A boy wearing a white t-shirt is on the right side and looking at the food while a man wearing blue t-shirt is holding a black spatula and he is picking up the food."], "durations": null, "exact_frames_per_prompt": [59], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/0Ata9VACjv8.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 63, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2614_2_0"}, {"texts": ["The boy starts touching the piece of food on the table while a boy wearing black t-shirt is on the left."], "durations": null, "exact_frames_per_prompt": [17], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/0Ata9VACjv8.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 63, "npz_gt_video_start_frame": 63, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 63, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2614_2_1"}, {"texts": ["A group of three cars are moving in different directions on a wet road."], "durations": null, "exact_frames_per_prompt": [12], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/0B6ThOWbrpU.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 16, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2617_0_ms_0"}, {"texts": ["A woman wearing a black dress is standing and presenting the weather forecast report from a screen."], "durations": null, "exact_frames_per_prompt": [65], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/0B6ThOWbrpU.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 15, "npz_gt_video_start_frame": 15, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 15, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2617_1_ms_0"}, {"texts": ["A boy wearing a multicoloured striped vest is folding a blue-white t-shirt on the bed."], "durations": null, "exact_frames_per_prompt": [34], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-KtT7Q730Yg.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 38, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2618_0_0"}, {"texts": ["The boy wearing a multicoloured striped vest starts jumping on the floor and tapping on the bed."], "durations": null, "exact_frames_per_prompt": [14], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-KtT7Q730Yg.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 38, "npz_gt_video_start_frame": 38, "npz_gt_video_end_frame": 52, "skip_frames_after_generation": 38, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2618_0_1"}, {"texts": ["A girl wearing pink-white-blue t-shirt is sitting on the gray surface."], "durations": null, "exact_frames_per_prompt": [8], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/17EMC6XiBM0.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 12, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2619_0_0"}, {"texts": ["The girl picks something."], "durations": null, "exact_frames_per_prompt": [35], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/17EMC6XiBM0.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 12, "npz_gt_video_start_frame": 12, "npz_gt_video_end_frame": 47, "skip_frames_after_generation": 12, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2619_0_1"}, {"texts": ["The girl throws it into the water."], "durations": null, "exact_frames_per_prompt": [33], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/17EMC6XiBM0.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 47, "npz_gt_video_start_frame": 47, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 47, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2619_0_2"}, {"texts": ["A man on the right side wearing a camouflage uniform is standing along with the man in grey t-shirt and participate in the game.And the man in middle moves his hand to start the game."], "durations": null, "exact_frames_per_prompt": [17], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/0JMurscrsBs.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 21, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2622_1_0"}, {"texts": ["The man is eating watermelon while the man in grey t-shirt finish the eating and throw away the watermelon rind.And the people all are laughing while watching it."], "durations": null, "exact_frames_per_prompt": [34], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/0JMurscrsBs.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 21, "npz_gt_video_start_frame": 21, "npz_gt_video_end_frame": 55, "skip_frames_after_generation": 21, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2622_1_1"}, {"texts": ["A man in the middle wearing a camouflage uniform is moving his hands while two men are eating watermelon."], "durations": null, "exact_frames_per_prompt": [11], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/0JMurscrsBs.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 15, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2622_2_0"}, {"texts": ["The man is walking another two men appear on the right side."], "durations": null, "exact_frames_per_prompt": [35], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/0JMurscrsBs.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 15, "npz_gt_video_start_frame": 15, "npz_gt_video_end_frame": 50, "skip_frames_after_generation": 15, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2622_2_1"}, {"texts": ["A man wearing a shirt and pants holding the rope is walking in front of the camel on the soil surface along with carrying a person on its back, and another brown camel is sitting under the tree on the right."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/09AinCnKAE8.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2623_0_0"}, {"texts": ["A child is sitting on the back of a camel walking on the soil surface while a man wearing pink shirt and pants is holding a camel leash and another camel is sitting on the right side."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/09AinCnKAE8.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2623_1_0"}, {"texts": ["A white camel on the left side is walking towards the right side, keeping a child on its back and a brown camel is standing front on the right side on a brown soil surface."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/09AinCnKAE8.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2623_2_0"}, {"texts": ["A man wearing a blue shirt is standing next to the grill and talking. "], "durations": null, "exact_frames_per_prompt": [9], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/0qZT35lsw14.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 13, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2624_0_0"}, {"texts": ["The man holds the handle of the griller."], "durations": null, "exact_frames_per_prompt": [58], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/0qZT35lsw14.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 13, "npz_gt_video_start_frame": 13, "npz_gt_video_end_frame": 71, "skip_frames_after_generation": 13, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2624_0_1"}, {"texts": ["A person in black and white clothing is at first puts his hand on a black napkin."], "durations": null, "exact_frames_per_prompt": [19], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/25QmM3V_kGQ.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 23, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2626_0_0"}, {"texts": ["The person removes his hands."], "durations": null, "exact_frames_per_prompt": [24], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/25QmM3V_kGQ.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 23, "npz_gt_video_start_frame": 23, "npz_gt_video_end_frame": 47, "skip_frames_after_generation": 23, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2626_0_1"}, {"texts": ["The person again puts his hands on the napkin, and flips it to the other side."], "durations": null, "exact_frames_per_prompt": [21], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/25QmM3V_kGQ.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 47, "npz_gt_video_start_frame": 47, "npz_gt_video_end_frame": 68, "skip_frames_after_generation": 47, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2626_0_2"}, {"texts": ["The person folded the napkin."], "durations": null, "exact_frames_per_prompt": [12], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/25QmM3V_kGQ.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 68, "npz_gt_video_start_frame": 68, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 68, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2626_0_3"}, {"texts": ["A man wearing a blue shirt, blue gloves, and black pants is standing behind the table and is picking up the knife with his right hand."], "durations": null, "exact_frames_per_prompt": [12], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/1GHkC3Siirw.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 16, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2628_0_0"}, {"texts": ["The man is spreading the white cream on the brown bread with the knife while holding the bread with his left hand."], "durations": null, "exact_frames_per_prompt": [64], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/1GHkC3Siirw.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 16, "npz_gt_video_start_frame": 16, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 16, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2628_0_1"}, {"texts": ["A person on the right side wearing black pants is sitting on a wooden dock and feeding the orange koi fishes in the water while a baby wearing a white top is standing on the dock."], "durations": null, "exact_frames_per_prompt": [69], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/0GKDxzjXSdg.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 73, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2629_0_0"}, {"texts": ["A girl wearing a purple top is standing and coating something in the breadcrumbs in a glass bowl and looking in front."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/2UVGQ78T1vs.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_262_0_0"}, {"texts": ["A man wearing a red t-shirt and black jeans is sitting while bending his legs on the left side of the dock, holding a seed packet in his left hand and a group of fish is swimming into pond."], "durations": null, "exact_frames_per_prompt": [18], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/0GKDxzjXSdg.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 22, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2630_1_0"}, {"texts": ["The man is dropping the seeds into the pond and a baby wearing a white t-shirt is banding and then gets up on the left side."], "durations": null, "exact_frames_per_prompt": [51], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/0GKDxzjXSdg.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 22, "npz_gt_video_start_frame": 22, "npz_gt_video_end_frame": 73, "skip_frames_after_generation": 22, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2630_1_1"}, {"texts": ["A child whose upper half-body is visible wearing a white shirt is standing on the dock.  while fishes are moving in the water."], "durations": null, "exact_frames_per_prompt": [23], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/0GKDxzjXSdg.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 27, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2630_2_0"}, {"texts": ["The child starts picking up the seeds from the docks surface while tilting his body."], "durations": null, "exact_frames_per_prompt": [46], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/0GKDxzjXSdg.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 27, "npz_gt_video_start_frame": 27, "npz_gt_video_end_frame": 73, "skip_frames_after_generation": 27, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2630_2_1"}, {"texts": ["A boy wearing a black coat is standing near a snake cage and holding a white snake in his hands."], "durations": null, "exact_frames_per_prompt": [47], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/5B9pNSUOCzU.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 51, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2632_0_0"}, {"texts": ["The boy starts walking while carrying the snake."], "durations": null, "exact_frames_per_prompt": [29], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/5B9pNSUOCzU.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 51, "npz_gt_video_start_frame": 51, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 51, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2632_0_1"}, {"texts": ["A white snake is held by the boy, and the snake is moving in his hands."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/5B9pNSUOCzU.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2632_1_0"}, {"texts": ["A man wearing a red t-shirt is sitting showing his abs."], "durations": null, "exact_frames_per_prompt": [45], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-xUJmgXYHzc.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 49, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2635_0_0"}, {"texts": ["The man wearing a red t-shirt takes a burger from the table."], "durations": null, "exact_frames_per_prompt": [31], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-xUJmgXYHzc.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 49, "npz_gt_video_start_frame": 49, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 49, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2635_0_1"}, {"texts": ["A man wearing a white vest and blue shorts sitting in a wheelchair is pulling it backward."], "durations": null, "exact_frames_per_prompt": [19], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/456Ep7tqQJw.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 23, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2636_0_0"}, {"texts": ["The man wearing a white vest and blue shorts is pressing a button and talking."], "durations": null, "exact_frames_per_prompt": [20], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/456Ep7tqQJw.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 23, "npz_gt_video_start_frame": 23, "npz_gt_video_end_frame": 43, "skip_frames_after_generation": 23, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2636_0_1"}, {"texts": ["The man wearing a white vest and blue shorts is moving."], "durations": null, "exact_frames_per_prompt": [37], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/456Ep7tqQJw.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 43, "npz_gt_video_start_frame": 43, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 43, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2636_0_2"}, {"texts": ["A person whose only upper body is visible wearing a red t-shirt is folding the white paper on the notebook."], "durations": null, "exact_frames_per_prompt": [32], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-mOiPZGmKCQ.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 36, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2637_0_0"}, {"texts": ["The person is pasting the paper while sitting on the right side."], "durations": null, "exact_frames_per_prompt": [44], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-mOiPZGmKCQ.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 36, "npz_gt_video_start_frame": 36, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 36, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2637_0_1"}, {"texts": ["A person whose only hands are visible is putting the electric drill in an apple."], "durations": null, "exact_frames_per_prompt": [38], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/2RIBdKujTiY.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 42, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2638_0_0"}, {"texts": ["The person starts rotating it while holding a scrapper."], "durations": null, "exact_frames_per_prompt": [38], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/2RIBdKujTiY.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 42, "npz_gt_video_start_frame": 42, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 42, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2638_0_1"}, {"texts": ["A girl wearing a white top is walking while holding a packet, cutting it with a scissor."], "durations": null, "exact_frames_per_prompt": [29], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/0xA7JQTsgNk.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 33, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2639_0_0"}, {"texts": ["The girl taking out an object from the packet."], "durations": null, "exact_frames_per_prompt": [47], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/0xA7JQTsgNk.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 33, "npz_gt_video_start_frame": 33, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 33, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2639_0_1"}, {"texts": ["A man standing in a black sweater is holding a white box in his hand while a woman wearing a purple sweater is standing near the group of people and talking to them, a group of the people are sitting on the chair."], "durations": null, "exact_frames_per_prompt": [37], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/0bJjw6sFnbc.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 41, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_263_1_0"}, {"texts": ["The man is moving on a brown surface while the woman wearing purple sweater is sitting on a chair."], "durations": null, "exact_frames_per_prompt": [39], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/0bJjw6sFnbc.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 41, "npz_gt_video_start_frame": 41, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 41, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_263_1_1"}, {"texts": ["A woman standing in a pink sweater on the right side while a man wearing a black jacket is holding a box and moving around."], "durations": null, "exact_frames_per_prompt": [56], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/0bJjw6sFnbc.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 60, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_263_2_0"}, {"texts": ["The woman starts sitting on a brown chair while a group of people are sitting on chairs."], "durations": null, "exact_frames_per_prompt": [20], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/0bJjw6sFnbc.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 60, "npz_gt_video_start_frame": 60, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 60, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_263_2_1"}, {"texts": ["A girl wearing a grey t-shirt with prints is holding a bottle of orange liquid."], "durations": null, "exact_frames_per_prompt": [9], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/sdpCun_Qi-8.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 13, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2642_0_0"}, {"texts": ["The girl is posing, then raising her hands and moving her hands."], "durations": null, "exact_frames_per_prompt": [59], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/sdpCun_Qi-8.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 13, "npz_gt_video_start_frame": 13, "npz_gt_video_end_frame": 72, "skip_frames_after_generation": 13, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2642_0_1"}, {"texts": ["The girl holds the bottle towards her mouth."], "durations": null, "exact_frames_per_prompt": [8], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/sdpCun_Qi-8.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 72, "npz_gt_video_start_frame": 72, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 72, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2642_0_2"}, {"texts": ["A man wearing a white vest and gray jeans is sitting on a chair on the left side and counting the money with his hands."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-F-p4_qAti8.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2643_0_0"}, {"texts": ["A person whom hands are visible is fasting a nut with a wrench."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/0OI5F5zFgzA.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2653_0_0"}, {"texts": ["A person wearing black clothes is preparing sushi roll."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/17lO7VKLZOs.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2654_0_0"}, {"texts": ["A woman in black and white clothes is at first speaking to the woman in white hair while others start clapping"], "durations": null, "exact_frames_per_prompt": [59], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/0D1LA6j5DeI.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 63, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2655_2_0"}, {"texts": ["The woman shakes her hand with the girl in a blue uniform."], "durations": null, "exact_frames_per_prompt": [11], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/0D1LA6j5DeI.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 63, "npz_gt_video_start_frame": 63, "npz_gt_video_end_frame": 74, "skip_frames_after_generation": 63, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2655_2_1"}, {"texts": ["The woman hugs her."], "durations": null, "exact_frames_per_prompt": [5], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/0D1LA6j5DeI.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 74, "npz_gt_video_start_frame": 74, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 74, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2655_2_2"}, {"texts": ["A girl wearing a uniform is at first walking in the left direction."], "durations": null, "exact_frames_per_prompt": [57], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/0D1LA6j5DeI.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 7, "npz_gt_video_start_frame": 7, "npz_gt_video_end_frame": 64, "skip_frames_after_generation": 7, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2655_3_0"}, {"texts": ["The girl shakes her hand with the woman in black and white clothes."], "durations": null, "exact_frames_per_prompt": [12], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/0D1LA6j5DeI.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 64, "npz_gt_video_start_frame": 64, "npz_gt_video_end_frame": 76, "skip_frames_after_generation": 64, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2655_3_1"}, {"texts": ["The girl hugs her."], "durations": null, "exact_frames_per_prompt": [4], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/0D1LA6j5DeI.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 76, "npz_gt_video_start_frame": 76, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 76, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2655_3_2"}, {"texts": ["A girl wearing a white shirt and a blue blazer is walking while the crowd is applauding for her"], "durations": null, "exact_frames_per_prompt": [55], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/0D1LA6j5DeI.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 8, "npz_gt_video_start_frame": 8, "npz_gt_video_end_frame": 63, "skip_frames_after_generation": 8, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2656_0_0"}, {"texts": ["The girl is doing a handshake with the first woman."], "durations": null, "exact_frames_per_prompt": [8], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/0D1LA6j5DeI.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 63, "npz_gt_video_start_frame": 63, "npz_gt_video_end_frame": 71, "skip_frames_after_generation": 63, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2656_0_1"}, {"texts": ["The girl is hugging her after."], "durations": null, "exact_frames_per_prompt": [9], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/0D1LA6j5DeI.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 71, "npz_gt_video_start_frame": 71, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 71, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2656_0_2"}, {"texts": ["A man wearing a white-red shirt is sitting and holding a baby on his lap, feeding the baby with a blue spoon while the baby looks into the front up  camera."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-H2CqflbVCs.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2658_0_0"}, {"texts": ["A baby wearing a blue dress is lying on a man's lap, looking in front and eating food."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-H2CqflbVCs.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2658_1_0"}, {"texts": ["A man wearing a white shirt is standing, speaking, moving his hands, and touching a tie."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-hYdiA_v1_o.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2659_0_0"}, {"texts": ["A man wearing blue jeans is at first holding the boy from behind and a woman wearing a pink-white t-shirt is trying to plucking fruit from the tree with a stick."], "durations": null, "exact_frames_per_prompt": [52], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/1Vl5p7nuRNM.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 56, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_265_0_0"}, {"texts": ["The man wearing blue jeans leaves the boy and watches him."], "durations": null, "exact_frames_per_prompt": [24], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/1Vl5p7nuRNM.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 56, "npz_gt_video_start_frame": 56, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 56, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_265_0_1"}, {"texts": ["A boy wearing brown shorts is at first held from behind by the man while the woman is wearing a blue pant looking towards the tree showing something to  him"], "durations": null, "exact_frames_per_prompt": [51], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/1Vl5p7nuRNM.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 55, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_265_1_0"}, {"texts": ["The boy turns towards the woman and starts walking towards her while the man slightly runs on the right side"], "durations": null, "exact_frames_per_prompt": [13], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/1Vl5p7nuRNM.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 55, "npz_gt_video_start_frame": 55, "npz_gt_video_end_frame": 68, "skip_frames_after_generation": 55, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_265_1_1"}, {"texts": ["The boy holds her."], "durations": null, "exact_frames_per_prompt": [12], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/1Vl5p7nuRNM.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 68, "npz_gt_video_start_frame": 68, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 68, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_265_1_2"}, {"texts": ["A woman wearing a pink top is holding a stick in one hand while the man wearing a yellow t-shirt is standing leaning forward and holding the kid wearing a white t-shirt."], "durations": null, "exact_frames_per_prompt": [17], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/1Vl5p7nuRNM.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 21, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_265_2_0"}, {"texts": ["The woman is touching the leaves from the other hand, while being held by the boy from the front side."], "durations": null, "exact_frames_per_prompt": [58], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/1Vl5p7nuRNM.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 21, "npz_gt_video_start_frame": 21, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 21, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_265_2_1"}, {"texts": ["A man wearing white shirt is standing, speaking, and moving his hands, and touching a tie."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-hYdiA_v1_o.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2660_0_0"}, {"texts": ["A woman wearing blue and white cloth is sitting and presenting the weather forecast report on a television."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/0C8ryVH1J8w.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2662_0_0"}, {"texts": ["A woman whose hands are visible is dipping the chicken pieces into the black sauce."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-fzmWn0p2Dk.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2663_0_0"}, {"texts": ["A girl whose upper body is visible wearing a printed pink t-shirt and specs is mixing the yellow liquid while holding a steel spatula in the steel pot utensil while standing."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/06oYtk5kQnM.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2664_0_0"}, {"texts": ["A person whose hands are visible is cutting the top of a pineapple with a knife."], "durations": null, "exact_frames_per_prompt": [49], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/1U2B7T6tbFI.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 53, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2666_0_0"}, {"texts": ["The person puts the top part away."], "durations": null, "exact_frames_per_prompt": [13], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/1U2B7T6tbFI.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 53, "npz_gt_video_start_frame": 53, "npz_gt_video_end_frame": 66, "skip_frames_after_generation": 53, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2666_0_1"}, {"texts": ["The person starts cutting the bottom of the pineapple with the knife."], "durations": null, "exact_frames_per_prompt": [14], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/1U2B7T6tbFI.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 66, "npz_gt_video_start_frame": 66, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 66, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2666_0_2"}, {"texts": ["A woman whose head is visible is making her eyebrows with a black eyebrow pencil."], "durations": null, "exact_frames_per_prompt": [71], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-jD6Q23D9Q0.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 9, "npz_gt_video_start_frame": 9, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 9, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2669_0_0"}, {"texts": ["A person wearing a protection suit is at first bending towards the floor, and gets up."], "durations": null, "exact_frames_per_prompt": [10], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/6_U-dyFOVZw.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 14, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2671_0_0"}, {"texts": ["The person points toward the beehive frames."], "durations": null, "exact_frames_per_prompt": [29], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/6_U-dyFOVZw.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 14, "npz_gt_video_start_frame": 14, "npz_gt_video_end_frame": 43, "skip_frames_after_generation": 14, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2671_0_1"}, {"texts": ["The person again bends."], "durations": null, "exact_frames_per_prompt": [17], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/6_U-dyFOVZw.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 43, "npz_gt_video_start_frame": 43, "npz_gt_video_end_frame": 60, "skip_frames_after_generation": 43, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2671_0_2"}, {"texts": ["The person picks up the metal object and places it on the beehive frames."], "durations": null, "exact_frames_per_prompt": [20], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/6_U-dyFOVZw.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 60, "npz_gt_video_start_frame": 60, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 60, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2671_0_3"}, {"texts": ["A woman whose only hands and lower body is visible is mixing food in a bowl with a spoon."], "durations": null, "exact_frames_per_prompt": [69], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/0Ee1sPKgpSg.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 73, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2672_0_0"}, {"texts": ["A girl on the right side wearing a dark blue jacket is sitting on a bench with a white-black guinea pig while a girl wearing a pink jacket is sitting on the left with a brown-white guinea and a group of people are moving here and there on the green surface."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/0xZreisqYSo.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2676_0_0"}, {"texts": ["A girl in the middle wearing a pink jacket is sitting on the bench with a white-brown guinea pig while the girl wearing violet/black jacket with the white-black guinea pig."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/0xZreisqYSo.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2676_1_0"}, {"texts": ["A girl wearing a purple sweater is sitting on a bench while a girl wearing a pint jacket is sitting on a bench, holding a brown-white guinea pig in her lap, and petting it; a person wearing a black jeans is standing at the back; a person wearing black clothes is sitting on the right, and a few people are walking at the back."], "durations": null, "exact_frames_per_prompt": [44], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/0xZreisqYSo.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 36, "npz_gt_video_start_frame": 36, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 36, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2676_2_0"}, {"texts": ["A white-brown guinea pig is sitting in the lap of the second girl.\n while the white-black guinea pig on the first girl."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/0xZreisqYSo.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2676_5_0"}, {"texts": ["A girl wearing a blue jacket is sitting on a bench and caressing a grey-white guinea pig while a baby girl wearing pink outfit holding an animal"], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/0xZreisqYSo.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2677_0_0"}, {"texts": ["A girl wearing a pink jacket is sitting on a bench and caressing a brown-white guinea pig while a girl wearing a black jacket sitting on a bench and caressing the black-white guinea pig, some people are walking on the green surface in the backside, a girl wearing a purple sweater is sitting beside the first girl."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/0xZreisqYSo.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2677_1_0"}, {"texts": ["A brown-white guinea pig is sitting in the lap of the second girl and getting caressed by the second girl while the other girl is caressed the another guinea pig in her lap."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/0xZreisqYSo.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2677_8_0"}, {"texts": ["A man whose hand is visible is opening the engine cap."], "durations": null, "exact_frames_per_prompt": [38], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/0L4f4U_9lcU.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 42, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2678_0_0"}, {"texts": ["The man puts the funnel inside the engine and starts pouring the engine oil."], "durations": null, "exact_frames_per_prompt": [38], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/0L4f4U_9lcU.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 42, "npz_gt_video_start_frame": 42, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 42, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2678_0_1"}, {"texts": ["A man wearing a brown cap is sitting on his knee and ice fishing with an ice fishing tool."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-WhyUj4zd2c.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2680_0_0"}, {"texts": ["A man wearing a white t-shirt is playing golf on the green surface."], "durations": null, "exact_frames_per_prompt": [68], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/15TRan7ygBY.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 72, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2681_0_0"}, {"texts": ["A man wearing a black t-shirt and graphic pants is standing and holding a katana sword."], "durations": null, "exact_frames_per_prompt": [24], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/0AxgY7y4X30.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 28, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2682_0_0"}, {"texts": ["The man is cutting a pineapple."], "durations": null, "exact_frames_per_prompt": [6], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/0AxgY7y4X30.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 28, "npz_gt_video_start_frame": 28, "npz_gt_video_end_frame": 34, "skip_frames_after_generation": 28, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2682_0_1"}, {"texts": ["A man wearing a brown jacket is shearing the Jacob sheep."], "durations": null, "exact_frames_per_prompt": [9], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/03idfoav5Zk.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 13, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2687_0_0"}, {"texts": ["The man takes the wool from the straw surface while the sheep with wool on it moves towards another sheep"], "durations": null, "exact_frames_per_prompt": [15], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/03idfoav5Zk.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 13, "npz_gt_video_start_frame": 13, "npz_gt_video_end_frame": 28, "skip_frames_after_generation": 13, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2687_0_1"}, {"texts": ["A black-white Jacob sheep is getting sheared by the man."], "durations": null, "exact_frames_per_prompt": [8], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/03idfoav5Zk.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 12, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2687_1_0"}, {"texts": ["The Jacob sheep is standing on the straw surface while another sheep with fur is standing on the right and moving toward the first sheep."], "durations": null, "exact_frames_per_prompt": [48], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/03idfoav5Zk.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 12, "npz_gt_video_start_frame": 12, "npz_gt_video_end_frame": 60, "skip_frames_after_generation": 12, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2687_1_1"}, {"texts": ["A grey sheep is walking on the straw surface while another person lifts something from earth surface"], "durations": null, "exact_frames_per_prompt": [32], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/03idfoav5Zk.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 12, "npz_gt_video_start_frame": 12, "npz_gt_video_end_frame": 44, "skip_frames_after_generation": 12, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2687_2_0"}, {"texts": ["A woman wearing a black dress is standing and is speaking into the mic while looking front and at the podium, moving her left hand and then holds the podium with her left hand."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-CyUVQGZSrE.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_268_0_0"}, {"texts": ["A man wearing red clothes is holding a sheep and doing sheep shearing."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/0i0Z45QpHX8.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2691_0_0"}, {"texts": ["A sheep is lying on the wooden surface and being shared by the man in red clothes."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/0i0Z45QpHX8.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2691_2_0"}, {"texts": ["A man wearing a dark blue clothes whose hands are only visible is opening a small plastic pouch."], "durations": null, "exact_frames_per_prompt": [39], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/21jymvUzKus.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 43, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2692_0_0"}, {"texts": ["The man is  taking out the paper."], "durations": null, "exact_frames_per_prompt": [23], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/21jymvUzKus.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 43, "npz_gt_video_start_frame": 43, "npz_gt_video_end_frame": 66, "skip_frames_after_generation": 43, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2692_0_1"}, {"texts": ["The man is lifting up the green tag on the wooden surface."], "durations": null, "exact_frames_per_prompt": [14], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/21jymvUzKus.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 66, "npz_gt_video_start_frame": 66, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 66, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2692_0_2"}, {"texts": ["A man wearing a white t-shirt is speaking."], "durations": null, "exact_frames_per_prompt": [18], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/1umxvmop4Xg.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 22, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2694_0_0"}, {"texts": ["The man is eating something."], "durations": null, "exact_frames_per_prompt": [44], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/1umxvmop4Xg.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 22, "npz_gt_video_start_frame": 22, "npz_gt_video_end_frame": 66, "skip_frames_after_generation": 22, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2694_0_1"}, {"texts": ["The man is also doing hand gestures."], "durations": null, "exact_frames_per_prompt": [15], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/1umxvmop4Xg.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 66, "npz_gt_video_start_frame": 66, "npz_gt_video_end_frame": 81, "skip_frames_after_generation": 66, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2694_0_2"}, {"texts": ["A man wearing a black shirt is standing in the front."], "durations": null, "exact_frames_per_prompt": [8], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/4BllUgStR2I.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 12, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2695_0_0"}, {"texts": ["The man is making a drink."], "durations": null, "exact_frames_per_prompt": [34], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/4BllUgStR2I.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 12, "npz_gt_video_start_frame": 12, "npz_gt_video_end_frame": 46, "skip_frames_after_generation": 12, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2695_0_1"}, {"texts": ["The man takes a steel container."], "durations": null, "exact_frames_per_prompt": [14], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/4BllUgStR2I.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 46, "npz_gt_video_start_frame": 46, "npz_gt_video_end_frame": 60, "skip_frames_after_generation": 46, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2695_0_2"}, {"texts": ["The man starts doing flair."], "durations": null, "exact_frames_per_prompt": [20], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/4BllUgStR2I.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 60, "npz_gt_video_start_frame": 60, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 60, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2695_0_3"}, {"texts": ["A person whose hand is visible, holding a paper bag and putting a paper in it."], "durations": null, "exact_frames_per_prompt": [77], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/273F_L2VRo8.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 81, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2696_0_0"}, {"texts": ["A man whose hands are visible wearing a silver watch in his left hand is putting the black book cover on the table."], "durations": null, "exact_frames_per_prompt": [22], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/273F_L2VRo8.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 26, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2697_0_0"}, {"texts": ["The man is picking up the folded brown card."], "durations": null, "exact_frames_per_prompt": [24], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/273F_L2VRo8.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 26, "npz_gt_video_start_frame": 26, "npz_gt_video_end_frame": 50, "skip_frames_after_generation": 26, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2697_0_1"}, {"texts": ["The man is opening the folded brown card."], "durations": null, "exact_frames_per_prompt": [28], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/273F_L2VRo8.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 50, "npz_gt_video_start_frame": 50, "npz_gt_video_end_frame": 78, "skip_frames_after_generation": 50, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2697_0_2"}, {"texts": ["The man is putting the brown card on the black book cover."], "durations": null, "exact_frames_per_prompt": [3], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/273F_L2VRo8.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 78, "npz_gt_video_start_frame": 78, "npz_gt_video_end_frame": 81, "skip_frames_after_generation": 78, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2697_0_3"}, {"texts": ["A person whose hands are visible is peeling a potato with a peeler."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/0TQtPfWYpU4.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_26_0_0"}, {"texts": ["A person wearing a brown jacket is standing on the left side, holding a white plate with mutton in it and picking the mutton pieces and putting it on the grill."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/1tmPfwetaRs.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2703_0_0"}, {"texts": ["A man wearing a black t-shirt is playing golf while speaking and instructing."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/1zzA9uG8JFw.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2705_0_0"}, {"texts": ["A man wearing a black suit is speaking."], "durations": null, "exact_frames_per_prompt": [29], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/6lLJU6qWTUQ.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 33, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2710_0_0"}, {"texts": ["The man is tying a red bow tie on his neck with his hands."], "durations": null, "exact_frames_per_prompt": [47], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/6lLJU6qWTUQ.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 33, "npz_gt_video_start_frame": 33, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 33, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2710_0_1"}, {"texts": ["A boy wearing a green jacket is sitting and eating food."], "durations": null, "exact_frames_per_prompt": [21], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/3NKLGx_EphA.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 25, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2712_0_0"}, {"texts": ["The boy is moving his camera."], "durations": null, "exact_frames_per_prompt": [55], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/3NKLGx_EphA.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 25, "npz_gt_video_start_frame": 25, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 25, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2712_0_1"}, {"texts": ["A woman wearing white-black clothes is standing and combing a black dog with a comb."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/1QDd-c4yJDM.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2713_0_0"}, {"texts": ["A black dog is standing on the gray surface, and getting combed by a woman."], "durations": null, "exact_frames_per_prompt": [54], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/1QDd-c4yJDM.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 58, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2713_1_0"}, {"texts": ["A woman wearing a grey t-shirt is sitting on the bed."], "durations": null, "exact_frames_per_prompt": [32], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/rR70TWY717E.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 36, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2717_0_0"}, {"texts": ["The woman wearing a grey t-shirt is trying to open the bottle with the bottle opener."], "durations": null, "exact_frames_per_prompt": [44], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/rR70TWY717E.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 36, "npz_gt_video_start_frame": 36, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 36, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2717_0_1"}, {"texts": ["A man wearing a black t-shirt is standing on the floor and cutting a watermelon with a knife."], "durations": null, "exact_frames_per_prompt": [48], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/a6iwL9KxxkM.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 7, "npz_gt_video_start_frame": 7, "npz_gt_video_end_frame": 55, "skip_frames_after_generation": 7, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2718_0_0"}, {"texts": ["The man puts the half watermelon on the counter."], "durations": null, "exact_frames_per_prompt": [8], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/a6iwL9KxxkM.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 55, "npz_gt_video_start_frame": 55, "npz_gt_video_end_frame": 63, "skip_frames_after_generation": 55, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2718_0_1"}, {"texts": ["The man again starts cutting the watermelon."], "durations": null, "exact_frames_per_prompt": [17], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/a6iwL9KxxkM.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 63, "npz_gt_video_start_frame": 63, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 63, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2718_0_2"}, {"texts": ["A man wearing a light green shirt, blue jeans, a gray cap, and goggles is sitting on the grass, holding a cactus with a stick and roasting it over the fire."], "durations": null, "exact_frames_per_prompt": [77], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/0YaRP4EK7ZY.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 81, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2720_0_0"}, {"texts": ["A man whose hands and head are visible, is pulling out the yellow object from the car's machine."], "durations": null, "exact_frames_per_prompt": [39], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/1zGMNq1b62A.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 43, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2721_0_0"}, {"texts": ["The man sides the black tub on the surface, wiping the car engine with the yellow cloth."], "durations": null, "exact_frames_per_prompt": [17], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/1zGMNq1b62A.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 43, "npz_gt_video_start_frame": 43, "npz_gt_video_end_frame": 60, "skip_frames_after_generation": 43, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2721_0_1"}, {"texts": ["The man is putting the white object into the engine of the car."], "durations": null, "exact_frames_per_prompt": [12], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/1zGMNq1b62A.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 60, "npz_gt_video_start_frame": 60, "npz_gt_video_end_frame": 72, "skip_frames_after_generation": 60, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2721_0_2"}, {"texts": ["A man wearing a blue shirt is holding a pineapple."], "durations": null, "exact_frames_per_prompt": [39], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/38u4-bjUjE0.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 43, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2722_0_0"}, {"texts": ["The man is cutting a pineapple."], "durations": null, "exact_frames_per_prompt": [38], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/38u4-bjUjE0.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 43, "npz_gt_video_start_frame": 43, "npz_gt_video_end_frame": 81, "skip_frames_after_generation": 43, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2722_0_1"}, {"texts": ["A woman wearing a blue sweater is milking the cow."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/0n1We0bkcNw.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2725_0_0"}, {"texts": ["A white cow is standing and getting milked by a woman."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/0n1We0bkcNw.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2725_1_0"}, {"texts": ["A person whose hand is visible wearing a red t-shirt is taking measurements on a paper sheet."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/0bcPHSkki4I.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2731_0_0"}, {"texts": ["A boy wearing a black t-shirt is sitting and eating a burger while a man wearing a grey t-shirt is sitting left side of the boy and looking at him."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/2BqRWvR7pNA.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2733_0_0"}, {"texts": ["A person whose only hands are visible is wearing white gloves and tying a white dressing on the other person's hand."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-QspPN2hnRQ.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2734_0_0"}, {"texts": ["A person wearing a black t-shirt is getting the dressing tied by another person using a white supporter for his hand."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-QspPN2hnRQ.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2734_1_0"}, {"texts": ["A boy wearing a blue t-shirt is sitting on the grassy surface and he is at first caressing the rabbit with one hand on his ears while the other girl sits down and caressing the rabbit."], "durations": null, "exact_frames_per_prompt": [10], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/1FsP1VEEOaE.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 14, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2735_0_0"}, {"texts": ["The boy wearing a blue t-shirt is closing his both ears with his hands while the kids gathered near the rabbit."], "durations": null, "exact_frames_per_prompt": [66], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/1FsP1VEEOaE.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 14, "npz_gt_video_start_frame": 14, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 14, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2735_0_1"}, {"texts": ["A woman wearing blue jeans is standing on the right side of the boy in a blue t-shirt while a boy wearing a blue t-shirt sits on the grassy surface while closing his ears with his hands, a group of kids comes and sits on the grassy surface, and a group of people are standing on the grassy surface."], "durations": null, "exact_frames_per_prompt": [53], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/1FsP1VEEOaE.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 57, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2735_2_0"}, {"texts": ["A white rabbit is sitting on the grassy surface and being caressed while the children surrounds the rabbit and try to caress him"], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/1FsP1VEEOaE.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2735_4_0"}, {"texts": ["A woman wearing a white printed t-shirt is sitting on a wooden chair and drinking a beverage from a straw while talking and smiling."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/2rhkA8PYXu0.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2736_0_0"}, {"texts": ["A brown horse is tied with the rope."], "durations": null, "exact_frames_per_prompt": [30], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/2JwNALMN1sE.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 34, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2739_0_0"}, {"texts": ["The horse is moving his head on the cleaning brush."], "durations": null, "exact_frames_per_prompt": [46], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/2JwNALMN1sE.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 34, "npz_gt_video_start_frame": 34, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 34, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2739_0_1"}, {"texts": ["A person wearing a green cloth, whose hand is visible, is holding the cleaning brush and rubbing on the horse's head."], "durations": null, "exact_frames_per_prompt": [52], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/2JwNALMN1sE.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 28, "npz_gt_video_start_frame": 28, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 28, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2739_1_0"}, {"texts": ["A person whose only hands are visible is pouring the egg white into the bowl and holding the egg with egg yolk."], "durations": null, "exact_frames_per_prompt": [31], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/3bbuOrmqm74.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 35, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_273_0_0"}, {"texts": ["A person whose hands and lower body are visible is setting up a red napkin on a table."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/1N_q5clBlNA.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2742_0_0"}, {"texts": ["A white-black dog is walking after the first dog while a kid wearing a black t-shirt is walking behind the dog while holding the dog's belt."], "durations": null, "exact_frames_per_prompt": [71], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-nt0YjZdboY.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 9, "npz_gt_video_start_frame": 9, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 9, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2743_2_0"}, {"texts": ["A baby boy wearing a black t-shirt is walking after the dogs on the floor."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-nt0YjZdboY.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2743_3_0"}, {"texts": ["A man wearing a yellow turban is walking in the desert."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-gcoV6rYgyE.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2744_0_0"}, {"texts": ["A person wearing black jacket and yellow turban is walking in the desert in forward direction."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-gcoV6rYgyE.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2745_0_0"}, {"texts": ["A man wearing purple clothes is walking on the sand surface while holding the first camel's harness.\n while others are sitting on the camel's back and enjoying the ride"], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-MoQEuZbzDY.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2746_1_0"}, {"texts": ["A camel is carrying two men on its back and walking on the sand surface ahead of the other camel while a man wearing blue robe is lifting camel"], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-MoQEuZbzDY.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2746_2_0"}, {"texts": ["A camel is carrying two women on its back, is tied to the first camel by a rope, and is walking on the sand surface behind the first camel while the first camel carries two other men and is being guided by a person walking infront."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-MoQEuZbzDY.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2746_3_0"}, {"texts": ["A man wearing a blue cap is eating something.\n"], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/2doPoIHWJCo.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2747_0_0"}, {"texts": ["A boy wearing black clothes and a cap is sitting, laughing and eating something."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/2doPoIHWJCo.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2748_0_0"}, {"texts": ["A man wearing a white t-shirt is standing, taking red chillies from the chopping board."], "durations": null, "exact_frames_per_prompt": [33], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/5SesONQCA5k.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 37, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2749_0_0"}, {"texts": ["The man is making sausage by pressing from the top."], "durations": null, "exact_frames_per_prompt": [43], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/5SesONQCA5k.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 37, "npz_gt_video_start_frame": 37, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 37, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2749_0_1"}, {"texts": ["A woman wearing a gray t-shirt and blue jeans is standing and speaking."], "durations": null, "exact_frames_per_prompt": [30], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/3Ov86yTJik4.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 34, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2750_0_0"}, {"texts": ["The woman picking up the bag."], "durations": null, "exact_frames_per_prompt": [23], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/3Ov86yTJik4.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 34, "npz_gt_video_start_frame": 34, "npz_gt_video_end_frame": 57, "skip_frames_after_generation": 34, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2750_0_1"}, {"texts": ["The woman dropping down the bag."], "durations": null, "exact_frames_per_prompt": [7], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/3Ov86yTJik4.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 57, "npz_gt_video_start_frame": 57, "npz_gt_video_end_frame": 64, "skip_frames_after_generation": 57, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2750_0_2"}, {"texts": ["The woman opening the washing machine door."], "durations": null, "exact_frames_per_prompt": [17], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/3Ov86yTJik4.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 64, "npz_gt_video_start_frame": 64, "npz_gt_video_end_frame": 81, "skip_frames_after_generation": 64, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2750_0_3"}, {"texts": ["A person whose hand is visible is making scrambled eggs in a pan."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/0p55jTqnXD8.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2751_0_0"}, {"texts": ["A person is peeling an apple.\n"], "durations": null, "exact_frames_per_prompt": [34], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/03xO1H48Kas.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 38, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2752_0_0"}, {"texts": ["A man wearing a grey checked t-shirt is holding a vegetable peeler and shaking his hands in the sink."], "durations": null, "exact_frames_per_prompt": [9], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/03xO1H48Kas.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 13, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2753_0_0"}, {"texts": ["The man takes an apple."], "durations": null, "exact_frames_per_prompt": [21], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/03xO1H48Kas.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 13, "npz_gt_video_start_frame": 13, "npz_gt_video_end_frame": 34, "skip_frames_after_generation": 13, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2753_0_1"}, {"texts": ["The man starts putting the apple on the drilling machine."], "durations": null, "exact_frames_per_prompt": [4], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/03xO1H48Kas.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 34, "npz_gt_video_start_frame": 34, "npz_gt_video_end_frame": 38, "skip_frames_after_generation": 34, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2753_0_2"}, {"texts": ["A man in a white-grey check shirt bends over, a sheep lies on his leg, and he cuts the sheep's wool with scissors."], "durations": null, "exact_frames_per_prompt": [49], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-yS8lc7rxzk.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 53, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2754_0_0"}, {"texts": ["A white sheep is lying on the legs of the man and getting a wool cut with the scissors by the man."], "durations": null, "exact_frames_per_prompt": [49], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-yS8lc7rxzk.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 53, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2754_1_0"}, {"texts": ["A person whose hand is visible is holding a spatula and a bowl."], "durations": null, "exact_frames_per_prompt": [10], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/282ALbZe7Ek.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 14, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2755_0_0"}, {"texts": ["The person whose hand is visible is pouring the egg mixture in a frying pan."], "durations": null, "exact_frames_per_prompt": [66], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/282ALbZe7Ek.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 14, "npz_gt_video_start_frame": 14, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 14, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2755_0_1"}, {"texts": ["A girl wearing a gray t-shirt and jeans is standing and holding a dead snake while a boy wearing a blue t-shirt is sitting on the cement road and looking at the girl."], "durations": null, "exact_frames_per_prompt": [50], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/1QBfBqXASIc.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 54, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2756_0_0"}, {"texts": ["The girl throws the snake on the ground while the boy touches the ground with his left hand."], "durations": null, "exact_frames_per_prompt": [16], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/1QBfBqXASIc.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 54, "npz_gt_video_start_frame": 54, "npz_gt_video_end_frame": 70, "skip_frames_after_generation": 54, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2756_0_1"}, {"texts": ["A boy wearing a gray vest and jeans is sitting on his knee and looking at the dead snake while the girl wearing grey t-shirt is holding the snake and then throws the snake on ground"], "durations": null, "exact_frames_per_prompt": [66], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/1QBfBqXASIc.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 70, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2756_1_0"}, {"texts": ["A man wearing a gray jacket and black pants is sitting on the first donkey and is riding a donkey behind the second donkey while a man wearing an orange jacket is walking and pulling the rope on the donkey, then puts his hand in the pocket of the donkey's cover."], "durations": null, "exact_frames_per_prompt": [55], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/5Y52IdfKNME.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 59, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2757_1_0"}, {"texts": ["A man wearing an orange jacket and black pants is walking on the road with the first donkey while holding a harness of the donkey while a woman on a gray jacket sits on a second donkey and riding on the road."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/5Y52IdfKNME.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2757_3_0"}, {"texts": ["A black donkey is walking on the road ahead of the first donkey while the woman in the gray hoodie sits on the donkey and riding on the surface"], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/5Y52IdfKNME.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2757_5_0"}, {"texts": ["A woman wearing an orange jacket is riding a brown horse and crossing the obstacles with it."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/0JHo32zbmoU.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2758_0_0"}, {"texts": ["A girl wearing a white jacket is standing and playing golf."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/1dgmi48YLwc.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2761_0_0"}, {"texts": ["A woman wearing black pants is moving her golf stick while standing."], "durations": null, "exact_frames_per_prompt": [8], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/1dgmi48YLwc.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 12, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2762_0_0"}, {"texts": ["The woman bends to pick a golf tee."], "durations": null, "exact_frames_per_prompt": [41], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/1dgmi48YLwc.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 12, "npz_gt_video_start_frame": 12, "npz_gt_video_end_frame": 53, "skip_frames_after_generation": 12, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2762_0_1"}, {"texts": ["A man wearing a blue t-shirt is speaking."], "durations": null, "exact_frames_per_prompt": [23], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/2cdNOY6zKHU.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 27, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2763_0_0"}, {"texts": ["The man is playing golf in the golf course."], "durations": null, "exact_frames_per_prompt": [29], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/2cdNOY6zKHU.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 27, "npz_gt_video_start_frame": 27, "npz_gt_video_end_frame": 56, "skip_frames_after_generation": 27, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2763_0_1"}, {"texts": ["A man wearing blue t-shirt is standing and speaking."], "durations": null, "exact_frames_per_prompt": [24], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/2cdNOY6zKHU.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 28, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2764_0_0"}, {"texts": ["The man is holding a golf stick."], "durations": null, "exact_frames_per_prompt": [16], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/2cdNOY6zKHU.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 28, "npz_gt_video_start_frame": 28, "npz_gt_video_end_frame": 44, "skip_frames_after_generation": 28, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2764_0_1"}, {"texts": ["The man is hitting a golf ball with his golf stick."], "durations": null, "exact_frames_per_prompt": [12], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/2cdNOY6zKHU.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 44, "npz_gt_video_start_frame": 44, "npz_gt_video_end_frame": 56, "skip_frames_after_generation": 44, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2764_0_2"}, {"texts": ["A boy whose only upper body is visible wearing a black t-shirt is sitting on a brown couch and is drinking milk from the drinking straw while holding a drinking straw in his right hand, and is dipping the drinking straw into the transparent glass and speaking."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/0MV9MX1-sJI.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2765_0_0"}, {"texts": ["A baby wearing a red striped shirt is sitting on a black floor holding a book and reading it."], "durations": null, "exact_frames_per_prompt": [42], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/08fZUmboS4g.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 46, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2766_0_0"}, {"texts": ["The baby flipping its page."], "durations": null, "exact_frames_per_prompt": [34], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/08fZUmboS4g.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 46, "npz_gt_video_start_frame": 46, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 46, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2766_0_1"}, {"texts": ["A person wearing a red sweatshirt is sitting and folding the green-white paper on the wooden table."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/1uRgl3lXFZI.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2767_0_0"}, {"texts": ["A person wearing a red top is sitting, folding paper, and making origami."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/1uRgl3lXFZI.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2768_0_0"}, {"texts": ["A man wearing a dark gray dress is lying under the car on the gray road and speaking while looking in front.\n"], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-Zj4nqoOtJ4.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2769_0_0"}, {"texts": ["A girl wearing a light-yellow top is standing on the grey floor and folding a pink t-shirt."], "durations": null, "exact_frames_per_prompt": [59], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/0JkfkBxyv6A.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 63, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_276_0_0"}, {"texts": ["The girl is raising her hand and showing fingers while holding the pink t-shirt."], "durations": null, "exact_frames_per_prompt": [17], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/0JkfkBxyv6A.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 63, "npz_gt_video_start_frame": 63, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 63, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_276_0_1"}, {"texts": ["A woman wearing a white shirt is standing and rolling sushi."], "durations": null, "exact_frames_per_prompt": [56], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/2tz76SScyE0.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 9, "npz_gt_video_start_frame": 9, "npz_gt_video_end_frame": 65, "skip_frames_after_generation": 9, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2770_1_0"}, {"texts": ["The woman takes a knife from the counter."], "durations": null, "exact_frames_per_prompt": [16], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/2tz76SScyE0.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 65, "npz_gt_video_start_frame": 65, "npz_gt_video_end_frame": 81, "skip_frames_after_generation": 65, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2770_1_1"}, {"texts": ["A girl wearing a printed pink top and pink shorts is sitting on a chair with her knees, holding a fork and then speaking while rolling the noodles with a fork in the bowl."], "durations": null, "exact_frames_per_prompt": [52], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/18nXGrBvNJE.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 56, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2771_0_0"}, {"texts": ["The girl starts eating the noodles."], "durations": null, "exact_frames_per_prompt": [24], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/18nXGrBvNJE.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 56, "npz_gt_video_start_frame": 56, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 56, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2771_0_1"}, {"texts": ["A boy wearing a green t-shirt is holding a piece of watermelon and walking in a restaurant."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-ZlDSY-03Z4.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2772_0_0"}, {"texts": ["A man wearing black clothes picks up a golf stick."], "durations": null, "exact_frames_per_prompt": [16], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-05APhKiIXc.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 20, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2773_0_0"}, {"texts": ["The man walks towards the right."], "durations": null, "exact_frames_per_prompt": [53], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-05APhKiIXc.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 20, "npz_gt_video_start_frame": 20, "npz_gt_video_end_frame": 73, "skip_frames_after_generation": 20, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2773_0_1"}, {"texts": ["The man is adjusting the red flag on the green grass surface."], "durations": null, "exact_frames_per_prompt": [7], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-05APhKiIXc.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 73, "npz_gt_video_start_frame": 73, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 73, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2773_0_2"}, {"texts": ["A man wearing black clothes is walking while holding the golf stick in his hand."], "durations": null, "exact_frames_per_prompt": [67], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-05APhKiIXc.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 71, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2774_0_0"}, {"texts": ["The man touches the red flag."], "durations": null, "exact_frames_per_prompt": [9], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-05APhKiIXc.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 71, "npz_gt_video_start_frame": 71, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 71, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2774_0_1"}, {"texts": ["A person whose only a hand is visible is wearing a black cloth and writing on a whiteboard with a marker."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-nAuWk_Hd5M.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2775_0_0"}, {"texts": ["A woman wearing a black t-shirt is standing on the soil surface."], "durations": null, "exact_frames_per_prompt": [17], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-wgCR4wk_dU.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 21, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2776_0_0"}, {"texts": ["The woman is feeding a group of goats, holding a white cup then starts showing her hands."], "durations": null, "exact_frames_per_prompt": [57], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-wgCR4wk_dU.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 21, "npz_gt_video_start_frame": 21, "npz_gt_video_end_frame": 78, "skip_frames_after_generation": 21, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2776_0_1"}, {"texts": ["A boy wearing a black t-shirt is walking while a man wearing colorful shirt sitting on the left side drinking something"], "durations": null, "exact_frames_per_prompt": [12], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/--uWuXty1jw.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 16, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2778_0_0"}, {"texts": ["The boy puts the bag on the table.  while a boy wearing colorful shirt sitting on the left side put the cup on the table"], "durations": null, "exact_frames_per_prompt": [11], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/--uWuXty1jw.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 16, "npz_gt_video_start_frame": 16, "npz_gt_video_end_frame": 27, "skip_frames_after_generation": 16, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2778_0_1"}, {"texts": ["The boy starts opening it while a boy wearing colorful shirt sitting on the left side washing his mouth in tissues"], "durations": null, "exact_frames_per_prompt": [24], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/--uWuXty1jw.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 27, "npz_gt_video_start_frame": 27, "npz_gt_video_end_frame": 51, "skip_frames_after_generation": 27, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2778_0_2"}, {"texts": ["A boy wearing a purple-white shirt is sitting and drinking something while a boy wearing black clothes comes by holding a black bag, he keeps it on the table while looking at the first boy."], "durations": null, "exact_frames_per_prompt": [24], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/--uWuXty1jw.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 28, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2778_1_0"}, {"texts": ["The boy is cleaning his mouth with a tissue paper while a man wearing a white t-shirt is sitting opposite the boy and smiling."], "durations": null, "exact_frames_per_prompt": [23], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/--uWuXty1jw.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 28, "npz_gt_video_start_frame": 28, "npz_gt_video_end_frame": 51, "skip_frames_after_generation": 28, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2778_1_1"}, {"texts": ["A person wearing a blue t-shirt is sitting on a black floor and wrapping a white sheet in a horse leg."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/1DpLvdJgWHg.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2780_0_0"}, {"texts": ["A brown horse is standing on a black surface while a woman is wrapping a cloth on horse leg."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/1DpLvdJgWHg.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2780_1_0"}, {"texts": ["A man wearing a green t-shirt is barbecuing food on a black barbecue."], "durations": null, "exact_frames_per_prompt": [59], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-rBz9EEx23M.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 63, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2781_0_0"}, {"texts": ["The man closes the barbecue lid."], "durations": null, "exact_frames_per_prompt": [17], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-rBz9EEx23M.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 63, "npz_gt_video_start_frame": 63, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 63, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2781_0_1"}, {"texts": ["A woman whose hands are visible, is breaking an egg into the pan."], "durations": null, "exact_frames_per_prompt": [24], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/5cQJJfPnMPo.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 28, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2782_0_0"}, {"texts": ["The woman added cheese, and started to saute the food."], "durations": null, "exact_frames_per_prompt": [52], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/5cQJJfPnMPo.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 28, "npz_gt_video_start_frame": 28, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 28, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2782_0_1"}, {"texts": ["A man wearing black clothes is standing in the kitchen and is flipping the food in the pan."], "durations": null, "exact_frames_per_prompt": [31], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-RW9M6NXMHs.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 35, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2785_0_0"}, {"texts": ["A man wearing a black t-shirt is standing and flipping the pancake from a black pan."], "durations": null, "exact_frames_per_prompt": [31], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-RW9M6NXMHs.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 35, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2786_0_0"}, {"texts": ["A woman wearing gray clothes is standing, doing hand gestures and talking on the grass surface.\n"], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-f0mGPf6TrM.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2787_0_0"}, {"texts": ["A girl wearing shades is holding a burger and is eating it while sitting inside a car."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/12Icqn94FZc.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2788_0_0"}, {"texts": ["A woman wearing a maroon top, sitting in a car speaking, eating, and holding a burger."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/12Icqn94FZc.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2789_0_0"}, {"texts": ["A person whose only hand is visible is caressing a brown bird while the bird pecks his own body."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/0Wo98IdOc08.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2791_0_0"}, {"texts": ["A brown bird is sitting inside a cage and scratching itself while a man scratching the bird from finger"], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/0Wo98IdOc08.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2791_1_0"}, {"texts": ["A man wearing a red t-shirt is sitting. "], "durations": null, "exact_frames_per_prompt": [26], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/3jMmnh_3OG4.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 30, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2792_0_0"}, {"texts": ["The man is opening the thread."], "durations": null, "exact_frames_per_prompt": [22], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/3jMmnh_3OG4.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 30, "npz_gt_video_start_frame": 30, "npz_gt_video_end_frame": 52, "skip_frames_after_generation": 30, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2792_0_1"}, {"texts": ["The man is taking out the bag dipped in the white bucket."], "durations": null, "exact_frames_per_prompt": [28], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/3jMmnh_3OG4.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 52, "npz_gt_video_start_frame": 52, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 52, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2792_0_2"}, {"texts": ["A woman wearing a white dress is holding an award and speaking on a mic."], "durations": null, "exact_frames_per_prompt": [69], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/1ncfAzpVtPM.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 11, "npz_gt_video_start_frame": 11, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 11, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2797_0_0"}, {"texts": ["A person wearing a blue t-shirt holding a golf stick and touching the bottom of the golf stick."], "durations": null, "exact_frames_per_prompt": [38], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/0MbrHvntEr0.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 42, "npz_gt_video_start_frame": 42, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 42, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2799_1_0"}, {"texts": ["A woman wearing a grey top is sitting on the right side and caressing a cat while the cat is standing and moving his head."], "durations": null, "exact_frames_per_prompt": [44], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-D03cnNac3g.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 48, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_27_0_0"}, {"texts": ["The woman moves her hand in the front then the cat tap her."], "durations": null, "exact_frames_per_prompt": [32], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-D03cnNac3g.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 48, "npz_gt_video_start_frame": 48, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 48, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_27_0_1"}, {"texts": ["A brown-white-black cat is standing with its two leg on the thigh of the woman and getting caressed by the woman then starts moving its head."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-D03cnNac3g.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_27_1_0"}, {"texts": ["A person wearing a blue t-shirt holding a golf stick and touching the bottom of a golf stick."], "durations": null, "exact_frames_per_prompt": [38], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/0MbrHvntEr0.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 42, "npz_gt_video_start_frame": 42, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 42, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2800_1_0"}, {"texts": ["A baby boy wearing a green designer t-shirt and green pajamas is sitting on a brown couch, and putting coins into a piggy bank."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/0hGwO9SlZ6s.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2801_0_0"}, {"texts": ["A girl wearing a peach coloured t-shirt is sitting in the front and crying while joining her hands."], "durations": null, "exact_frames_per_prompt": [77], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-QFr0bEZ61k.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 81, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2802_0_0"}, {"texts": ["A person whose hands and chest is visible is adjusting the cookie batter in a silver dish with a knife."], "durations": null, "exact_frames_per_prompt": [68], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/1bgiMcUBDes.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 72, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2805_0_0"}, {"texts": ["The person is putting the dish into an oven."], "durations": null, "exact_frames_per_prompt": [8], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/1bgiMcUBDes.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 72, "npz_gt_video_start_frame": 72, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 72, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2805_0_1"}, {"texts": ["A person whose hand is visible is holding the blue bow tie."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-iHP2acD4wQ.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2810_0_0"}, {"texts": ["A person whose hand is visible is holding a necktie and arranging it."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-iHP2acD4wQ.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2811_0_0"}, {"texts": ["A woman wearing a blue suit is milking the cow."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/0SYsQfmAofo.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2813_0_0"}, {"texts": ["A white cow is getting milked by the woman in a blue suit."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/0SYsQfmAofo.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2813_1_0"}, {"texts": ["A woman wearing multi-colored clothes is milking a white cow with her hands."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/0SYsQfmAofo.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2814_0_0"}, {"texts": ["A white cow is standing and is being milked by the woman."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/0SYsQfmAofo.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2814_1_0"}, {"texts": ["A person whose hand is visible is writing on a white sheet with a pen."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/2HQkvi74tD4.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2815_0_0"}, {"texts": ["A girl whose only hands are visible is at first applying glue on the envelope."], "durations": null, "exact_frames_per_prompt": [29], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/1bGx_V_ylEk.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 33, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2816_0_0"}, {"texts": ["The girl whose only hands are visible is closing the envelope."], "durations": null, "exact_frames_per_prompt": [47], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/1bGx_V_ylEk.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 33, "npz_gt_video_start_frame": 33, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 33, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2816_0_1"}, {"texts": ["A woman wearing a olive green jacket is walking and holding the dog leash on the left hand.\n while a black brown dog is running forward."], "durations": null, "exact_frames_per_prompt": [47], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-YCuMrVTf0U.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 19, "npz_gt_video_start_frame": 19, "npz_gt_video_end_frame": 66, "skip_frames_after_generation": 19, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2817_0_0"}, {"texts": ["A black-brown dog is running at the right side to left side on the brown road and snow surface while the person in the black snow suit walking along with black-brown dog."], "durations": null, "exact_frames_per_prompt": [53], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-YCuMrVTf0U.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 57, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2817_1_0"}, {"texts": ["A boy wearing white shorts and white shoes is playing golf."], "durations": null, "exact_frames_per_prompt": [54], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/0kRMZvqF1O0.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 58, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2818_0_0"}, {"texts": ["The boy wearing white shorts and white shoes is picking a golf ball."], "durations": null, "exact_frames_per_prompt": [21], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/0kRMZvqF1O0.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 58, "npz_gt_video_start_frame": 58, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 58, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2818_0_1"}, {"texts": ["A boy wearing white shorts is playing golf."], "durations": null, "exact_frames_per_prompt": [55], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/0kRMZvqF1O0.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 59, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2819_0_0"}, {"texts": ["The boy is picking a golf ball."], "durations": null, "exact_frames_per_prompt": [21], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/0kRMZvqF1O0.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 59, "npz_gt_video_start_frame": 59, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 59, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2819_0_1"}, {"texts": ["A woman wearing a multi-colored top is holding the slice of watermelon and dropping the diced watermelon in a bowl."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/Acqr9yFH9Ts.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_281_0_0"}, {"texts": ["A person is getting his back tattooed with a skull and a dagger."], "durations": null, "exact_frames_per_prompt": [51], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/0z38AVmVQLg.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 55, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2820_0_0"}, {"texts": ["A person wearing black gloves holding a tattoo machine is drawing a tattoo on the back of the first person."], "durations": null, "exact_frames_per_prompt": [27], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/0z38AVmVQLg.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 31, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2820_1_0"}, {"texts": ["The person then cleans it with a tissue."], "durations": null, "exact_frames_per_prompt": [24], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/0z38AVmVQLg.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 31, "npz_gt_video_start_frame": 31, "npz_gt_video_end_frame": 55, "skip_frames_after_generation": 31, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2820_1_1"}, {"texts": ["A woman in different clothes is sitting in the news studio and broadcasting the news.\n"], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/33bvixQ1sL0.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2823_0_ms_0"}, {"texts": ["A man wearing an olive green jacket is sitting on the dry leaf surface while the brown dog sits with him and looks into the bag."], "durations": null, "exact_frames_per_prompt": [17], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/5BJF1ryRtxY.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 21, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2826_0_0"}, {"texts": ["The man wearing an olive green jacket opens a small bag while the dog stands up and turns left and searches for something on the surface."], "durations": null, "exact_frames_per_prompt": [59], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/5BJF1ryRtxY.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 21, "npz_gt_video_start_frame": 21, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 21, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2826_0_1"}, {"texts": ["A brown dog is sitting near the man on the right side."], "durations": null, "exact_frames_per_prompt": [28], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/5BJF1ryRtxY.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 32, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2826_1_0"}, {"texts": ["The dog starts walking while the man opens the bag"], "durations": null, "exact_frames_per_prompt": [48], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/5BJF1ryRtxY.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 32, "npz_gt_video_start_frame": 32, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 32, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2826_1_1"}, {"texts": ["A woman wearing a red cloth standing on the left side is cooking noodles while adding, nuts, and coriander"], "durations": null, "exact_frames_per_prompt": [56], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/0szCTeTOWg0.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 60, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2827_0_0"}, {"texts": ["The woman starts steering the noodle with the help of a wooden spatula."], "durations": null, "exact_frames_per_prompt": [20], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/0szCTeTOWg0.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 60, "npz_gt_video_start_frame": 60, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 60, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2827_0_1"}, {"texts": ["An elephant is walking on the soil surface while carrying a group of four kids."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-nAsMzrGKds.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2829_3_0"}, {"texts": ["A person wearing a multi-color t-shirt is standing and putting watermelon pieces into a bowl."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/Acqr9yFH9Ts.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_282_0_0"}, {"texts": ["A man whom hands are visible wearing a blue shirt is folding the necktie."], "durations": null, "exact_frames_per_prompt": [50], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/8h3FD2KgWyQ.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 54, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2831_0_0"}, {"texts": ["The man is ghosting the necktie with his hands on the table."], "durations": null, "exact_frames_per_prompt": [26], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/8h3FD2KgWyQ.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 54, "npz_gt_video_start_frame": 54, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 54, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2831_0_1"}, {"texts": ["A girl wearing a yellow t-shirt is eating a carrot.\n"], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/1xDhfBuATZ0.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2832_0_0"}, {"texts": ["A kid wearing a blue striped t-shirt is catching the fish from the water with a fishing net while a woman wearing pants is sitting beside the kid is catching a fish too, and putting it in the bowl; and on the other side is a kid wearing a white jacket is catching fish using a net."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/02CYW9ylCuA.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2835_0_0"}, {"texts": ["A person wearing blue jeans is sitting, and catching a fish while a kid is sitting infront of her and also scooping from the water full of fish."], "durations": null, "exact_frames_per_prompt": [28], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/02CYW9ylCuA.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 32, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2835_2_0"}, {"texts": ["The person puts it into the bowl and the boy does as well."], "durations": null, "exact_frames_per_prompt": [19], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/02CYW9ylCuA.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 32, "npz_gt_video_start_frame": 32, "npz_gt_video_end_frame": 51, "skip_frames_after_generation": 32, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2835_2_1"}, {"texts": ["An orange fish is caught in a net and the person."], "durations": null, "exact_frames_per_prompt": [17], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/02CYW9ylCuA.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 21, "npz_gt_video_start_frame": 21, "npz_gt_video_end_frame": 38, "skip_frames_after_generation": 21, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2835_6_0"}, {"texts": ["The person puts it into the bowl."], "durations": null, "exact_frames_per_prompt": [20], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/02CYW9ylCuA.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 38, "npz_gt_video_start_frame": 38, "npz_gt_video_end_frame": 58, "skip_frames_after_generation": 38, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2835_6_1"}, {"texts": ["A kid sitting on a stool wearing a blue striped t-shirt and brown trousers is catching the fish from the water with a fishing net.\n a woman with yellow t-shirt is helping  the kid to catch the fish with a fishing net."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/02CYW9ylCuA.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2836_0_0"}, {"texts": ["A person wearing blue jeans is sitting and catching a fish while the other person is helping in the same activity"], "durations": null, "exact_frames_per_prompt": [30], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/02CYW9ylCuA.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 34, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2836_2_0"}, {"texts": ["The person wearing blue jeans puts it into the bowl while other kid holds the fish net"], "durations": null, "exact_frames_per_prompt": [46], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/02CYW9ylCuA.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 34, "npz_gt_video_start_frame": 34, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 34, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2836_2_1"}, {"texts": ["An orange fish is caught in a net."], "durations": null, "exact_frames_per_prompt": [20], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/02CYW9ylCuA.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 20, "npz_gt_video_start_frame": 20, "npz_gt_video_end_frame": 40, "skip_frames_after_generation": 20, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2836_6_0"}, {"texts": ["The person puts the fish into the bowl and the person stood up."], "durations": null, "exact_frames_per_prompt": [16], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/02CYW9ylCuA.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 40, "npz_gt_video_start_frame": 40, "npz_gt_video_end_frame": 56, "skip_frames_after_generation": 40, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2836_6_1"}, {"texts": ["A man wearing a blue t-shirt is holding a glass mug and at first pouring the liquid into the glass mug."], "durations": null, "exact_frames_per_prompt": [15], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/36RwEWdQ8pI.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 19, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2837_0_0"}, {"texts": ["Then the man puts the other glass on the wooden surface."], "durations": null, "exact_frames_per_prompt": [39], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/36RwEWdQ8pI.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 19, "npz_gt_video_start_frame": 19, "npz_gt_video_end_frame": 58, "skip_frames_after_generation": 19, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2837_0_1"}, {"texts": ["The man drinks the beverage."], "durations": null, "exact_frames_per_prompt": [22], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/36RwEWdQ8pI.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 58, "npz_gt_video_start_frame": 58, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 58, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2837_0_2"}, {"texts": ["A colourful fish is held and rubbed by a person in an aquarium."], "durations": null, "exact_frames_per_prompt": [55], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/02MhTVQbVwg.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 59, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_283_1_0"}, {"texts": ["A woman wearing a maroon top is seeing herself in the mirror and is touching her eyebrows."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/0JaknYWCvKo.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2841_0_0"}, {"texts": ["A woman wearing a red t-shirt is holding an eyebrow pencil."], "durations": null, "exact_frames_per_prompt": [31], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/0JaknYWCvKo.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 35, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2842_0_0"}, {"texts": ["The woman is touching her face with her finger in front of a mirror."], "durations": null, "exact_frames_per_prompt": [45], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/0JaknYWCvKo.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 35, "npz_gt_video_start_frame": 35, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 35, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2842_0_1"}, {"texts": ["A girl wearing a white top with red-white stripes and black pants is sitting on the bed and opening a toy laptop."], "durations": null, "exact_frames_per_prompt": [55], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/0hhfOGkn8AU.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 59, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2844_0_0"}, {"texts": ["The girl wearing a white top with red-white stripes and black pants is showing it to the left side."], "durations": null, "exact_frames_per_prompt": [21], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/0hhfOGkn8AU.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 59, "npz_gt_video_start_frame": 59, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 59, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2844_0_1"}, {"texts": ["A girl wearing a white t-shirt is sitting on the bed, opening a pink laptop."], "durations": null, "exact_frames_per_prompt": [56], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/0hhfOGkn8AU.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 60, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2845_0_0"}, {"texts": ["The girl is showing the laptop."], "durations": null, "exact_frames_per_prompt": [20], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/0hhfOGkn8AU.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 60, "npz_gt_video_start_frame": 60, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 60, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2845_0_1"}, {"texts": ["A man wearing a scuba diving suit is scuba diving and his hand is held by a person."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-vWpXbRi1XY.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2846_0_0"}, {"texts": ["A girl wearing a light green top is riding a black horse on a green and brown surface."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/0BCcrjsLoMY.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2847_0_0"}, {"texts": ["A black horse is walking on the green and brown surface, while a girl is sitting on its back."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/0BCcrjsLoMY.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2847_1_0"}, {"texts": ["A woman wearing a light blue t-shirt and white trousers is riding a black horse."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/0BCcrjsLoMY.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2848_0_0"}, {"texts": ["A black horse is walking in the green grass field and being ridden by the woman."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/0BCcrjsLoMY.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2848_1_0"}, {"texts": ["A girl wearing a light green t-shirt is riding on the horse"], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/0BCcrjsLoMY.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2849_0_0"}, {"texts": ["A black horse is walking while carrying a girl on its back."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/0BCcrjsLoMY.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2849_1_0"}, {"texts": ["A person wearing a white t-shirt is standing and ironing a dark blue trouser with a steam iron."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-3SmhZu4ZWc.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2850_0_0"}, {"texts": ["A girl wearing a green dress is sitting on a baby chair and eating cake with a yellow-green designer spoon."], "durations": null, "exact_frames_per_prompt": [45], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/48f5oAtAusA.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 49, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2851_0_0"}, {"texts": ["The girl starts eating with her hand."], "durations": null, "exact_frames_per_prompt": [31], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/48f5oAtAusA.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 49, "npz_gt_video_start_frame": 49, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 49, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2851_0_1"}, {"texts": ["A woman wearing a light blue top is standing in the kitchen holding a tong and mixing a salad in a glass bowl."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/2eaQP1LdXS8.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2852_0_0"}, {"texts": ["A man wearing a white chef's uniform is standing behind the counter and is picking up the food."], "durations": null, "exact_frames_per_prompt": [21], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-tgWzmdueYI.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 25, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2854_1_0"}, {"texts": ["The man is putting the food on the plate."], "durations": null, "exact_frames_per_prompt": [23], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-tgWzmdueYI.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 25, "npz_gt_video_start_frame": 25, "npz_gt_video_end_frame": 48, "skip_frames_after_generation": 25, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2854_1_1"}, {"texts": ["The man is holding the plate and putting it on the white counter."], "durations": null, "exact_frames_per_prompt": [20], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-tgWzmdueYI.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 48, "npz_gt_video_start_frame": 48, "npz_gt_video_end_frame": 68, "skip_frames_after_generation": 48, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2854_1_2"}, {"texts": ["The man is picking up the plate and putting it on the glass slab."], "durations": null, "exact_frames_per_prompt": [12], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-tgWzmdueYI.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 68, "npz_gt_video_start_frame": 68, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 68, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2854_1_3"}, {"texts": ["A man wearing a royal blue shirt and light cream pants is moving his hand back and forth while holding a golf stick a woman wearing a pink t-shirt and black trouser is listening to man's instructions."], "durations": null, "exact_frames_per_prompt": [19], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/1lEmIh8jZ6A.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 23, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2857_0_0"}, {"texts": ["The man is standing and moving his golf stick back and forth."], "durations": null, "exact_frames_per_prompt": [26], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/1lEmIh8jZ6A.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 23, "npz_gt_video_start_frame": 23, "npz_gt_video_end_frame": 49, "skip_frames_after_generation": 23, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2857_0_1"}, {"texts": ["The man is standing on the right side of the woman."], "durations": null, "exact_frames_per_prompt": [31], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/1lEmIh8jZ6A.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 49, "npz_gt_video_start_frame": 49, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 49, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2857_0_2"}, {"texts": ["A woman wearing a dark gray top and black jeans is sitting while holding white papers, on the right side of the couch and is talking to the second woman while showing the papers to the second woman while the person wearing blue shirt sitting on the right side of the couch."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/6Am-TQ0OeFU.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2858_4_0"}, {"texts": ["A woman wearing a flower printed black dress is sitting in the middle of the man and the first woman."], "durations": null, "exact_frames_per_prompt": [28], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/6Am-TQ0OeFU.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 32, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2858_5_0"}, {"texts": ["The woman wearing a flower printed black dress is speaking while moving her hands a grey dress woman and the man wearing blue checks shirt are listening"], "durations": null, "exact_frames_per_prompt": [48], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/6Am-TQ0OeFU.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 32, "npz_gt_video_start_frame": 32, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 32, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2858_5_1"}, {"texts": ["A man wearing a white striped blue shirt and dark blue pants is sitting on the left side of the couch while the two women are talking to each other."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/6Am-TQ0OeFU.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2858_6_0"}, {"texts": ["A boy wearing blue clothes is eating kale chips."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/0o1VQXMWwxo.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2859_0_0"}, {"texts": ["A man wearing a green-spotted black shirt and white shorts is standing on the left side while holding a fry pan with the yellow pancake."], "durations": null, "exact_frames_per_prompt": [17], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-e6Zra4ke60.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 21, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_285_0_0"}, {"texts": ["The man wearing a green-spotted black shirt and white shorts starts tossing the pancake in the air and catching it in the fry pan."], "durations": null, "exact_frames_per_prompt": [41], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-e6Zra4ke60.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 21, "npz_gt_video_start_frame": 21, "npz_gt_video_end_frame": 62, "skip_frames_after_generation": 21, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_285_0_1"}, {"texts": ["A baby wearing a blue cloth is crawling on the gray surface while a girl wearing pink dress is playing with the birds"], "durations": null, "exact_frames_per_prompt": [20], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-SfAPNOB3aU.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 24, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2860_0_0"}, {"texts": ["The baby is touching the birds."], "durations": null, "exact_frames_per_prompt": [43], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-SfAPNOB3aU.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 24, "npz_gt_video_start_frame": 24, "npz_gt_video_end_frame": 67, "skip_frames_after_generation": 24, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2860_0_1"}, {"texts": ["A girl wearing a pink striped white t-shirt, pink shorts and black slippers is sitting on the gray surface while holding a stick and is moving the stick towards the birds a baby wearing a blue dress is also playing  with the birds"], "durations": null, "exact_frames_per_prompt": [63], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-SfAPNOB3aU.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 67, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2860_1_0"}, {"texts": ["A person whose only legs are visible, wearing gray jeans and red shoes, is sitting while folding legs and holding the white bird a baby wearing a blue cloth is crawling on the gray surface."], "durations": null, "exact_frames_per_prompt": [12], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-SfAPNOB3aU.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 16, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2860_4_0"}, {"texts": ["A person whose only hand is visible is cooking vegetables in a black container.\n"], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-Fo1A96jv44.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2861_0_0"}, {"texts": ["A person whose hand is visible is holding a spatula and mixing food in a frying pan."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-Fo1A96jv44.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2862_0_0"}, {"texts": ["A person wearing white clothes is standing and holding a knife and cutting food."], "durations": null, "exact_frames_per_prompt": [44], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/07on_8rbwyM.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 48, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2865_0_0"}, {"texts": ["The person is putting the food in a plate."], "durations": null, "exact_frames_per_prompt": [32], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/07on_8rbwyM.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 48, "npz_gt_video_start_frame": 48, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 48, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2865_0_1"}, {"texts": ["A baby wearing pink and an off-white clothes is giggling and laughing hysterically with fingers in his mouth a man wearing a grey shirt and black trouser is also laughing along with the baby"], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/1K2RKfnb1bo.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2866_0_0"}, {"texts": ["A man wearing a check shirt is lying on the bed and looking at the baby and laughing with him."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/1K2RKfnb1bo.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2866_1_0"}, {"texts": ["A man wearing navy blue pants is lying on the gray floor and laughing while the baby is also laughing"], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/1K2RKfnb1bo.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2867_0_0"}, {"texts": ["A baby wearing a white cloth is sitting, watching the man and laughing."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/1K2RKfnb1bo.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2867_1_0"}, {"texts": ["A person wearing a black chef jacket is peeling a green apple with a peeler."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-JEFz5Z0EWU.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2868_0_0"}, {"texts": ["A white sheep is lying on a blue surface and getting hair trimmed by the man."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-PL_D_VzGCA.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2869_1_0"}, {"texts": ["A man wearing a cap is standing, holding a trimmer and trimming sheep's hair."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-PL_D_VzGCA.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2869_2_0"}, {"texts": ["A woman wearing a green dress is speaking while looking in a straight direction.\n"], "durations": null, "exact_frames_per_prompt": [34], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/3wqTy6TXMQA.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 46, "npz_gt_video_start_frame": 46, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 46, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_286_1_ms_0"}, {"texts": ["A white sheep is lying on a blue surface and getting trimmed by the first man a boy wearing  a pink t-shirt is watching the sheep while it is getting trimmed"], "durations": null, "exact_frames_per_prompt": [49], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-PL_D_VzGCA.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 53, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2870_1_0"}, {"texts": ["A man wearing a cap is standing, holding a trimmer and trimming hair of the sheep while the kid watching him."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-PL_D_VzGCA.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2870_2_0"}, {"texts": ["A boy wearing red t-shirt is standing and watching the first man while the first man is trimming the sheep."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-PL_D_VzGCA.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2870_3_0"}, {"texts": ["A white-brown goat on the left is standing behind the fence."], "durations": null, "exact_frames_per_prompt": [34], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/06HpOpJpJUU.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 38, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2871_0_0"}, {"texts": ["The goat is being given some food by the girl."], "durations": null, "exact_frames_per_prompt": [41], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/06HpOpJpJUU.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 38, "npz_gt_video_start_frame": 38, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 38, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2871_0_1"}, {"texts": ["A person whose hands are visible is making a fan from a gift wrap."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-LzyV1PtJXE.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2873_0_0"}, {"texts": ["A woman wearing black clothes is showing a brush."], "durations": null, "exact_frames_per_prompt": [14], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/1jjWCAqF0Ds.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 35, "npz_gt_video_start_frame": 35, "npz_gt_video_end_frame": 49, "skip_frames_after_generation": 35, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2874_0_ms_0"}, {"texts": ["The woman starts cleaning the brown horse with a brush."], "durations": null, "exact_frames_per_prompt": [31], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/1jjWCAqF0Ds.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 49, "npz_gt_video_start_frame": 49, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 49, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2874_0_ms_1"}, {"texts": ["A brown horse is standing and getting cleaned by a woman."], "durations": null, "exact_frames_per_prompt": [45], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/1jjWCAqF0Ds.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 35, "npz_gt_video_start_frame": 35, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 35, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2874_1_ms_0"}, {"texts": ["A person wearing white clothes is standing near the table."], "durations": null, "exact_frames_per_prompt": [30], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-0aDlftNdyw.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 34, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2875_0_0"}, {"texts": ["The person starts eating watermelon."], "durations": null, "exact_frames_per_prompt": [16], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-0aDlftNdyw.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 34, "npz_gt_video_start_frame": 34, "npz_gt_video_end_frame": 50, "skip_frames_after_generation": 34, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2875_0_1"}, {"texts": ["A person whose upper body is visible wearing a white shirt is holding the watermelon on the table."], "durations": null, "exact_frames_per_prompt": [29], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-0aDlftNdyw.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 33, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2876_0_0"}, {"texts": ["The person is tilting the body and starts eating the watermelon from the mouth."], "durations": null, "exact_frames_per_prompt": [47], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-0aDlftNdyw.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 33, "npz_gt_video_start_frame": 33, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 33, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2876_0_1"}, {"texts": ["A person whose only finger is visible is ironing the cloth with an iron."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-f3onW_tZK4.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2877_0_0"}, {"texts": ["A person whose hand is visible is ironing the painting on a paper."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-f3onW_tZK4.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2878_0_0"}, {"texts": ["A person is caressing the head and the face of a white cat."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/1v2pjy9QJqg.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2879_0_0"}, {"texts": ["A white cat is getting its head and face caressed by a person."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/1v2pjy9QJqg.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2879_1_0"}, {"texts": ["A person whose only a hand is visible is flipping the egg white with a spatula on a grill pan."], "durations": null, "exact_frames_per_prompt": [53], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/0xciMnHCZL0.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 57, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2882_0_0"}, {"texts": ["A person whose hand is visible, holding a spatula and frying the egg on the frying grill.\n"], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/0xciMnHCZL0.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_2883_0_0"}, {"texts": ["A person whose only hand is visible is walking with the dog on the road."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/0x1l1uv8Xrw.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_288_1_0"}, {"texts": ["A person whom hands are visible is rolling the sushi mat."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/6DhdKNz3opI.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_28_0_0"}, {"texts": ["A girl wearing blue-white striped cloth is standing and holding a bottle in her right hand."], "durations": null, "exact_frames_per_prompt": [36], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/0wzdr5YlJ1U.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 40, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_292_0_0"}, {"texts": ["The girl is tapping her hand on her thighs. "], "durations": null, "exact_frames_per_prompt": [25], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/0wzdr5YlJ1U.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 40, "npz_gt_video_start_frame": 40, "npz_gt_video_end_frame": 65, "skip_frames_after_generation": 40, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_292_0_1"}, {"texts": ["The girl is taking a cream out on her palm from the bottle."], "durations": null, "exact_frames_per_prompt": [16], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/0wzdr5YlJ1U.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 65, "npz_gt_video_start_frame": 65, "npz_gt_video_end_frame": 81, "skip_frames_after_generation": 65, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_292_0_2"}, {"texts": ["A girl wearing a blue t-shirt is riding a horse on the soil surface."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/5X7u1YubmsU.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_295_0_0"}, {"texts": ["A brown horse is running on the soil surface while carrying the girl on its back."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/5X7u1YubmsU.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_295_1_0"}, {"texts": ["A person whose hands are visible is counting the banknotes."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/1rNPVU8Ookc.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_29_0_0"}, {"texts": ["A man wearing a cream shirt is sitting, holding a glass of beer, talking, and moving his hand."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/2dTYAsw8_w0.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_300_0_0"}, {"texts": ["A man wearing a light grey shirt is sitting and holding a beer glass in his right hand."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/2dTYAsw8_w0.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_301_0_0"}, {"texts": ["A man wearing a blue t-shirt is sitting on the animal and moving forward."], "durations": null, "exact_frames_per_prompt": [43], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/0fJDwTxsT54.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 47, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_302_0_0"}, {"texts": ["A man wearing dark blue clothes is eating and standing."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/1MNYX74lHzM.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_303_0_0"}, {"texts": ["A person whose hands are only visible is folding paper sheets then throwing it."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/0OjixsJenPg.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_304_0_0"}, {"texts": ["A woman wearing a grey t-shirt and pink jeans is standing, taking out cookies from the tray."], "durations": null, "exact_frames_per_prompt": [22], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-jswxUe334A.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 26, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_310_0_0"}, {"texts": ["The woman wearing a grey t-shirt and pink jeans is inserting them into the plate."], "durations": null, "exact_frames_per_prompt": [54], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-jswxUe334A.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 26, "npz_gt_video_start_frame": 26, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 26, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_310_0_1"}, {"texts": ["A man wearing a dark-grey suit is standing on the right and talking on a mobile phone."], "durations": null, "exact_frames_per_prompt": [34], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/43uzshYztFQ.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 46, "npz_gt_video_start_frame": 46, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 46, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_316_1_0"}, {"texts": ["A man wearing a blue shirt is lifting up a lawn mower machine with his hands."], "durations": null, "exact_frames_per_prompt": [51], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/0sPCFkic_Kk.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 55, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_31_0_0"}, {"texts": ["The man putting it back down on the gray surface."], "durations": null, "exact_frames_per_prompt": [25], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/0sPCFkic_Kk.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 55, "npz_gt_video_start_frame": 55, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 55, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_31_0_1"}, {"texts": ["A woman whose upper half-body is visible wearing a printed dark blue top and a flower-printed scarf on her neck is sitting and is tying a knot in a scarf with her hands and wearing it around her neck."], "durations": null, "exact_frames_per_prompt": [77], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/2xsbzoo76aA.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 81, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_322_0_0"}, {"texts": ["A man wearing a maroon checked shirt is picking a hot dog from a white plate and then starts eating the hot dog."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/2yfY6KhdZ4A.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_323_0_0"}, {"texts": ["A girl wearing a white dress is standing and interpreting the sign language with her hands through music by acting as if she was playing guitar and moving her hands here and there."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-o0ZoWKge2k.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_324_0_0"}, {"texts": ["A woman wearing a white dress is standing and doing the act of playing air guitar."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-o0ZoWKge2k.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_325_0_0"}, {"texts": ["A girl wearing a white dress is standing in the front, moving her hands and doing an action like playing a guitar."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-o0ZoWKge2k.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_326_0_0"}, {"texts": ["A man wearing black cloth is standing and holding a black tire."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-B2CPFQLtu0.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_327_0_0"}, {"texts": ["A man wearing a dark grey t-shirt is standing near the tire changing machine and touches the tire."], "durations": null, "exact_frames_per_prompt": [61], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-B2CPFQLtu0.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 65, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_328_0_0"}, {"texts": ["The man inflates the tire with the help of an air pipe."], "durations": null, "exact_frames_per_prompt": [15], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-B2CPFQLtu0.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 65, "npz_gt_video_start_frame": 65, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 65, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_328_0_1"}, {"texts": ["A man wearing a dark grey t-shirt is standing near the tire changing machine and touches the tire."], "durations": null, "exact_frames_per_prompt": [62], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-B2CPFQLtu0.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 66, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_329_0_0"}, {"texts": ["The man inflates the tire with the help of an air pipe."], "durations": null, "exact_frames_per_prompt": [14], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-B2CPFQLtu0.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 66, "npz_gt_video_start_frame": 66, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 66, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_329_0_1"}, {"texts": ["A person whose hand is visible is holding a cup of water and feeding water to the first and second parrot."], "durations": null, "exact_frames_per_prompt": [37], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-H9lVjKQw2o.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 41, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_32_0_0"}, {"texts": ["A kid wearing a red-gray t-shirt, holding a cup of water and feeding water to the fourth parrot."], "durations": null, "exact_frames_per_prompt": [28], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-H9lVjKQw2o.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 52, "npz_gt_video_start_frame": 52, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 52, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_32_2_0"}, {"texts": ["A parrot is standing on the right side of the first parrot and drinking water from the same cup of water a person whose hand is visible and feeding the parrots."], "durations": null, "exact_frames_per_prompt": [29], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-H9lVjKQw2o.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 33, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_32_4_0"}, {"texts": ["A parrot is standing on the railing and drinking water from the cup of water, then looking at the kid wearing a red-grey t-shirt."], "durations": null, "exact_frames_per_prompt": [25], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-H9lVjKQw2o.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 55, "npz_gt_video_start_frame": 55, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 55, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_32_6_0"}, {"texts": ["A girl wearing a multicolored striped dress is sitting on the floor and reading a book."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/0kOi2iCwNi8.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_330_0_0"}, {"texts": ["A woman in the white clothing is at first raising her hands up in the air."], "durations": null, "exact_frames_per_prompt": [22], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/07ILndvuKUc.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 11, "npz_gt_video_start_frame": 11, "npz_gt_video_end_frame": 33, "skip_frames_after_generation": 11, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_331_1_0"}, {"texts": ["The woman in the white clothing is piercing the food from her hands and putting it on the foil tray."], "durations": null, "exact_frames_per_prompt": [47], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/07ILndvuKUc.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 33, "npz_gt_video_start_frame": 33, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 33, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_331_1_1"}, {"texts": ["A girl on the left side is standing and speaking a girl wearing a white t-shirt is breaking food into pieces"], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/07ILndvuKUc.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_332_0_0"}, {"texts": ["A girl wearing a white t-shirt whose hands are up."], "durations": null, "exact_frames_per_prompt": [17], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/07ILndvuKUc.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 11, "npz_gt_video_start_frame": 11, "npz_gt_video_end_frame": 28, "skip_frames_after_generation": 11, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_332_1_0"}, {"texts": ["The girl takes some cake pieces from the tray."], "durations": null, "exact_frames_per_prompt": [11], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/07ILndvuKUc.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 28, "npz_gt_video_start_frame": 28, "npz_gt_video_end_frame": 39, "skip_frames_after_generation": 28, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_332_1_1"}, {"texts": ["The girl breaks the pieces."], "durations": null, "exact_frames_per_prompt": [4], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/07ILndvuKUc.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 39, "npz_gt_video_start_frame": 39, "npz_gt_video_end_frame": 44, "skip_frames_after_generation": 39, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_332_1_2"}, {"texts": ["The girl puts them into the tray."], "durations": null, "exact_frames_per_prompt": [35], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/07ILndvuKUc.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 44, "npz_gt_video_start_frame": 44, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 44, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_332_1_3"}, {"texts": ["A man wearing a multi color t-shirt is sitting on the left side, moving his hands and talking a man wearing a grey and white checks shirt is explaining something to the people"], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-YyWVl1jNx0.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_337_0_0"}, {"texts": ["The man wearing a checkered shirt, standing in the center with one hand on the chair a man wearing the multi color t-shirt explaining something."], "durations": null, "exact_frames_per_prompt": [43], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-YyWVl1jNx0.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 47, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_337_1_0"}, {"texts": ["The other hand is pointing to the food a man wearing a white shirt is sitting on the chair on the right side, moving his head."], "durations": null, "exact_frames_per_prompt": [33], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-YyWVl1jNx0.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 47, "npz_gt_video_start_frame": 47, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 47, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_337_1_1"}, {"texts": ["A man wearing a white shirt is sitting on the chair on the right side, moving his head the man is wearing a checkered shirt, standing in the center with one hand on the chair, the other hand is pointing to the food while the other man wearing a multi colored shirt explaining something"], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-YyWVl1jNx0.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_337_2_0"}, {"texts": ["A woman wearing a black dress is standing and decorating a plate with a red napkin and a metallic object."], "durations": null, "exact_frames_per_prompt": [45], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/1IjUBBAglvA.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 49, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_345_0_0"}, {"texts": ["The woman looking in front while smiling."], "durations": null, "exact_frames_per_prompt": [31], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/1IjUBBAglvA.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 49, "npz_gt_video_start_frame": 49, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 49, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_345_0_1"}, {"texts": ["A woman wearing a black top is standing in a kitchen and is peeling a potato with a peeler."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-1jQapks1hI.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_347_0_0"}, {"texts": ["A woman wearing a black top is standing in a kitchen and peeling a potato with a peeler."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-1jQapks1hI.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_348_0_0"}, {"texts": ["A person is draining out engine oil from the engine of a car."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/2e8A9W9b_WE.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_349_0_0"}, {"texts": ["A girl wearing a pink dress is running on the white surface with a small white dog."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-OlQoUmWtdw.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_34_0_0"}, {"texts": ["A small white dog is also running on the white surface with the girl."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-OlQoUmWtdw.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_34_1_0"}, {"texts": ["A girl wearing a blue top is at first standing near the women and watching her while moving body here and there while the girl wearing pink colour dress playing with the doll"], "durations": null, "exact_frames_per_prompt": [57], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-7QhBZO3t-A.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 61, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_350_0_0"}, {"texts": ["The girl wearing a blue top turns and starts walking in the backward direction while the women trying to open the box"], "durations": null, "exact_frames_per_prompt": [19], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-7QhBZO3t-A.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 61, "npz_gt_video_start_frame": 61, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 61, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_350_0_1"}, {"texts": ["A girl wearing a pink t-shirt is standing on the right side near the woman and holding a soft toy a woman wearing a black printed t-shirt is removing the tape from a cardboard box"], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-7QhBZO3t-A.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_350_1_0"}, {"texts": ["A woman wearing a black printed t-shirt is removing the tape from a cardboard box a girl wearing a pink t-shirt is standing on the right side near the woman and holding a soft toy and the girl wearing a blue top turns and starts walking in the backward direction."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-7QhBZO3t-A.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_350_2_0"}, {"texts": ["A person wearing a blue t-shirt is sitting with a dog and tickling the dog's stomach."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/1jMZZo7qrr8.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_354_0_0"}, {"texts": ["A brown dog is sitting in the lap of the person a person wearing a blue t-shirt and tickling the dog's stomach."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/1jMZZo7qrr8.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_354_1_0"}, {"texts": ["A woman wearing a blue dress is standing on the left side of the first woman is arranging a bed sheet."], "durations": null, "exact_frames_per_prompt": [31], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/ip5eOs8IcMw.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 35, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_357_1_0"}, {"texts": ["The woman then moves toward the right."], "durations": null, "exact_frames_per_prompt": [45], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/ip5eOs8IcMw.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 35, "npz_gt_video_start_frame": 35, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 35, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_357_1_1"}, {"texts": ["A girl whose hands are visible is holding a book and binding the book with a needle and thread."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-60KGw7ow0k.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_359_0_0"}, {"texts": ["A man wearing spectacles is speaking facing the front."], "durations": null, "exact_frames_per_prompt": [11], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/0Jzz8XkT5i4.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 15, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_35_0_0"}, {"texts": ["The man is showing some cooking meat in a pan."], "durations": null, "exact_frames_per_prompt": [41], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/0Jzz8XkT5i4.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 15, "npz_gt_video_start_frame": 15, "npz_gt_video_end_frame": 56, "skip_frames_after_generation": 15, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_35_0_1"}, {"texts": ["A woman wearing a blue top is sitting in a room and speaking sign language."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/7YNc6HhaFTQ.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_363_0_0"}, {"texts": ["A person whose hands are visible, wearing a black cloth, is standing and whisking the eggs in a bowl."], "durations": null, "exact_frames_per_prompt": [45], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/2zFyhKWjBSY.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 49, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_364_1_0"}, {"texts": ["The person is picking up the cheese from the white plate."], "durations": null, "exact_frames_per_prompt": [31], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/2zFyhKWjBSY.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 49, "npz_gt_video_start_frame": 49, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 49, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_364_1_1"}, {"texts": ["A man wearing a white t-shirt is sitting, eating a burger."], "durations": null, "exact_frames_per_prompt": [26], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/1dmfliZM2NI.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 30, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_365_0_0"}, {"texts": ["The man is drinking from the can."], "durations": null, "exact_frames_per_prompt": [50], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/1dmfliZM2NI.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 30, "npz_gt_video_start_frame": 30, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 30, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_365_0_1"}, {"texts": ["A person whose finger is visible is holding a black cooking spoon and he is cooking."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/63PLL5HEAn8.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_367_0_0"}, {"texts": ["A person whose hands are visible is cleaning the oil filler cap o ring."], "durations": null, "exact_frames_per_prompt": [44], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/2elLtiD6lQQ.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 36, "npz_gt_video_start_frame": 36, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 36, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_368_0_0"}, {"texts": ["A person whose only half-body is visible is standing and preparing sushi on a white chopping board."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/4MPCEhet8aM.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_369_0_0"}, {"texts": ["A person wearing a white t-shirt is sitting holding a packet."], "durations": null, "exact_frames_per_prompt": [32], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/3DmlVvjxTic.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 36, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_370_0_0"}, {"texts": ["The person is eating carrot pieces from the packet."], "durations": null, "exact_frames_per_prompt": [44], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/3DmlVvjxTic.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 36, "npz_gt_video_start_frame": 36, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 36, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_370_0_1"}, {"texts": ["A person whose only hands are visible is holding a news paper and moving it."], "durations": null, "exact_frames_per_prompt": [43], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/4xM_6HLxBXA.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 47, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_371_0_0"}, {"texts": ["A person whose only hand and leg is visible is holding a newspaper."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/4xM_6HLxBXA.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_372_0_ms_0"}, {"texts": ["A baby boy wearing printed clothes is turning pages of a picture book while sitting on a brown sofa."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/19k_Lu-S1rM.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_375_0_0"}, {"texts": ["A woman wearing a green top is standing and speaking."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/4fQMV3HcdF8.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_376_0_ms_0"}, {"texts": ["A girl wearing red clothes is sitting and eating chocolates."], "durations": null, "exact_frames_per_prompt": [39], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/54k8Tt0h3MU.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 43, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_377_0_0"}, {"texts": ["And then smiling."], "durations": null, "exact_frames_per_prompt": [37], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/54k8Tt0h3MU.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 43, "npz_gt_video_start_frame": 43, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 43, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_377_0_1"}, {"texts": ["A girl wearing a pink t-shirt is sitting and eating a chocolate stuffed food item with her hands."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/54k8Tt0h3MU.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_378_0_0"}, {"texts": ["A man on the right side wearing a high-visibility jacket is leaning forward and drilling into the ice surface with an earth auger machine while a man wearing black Tshirt is filming"], "durations": null, "exact_frames_per_prompt": [41], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-psBKORmrzk.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 9, "npz_gt_video_start_frame": 9, "npz_gt_video_end_frame": 50, "skip_frames_after_generation": 9, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_37_1_ms_0"}, {"texts": ["A man, wearing a black jacket and brown pants, is standing on the left side and holding a camera in his hands while the person wearing yellow and black jacket starts to drill on the ice surface."], "durations": null, "exact_frames_per_prompt": [38], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-psBKORmrzk.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 10, "npz_gt_video_start_frame": 10, "npz_gt_video_end_frame": 48, "skip_frames_after_generation": 10, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_37_2_ms_0"}, {"texts": ["The man starts drilling into the ice surface with an earth auger machine."], "durations": null, "exact_frames_per_prompt": [32], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-psBKORmrzk.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 48, "npz_gt_video_start_frame": 48, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 48, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_37_2_ms_1"}, {"texts": ["A man wearing a high-visibility jacket is standing on the backside the other man is trying to dig the snow while a man wearing black Tshirt is filming"], "durations": null, "exact_frames_per_prompt": [73], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-psBKORmrzk.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 7, "npz_gt_video_start_frame": 7, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 7, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_37_3_ms_0"}, {"texts": ["A lady is doing her eyebrow makeup."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-WesM6ZQj-I.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_381_0_0"}, {"texts": ["A boy wearing a yellow t-shirt is leaning on the white wall."], "durations": null, "exact_frames_per_prompt": [11], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/2DZw8PbWK0Y.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 15, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_385_0_ms_0"}, {"texts": ["A man wearing a blue t-shirt is getting a tattoo on his wrist while the other person is making the tattoo"], "durations": null, "exact_frames_per_prompt": [65], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/2DZw8PbWK0Y.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 15, "npz_gt_video_start_frame": 15, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 15, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_385_1_ms_0"}, {"texts": ["A man in black clothes is drawing a tattoo on the wrist of the man in blue clothes."], "durations": null, "exact_frames_per_prompt": [65], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/2DZw8PbWK0Y.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 15, "npz_gt_video_start_frame": 15, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 15, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_385_2_ms_0"}, {"texts": ["A boy wearing a white vest is taking out something with a spoon."], "durations": null, "exact_frames_per_prompt": [45], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/2yimbyG6I_8.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 49, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_386_0_0"}, {"texts": ["The boy is eating while standing."], "durations": null, "exact_frames_per_prompt": [31], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/2yimbyG6I_8.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 49, "npz_gt_video_start_frame": 49, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 49, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_386_0_1"}, {"texts": ["A boy wearing a white tank t-shirt is standing."], "durations": null, "exact_frames_per_prompt": [22], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/2yimbyG6I_8.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 26, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_387_0_0"}, {"texts": ["The boy wearing a white tank t-shirt is tasting something using a spoon from a yellow cup."], "durations": null, "exact_frames_per_prompt": [54], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/2yimbyG6I_8.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 26, "npz_gt_video_start_frame": 26, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 26, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_387_0_1"}, {"texts": ["A baby wearing a white-blue body suit is sitting on a baby chair watching the person and laughing as the person tears the piece of paper."], "durations": null, "exact_frames_per_prompt": [69], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/0B7YiFODRFA.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 73, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_38_0_0"}, {"texts": ["A person whose half-body is visible is sitting and tearing the paper in front of the baby while the baby is laughing"], "durations": null, "exact_frames_per_prompt": [69], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/0B7YiFODRFA.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 73, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_38_1_0"}, {"texts": ["A girl wearing a white woolen cloth is sitting on the brown wooden floor and holding a gift box."], "durations": null, "exact_frames_per_prompt": [41], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/1pe4CEs--5A.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 45, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_390_0_0"}, {"texts": ["The girl is putting the grey wrapping paper on the gift box."], "durations": null, "exact_frames_per_prompt": [35], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/1pe4CEs--5A.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 45, "npz_gt_video_start_frame": 45, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 45, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_390_0_1"}, {"texts": ["A dog is walking on the green grass field with the first person."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/0PTCyNDpb7c.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_392_4_0"}, {"texts": ["A white car is passing by a road."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/0PTCyNDpb7c.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_392_5_0"}, {"texts": ["A man wearing a black t-shirt and a helmet is holding a tire inflator nozzle."], "durations": null, "exact_frames_per_prompt": [8], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/1eUYPK0z8XU.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 12, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_393_0_0"}, {"texts": ["The man wearing a black t-shirt and a helmet is blowing the air into the tire from the air inflator."], "durations": null, "exact_frames_per_prompt": [34], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/1eUYPK0z8XU.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 12, "npz_gt_video_start_frame": 12, "npz_gt_video_end_frame": 46, "skip_frames_after_generation": 12, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_393_0_1"}, {"texts": ["The man wearing a black t-shirt and a helmet stands."], "durations": null, "exact_frames_per_prompt": [34], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/1eUYPK0z8XU.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 46, "npz_gt_video_start_frame": 46, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 46, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_393_0_2"}, {"texts": ["A woman whose hands are visible is folding a white paper on the grey table."], "durations": null, "exact_frames_per_prompt": [59], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/20dRlSxNx6Y.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 21, "npz_gt_video_start_frame": 21, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 21, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_394_0_0"}, {"texts": ["A man wearing a black suit is sitting a woman wearing a white bridal dress is also sitting."], "durations": null, "exact_frames_per_prompt": [30], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-RnFoMT98A4.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 34, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_395_0_0"}, {"texts": ["The man wearing a black suit is feeding the cake to the woman with a white a fork the woman is feeding the cake to person one with a white fork."], "durations": null, "exact_frames_per_prompt": [46], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-RnFoMT98A4.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 34, "npz_gt_video_start_frame": 34, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 34, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_395_0_1"}, {"texts": ["A woman wearing a white bridal dress is also sitting a man wearing a black suit is sitting beside the woman"], "durations": null, "exact_frames_per_prompt": [34], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-RnFoMT98A4.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 38, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_395_1_0"}, {"texts": ["The woman is feeding the cake to person one with a white fork the man wearing a black suit is feeding the cake."], "durations": null, "exact_frames_per_prompt": [42], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-RnFoMT98A4.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 38, "npz_gt_video_start_frame": 38, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 38, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_395_1_1"}, {"texts": ["A man wearing a dark gray t-shirt and black pants is setting the white cloth on the ironing board table."], "durations": null, "exact_frames_per_prompt": [10], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/2nvUj3XVbbg.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 14, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_396_0_0"}, {"texts": ["The man wearing a dark gray t-shirt and black pants is ironing the cloth with an ironing machine while standing."], "durations": null, "exact_frames_per_prompt": [66], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/2nvUj3XVbbg.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 14, "npz_gt_video_start_frame": 14, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 14, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_396_0_1"}, {"texts": ["A woman wearing a black t-shirt is standing on the left side and rubbing her finger on the cheek of the other woman."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/3yUhfGuvVnM.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_39_0_0"}, {"texts": ["A woman is standing and getting rubbed on her cheek by the first woman."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/3yUhfGuvVnM.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_39_1_0"}, {"texts": ["A girl wearing a black t-shirt is standing and moving her hand."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-V8ApSSg0Kg.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_401_0_0"}, {"texts": ["A woman wearing a light blue top is sitting on a chair and holding a newspaper."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/8L19e-kEv7U.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_403_0_0"}, {"texts": ["A girl wearing a pink top is sitting on the right side and eating cheese a boy wearing a check shirt putting the hand in the purple packet of cheese."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/0b30eRmVdXg.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_405_1_0"}, {"texts": ["A boy wearing a grey-white checked shirt is standing next to the first girl with his one hand in the purple packet of cheese a girl wearing pink colour dress trying to eat cheese."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/0b30eRmVdXg.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_405_2_0"}, {"texts": ["A man wearing a dark-green t-shirt with white borders is standing on the left side and putting cooked vegetables in a white bowl with a serving spoon a woman wearing a pink-green kurta is standing on the right side and looking at the action of the man."], "durations": null, "exact_frames_per_prompt": [61], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/3kijrhAf7zc.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 65, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_407_0_0"}, {"texts": ["The man wearing a dark-green t-shirt with white borders started taking something from a steel bowl."], "durations": null, "exact_frames_per_prompt": [15], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/3kijrhAf7zc.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 65, "npz_gt_video_start_frame": 65, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 65, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_407_0_1"}, {"texts": ["A woman wearing a pink-green kurta is standing on the right side and looking at the action of the man a man wearing a dark-green t-shirt with white borders is standing on the left side and putting cooked vegetables in a white bowl with a serving spoon."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/3kijrhAf7zc.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_407_1_0"}, {"texts": ["A man wearing a blue t-shirt is sitting on the gray floor and covering the mattress."], "durations": null, "exact_frames_per_prompt": [38], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/jdGFLI1LIno.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 42, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_408_0_0"}, {"texts": ["The man stands up."], "durations": null, "exact_frames_per_prompt": [27], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/jdGFLI1LIno.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 42, "npz_gt_video_start_frame": 42, "npz_gt_video_end_frame": 69, "skip_frames_after_generation": 42, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_408_0_1"}, {"texts": ["The man carries a white cloth in his hand."], "durations": null, "exact_frames_per_prompt": [11], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/jdGFLI1LIno.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 69, "npz_gt_video_start_frame": 69, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 69, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_408_0_2"}, {"texts": ["A man wearing a blue t-shirt is standing on a white boat and is picking up a silver fish."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/2L_1KFJL0uA.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_409_0_0"}, {"texts": ["A silver fish is being picked up by the man."], "durations": null, "exact_frames_per_prompt": [70], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/2L_1KFJL0uA.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 74, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_409_1_0"}, {"texts": ["The fish is moving on the white floor."], "durations": null, "exact_frames_per_prompt": [6], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/2L_1KFJL0uA.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 74, "npz_gt_video_start_frame": 74, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 74, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_409_1_1"}, {"texts": ["A fish is held by a man."], "durations": null, "exact_frames_per_prompt": [61], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/2L_1KFJL0uA.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 65, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_410_1_0"}, {"texts": ["Then he put it down."], "durations": null, "exact_frames_per_prompt": [7], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/2L_1KFJL0uA.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 65, "npz_gt_video_start_frame": 65, "npz_gt_video_end_frame": 72, "skip_frames_after_generation": 65, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_410_1_1"}, {"texts": ["The fish died."], "durations": null, "exact_frames_per_prompt": [8], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/2L_1KFJL0uA.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 72, "npz_gt_video_start_frame": 72, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 72, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_410_1_2"}, {"texts": ["A man wearing a black suit is standing, holding papers and delivering speech into a microphone."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-1GRj5UvVBA.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_411_0_0"}, {"texts": ["A boy wearing a black jacket and black pants is standing on the right side of the car, holding the pipe the Another man comes and pulls down his pants."], "durations": null, "exact_frames_per_prompt": [38], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/4GtqaxwORHQ.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 42, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_412_0_0"}, {"texts": ["The boy is pulling up his pants."], "durations": null, "exact_frames_per_prompt": [17], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/4GtqaxwORHQ.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 42, "npz_gt_video_start_frame": 42, "npz_gt_video_end_frame": 59, "skip_frames_after_generation": 42, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_412_0_1"}, {"texts": ["A boy wearing a cloth is pulling the pants of the first boy the boy is pulling up his pants."], "durations": null, "exact_frames_per_prompt": [50], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/4GtqaxwORHQ.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 30, "npz_gt_video_start_frame": 30, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 30, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_412_1_0"}, {"texts": ["A man wearing a black hoodie is filling fuel in the car."], "durations": null, "exact_frames_per_prompt": [29], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/4GtqaxwORHQ.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 33, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_413_0_0"}, {"texts": ["The another man comes and pulls down his pants."], "durations": null, "exact_frames_per_prompt": [47], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/4GtqaxwORHQ.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 33, "npz_gt_video_start_frame": 33, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 33, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_413_0_1"}, {"texts": ["A person whose hand is visible is holding a pineapple."], "durations": null, "exact_frames_per_prompt": [67], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/0_OVwopYX-s.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 13, "npz_gt_video_start_frame": 13, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 13, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_415_0_0"}, {"texts": ["A boy wearing a red t-shirt is standing on the floor and holding a blue trouser."], "durations": null, "exact_frames_per_prompt": [31], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-riGpxoagdg.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 35, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_417_0_0"}, {"texts": ["The boy starts folding the trouser on the bed."], "durations": null, "exact_frames_per_prompt": [45], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-riGpxoagdg.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 35, "npz_gt_video_start_frame": 35, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 35, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_417_0_1"}, {"texts": ["A person wearing a green cloth whose only hand is visible is holding a pencil and writing on the white paper."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/0zz4NLgfYgA.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_418_0_0"}, {"texts": ["A woman wearing a green t-shirt and black trousers is standing on the left and holding a cow's tail.\n a man wearing black hoddie is looking at the cow"], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-tTErlr0HuI.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_419_0_0"}, {"texts": ["A white cow with black spots is standing and getting milked a woman with green t-shirt is holding the cow's tail."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-tTErlr0HuI.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_419_3_0"}, {"texts": ["A baby wearing a pink cloth is lying on the white surface and crying."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/0piS75q1Uzk.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_423_0_0"}, {"texts": ["A man wearing a grey jacket is sitting on a black chair and counting the money."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/06FObXSod4E.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_424_0_0"}, {"texts": ["A man wearing a gray vest and black shorts is helping a woman and starts walking a woman with red stripped vest moves back as she is scared"], "durations": null, "exact_frames_per_prompt": [77], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-gJmR_kGWXE.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 81, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_428_0_0"}, {"texts": ["A woman wearing a white top and blue skirt is standing, watching the first man a woman wearing a multi-color top and black tights is standing placing the pipes."], "durations": null, "exact_frames_per_prompt": [39], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-gJmR_kGWXE.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 43, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_428_1_0"}, {"texts": ["The woman earing a white top and blue skirt is showing signal a woman wearing a multi-color top and black tights is standing showing a thumbs signal."], "durations": null, "exact_frames_per_prompt": [20], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-gJmR_kGWXE.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 43, "npz_gt_video_start_frame": 43, "npz_gt_video_end_frame": 63, "skip_frames_after_generation": 43, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_428_1_1"}, {"texts": ["The woman wearing a white top and blue skirt makes a shocking face while the man comes towards the pipes and doing something"], "durations": null, "exact_frames_per_prompt": [18], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-gJmR_kGWXE.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 63, "npz_gt_video_start_frame": 63, "npz_gt_video_end_frame": 81, "skip_frames_after_generation": 63, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_428_1_2"}, {"texts": ["A woman wearing a multi-color top and black tights is standing, placing the pipes a woman wearing a white top and blue skirt is standing,and also placing the pipes"], "durations": null, "exact_frames_per_prompt": [44], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-gJmR_kGWXE.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 48, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_428_2_0"}, {"texts": ["The woman is showing a thumbs signal while the man walks towards the pipes and doing something"], "durations": null, "exact_frames_per_prompt": [32], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-gJmR_kGWXE.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 48, "npz_gt_video_start_frame": 48, "npz_gt_video_end_frame": 81, "skip_frames_after_generation": 48, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_428_2_1"}, {"texts": ["A man wearing a black jacket and light brown jeans is standing and spreading the red sauce on the meat slice with the wooden spatula while holding it in his right hand and holding a bowl in his left hand while the man wearing black t-shirt stands in front and says something"], "durations": null, "exact_frames_per_prompt": [51], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-QJz_YH0TMA.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 5, "npz_gt_video_start_frame": 5, "npz_gt_video_end_frame": 56, "skip_frames_after_generation": 5, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_431_1_0"}, {"texts": ["The man is standing on the right side and talking to the third man."], "durations": null, "exact_frames_per_prompt": [24], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-QJz_YH0TMA.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 56, "npz_gt_video_start_frame": 56, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 56, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_431_1_1"}, {"texts": ["A man wearing a black jacket, brown jeans, and a black woolen cap, is standing and putting something on the table, then standing on the right side and talking to the first man a man with black t-shirt in the left is filming their actions"], "durations": null, "exact_frames_per_prompt": [32], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-QJz_YH0TMA.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 36, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_431_2_0"}, {"texts": ["A person whose hands are visible is drawing a tattoo on the upper back side of a woman while the woman is lying on a bed  and having a tattoo drawn on the upper side of her back."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-pOhuX7_wPo.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_432_1_0"}, {"texts": ["A boy wearing a black t-shirt is sitting and writing on a white sheet with a pen a man with a blue hoddie on the right is laughing by looking at them."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/1DP7shELdso.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_434_1_0"}, {"texts": ["A girl wearing a black top is sitting and writing on a white sheet a boy wearing a black t-shirt is writing something on white paper."], "durations": null, "exact_frames_per_prompt": [33], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/1DP7shELdso.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 37, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_434_2_0"}, {"texts": ["The girl wearing a black top stands up a man with a blue hoddie on the right is laughing by looking at them."], "durations": null, "exact_frames_per_prompt": [43], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/1DP7shELdso.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 37, "npz_gt_video_start_frame": 37, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 37, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_434_2_1"}, {"texts": ["A man wearing a blue sweatshirt is sitting and laughing while the two boys were writing their notes and the girl jumped up and down and writing the notes."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/1DP7shELdso.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_434_3_0"}, {"texts": ["A woman wearing a white shirt and a christmas cap sits on the brown floor with the baby and holding a paper bag a baby wearing a red dress is sitting on the woman's lap and the dog comes towards the baby"], "durations": null, "exact_frames_per_prompt": [69], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/0foVlayF72U.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 73, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_438_0_0"}, {"texts": ["The woman wearing a white shirt and a christmas cap puts it on the right side."], "durations": null, "exact_frames_per_prompt": [7], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/0foVlayF72U.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 73, "npz_gt_video_start_frame": 73, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 73, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_438_0_1"}, {"texts": ["A baby wearing a red dress is sitting on the woman's lap a woman wearing a white shirt and a Christmas cap sits on the brown floor with the baby and holds a paper bag and the dog comes towards the baby."], "durations": null, "exact_frames_per_prompt": [59], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/0foVlayF72U.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 63, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_438_1_0"}, {"texts": ["The baby is holding a piece of white wrapping paper in his hand the woman puts a bag on the left side."], "durations": null, "exact_frames_per_prompt": [17], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/0foVlayF72U.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 63, "npz_gt_video_start_frame": 63, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 63, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_438_1_1"}, {"texts": ["A girl wearing a pink sweatshirt and black pants is walking under the trees a woman wearing blue top is holding baby with pink t-shirt and and pink pant walks on green surface"], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-n5JEmTInzM.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_439_1_0"}, {"texts": ["A woman wearing a blue top and black jeans is holding a second girl."], "durations": null, "exact_frames_per_prompt": [23], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-n5JEmTInzM.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 23, "npz_gt_video_start_frame": 23, "npz_gt_video_end_frame": 46, "skip_frames_after_generation": 23, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_439_2_0"}, {"texts": ["The woman starts walking."], "durations": null, "exact_frames_per_prompt": [19], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-n5JEmTInzM.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 46, "npz_gt_video_start_frame": 46, "npz_gt_video_end_frame": 65, "skip_frames_after_generation": 46, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_439_2_1"}, {"texts": ["The woman bends over."], "durations": null, "exact_frames_per_prompt": [9], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-n5JEmTInzM.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 65, "npz_gt_video_start_frame": 65, "npz_gt_video_end_frame": 74, "skip_frames_after_generation": 65, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_439_2_2"}, {"texts": ["The woman gets up."], "durations": null, "exact_frames_per_prompt": [6], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-n5JEmTInzM.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 74, "npz_gt_video_start_frame": 74, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 74, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_439_2_3"}, {"texts": ["A man wearing light grey clothes is sitting on the grey surface and fitting exercise bandages and vet wrap around the leg of a horse."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-3L-WLPIOuo.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_440_0_0"}, {"texts": ["A black horse is standing on the grey surface while a man fitting exercise bandages and vet wrap around the horse leg."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-3L-WLPIOuo.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_440_1_0"}, {"texts": ["A white-brown cow is standing on yellow grasses."], "durations": null, "exact_frames_per_prompt": [20], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/2kxYniFmH0c.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 60, "npz_gt_video_start_frame": 60, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 60, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_441_0_0"}, {"texts": ["A woman wearing white dress is standing and wrapping a plaster to the second person's hand."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-X7BUQa7Dr0.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_442_0_0"}, {"texts": ["A man wearing a blue cloth is sitting on a bed and getting a plaster in hand while the women plastering the man and saying something"], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-X7BUQa7Dr0.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_442_1_0"}, {"texts": ["A person wearing brown shirt is tying a knot on an orange wire which is tied on a wood log."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-LIY-OiDBaI.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_443_0_0"}, {"texts": ["A man wearing a black printed t-shirt is walking on a muddy surface while talking."], "durations": null, "exact_frames_per_prompt": [26], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/2wWwaExlzDk.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 30, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_444_0_0"}, {"texts": ["The man wearing a black printed t-shirt drinking from a glass bottle."], "durations": null, "exact_frames_per_prompt": [50], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/2wWwaExlzDk.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 30, "npz_gt_video_start_frame": 30, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 30, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_444_0_1"}, {"texts": ["A person whose hand is visible is holding a air filter and fixing it."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-0M6S1qBn8s.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_446_0_0"}, {"texts": ["A baby girl wearing a white t-shirt is sitting while holding a book and watching."], "durations": null, "exact_frames_per_prompt": [33], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/0QJYsKM0r9c.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 37, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_448_0_0"}, {"texts": ["The baby girl turning the pages of the book."], "durations": null, "exact_frames_per_prompt": [43], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/0QJYsKM0r9c.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 37, "npz_gt_video_start_frame": 37, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 37, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_448_0_1"}, {"texts": ["A baby girl wearing a white t-shirt is sitting, holding, and turning the pages of the book."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/0QJYsKM0r9c.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_449_0_0"}, {"texts": ["A white sheep is lying on the brown floor a man with green vest kicks the sheep"], "durations": null, "exact_frames_per_prompt": [59], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-MN-Zv7zQsU.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 63, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_451_0_0"}, {"texts": ["The sheep stands up and moves a man with blue hoddie comes forward and moves the wool"], "durations": null, "exact_frames_per_prompt": [17], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-MN-Zv7zQsU.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 63, "npz_gt_video_start_frame": 63, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 63, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_451_0_1"}, {"texts": ["A man wearing a green shirt and black pants is standing and cutting the wool from the sheep."], "durations": null, "exact_frames_per_prompt": [51], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-MN-Zv7zQsU.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 55, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_451_1_0"}, {"texts": ["The man is pushing the sheep a man wearing a blue jacket is moving wool with the legs."], "durations": null, "exact_frames_per_prompt": [25], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-MN-Zv7zQsU.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 55, "npz_gt_video_start_frame": 55, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 55, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_451_1_1"}, {"texts": ["A man wearing a blue jacket is standing on the brown floor and looking at the sheep, then moving wool from the legs a man wearing a green shirt and black pants is pushing the sheep."], "durations": null, "exact_frames_per_prompt": [13], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-MN-Zv7zQsU.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 67, "npz_gt_video_start_frame": 67, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 67, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_451_2_0"}, {"texts": ["A person whose hand is visible is holding a watermelon on which a coin is inserted,and then rolling the coin on the watermelon."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/8q7VsYnOm3U.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_452_0_0"}, {"texts": ["A woman, whom hands are visible only, is aligning some black stripes in the middle of the two boards."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/1IMWCMxxSVU.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_457_0_0"}, {"texts": ["A person whose only hands and legs are visible is sitting on the floor and playing with a paper toy."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/11Zqz-Y_pxA.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_459_0_0"}, {"texts": ["A girl wearing a pink top is sitting on a green mat and eating ice cream with a grey spoon."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/0KUHoXyYiIs.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_464_0_0"}, {"texts": ["A person, wearing a blue-black cloth, is holding a half torn white paper in his left hand."], "durations": null, "exact_frames_per_prompt": [16], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/02lHl-AlVZw.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 20, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_466_0_0"}, {"texts": ["The video starts to play in reverse order and the paper is back to the stage before it was torn by the person."], "durations": null, "exact_frames_per_prompt": [27], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/02lHl-AlVZw.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 20, "npz_gt_video_start_frame": 20, "npz_gt_video_end_frame": 47, "skip_frames_after_generation": 20, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_466_0_1"}, {"texts": ["A person wearing blue-black clothes, is standing and holding half torn white paper in the left hand."], "durations": null, "exact_frames_per_prompt": [16], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/02lHl-AlVZw.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 20, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_467_1_0"}, {"texts": ["The video starts to play in the reverse. The paper is back to the stage before it was torn by the person"], "durations": null, "exact_frames_per_prompt": [27], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/02lHl-AlVZw.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 20, "npz_gt_video_start_frame": 20, "npz_gt_video_end_frame": 47, "skip_frames_after_generation": 20, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_467_1_1"}, {"texts": ["A person whose hands are visible is feeding water to a black bird with the help of a yellow water feeder."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/0fW4pakAOcY.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_46_0_0"}, {"texts": ["A man wearing a white shirt and black pants standing on the right side is speaking while the man wearing a white shirt and black pants is tucking the bedspread under the mattress."], "durations": null, "exact_frames_per_prompt": [14], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/I_nL-KkVdKY.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 18, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_471_0_0"}, {"texts": ["The man leans forward to remove creases from the bed."], "durations": null, "exact_frames_per_prompt": [63], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/I_nL-KkVdKY.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 18, "npz_gt_video_start_frame": 18, "npz_gt_video_end_frame": 81, "skip_frames_after_generation": 18, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_471_0_1"}, {"texts": ["A man wearing a white shirt and black pants is tucking the bedspread under the mattress while a man leans forward to remove creases from the bed."], "durations": null, "exact_frames_per_prompt": [77], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/I_nL-KkVdKY.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 81, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_471_1_0"}, {"texts": ["A man wearing a dark gray jacket, a light gray t-shirt and goggles is standing on the right side of the car and moving his hand while speaking."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/45HP71WKfo8.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_476_0_0"}, {"texts": ["A man wearing a white t-shirt, black shorts and white shoes is playing golf a man wearing a blue t-shirt, gray shorts and black shoes is lying on the green grass surface while holding a mobile phone and is recording the golf played by the first man."], "durations": null, "exact_frames_per_prompt": [62], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-BtQqYdMQU8.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 66, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_477_0_ms_0"}, {"texts": ["A man wearing a blue t-shirt, gray shorts and black shoes is lying on the green grass surface while holding a mobile phone and is recording the golf played by the first man a man wearing a white t-shirt, black shorts and white shoes is playing golf."], "durations": null, "exact_frames_per_prompt": [62], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-BtQqYdMQU8.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 66, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_477_1_0"}, {"texts": ["A man wearing a gray t-shirt and checkered gray pants is sitting while bending his legs and is looking at the first man and then looking towards the right side while a man with blue t-shirt films the man in white t-shirt"], "durations": null, "exact_frames_per_prompt": [37], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-BtQqYdMQU8.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 41, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_477_2_ms_0"}, {"texts": ["A man whose only upper body is visible wearing a checked blue shirt, is speaking while moving hands while standing."], "durations": null, "exact_frames_per_prompt": [13], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-BtQqYdMQU8.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 67, "npz_gt_video_start_frame": 67, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 67, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_477_3_ms_0"}, {"texts": ["A boy wearing orange-gray cloth is sitting and holding an ice-cream."], "durations": null, "exact_frames_per_prompt": [16], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/0uJONCBOI-E.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 20, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_478_0_0"}, {"texts": ["The boy wearing orange-gray cloth is putting it in his mouth."], "durations": null, "exact_frames_per_prompt": [60], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/0uJONCBOI-E.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 20, "npz_gt_video_start_frame": 20, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 20, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_478_0_1"}, {"texts": ["A woman of whom only face and hands are visible is rubbing her hands on her neck."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/23t0tlA0IqM.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_479_0_0"}, {"texts": ["A person whose hands are visible is opening a colorful box."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/0D5LYM_0-ro.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_47_0_0"}, {"texts": ["A group of people is standing behind the first group in the audience area."], "durations": null, "exact_frames_per_prompt": [47], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/0ruWrr2x65U.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 51, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_480_1_ms_0"}, {"texts": ["A man wearing a light brown shirt is standing and playing golf."], "durations": null, "exact_frames_per_prompt": [41], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/1W2DduDv7Sk.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 45, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_481_0_0"}, {"texts": ["A person whose only a leg is visible is wearing white slippers and riding on a blue boat."], "durations": null, "exact_frames_per_prompt": [11], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/1Ynzd92PKQM.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 15, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_482_2_0"}, {"texts": ["A boy wearing a green t-shirt is standing behind the countertop and eating a big slice of watermelon while the person in maroon t-shirt is taking something from the refrigerator."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/31LazXkQKyw.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_483_0_0"}, {"texts": ["A man wearing a dark brown t-shirt is standing behind the boy moving his hand."], "durations": null, "exact_frames_per_prompt": [63], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/31LazXkQKyw.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 67, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_483_1_0"}, {"texts": ["The man is touching the refrigerator door while a bog with green t-shirt is eating watermelon"], "durations": null, "exact_frames_per_prompt": [13], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/31LazXkQKyw.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 67, "npz_gt_video_start_frame": 67, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 67, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_483_1_1"}, {"texts": ["A boy wearing a red life jacket and printed shorts is eating an ice-cream with a spoon while walking on the brown surface."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/0xXEfgNiJbQ.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_484_1_0"}, {"texts": ["A child wearing a white t-shirt and blue shorts is walking while holding an object and then starts crawling on the brown surface a boy wearing a red life jacket and printed shorts walking on the brown surface."], "durations": null, "exact_frames_per_prompt": [29], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/0xXEfgNiJbQ.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 51, "npz_gt_video_start_frame": 51, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 51, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_484_3_0"}, {"texts": ["A man wearing a purple t-shirt and black trousers is sitting and giving instructions about the black tires."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/0nHDmrUwh_g.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_485_0_0"}, {"texts": ["A person whose hands are visible is touching a white surface."], "durations": null, "exact_frames_per_prompt": [10], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/0nHDmrUwh_g.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 14, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_486_0_0"}, {"texts": ["The person whose hands are visible is touching the tyre."], "durations": null, "exact_frames_per_prompt": [66], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/0nHDmrUwh_g.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 14, "npz_gt_video_start_frame": 14, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 14, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_486_0_1"}, {"texts": ["A man wearing purple clothes is sitting and talking."], "durations": null, "exact_frames_per_prompt": [25], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/0nHDmrUwh_g.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 33, "npz_gt_video_start_frame": 33, "npz_gt_video_end_frame": 58, "skip_frames_after_generation": 33, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_486_1_0"}, {"texts": ["The man wearing purple clothes is touching the tyre."], "durations": null, "exact_frames_per_prompt": [22], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/0nHDmrUwh_g.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 58, "npz_gt_video_start_frame": 58, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 58, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_486_1_1"}, {"texts": [" A man wearing white clothes is sitting and holding a vodka glass"], "durations": null, "exact_frames_per_prompt": [63], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-ueQpq7t1VE.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 67, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_488_0_0"}, {"texts": ["The man is drinking."], "durations": null, "exact_frames_per_prompt": [13], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-ueQpq7t1VE.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 67, "npz_gt_video_start_frame": 67, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 67, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_488_0_1"}, {"texts": ["A man whose hands are visible is opening a box on a white surface."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/0D5LYM_0-ro.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_48_0_0"}, {"texts": ["A woman wearing a yellow towel on her head is sitting and spreading cream on her face."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/2BG6Plw5E5U.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_490_0_0"}, {"texts": ["A person whose hand is visible is shredding papers in a shredding machine while the cat is moving around"], "durations": null, "exact_frames_per_prompt": [50], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/5i4pUhkfU_k.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 8, "npz_gt_video_start_frame": 8, "npz_gt_video_end_frame": 58, "skip_frames_after_generation": 8, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_492_0_0"}, {"texts": ["A black white cat is climbing on a shredding machine, then sits on the floor."], "durations": null, "exact_frames_per_prompt": [31], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/5i4pUhkfU_k.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 35, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_492_1_0"}, {"texts": ["The cat starts walking."], "durations": null, "exact_frames_per_prompt": [36], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/5i4pUhkfU_k.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 35, "npz_gt_video_start_frame": 35, "npz_gt_video_end_frame": 71, "skip_frames_after_generation": 35, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_492_1_1"}, {"texts": ["A man wearing black suit, white shirt and red tie is standing in front."], "durations": null, "exact_frames_per_prompt": [29], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/5Ulam2i8QrA.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 33, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_493_0_0"}, {"texts": ["The man wearing black suit, white shirt and red tie moves his neck."], "durations": null, "exact_frames_per_prompt": [47], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/5Ulam2i8QrA.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 33, "npz_gt_video_start_frame": 33, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 33, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_493_0_1"}, {"texts": ["A man on the right wearing a red hoodie is holding a snake in his hands."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/1vGvrYf13QM.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_494_0_0"}, {"texts": ["A grey rat snake is in the hands of the first man."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/1vGvrYf13QM.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_494_2_0"}, {"texts": ["A woman wearing a white dress is sitting on a sofa and reading a book while the baby is laughing by seeing the woman"], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/0CCkipT9C-k.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_495_0_0"}, {"texts": ["A man wearing a red t-shirt is holding a rope tied to a dead fish while a boy is standing behind and watching what the man is doing"], "durations": null, "exact_frames_per_prompt": [19], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-7Jqn00FWHA.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 23, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_496_0_0"}, {"texts": ["The man throws it into the water.  while the boy is trying to hold the rope"], "durations": null, "exact_frames_per_prompt": [45], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-7Jqn00FWHA.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 23, "npz_gt_video_start_frame": 23, "npz_gt_video_end_frame": 68, "skip_frames_after_generation": 23, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_496_0_1"}, {"texts": ["The man tries to hold the rope tied to a dead fish."], "durations": null, "exact_frames_per_prompt": [12], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-7Jqn00FWHA.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 68, "npz_gt_video_start_frame": 68, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 68, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_496_0_2"}, {"texts": ["A boy wearing a gray t-shirt is standing in the back while holding a rope while the man with red t-shirt is throwing fish's face in  the sea"], "durations": null, "exact_frames_per_prompt": [49], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-7Jqn00FWHA.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 53, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_496_1_0"}, {"texts": ["The boy wearing a gray t-shirt is moving and trying to pull the rope while the man in the red t-shirt is also pulling the rope."], "durations": null, "exact_frames_per_prompt": [27], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-7Jqn00FWHA.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 53, "npz_gt_video_start_frame": 53, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 53, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_496_1_1"}, {"texts": ["A black fish is swimming in the sea."], "durations": null, "exact_frames_per_prompt": [16], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-7Jqn00FWHA.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 37, "npz_gt_video_start_frame": 37, "npz_gt_video_end_frame": 53, "skip_frames_after_generation": 37, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_497_0_0"}, {"texts": ["A man wearing a red t-shirt is holding a rope tied to a dead fish a boy is watching by standing beside the man"], "durations": null, "exact_frames_per_prompt": [10], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-7Jqn00FWHA.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 14, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_497_1_0"}, {"texts": ["The man throws it into the water while the boy is trying to hold the rope tightly"], "durations": null, "exact_frames_per_prompt": [54], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-7Jqn00FWHA.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 14, "npz_gt_video_start_frame": 14, "npz_gt_video_end_frame": 68, "skip_frames_after_generation": 14, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_497_1_1"}, {"texts": ["The man tries to hold it."], "durations": null, "exact_frames_per_prompt": [12], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-7Jqn00FWHA.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 68, "npz_gt_video_start_frame": 68, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 68, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_497_1_2"}, {"texts": ["A boy wearing a light gray t-shirt is standing in the back while the man with the red t-shirt is throwing fish's face in  the sea."], "durations": null, "exact_frames_per_prompt": [13], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-7Jqn00FWHA.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 17, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_497_2_0"}, {"texts": ["The boy wearing a light gray t-shirt is holding the rope, moving a man wearing a red t-shirt throws the rope attached to some object into the sea"], "durations": null, "exact_frames_per_prompt": [36], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-7Jqn00FWHA.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 17, "npz_gt_video_start_frame": 17, "npz_gt_video_end_frame": 53, "skip_frames_after_generation": 17, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_497_2_1"}, {"texts": ["The boy wearing a light gray t-shirt is trying to hold the rope while the man in the red t-shirt is also pulling the rope."], "durations": null, "exact_frames_per_prompt": [27], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-7Jqn00FWHA.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 53, "npz_gt_video_start_frame": 53, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 53, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_497_2_2"}, {"texts": ["A person wearing a red sweater is sitting on the floor, and unwrapping something."], "durations": null, "exact_frames_per_prompt": [43], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/0PLaN0hmMDY.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 47, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_499_0_0"}, {"texts": ["The person is cutting a paper with a scissor."], "durations": null, "exact_frames_per_prompt": [33], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/0PLaN0hmMDY.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 47, "npz_gt_video_start_frame": 47, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 47, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_499_0_1"}, {"texts": ["A man wearing a white shirt and grey pants is standing, holding a golf stick in his hand, putting the stick near the golf ball."], "durations": null, "exact_frames_per_prompt": [26], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/01PhSo0gdcE.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 30, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_501_0_0"}, {"texts": ["The man hits the ball."], "durations": null, "exact_frames_per_prompt": [50], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/01PhSo0gdcE.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 30, "npz_gt_video_start_frame": 30, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 30, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_501_0_1"}, {"texts": ["A boy wearing a printed t-shirt is standing on the right side while other boy trying to pouring something in a glass."], "durations": null, "exact_frames_per_prompt": [32], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/71fTeuqx3Ys.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 36, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_502_1_0"}, {"texts": ["The boy looking at the glass while smiling."], "durations": null, "exact_frames_per_prompt": [44], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/71fTeuqx3Ys.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 36, "npz_gt_video_start_frame": 36, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 36, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_502_1_1"}, {"texts": ["A man wearing a yellow-black jacket is moving his hand."], "durations": null, "exact_frames_per_prompt": [32], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/16_opdOffmQ.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 9, "npz_gt_video_start_frame": 9, "npz_gt_video_end_frame": 41, "skip_frames_after_generation": 9, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_504_0_0"}, {"texts": ["The man is putting a black grill on the wooden blocks."], "durations": null, "exact_frames_per_prompt": [22], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/16_opdOffmQ.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 41, "npz_gt_video_start_frame": 41, "npz_gt_video_end_frame": 63, "skip_frames_after_generation": 41, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_504_0_1"}, {"texts": ["An old man wearing a dark-green t-shirt is in bending position and brushing the legs of the horse."], "durations": null, "exact_frames_per_prompt": [56], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/1S_Dw8RCMrQ.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 60, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_508_0_0"}, {"texts": ["The old man stands up and starts brushing the upper body of the horse."], "durations": null, "exact_frames_per_prompt": [20], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/1S_Dw8RCMrQ.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 60, "npz_gt_video_start_frame": 60, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 60, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_508_0_1"}, {"texts": ["A black horse is standing on a grey surface and getting its legs and upper body crushed by the old man."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/1S_Dw8RCMrQ.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_508_2_0"}, {"texts": ["A woman wearing shorts is at first walking along with a dog in a straight direction."], "durations": null, "exact_frames_per_prompt": [16], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/3oERY0un6Yo.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 20, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_509_0_0"}, {"texts": ["The woman turns into backward direction and the dog also moving with her"], "durations": null, "exact_frames_per_prompt": [49], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/3oERY0un6Yo.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 20, "npz_gt_video_start_frame": 20, "npz_gt_video_end_frame": 69, "skip_frames_after_generation": 20, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_509_0_1"}, {"texts": ["The woman lifts the collar belt of the dog."], "durations": null, "exact_frames_per_prompt": [11], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/3oERY0un6Yo.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 69, "npz_gt_video_start_frame": 69, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 69, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_509_0_2"}, {"texts": ["A black dog is at first walking in a straight direction along with the woman in shorts."], "durations": null, "exact_frames_per_prompt": [23], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/3oERY0un6Yo.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 27, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_509_1_0"}, {"texts": ["The dog turns into the backward direction and then sits on the ground facing the right direction as the woman in blue t-shirt feeds it something"], "durations": null, "exact_frames_per_prompt": [45], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/3oERY0un6Yo.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 27, "npz_gt_video_start_frame": 27, "npz_gt_video_end_frame": 72, "skip_frames_after_generation": 27, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_509_1_1"}, {"texts": ["The dog jumps on the ground while the woman removes the rope from her neck."], "durations": null, "exact_frames_per_prompt": [8], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/3oERY0un6Yo.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 72, "npz_gt_video_start_frame": 72, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 72, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_509_1_2"}, {"texts": ["A man wearing a black jacket is standing and taking out an apple from the red fruit picker."], "durations": null, "exact_frames_per_prompt": [29], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/1BPIqXezHjw.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 33, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_50_0_0"}, {"texts": ["A woman wearing a blue t-shirt and white shorts is walking with a dog in the park."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/3oERY0un6Yo.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_510_0_0"}, {"texts": ["A black dog walking with the first woman."], "durations": null, "exact_frames_per_prompt": [33], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/3oERY0un6Yo.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 37, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_510_1_0"}, {"texts": ["The black dog sits down girl moved a bit to the right side"], "durations": null, "exact_frames_per_prompt": [32], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/3oERY0un6Yo.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 37, "npz_gt_video_start_frame": 37, "npz_gt_video_end_frame": 69, "skip_frames_after_generation": 37, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_510_1_1"}, {"texts": ["The black dog starts jumping."], "durations": null, "exact_frames_per_prompt": [11], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/3oERY0un6Yo.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 69, "npz_gt_video_start_frame": 69, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 69, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_510_1_2"}, {"texts": ["A boy wearing a grey printed t-shirt is licking an ice-cream as well as wiping his face from his hand."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/0FnVWxvziQQ.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_513_0_0"}, {"texts": ["A man wearing a gray t-shirt and dark blue pants is sitting on the wooden chair while a man wearing an orange t-shirt and cream pants is sitting in a wooden chair, holding a water bottle in one hand and a black bottle in the other hand and opened the cap of the glass bottle with the help of water bottle."], "durations": null, "exact_frames_per_prompt": [55], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/SrzPuu_K-vw.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 59, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_514_1_0"}, {"texts": ["The man wearing a gray t-shirt and dark blue pants is smiling while looking at the first man while the first man is trying to stand up"], "durations": null, "exact_frames_per_prompt": [21], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/SrzPuu_K-vw.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 59, "npz_gt_video_start_frame": 59, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 59, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_514_1_1"}, {"texts": ["A person whose only hands are visible is making food."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/2hbNicjSNec.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_515_0_0"}, {"texts": ["A woman wearing black clothes is at first spills the food from the frypan while tossing the food."], "durations": null, "exact_frames_per_prompt": [21], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/11Qs_g4SmCM.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 25, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_517_0_0"}, {"texts": ["Then she picks up the food and puts it into the fry pan properly."], "durations": null, "exact_frames_per_prompt": [55], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/11Qs_g4SmCM.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 25, "npz_gt_video_start_frame": 25, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 25, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_517_0_1"}, {"texts": ["A woman wearing black clothes is flipping an omelette in a pan."], "durations": null, "exact_frames_per_prompt": [16], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/11Qs_g4SmCM.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 20, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_518_0_0"}, {"texts": ["The woman picks it up from the wall to put it back in the pan."], "durations": null, "exact_frames_per_prompt": [59], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/11Qs_g4SmCM.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 20, "npz_gt_video_start_frame": 20, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 20, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_518_0_1"}, {"texts": ["A person wearing gray clothes is standing and holding a knife and watermelon and peeling it on a brown board."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/agGBonJCCiQ.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_51_0_0"}, {"texts": ["A person whose only hands are visible is folding the cloth on the green surface."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/155oJFYi4W0.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_520_0_0"}, {"texts": ["A man wearing a blue jacket, gray pants, and a white bee veil is putting his hand towards the honeycomb."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/7ZHSWPrJrI0.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_521_1_0"}, {"texts": ["A woman is explaining and cooking food."], "durations": null, "exact_frames_per_prompt": [40], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/3RLO4C-MXnw.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 44, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_522_0_0"}, {"texts": ["Then sits down to explain more."], "durations": null, "exact_frames_per_prompt": [24], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/3RLO4C-MXnw.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 44, "npz_gt_video_start_frame": 44, "npz_gt_video_end_frame": 68, "skip_frames_after_generation": 44, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_522_0_1"}, {"texts": ["A man wearing a black suit is standing on the right side and explaining the weather forecasting report while touching the big display."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-3etcrOAM-E.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_524_0_0"}, {"texts": ["A man is lying in bed, covered in a white blanket, and eating a carrot."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/2MlmjE0Il60.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_529_0_0"}, {"texts": ["A man is lying in the bed and eating a carrot."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/2MlmjE0Il60.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_530_0_0"}, {"texts": ["A girl wearing a pink cloth is sitting on the light brown surface and folding the clothes."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/17KlsgcoHJM.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_531_0_0"}, {"texts": ["A boy wearing a dark blue t-shirt and red pants is sitting on the gray surface and showing his hands with an open palm while the other kids around him are watching the boy"], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-dWIkANNXL8.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_532_1_0"}, {"texts": ["A man wearing a khaki dress is sitting on the gray surface and holding a baby snake and showing it to the group of kids and the boy a boy wearing blue t-shirt moved his hands towards the snake for catching"], "durations": null, "exact_frames_per_prompt": [22], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-dWIkANNXL8.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 26, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_532_2_0"}, {"texts": ["A boy wearing a dark blue t-shirt and red shorts is sitting on the gray surface and showing his hands with an open palm a girl in black and blue t-shirt closing her hand together and moving her head left and right"], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-dWIkANNXL8.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_533_1_0"}, {"texts": ["A man wearing a light brown uniform is sitting on the gray surface and holding a baby snake while the kids in front of him are listening to the man"], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-dWIkANNXL8.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_533_2_0"}, {"texts": ["A black baby snake is being holded by a man while the kids are watching it"], "durations": null, "exact_frames_per_prompt": [26], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-dWIkANNXL8.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 30, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_533_3_0"}, {"texts": ["A person wearing a grey jacket is sitting and cutting a brown potato with his hands."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/3fLJMqLtbBU.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_537_0_0"}, {"texts": ["A woman wearing black clothes is standing and holding a pan."], "durations": null, "exact_frames_per_prompt": [23], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/0qyu0s-bugk.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 27, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_538_0_0"}, {"texts": ["The woman is flipping the bread."], "durations": null, "exact_frames_per_prompt": [53], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/0qyu0s-bugk.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 27, "npz_gt_video_start_frame": 27, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 27, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_538_0_1"}, {"texts": ["A black dog is sitting on the floor while a woman lifts the clothes from the floor."], "durations": null, "exact_frames_per_prompt": [8], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/3YeFK_mBl48.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 12, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_53_1_0"}, {"texts": ["The dog starts walking after the woman while holding a cloth in its mouth the women carrying clothes to the washing area"], "durations": null, "exact_frames_per_prompt": [28], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/3YeFK_mBl48.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 12, "npz_gt_video_start_frame": 12, "npz_gt_video_end_frame": 41, "skip_frames_after_generation": 12, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_53_1_1"}, {"texts": ["The dog then puts the cloth in the washing machine while a woman puts the clothes in the washing machine"], "durations": null, "exact_frames_per_prompt": [38], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/3YeFK_mBl48.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 41, "npz_gt_video_start_frame": 41, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 41, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_53_1_2"}, {"texts": ["A girl wearing a blue top and black bottoms is making the bed."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/jxJEVdbO4O8.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_540_0_0"}, {"texts": ["A girl wearing a blue t-shirt and black leggings is sitting on her knees."], "durations": null, "exact_frames_per_prompt": [13], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/jxJEVdbO4O8.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 17, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_541_0_0"}, {"texts": ["The girl wearing a blue t-shirt and black leggings is tapping her hands on the bed while setting up the bed sheet on the bed."], "durations": null, "exact_frames_per_prompt": [63], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/jxJEVdbO4O8.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 17, "npz_gt_video_start_frame": 17, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 17, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_541_0_1"}, {"texts": ["A man wearing a brown t-shirt and red cap is standing behind the counter and making a drink in a blue bottle."], "durations": null, "exact_frames_per_prompt": [40], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/1vvqjBt16fs.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 44, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_547_0_0"}, {"texts": ["The man is pouring it in glass."], "durations": null, "exact_frames_per_prompt": [36], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/1vvqjBt16fs.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 44, "npz_gt_video_start_frame": 44, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 44, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_547_0_1"}, {"texts": ["A man wearing a red cap is standing, shaking a shaker other peoples are watching him."], "durations": null, "exact_frames_per_prompt": [38], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/1vvqjBt16fs.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 42, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_548_0_0"}, {"texts": ["The man wearing a red cap is pouring the drink into the glass while a man wearing a white shirt is clapping."], "durations": null, "exact_frames_per_prompt": [22], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/1vvqjBt16fs.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 42, "npz_gt_video_start_frame": 42, "npz_gt_video_end_frame": 64, "skip_frames_after_generation": 42, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_548_0_1"}, {"texts": ["A man wearing black trousers is standing while a man in brown t-shirt is pouring a drink in the glass"], "durations": null, "exact_frames_per_prompt": [46], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/1vvqjBt16fs.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 50, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_548_2_0"}, {"texts": ["The man is clapping and watching the first man and the other people in the crowd is watching the activity"], "durations": null, "exact_frames_per_prompt": [28], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/1vvqjBt16fs.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 50, "npz_gt_video_start_frame": 50, "npz_gt_video_end_frame": 78, "skip_frames_after_generation": 50, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_548_2_1"}, {"texts": ["A man wearing an olive green shirt is lying on a sofa."], "durations": null, "exact_frames_per_prompt": [51], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-Qh18sFtZD8.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 55, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_54_0_0"}, {"texts": ["The man wearing an olive green shirt moving his hands while speaking."], "durations": null, "exact_frames_per_prompt": [25], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-Qh18sFtZD8.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 55, "npz_gt_video_start_frame": 55, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 55, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_54_0_1"}, {"texts": ["A man wearing a black raincoat is arranging a white chair on the grey surface."], "durations": null, "exact_frames_per_prompt": [20], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/2Z-xGLcdOIM.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 24, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_551_0_0"}, {"texts": ["The man is sitting on it and reading a newspaper."], "durations": null, "exact_frames_per_prompt": [56], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/2Z-xGLcdOIM.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 24, "npz_gt_video_start_frame": 24, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 24, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_551_0_1"}, {"texts": ["A man wearing a light blue shirt is standing."], "durations": null, "exact_frames_per_prompt": [61], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/0dn9Djvm74w.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 65, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_553_0_0"}, {"texts": ["The man wearing a light blue shirt is tying a tie."], "durations": null, "exact_frames_per_prompt": [15], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/0dn9Djvm74w.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 65, "npz_gt_video_start_frame": 65, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 65, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_553_0_1"}, {"texts": ["A man wearing blue shorts lifts the black tire from the floor."], "durations": null, "exact_frames_per_prompt": [13], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/3BNVgA_GLfo.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 17, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_554_0_0"}, {"texts": ["The man puts it on the bicycle."], "durations": null, "exact_frames_per_prompt": [45], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/3BNVgA_GLfo.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 17, "npz_gt_video_start_frame": 17, "npz_gt_video_end_frame": 62, "skip_frames_after_generation": 17, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_554_0_1"}, {"texts": ["Then the man turns the bicycle."], "durations": null, "exact_frames_per_prompt": [19], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/3BNVgA_GLfo.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 62, "npz_gt_video_start_frame": 62, "npz_gt_video_end_frame": 81, "skip_frames_after_generation": 62, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_554_0_2"}, {"texts": ["A person wearing a blue t-shirt and blue shorts lifts the bicycle tire."], "durations": null, "exact_frames_per_prompt": [18], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/3BNVgA_GLfo.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 22, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_555_0_0"}, {"texts": ["The person puts it on the bicycle."], "durations": null, "exact_frames_per_prompt": [41], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/3BNVgA_GLfo.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 22, "npz_gt_video_start_frame": 22, "npz_gt_video_end_frame": 63, "skip_frames_after_generation": 22, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_555_0_1"}, {"texts": ["The person turned the bicycle."], "durations": null, "exact_frames_per_prompt": [18], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/3BNVgA_GLfo.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 63, "npz_gt_video_start_frame": 63, "npz_gt_video_end_frame": 81, "skip_frames_after_generation": 63, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_555_0_2"}, {"texts": ["A man wearing a white shirt and black pants is ironing a white shirt."], "durations": null, "exact_frames_per_prompt": [57], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-FzMbTz5t_c.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 61, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_557_0_0"}, {"texts": ["The man puts the iron on the right side and picks up the white shirt."], "durations": null, "exact_frames_per_prompt": [19], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-FzMbTz5t_c.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 61, "npz_gt_video_start_frame": 61, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 61, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_557_0_1"}, {"texts": ["A person whom hands are visible, holding a paper."], "durations": null, "exact_frames_per_prompt": [43], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/2kpsVr-iPUg.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 47, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_560_0_0"}, {"texts": ["The person starts folding it."], "durations": null, "exact_frames_per_prompt": [33], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/2kpsVr-iPUg.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 47, "npz_gt_video_start_frame": 47, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 47, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_560_0_1"}, {"texts": ["A baby wearing yellow clothes is lying on the white surface, laughing and moving their hands a baby wearing white dress closes his hand and starts smiling and moving there eyebrow"], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-XWjsz_0_OE.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_562_1_0"}, {"texts": ["A woman wearing white-black clothes is bending toward the baby boys baby boy wearing yellow sweater laughing by seeing the girl"], "durations": null, "exact_frames_per_prompt": [60], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-XWjsz_0_OE.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 20, "npz_gt_video_start_frame": 20, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 20, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_562_2_0"}, {"texts": ["A baby wearing a striped t-shirt is at first adding the toppings on his pizza."], "durations": null, "exact_frames_per_prompt": [30], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-mLImzB3iek.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 34, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_563_0_0"}, {"texts": ["The baby is holding the tray full of pizzas."], "durations": null, "exact_frames_per_prompt": [13], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-mLImzB3iek.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 34, "npz_gt_video_start_frame": 34, "npz_gt_video_end_frame": 47, "skip_frames_after_generation": 34, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_563_0_1"}, {"texts": ["The baby is looking inside the microwave."], "durations": null, "exact_frames_per_prompt": [4], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-mLImzB3iek.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 47, "npz_gt_video_start_frame": 47, "npz_gt_video_end_frame": 52, "skip_frames_after_generation": 47, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_563_0_2"}, {"texts": ["The baby turns his face in the left direction"], "durations": null, "exact_frames_per_prompt": [4], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-mLImzB3iek.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 52, "npz_gt_video_start_frame": 52, "npz_gt_video_end_frame": 56, "skip_frames_after_generation": 52, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_563_0_3"}, {"texts": ["A boy wearing a multi-color t-shirt and black bathrobe is standing, holding a pan, moving it, moving his other hand, and talking."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-XVjAQJ-IPI.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_564_0_0"}, {"texts": ["A man wearing a black designer t-shirt and black pajamas is sitting near a black tire, holding a coin in his hand. "], "durations": null, "exact_frames_per_prompt": [26], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/3f9iR3QJkgc.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 30, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_567_0_0"}, {"texts": ["The man puts the coin into the tire treads."], "durations": null, "exact_frames_per_prompt": [50], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/3f9iR3QJkgc.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 30, "npz_gt_video_start_frame": 30, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 30, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_567_0_1"}, {"texts": ["A woman wearing a black-white striped t-shirt and black pants is standing, holding a clothes ironing machine and ironing the white shirt on the ironing board."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/0LHuetA-mbw.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_568_0_0"}, {"texts": ["A woman whose lower body is visible is ironing a checkered shirt on an ironing board. "], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/0LHuetA-mbw.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_569_0_0"}, {"texts": ["A man wearing a black and white trousers is at first bending towards the snowy surface and pulls an object from the surface while a man wearing black clothes holding a cane is standing"], "durations": null, "exact_frames_per_prompt": [25], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/3MwrY-NOdC0.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 29, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_572_0_0"}, {"texts": ["The man wearing a black and white trousers throws the object."], "durations": null, "exact_frames_per_prompt": [12], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/3MwrY-NOdC0.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 29, "npz_gt_video_start_frame": 29, "npz_gt_video_end_frame": 41, "skip_frames_after_generation": 29, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_572_0_1"}, {"texts": ["The man wearing a black and white trousers starts running towards the other man."], "durations": null, "exact_frames_per_prompt": [22], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/3MwrY-NOdC0.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 41, "npz_gt_video_start_frame": 41, "npz_gt_video_end_frame": 63, "skip_frames_after_generation": 41, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_572_0_2"}, {"texts": ["The man wearing a black and white trousers slaps the other man on his back."], "durations": null, "exact_frames_per_prompt": [17], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/3MwrY-NOdC0.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 63, "npz_gt_video_start_frame": 63, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 63, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_572_0_3"}, {"texts": ["A man wearing a white shirt and gray jeans is sitting on a red chair and wearing a necktie while making a knot of the necktie."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/11auFH1JpL4.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_576_0_0"}, {"texts": ["A kid is wearing a red jacket and black pants is catching fish."], "durations": null, "exact_frames_per_prompt": [48], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/4epEf8NasFw.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 52, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_577_0_0"}, {"texts": ["The kid puts the fish on the ground."], "durations": null, "exact_frames_per_prompt": [28], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/4epEf8NasFw.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 52, "npz_gt_video_start_frame": 52, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 52, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_577_0_1"}, {"texts": ["A boy wearing a grey jacket is standing on a brown surface and takes a bite of food while girl wearing black t-shirt talking with someone"], "durations": null, "exact_frames_per_prompt": [9], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/1XklZnvQcGY.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 13, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_578_0_0"}, {"texts": ["The boy passes the food to the girl on the right side group of people standing back side are laughing"], "durations": null, "exact_frames_per_prompt": [34], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/1XklZnvQcGY.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 13, "npz_gt_video_start_frame": 13, "npz_gt_video_end_frame": 47, "skip_frames_after_generation": 13, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_578_0_1"}, {"texts": ["A girl wearing a grey t-shirt is standing on a brown surface, takes the food from the boy on the left side while the other girl wearing a black t-shirt licking her fingres"], "durations": null, "exact_frames_per_prompt": [26], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/1XklZnvQcGY.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 13, "npz_gt_video_start_frame": 13, "npz_gt_video_end_frame": 39, "skip_frames_after_generation": 13, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_578_1_0"}, {"texts": ["The girl wearing a grey t-shirt takes a bite."], "durations": null, "exact_frames_per_prompt": [27], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/1XklZnvQcGY.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 39, "npz_gt_video_start_frame": 39, "npz_gt_video_end_frame": 66, "skip_frames_after_generation": 39, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_578_1_1"}, {"texts": ["The girl wearing a grey t-shirt gives the food to the other person."], "durations": null, "exact_frames_per_prompt": [15], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/1XklZnvQcGY.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 66, "npz_gt_video_start_frame": 66, "npz_gt_video_end_frame": 81, "skip_frames_after_generation": 66, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_578_1_2"}, {"texts": ["A girl wearing a pink top and black pants is standing moving her hand into the washing machine."], "durations": null, "exact_frames_per_prompt": [14], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/7in1YseTaOM.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 18, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_579_0_0"}, {"texts": ["The girl takes a pink cloth from the woman and drops it on the bunch of cloth.  while the woman who's partly visible drops off the clothes in the washing machine."], "durations": null, "exact_frames_per_prompt": [46], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/7in1YseTaOM.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 18, "npz_gt_video_start_frame": 18, "npz_gt_video_end_frame": 64, "skip_frames_after_generation": 18, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_579_0_1"}, {"texts": ["The girl again picks the pink cloth."], "durations": null, "exact_frames_per_prompt": [16], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/7in1YseTaOM.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 64, "npz_gt_video_start_frame": 64, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 64, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_579_0_2"}, {"texts": ["A woman whose hands and head are visible gives a pink cloth to the girl baby girl putting the clothes into the machine"], "durations": null, "exact_frames_per_prompt": [41], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/7in1YseTaOM.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 45, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_579_1_0"}, {"texts": ["The woman starts picking clothes from the bunch of clothes and puts them into the washing machine while little girl helping the lady to put clothes in the laundry machine"], "durations": null, "exact_frames_per_prompt": [35], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/7in1YseTaOM.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 45, "npz_gt_video_start_frame": 45, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 45, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_579_1_1"}, {"texts": ["A man wearing a white shirt is standing in front of the screen and giving a weather description in the news studio."], "durations": null, "exact_frames_per_prompt": [77], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-QDuiwp7jvE.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 81, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_580_0_0"}, {"texts": ["A man, whose hand is visible, is holding a fork and taking out some cooked chickens from the bowl and putting them on the barbeque."], "durations": null, "exact_frames_per_prompt": [57], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/0cQ9qWe7YRA.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 6, "npz_gt_video_start_frame": 6, "npz_gt_video_end_frame": 63, "skip_frames_after_generation": 6, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_583_0_0"}, {"texts": ["A person whose hand is visible is placing a meat piece on the grill from the bowl."], "durations": null, "exact_frames_per_prompt": [57], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/0cQ9qWe7YRA.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 6, "npz_gt_video_start_frame": 6, "npz_gt_video_end_frame": 63, "skip_frames_after_generation": 6, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_584_0_0"}, {"texts": ["A man wearing a maroon t-shirt standing on a grass surface."], "durations": null, "exact_frames_per_prompt": [30], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-InKRIGbKoY.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 34, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_586_0_0"}, {"texts": ["The man is attaching a pendulum on a golf putter."], "durations": null, "exact_frames_per_prompt": [46], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-InKRIGbKoY.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 34, "npz_gt_video_start_frame": 34, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 34, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_586_0_1"}, {"texts": ["A man wearing a dark green shirt and dove gray pants is holding a spatula and sauteing the meat in a pan."], "durations": null, "exact_frames_per_prompt": [46], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-ToYMwdP3l8.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 50, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_587_0_0"}, {"texts": ["The man is holding the slices of lemon."], "durations": null, "exact_frames_per_prompt": [12], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-ToYMwdP3l8.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 50, "npz_gt_video_start_frame": 50, "npz_gt_video_end_frame": 62, "skip_frames_after_generation": 50, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_587_0_1"}, {"texts": ["The man is squeezing it over the meat."], "durations": null, "exact_frames_per_prompt": [18], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-ToYMwdP3l8.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 62, "npz_gt_video_start_frame": 62, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 62, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_587_0_2"}, {"texts": ["A baby whose upper half-body is visible wearing a gray-blue striped t-shirt is sitting in the stroller on the left side, eating an ice cream cone and then looking at a person."], "durations": null, "exact_frames_per_prompt": [64], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-ZAg8y-J9II.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 68, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_58_1_0"}, {"texts": ["A black-brown dog is standing on the gray surface someone is standing on the right side of the dog."], "durations": null, "exact_frames_per_prompt": [11], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/1CSOIiEJwx0.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 15, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_591_3_0"}, {"texts": ["The dog is looking at the people one person is giving chips to the dog."], "durations": null, "exact_frames_per_prompt": [65], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/1CSOIiEJwx0.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 15, "npz_gt_video_start_frame": 15, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 15, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_591_3_1"}, {"texts": ["A girl wearing a white t-shirt is sitting and eating the food while looking on the left side."], "durations": null, "exact_frames_per_prompt": [13], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/1zaz-wmBRg4.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 17, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_594_0_0"}, {"texts": ["The girl looking in front and coming forward."], "durations": null, "exact_frames_per_prompt": [13], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/1zaz-wmBRg4.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 17, "npz_gt_video_start_frame": 17, "npz_gt_video_end_frame": 30, "skip_frames_after_generation": 17, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_594_0_1"}, {"texts": ["The girl is touching her eye while speaking."], "durations": null, "exact_frames_per_prompt": [12], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/1zaz-wmBRg4.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 30, "npz_gt_video_start_frame": 30, "npz_gt_video_end_frame": 42, "skip_frames_after_generation": 30, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_594_0_2"}, {"texts": ["The girl backs off."], "durations": null, "exact_frames_per_prompt": [29], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/1zaz-wmBRg4.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 42, "npz_gt_video_start_frame": 42, "npz_gt_video_end_frame": 71, "skip_frames_after_generation": 42, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_594_0_3"}, {"texts": ["The girl is looking on the left side again."], "durations": null, "exact_frames_per_prompt": [9], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/1zaz-wmBRg4.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 71, "npz_gt_video_start_frame": 71, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 71, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_594_0_4"}, {"texts": ["A man wearing white clothes and spectacles is sitting on the chair and speaking while looking in front."], "durations": null, "exact_frames_per_prompt": [28], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/3_UbJ2bCq0A.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 32, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_596_0_ms_0"}, {"texts": ["A person wearing brown clothes is holding a fuel dispenser that is attached to a vehicle."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/09iXwL2KL2I.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_597_0_0"}, {"texts": ["A girl wearing a pink t-shirt and pink trousers is standing on the floor a lady instruct her with their finger."], "durations": null, "exact_frames_per_prompt": [12], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/1hlx6xGL0HA.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 16, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_5_0_0"}, {"texts": ["The girl wearing a pink t-shirt is pushing the clothes into the washing machine the lady is saying something to her."], "durations": null, "exact_frames_per_prompt": [64], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/1hlx6xGL0HA.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 16, "npz_gt_video_start_frame": 16, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 16, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_5_0_1"}, {"texts": ["A person whose hand is visible wearing a black sweater is instructing the girl."], "durations": null, "exact_frames_per_prompt": [13], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/1hlx6xGL0HA.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 17, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_5_1_0"}, {"texts": ["A man wearing dark green clothes is shearing a grey sheep with a shearing machine."], "durations": null, "exact_frames_per_prompt": [74], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-XL6pE2XX0I.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 78, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_600_0_0"}, {"texts": ["A grey sheep is sitting on a brown wooden sheet.\n a man in the green dress is shedding sheep's hair"], "durations": null, "exact_frames_per_prompt": [74], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-XL6pE2XX0I.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 78, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_600_1_0"}, {"texts": ["A man wearing dark gray pants is standing and holding a bee smoker machine."], "durations": null, "exact_frames_per_prompt": [44], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-xixa4bDH-s.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 48, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_601_0_0"}, {"texts": ["The man is sitting and starts pressing the smoker machine."], "durations": null, "exact_frames_per_prompt": [32], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-xixa4bDH-s.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 48, "npz_gt_video_start_frame": 48, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 48, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_601_0_1"}, {"texts": ["A boy wearing a purple t-shirt is standing on the white tiled floor, holding a pan of brown food. "], "durations": null, "exact_frames_per_prompt": [13], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/2AIPTsVvBpE.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 17, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_603_0_0"}, {"texts": ["The boy puts the pan on the stove."], "durations": null, "exact_frames_per_prompt": [13], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/2AIPTsVvBpE.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 17, "npz_gt_video_start_frame": 17, "npz_gt_video_end_frame": 30, "skip_frames_after_generation": 17, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_603_0_1"}, {"texts": ["The boy walks back and starts tossing the brown food with pan."], "durations": null, "exact_frames_per_prompt": [50], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/2AIPTsVvBpE.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 30, "npz_gt_video_start_frame": 30, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 30, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_603_0_2"}, {"texts": ["A woman wearing a red top is standing."], "durations": null, "exact_frames_per_prompt": [13], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/8e3MxL7e4yk.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 17, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_613_0_0"}, {"texts": ["The woman is chopping the watermelon with the help of a knife."], "durations": null, "exact_frames_per_prompt": [63], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/8e3MxL7e4yk.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 17, "npz_gt_video_start_frame": 17, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 17, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_613_0_1"}, {"texts": ["A man on the right is sitting and is eating the man in left move the fork"], "durations": null, "exact_frames_per_prompt": [12], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/4tcFOYWfdtY.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 16, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_614_0_0"}, {"texts": ["The man is drinking."], "durations": null, "exact_frames_per_prompt": [64], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/4tcFOYWfdtY.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 16, "npz_gt_video_start_frame": 16, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 16, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_614_0_1"}, {"texts": ["A man on the left is also eating with a white fork while a man sitting on the right side he was wearing multicolour brief he eating something and drinking something from the pouch and he starts pouring it on his face and then he starts talking with the other person."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/4tcFOYWfdtY.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_614_1_0"}, {"texts": ["A man sitting on the left wearing brown shorts is holding a fork and chewing food while a man sitting on the right wearing multicolored shorts is eating and drinking something from the pouch and starts pouring it on his face while talking to the other"], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/4tcFOYWfdtY.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_615_0_0"}, {"texts": ["A woman wearing a purple t-shirt, blue jeans, a black cap, and black boots is sitting on the horse while holding a horse harness and is riding on the road from left to right the black color horse walking on the street towards ahead and moving head up and down"], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-XwUSOjvgQw.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_617_0_0"}, {"texts": ["A man whose legs are visible, wearing brown pants and dark brown shoes, is walking with the horse on the road from left to right a lady, wearing a blue pant and black shoes, is riding on the horse on the road from left to right."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-XwUSOjvgQw.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_617_1_0"}, {"texts": ["A black horse is walking on the road from left to right."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-XwUSOjvgQw.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_617_2_0"}, {"texts": ["A person wearing a purple t-shirt and green shorts is putting food from a pink bucket into the goat feeder."], "durations": null, "exact_frames_per_prompt": [22], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/0fXB4HjzHyk.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 26, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_621_0_0"}, {"texts": ["A boy on the right wearing a green t-shirt is putting cheese on a pizza dough then starts spreading the cheese on the pizza dough with his hand and a boy on left side is also putting cheese on the same pizza dough."], "durations": null, "exact_frames_per_prompt": [50], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/0Dd-LEk-2iE.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 54, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_623_0_0"}, {"texts": ["A boy wearing a blue t-shirt is sitting on a wooden chair and taking cheese from a bowl of cheese then putting cheese on the pizza dough a boy with black hair putting cheese from table to pizza dough"], "durations": null, "exact_frames_per_prompt": [13], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/0Dd-LEk-2iE.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 17, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_623_1_0"}, {"texts": ["The boy starts tapping the cheese on the pizza dough."], "durations": null, "exact_frames_per_prompt": [37], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/0Dd-LEk-2iE.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 17, "npz_gt_video_start_frame": 17, "npz_gt_video_end_frame": 54, "skip_frames_after_generation": 17, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_623_1_1"}, {"texts": ["A person whose hands are visible is holding a blue paper swan."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/0x1NxfGd30s.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_625_0_0"}, {"texts": ["A person whose hand is visible is holding a paper craft and showing it."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/0x1NxfGd30s.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_626_0_0"}, {"texts": ["A man wearing a sweatshirt is feeding the dog."], "durations": null, "exact_frames_per_prompt": [67], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/3h1BB6bTX4A.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 13, "npz_gt_video_start_frame": 13, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 13, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_629_0_0"}, {"texts": ["A brown and white colored dog is at first looking to the other dogs."], "durations": null, "exact_frames_per_prompt": [14], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/3h1BB6bTX4A.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 18, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_629_1_0"}, {"texts": ["The dog is looking in an upward direction."], "durations": null, "exact_frames_per_prompt": [32], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/3h1BB6bTX4A.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 18, "npz_gt_video_start_frame": 18, "npz_gt_video_end_frame": 50, "skip_frames_after_generation": 18, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_629_1_1"}, {"texts": ["The dog is being fed by the man in a sweatshirt."], "durations": null, "exact_frames_per_prompt": [29], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/3h1BB6bTX4A.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 50, "npz_gt_video_start_frame": 50, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 50, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_629_1_2"}, {"texts": ["A man wearing a grey suit is sitting, keeping his hands on the table."], "durations": null, "exact_frames_per_prompt": [38], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/0hrqR84Ii2I.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 42, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_62_0_ms_0"}, {"texts": ["A person wearing black trousers is sitting on the snow surface, and getting the leg tied by another person."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/0M7duRovxvI.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_630_0_0"}, {"texts": ["A person wearing a red-black jacket is sitting on the snow surface and tying the leg of person one with a white cloth."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/0M7duRovxvI.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_630_1_0"}, {"texts": ["A man wearing a black suit, white shirt and yellow tie moves forward having a black device in his hand."], "durations": null, "exact_frames_per_prompt": [61], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-gcaBc_4dNc.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 65, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_633_0_0"}, {"texts": ["A woman is wearing a green top and blue shorts is sitting."], "durations": null, "exact_frames_per_prompt": [26], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-aGgP2rhewk.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 30, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_634_0_0"}, {"texts": ["The woman is catching fishes. "], "durations": null, "exact_frames_per_prompt": [8], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-aGgP2rhewk.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 30, "npz_gt_video_start_frame": 30, "npz_gt_video_end_frame": 38, "skip_frames_after_generation": 30, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_634_0_1"}, {"texts": ["The woman is putting the fishes in a bucket."], "durations": null, "exact_frames_per_prompt": [42], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-aGgP2rhewk.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 38, "npz_gt_video_start_frame": 38, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 38, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_634_0_2"}, {"texts": ["A woman wearing a black top is standing, showing a book."], "durations": null, "exact_frames_per_prompt": [21], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/09xhRsuGvFs.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 25, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_636_0_0"}, {"texts": ["The woman is turning the pages of the book."], "durations": null, "exact_frames_per_prompt": [56], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/09xhRsuGvFs.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 25, "npz_gt_video_start_frame": 25, "npz_gt_video_end_frame": 81, "skip_frames_after_generation": 25, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_636_0_1"}, {"texts": ["A woman in grey clothing is riding on the camel in the right direction at the same time the person wearing blue jacket rides on the camel"], "durations": null, "exact_frames_per_prompt": [38], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-kbnAxb3PzQ.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 42, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_638_0_0"}, {"texts": ["A man wearing a hat is riding on a camel in the straight direction."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-kbnAxb3PzQ.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_638_1_0"}, {"texts": ["A camel is moving in the straight direction and giving a ride to the man wearing a hat a man in a pink shirt is pulling the camel's rope forward"], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-kbnAxb3PzQ.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_638_5_0"}, {"texts": ["A woman wearing a green-black top is sitting behind the table and picking up the pasta with a fork from the plate."], "durations": null, "exact_frames_per_prompt": [55], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/--3ouPhoy2A.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 25, "npz_gt_video_start_frame": 25, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 25, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_641_0_0"}, {"texts": ["A person wearing a black t-shirt is sitting on the right side of the woman and picking up the noodles with a fork from the plate."], "durations": null, "exact_frames_per_prompt": [29], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/--3ouPhoy2A.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 33, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_641_1_0"}, {"texts": ["A man wearing black cloth is holding a transparent bottle and making the man wearing white clothes drink alcohol.  while a man wearing red shirt forcing white shirt person to drink and removed the bottle from the mouth and carrying him to room"], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-gmCbukFNco.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_643_2_0"}, {"texts": ["A man wearing a white vest is sitting and playing drums while eating a sandwich."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/0FIJ-cyhP2k.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_644_0_0"}, {"texts": ["A boy wearing white clothes is sitting and playing drums with his right hand while holding food in his left hand and eating the food.\n"], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/0FIJ-cyhP2k.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_645_0_0"}, {"texts": ["A girl wearing a blue and black jumpsuit is holding a fishing rod and bending towards the fishing hole."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/3oASaiFvmGQ.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_648_0_0"}, {"texts": ["A baby wearing a red top is sitting and playing on an elephant stuffed toy."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-Tr7AZF8aIY.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_649_0_0"}, {"texts": ["A person is sitting on a white bed and holding the baby while the child wearing a red t-shirt is sitting on a grey toy elephant."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-Tr7AZF8aIY.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_649_1_0"}, {"texts": ["A child wearing a red-white t-shirt is sitting on a grey toy elephant."], "durations": null, "exact_frames_per_prompt": [10], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-Tr7AZF8aIY.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 14, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_650_0_0"}, {"texts": ["The child lies on the toy elephant."], "durations": null, "exact_frames_per_prompt": [51], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-Tr7AZF8aIY.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 14, "npz_gt_video_start_frame": 14, "npz_gt_video_end_frame": 65, "skip_frames_after_generation": 14, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_650_0_1"}, {"texts": ["The child gets up."], "durations": null, "exact_frames_per_prompt": [15], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-Tr7AZF8aIY.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 65, "npz_gt_video_start_frame": 65, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 65, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_650_0_2"}, {"texts": ["A woman whose hands are visible is sitting on a bed and holding the child baby wearing red top is sat on toy elephant and started laughing"], "durations": null, "exact_frames_per_prompt": [36], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-Tr7AZF8aIY.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 40, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_650_1_0"}, {"texts": ["The woman is shaking the child baby laughing Infront of camera"], "durations": null, "exact_frames_per_prompt": [40], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-Tr7AZF8aIY.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 40, "npz_gt_video_start_frame": 40, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 40, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_650_1_1"}, {"texts": ["A man wearing a light brown t-shirt and brown pants is standing, keeping some pineapple slices on the left side of the counter-top."], "durations": null, "exact_frames_per_prompt": [13], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/0Ni4Ab8-Sjo.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 20, "npz_gt_video_start_frame": 20, "npz_gt_video_end_frame": 33, "skip_frames_after_generation": 20, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_651_0_0"}, {"texts": ["The man is cutting the other pineapple slices with a knife on the chopping board."], "durations": null, "exact_frames_per_prompt": [34], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/0Ni4Ab8-Sjo.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 33, "npz_gt_video_start_frame": 33, "npz_gt_video_end_frame": 67, "skip_frames_after_generation": 33, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_651_0_1"}, {"texts": ["The man puts the knife on the counter-top."], "durations": null, "exact_frames_per_prompt": [4], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/0Ni4Ab8-Sjo.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 67, "npz_gt_video_start_frame": 67, "npz_gt_video_end_frame": 71, "skip_frames_after_generation": 67, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_651_0_2"}, {"texts": ["The man walks to pick up the transparent box."], "durations": null, "exact_frames_per_prompt": [9], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/0Ni4Ab8-Sjo.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 71, "npz_gt_video_start_frame": 71, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 71, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_651_0_3"}, {"texts": ["A person whose hands are visible is cutting a watermelon with a knife."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/9pFBmmYcjB8.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_652_0_0"}, {"texts": ["A girl wearing a dark brown jacket is sitting on the brown tile stairs and eating an ice cream cone and the dog beside the girl is watching her"], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-jIqQuroXrY.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_653_0_0"}, {"texts": ["A big brown dog is standing near a girl and watching them a girl in a brown jacket  is moving left a bit and laughing and licking something"], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-jIqQuroXrY.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_653_1_0"}, {"texts": ["A woman wearing a purple dress is sitting on the chair on the left side and is moving her hand while speaking with the man."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/4QE2Kqowp88.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_658_0_0"}, {"texts": ["A man whose half body is visible wearing a gray t-shirt is standing and sauteing the food in a pan."], "durations": null, "exact_frames_per_prompt": [31], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-p46GSgNajs.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 35, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_659_0_0"}, {"texts": ["The man whose half body is visible wearing a gray t-shirt is adding cheese into the food."], "durations": null, "exact_frames_per_prompt": [45], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-p46GSgNajs.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 35, "npz_gt_video_start_frame": 35, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 35, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_659_0_1"}, {"texts": ["A woman is standing and reading the card to the girl."], "durations": null, "exact_frames_per_prompt": [28], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/0foWkOFyYG0.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 32, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_660_0_0"}, {"texts": ["A woman in the middle is holding a greeting card and reading while a girl wearing a red t-shirt is folding the blue bed sheet"], "durations": null, "exact_frames_per_prompt": [53], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/0foWkOFyYG0.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 57, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_661_0_0"}, {"texts": ["A man wearing an orange t-shirt and blue trousers is leaning forward and shaving a sheep."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-Qzs6sCMBZE.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_663_0_0"}, {"texts": ["A black sheep is lying on the carpet and getting shaved by a man."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-Qzs6sCMBZE.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_663_1_0"}, {"texts": ["A woman wearing a black top is reading a book."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-S5k_kb2ZFg.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_664_0_0"}, {"texts": ["A person whose hands are visible is holding a toy skateboard in the left hand and showing a toy skateboard."], "durations": null, "exact_frames_per_prompt": [20], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/3YW1znEPquY.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 24, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_665_0_0"}, {"texts": ["The person is putting a metal object with the right hand on the wheel of a toy skateboard."], "durations": null, "exact_frames_per_prompt": [56], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/3YW1znEPquY.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 24, "npz_gt_video_start_frame": 24, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 24, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_665_0_1"}, {"texts": ["A boy wearing a blue t-shirt is walking around the table and is pouring honey over the french toast."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/47h-5NgefNs.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_666_0_0"}, {"texts": ["A woman wearing a red sweater is standing and holding a red plastic sheet, full of potato peels."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/5bp_ZwqZOe4.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_667_0_0"}, {"texts": ["A woman wearing a red sweater is standing, holding a peeler tool."], "durations": null, "exact_frames_per_prompt": [23], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/5bp_ZwqZOe4.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 27, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_668_0_0"}, {"texts": ["The woman is lifting up a plastic bag filled with the peels of potatoes."], "durations": null, "exact_frames_per_prompt": [15], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/5bp_ZwqZOe4.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 27, "npz_gt_video_start_frame": 27, "npz_gt_video_end_frame": 43, "skip_frames_after_generation": 27, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_668_0_1"}, {"texts": ["The woman is putting it down and speaking."], "durations": null, "exact_frames_per_prompt": [35], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/5bp_ZwqZOe4.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 43, "npz_gt_video_start_frame": 43, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 43, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_668_0_2"}, {"texts": ["A woman wearing a red high neck is standing behind the counter-top and holding the peeler."], "durations": null, "exact_frames_per_prompt": [25], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/5bp_ZwqZOe4.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 29, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_669_0_0"}, {"texts": ["The woman is picking up the plastic sheet with some stuff."], "durations": null, "exact_frames_per_prompt": [21], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/5bp_ZwqZOe4.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 29, "npz_gt_video_start_frame": 29, "npz_gt_video_end_frame": 50, "skip_frames_after_generation": 29, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_669_0_1"}, {"texts": ["The woman is putting it back on the counter."], "durations": null, "exact_frames_per_prompt": [30], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/5bp_ZwqZOe4.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 50, "npz_gt_video_start_frame": 50, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 50, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_669_0_2"}, {"texts": ["A boy wearing black clothes is sitting and holding a fork and licking and then taking food from the plates."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/1cuTbaUR6nw.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_66_0_0"}, {"texts": ["A woman wearing a light purple shirt is turning on the dial of a washing machine with one hand and meddling with the curtain with the other while speaking."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/4y6NZrH8leI.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_672_0_0"}, {"texts": ["A woman wearing a grey shirt is standing with one hand on the curtain."], "durations": null, "exact_frames_per_prompt": [21], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/4y6NZrH8leI.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 25, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_673_0_0"}, {"texts": ["The woman is turning the dial of the machine with the other hand."], "durations": null, "exact_frames_per_prompt": [55], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/4y6NZrH8leI.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 25, "npz_gt_video_start_frame": 25, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 25, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_673_0_1"}, {"texts": ["A man whose hand is visible, is holding some paper and putting it into the paper shredder machine."], "durations": null, "exact_frames_per_prompt": [11], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/8PzGl4IW2-U.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 15, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_674_0_0"}, {"texts": ["A woman wearing a gray-white top is walking on the left side of the first woman."], "durations": null, "exact_frames_per_prompt": [19], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-9w1jNVB9fA.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 23, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_676_1_0"}, {"texts": ["The woman is eating something."], "durations": null, "exact_frames_per_prompt": [17], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-9w1jNVB9fA.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 23, "npz_gt_video_start_frame": 23, "npz_gt_video_end_frame": 40, "skip_frames_after_generation": 23, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_676_1_1"}, {"texts": ["The woman is moving her head in front."], "durations": null, "exact_frames_per_prompt": [19], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-9w1jNVB9fA.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 40, "npz_gt_video_start_frame": 40, "npz_gt_video_end_frame": 59, "skip_frames_after_generation": 40, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_676_1_2"}, {"texts": ["The woman walks towards the first woman."], "durations": null, "exact_frames_per_prompt": [21], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-9w1jNVB9fA.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 59, "npz_gt_video_start_frame": 59, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 59, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_676_1_3"}, {"texts": ["A woman is walking on the left side while eating something."], "durations": null, "exact_frames_per_prompt": [30], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-9w1jNVB9fA.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 34, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_677_1_0"}, {"texts": ["The woman moves her head in front."], "durations": null, "exact_frames_per_prompt": [25], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-9w1jNVB9fA.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 34, "npz_gt_video_start_frame": 34, "npz_gt_video_end_frame": 59, "skip_frames_after_generation": 34, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_677_1_1"}, {"texts": ["The woman walks towards the first woman."], "durations": null, "exact_frames_per_prompt": [21], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-9w1jNVB9fA.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 59, "npz_gt_video_start_frame": 59, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 59, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_677_1_2"}, {"texts": ["A man wearing a red t-shirt is walking on a green grass surface with a rope in hand attached to a camel while a woman wearing a black top is sitting on a camel."], "durations": null, "exact_frames_per_prompt": [22], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/0BCJpch3w8Q.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 5, "npz_gt_video_start_frame": 5, "npz_gt_video_end_frame": 27, "skip_frames_after_generation": 5, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_678_1_0"}, {"texts": ["A woman wearing a black top is sitting on a camel while a man wearing a red t-shirt is walking on a green grass surface with a rope in hand attached to a camel."], "durations": null, "exact_frames_per_prompt": [17], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/0BCJpch3w8Q.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 12, "npz_gt_video_start_frame": 12, "npz_gt_video_end_frame": 29, "skip_frames_after_generation": 12, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_678_2_0"}, {"texts": ["A camel is walking on a green grass surface with a woman sitting on its back while man wearing red t-shirt handling the camel"], "durations": null, "exact_frames_per_prompt": [21], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/0BCJpch3w8Q.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 8, "npz_gt_video_start_frame": 8, "npz_gt_video_end_frame": 29, "skip_frames_after_generation": 8, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_678_3_0"}, {"texts": ["A man wearing a red t-shirt is walking on a green grass surface with a rope in hand attached to a camel while a woman wearing a black top is sitting on a camel and smiling"], "durations": null, "exact_frames_per_prompt": [24], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/0BCJpch3w8Q.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 5, "npz_gt_video_start_frame": 5, "npz_gt_video_end_frame": 29, "skip_frames_after_generation": 5, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_679_1_0"}, {"texts": ["A woman wearing blue cloth is sitting on a camel a camel is moving in the straight direction and giving a ride to the woman"], "durations": null, "exact_frames_per_prompt": [17], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/0BCJpch3w8Q.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 12, "npz_gt_video_start_frame": 12, "npz_gt_video_end_frame": 29, "skip_frames_after_generation": 12, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_679_2_0"}, {"texts": ["A man wearing a brown t-shirt is at first juggling with the glasses."], "durations": null, "exact_frames_per_prompt": [18], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/1aEgKEUpsKE.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 22, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_682_0_0"}, {"texts": ["The man puts the glass on the countertop."], "durations": null, "exact_frames_per_prompt": [12], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/1aEgKEUpsKE.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 22, "npz_gt_video_start_frame": 22, "npz_gt_video_end_frame": 34, "skip_frames_after_generation": 22, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_682_0_1"}, {"texts": ["The man adding the beverage in the transparent glass."], "durations": null, "exact_frames_per_prompt": [23], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/1aEgKEUpsKE.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 34, "npz_gt_video_start_frame": 34, "npz_gt_video_end_frame": 57, "skip_frames_after_generation": 34, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_682_0_2"}, {"texts": ["The man covered the glass with the steel glass."], "durations": null, "exact_frames_per_prompt": [15], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/1aEgKEUpsKE.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 57, "npz_gt_video_start_frame": 57, "npz_gt_video_end_frame": 72, "skip_frames_after_generation": 57, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_682_0_3"}, {"texts": ["The man is shaking it."], "durations": null, "exact_frames_per_prompt": [8], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/1aEgKEUpsKE.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 72, "npz_gt_video_start_frame": 72, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 72, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_682_0_4"}, {"texts": ["A man wearing maroon cloth is standing, holding a glass and making drinks."], "durations": null, "exact_frames_per_prompt": [29], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/1aEgKEUpsKE.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 33, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_683_0_0"}, {"texts": ["The man is pouring in a glass."], "durations": null, "exact_frames_per_prompt": [47], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/1aEgKEUpsKE.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 33, "npz_gt_video_start_frame": 33, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 33, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_683_0_1"}, {"texts": ["A woman wearing a black-golden top is eating crunchy food."], "durations": null, "exact_frames_per_prompt": [29], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-ySTvnMuqo8.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 33, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_690_0_0"}, {"texts": ["The woman wearing a black-golden top is holding the food in her hand."], "durations": null, "exact_frames_per_prompt": [47], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-ySTvnMuqo8.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 33, "npz_gt_video_start_frame": 33, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 33, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_690_0_1"}, {"texts": ["A man wearing black clothes is shaving off the wool of a white sheep and a man wearing black t-shirt on the right is moving the wool aside."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-_rVqtj1lXo.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_691_0_0"}, {"texts": ["A man on the right wearing a cap is collecting the wool in a bag a sheep is lying on the gray surface and getting shaved by a man."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-_rVqtj1lXo.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_691_1_0"}, {"texts": ["A girl wearing a pink top is standing first in the queue, trying to milking an artificial cow while the other girls in the queue are moving back and sitting down"], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/0LdmTwbHPu8.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_692_1_0"}, {"texts": ["A girl wearing a colourful striped top is standing in the middle of the queue and the girl before her in pink t-shirt is trying to milking the artificial cow"], "durations": null, "exact_frames_per_prompt": [64], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/0LdmTwbHPu8.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 68, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_692_2_0"}, {"texts": ["The girl sits on the blue floor."], "durations": null, "exact_frames_per_prompt": [12], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/0LdmTwbHPu8.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 68, "npz_gt_video_start_frame": 68, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 68, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_692_2_1"}, {"texts": ["A girl wearing a light green t-shirt is standing at the last of the queue while a girl wearing a pink top sits down and doing something under the cow"], "durations": null, "exact_frames_per_prompt": [43], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/0LdmTwbHPu8.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 47, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_692_3_0"}, {"texts": ["The girl then sits on the blue floor while the other girl also sits on the floor."], "durations": null, "exact_frames_per_prompt": [33], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/0LdmTwbHPu8.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 47, "npz_gt_video_start_frame": 47, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 47, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_692_3_1"}, {"texts": ["A baby sitting on the purple baby chair is crying and looking in the front."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/0eHkPKES-Oc.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_693_0_0"}, {"texts": ["A man wearing a maroon checked lungi is sitting on a grey surface and cutting the pineapple crown leaves with a pruning knife."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/0wKfU6wVBeY.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_694_0_0"}, {"texts": ["A man wearing a gray jacket and white trousers is sitting on the floor, holding the dog leash, stands, and starts walking."], "durations": null, "exact_frames_per_prompt": [33], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/2nC4xpymkFI.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 37, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_696_0_0"}, {"texts": ["A person whose hands are visible, is folding a cloth on a black-brown surface."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/0rbogDaB-n0.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_697_0_0"}, {"texts": ["A girl whose hands are visible is folding a blue cloth on the brown-black printed bed."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/0rbogDaB-n0.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_698_0_0"}, {"texts": ["A girl in the front, painted green on her face, is sitting and eating a burger, then showing the burger while the other girl on the left having a drink"], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/06pj__z8U5c.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_699_0_0"}, {"texts": ["A girl is sitting behind the first girl and drinking from the cup."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/06pj__z8U5c.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_699_1_0"}, {"texts": ["A baby wearing a white vest is standing licking a white bowl while the other person is holding the spoon"], "durations": null, "exact_frames_per_prompt": [29], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/15X-JbkRS_w.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 33, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_69_0_0"}, {"texts": ["The baby puts it on the brown surface and the other baby with white t-shirt comes to her from right"], "durations": null, "exact_frames_per_prompt": [47], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/15X-JbkRS_w.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 33, "npz_gt_video_start_frame": 33, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 33, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_69_0_1"}, {"texts": ["A person whose only hand is visible is sitting holding a spoon while the baby with white vest is licking the white bowl"], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/15X-JbkRS_w.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_69_3_0"}, {"texts": ["A girl in the front, painted green on her face, is sitting while the other girl on the left having a drink"], "durations": null, "exact_frames_per_prompt": [39], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/06pj__z8U5c.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 43, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_700_0_0"}, {"texts": ["The girl is eating a burger, then showing the burger."], "durations": null, "exact_frames_per_prompt": [37], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/06pj__z8U5c.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 43, "npz_gt_video_start_frame": 43, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 43, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_700_0_1"}, {"texts": ["A girl is sitting behind the first girl while a girl in the front, painted green on her face, is sitting and eating a burger"], "durations": null, "exact_frames_per_prompt": [49], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/06pj__z8U5c.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 53, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_700_1_0"}, {"texts": ["The girl is drinking from the cup while the first girl showing the burger"], "durations": null, "exact_frames_per_prompt": [27], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/06pj__z8U5c.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 53, "npz_gt_video_start_frame": 53, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 53, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_700_1_1"}, {"texts": ["A boy wearing black shorts is holding a golf stick and watching the golf ball moving in the straight direction."], "durations": null, "exact_frames_per_prompt": [14], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/02gvQ8g9Wlc.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 18, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_701_0_0"}, {"texts": ["The boy turned and put the golf stick on the grass mat."], "durations": null, "exact_frames_per_prompt": [13], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/02gvQ8g9Wlc.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 18, "npz_gt_video_start_frame": 18, "npz_gt_video_end_frame": 31, "skip_frames_after_generation": 18, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_701_0_1"}, {"texts": ["The boy starts running."], "durations": null, "exact_frames_per_prompt": [6], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/02gvQ8g9Wlc.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 31, "npz_gt_video_start_frame": 31, "npz_gt_video_end_frame": 37, "skip_frames_after_generation": 31, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_701_0_2"}, {"texts": ["A person wearing red clothes is sitting on the dark brown elephant and is riding an elephant."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-mQv93RpSpY.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_702_3_0"}, {"texts": ["A person whose only leg is visible wearing red pants, is sitting on the light brown elephant and is riding an elephant."], "durations": null, "exact_frames_per_prompt": [34], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-mQv93RpSpY.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 46, "npz_gt_video_start_frame": 46, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 46, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_702_4_0"}, {"texts": ["A dark brown elephant is walking from left to right on the road."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-mQv93RpSpY.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_702_5_0"}, {"texts": ["A person whose hand is visible, takes out the egg from the boiling water."], "durations": null, "exact_frames_per_prompt": [16], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/1lwykC3RZb4.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 20, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_703_0_0"}, {"texts": ["The person puts it in ice cold water."], "durations": null, "exact_frames_per_prompt": [13], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/1lwykC3RZb4.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 20, "npz_gt_video_start_frame": 20, "npz_gt_video_end_frame": 33, "skip_frames_after_generation": 20, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_703_0_1"}, {"texts": ["The person takes the egg out of the water."], "durations": null, "exact_frames_per_prompt": [47], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/1lwykC3RZb4.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 33, "npz_gt_video_start_frame": 33, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 33, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_703_0_2"}, {"texts": ["A man wearing white bee protection suit is doing thumbs up and standing on a green grass surface."], "durations": null, "exact_frames_per_prompt": [13], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/6p8IFFyx8UI.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 31, "npz_gt_video_start_frame": 31, "npz_gt_video_end_frame": 44, "skip_frames_after_generation": 31, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_70_0_0"}, {"texts": ["A man wearing white bee protection suit is speaking facing front and right side."], "durations": null, "exact_frames_per_prompt": [17], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/6p8IFFyx8UI.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 47, "npz_gt_video_start_frame": 47, "npz_gt_video_end_frame": 64, "skip_frames_after_generation": 47, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_70_1_0"}, {"texts": ["A man whose hand is visible is picking a metal container lid."], "durations": null, "exact_frames_per_prompt": [31], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/5dP2EyeHKUs.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 11, "npz_gt_video_start_frame": 11, "npz_gt_video_end_frame": 42, "skip_frames_after_generation": 11, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_710_1_0"}, {"texts": ["The man is putting it back."], "durations": null, "exact_frames_per_prompt": [17], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/5dP2EyeHKUs.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 42, "npz_gt_video_start_frame": 42, "npz_gt_video_end_frame": 59, "skip_frames_after_generation": 42, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_710_1_1"}, {"texts": ["A boy wearing a light blue t-shirt is standing and is peeling an apple while a girl wearing pink t-shirt standing behind the boy"], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/151r492H6X8.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_712_0_0"}, {"texts": ["A girl wearing a pink top is eating an apple slice while the baby in white t-shirt is peeling the apple."], "durations": null, "exact_frames_per_prompt": [36], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/151r492H6X8.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 40, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_712_1_0"}, {"texts": ["The girl wearing a pink top is looking at the peeler while the baby is chewing something"], "durations": null, "exact_frames_per_prompt": [40], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/151r492H6X8.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 40, "npz_gt_video_start_frame": 40, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 40, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_712_1_1"}, {"texts": ["A white-brown goat is walking while the two girls were moving around."], "durations": null, "exact_frames_per_prompt": [9], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/1Bm_JDw1L9E.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 13, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_713_4_0"}, {"texts": ["The goat is eating on the soil surface while the girl wearing a pink top is trying to feed the goat"], "durations": null, "exact_frames_per_prompt": [16], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/1Bm_JDw1L9E.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 13, "npz_gt_video_start_frame": 13, "npz_gt_video_end_frame": 29, "skip_frames_after_generation": 13, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_713_4_1"}, {"texts": ["The goat starts eating from the hand of the second girl and the girls starts running in the backward direction."], "durations": null, "exact_frames_per_prompt": [41], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/1Bm_JDw1L9E.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 29, "npz_gt_video_start_frame": 29, "npz_gt_video_end_frame": 70, "skip_frames_after_generation": 29, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_713_4_2"}, {"texts": ["A brown goat on the left is standing and eating on the soil surface while a girl wearing pink top trying to feed the goat and runs away"], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/1Bm_JDw1L9E.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_713_5_0"}, {"texts": ["A person whom hand is visible is holding a spatula and stirring the vegetables in a frying pan."], "durations": null, "exact_frames_per_prompt": [50], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/0l7sy0L7lBI.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 30, "npz_gt_video_start_frame": 30, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 30, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_714_0_0"}, {"texts": ["A man wearing a black suit is sitting on a chair and speaking while looking at the papers while a crowd of people are listening to his speech"], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/4OcGdl476Po.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_716_1_0"}, {"texts": ["A baby wearing a white t-shirt is kneeling on the floor while the cat is sleeping on the floor"], "durations": null, "exact_frames_per_prompt": [58], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/29etCiaGuas.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 62, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_717_0_0"}, {"texts": ["The baby is playing near the cat."], "durations": null, "exact_frames_per_prompt": [14], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/29etCiaGuas.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 62, "npz_gt_video_start_frame": 62, "npz_gt_video_end_frame": 76, "skip_frames_after_generation": 62, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_717_0_1"}, {"texts": ["A cat is sitting on the floor near the baby and watching the baby."], "durations": null, "exact_frames_per_prompt": [72], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/29etCiaGuas.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 76, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_717_1_0"}, {"texts": ["A kid wearing a sky-blue t-shirt is sitting and moving towards the cat on the grey floor."], "durations": null, "exact_frames_per_prompt": [72], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/29etCiaGuas.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 76, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_718_0_0"}, {"texts": ["A white cat is sitting on the right side of the grey floor while the kid wearing a sky-blue t-shirt is sitting and moving towards the cat on the grey floor."], "durations": null, "exact_frames_per_prompt": [72], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/29etCiaGuas.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 76, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_718_1_0"}, {"texts": ["A person wearing a blue t-shirt is standing, his hands are lifted up."], "durations": null, "exact_frames_per_prompt": [13], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/0O8w8zn8qSc.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 17, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_719_0_0"}, {"texts": ["A bird comes and sits on his hand."], "durations": null, "exact_frames_per_prompt": [32], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/0O8w8zn8qSc.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 17, "npz_gt_video_start_frame": 17, "npz_gt_video_end_frame": 49, "skip_frames_after_generation": 17, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_719_0_1"}, {"texts": ["A gray-white bird comes from the left side and sits on the person's hand."], "durations": null, "exact_frames_per_prompt": [17], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/0O8w8zn8qSc.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 14, "npz_gt_video_start_frame": 14, "npz_gt_video_end_frame": 31, "skip_frames_after_generation": 14, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_719_1_0"}, {"texts": ["A child wearing a dark blue graphic t-shirt is sitting on the right side of the green table and eating a slice of fruit."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/18iLzeDp0Oo.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_71_0_0"}, {"texts": ["A kid wearing a red t-shirt is sitting on a baby high chair."], "durations": null, "exact_frames_per_prompt": [23], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/4f5Cmellhjw.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 27, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_720_0_0"}, {"texts": ["The kid is eating food."], "durations": null, "exact_frames_per_prompt": [53], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/4f5Cmellhjw.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 27, "npz_gt_video_start_frame": 27, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 27, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_720_0_1"}, {"texts": ["A man wearing white shoes is putting a shredder bin in a paper shredder machine.\n"], "durations": null, "exact_frames_per_prompt": [43], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/8k20SsluyhU.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 37, "npz_gt_video_start_frame": 37, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 37, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_723_0_0"}, {"texts": ["A boy wearing a black designer hoodie is standing and feeding a black goat."], "durations": null, "exact_frames_per_prompt": [30], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/1el2igcfozM.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 34, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_725_0_0"}, {"texts": ["The boy is pampering a black goat."], "durations": null, "exact_frames_per_prompt": [31], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/1el2igcfozM.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 34, "npz_gt_video_start_frame": 34, "npz_gt_video_end_frame": 65, "skip_frames_after_generation": 34, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_725_0_1"}, {"texts": ["A black goat is standing inside the fencing and eating something from the boy's hand."], "durations": null, "exact_frames_per_prompt": [30], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/1el2igcfozM.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 34, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_725_1_0"}, {"texts": ["The goat is getting pampered by the boy."], "durations": null, "exact_frames_per_prompt": [31], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/1el2igcfozM.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 34, "npz_gt_video_start_frame": 34, "npz_gt_video_end_frame": 65, "skip_frames_after_generation": 34, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_725_1_1"}, {"texts": ["A girl wearing a t-shirt is spreading a blanket on a bed."], "durations": null, "exact_frames_per_prompt": [46], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/hZ_N-1mP28Q.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 50, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_728_0_0"}, {"texts": ["A man wearing a red shirt and an oven glove is standing and putting a baking dish on the steel stand."], "durations": null, "exact_frames_per_prompt": [15], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/0Hj6wTbDOXQ.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 19, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_730_0_0"}, {"texts": ["A man wearing a black t-shirt is standing and performing flair bar-tending."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/2p1WmeaHWXI.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_736_0_0"}, {"texts": ["A girl wearing a pink top is sitting and is eating pink ice-cream."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/06iDqj6n6YI.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_737_0_0"}, {"texts": ["A person wearing black clothes is holding a potato and a peeler and peeling it in a black bucket."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/2LK56y2Pyag.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_741_0_0"}, {"texts": ["A man wearing a black t-shirt, a dark blue cap, and gray shorts is sitting on the first camel and woman is sitting back of that man while other man woke up the camel"], "durations": null, "exact_frames_per_prompt": [44], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-5ZUMpdBPyw.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 48, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_742_0_0"}, {"texts": ["The man is riding a camel on the soil surface ahead of the second camel."], "durations": null, "exact_frames_per_prompt": [31], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-5ZUMpdBPyw.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 48, "npz_gt_video_start_frame": 48, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 48, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_742_0_1"}, {"texts": ["A woman wearing a pink t-shirt, pink pants, and white shoes is sitting while holding a camel's harness behind the first man on the first camel.  while the other man woke up the camel"], "durations": null, "exact_frames_per_prompt": [54], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-5ZUMpdBPyw.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 58, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_742_1_0"}, {"texts": ["The woman is riding a camel on the soil surface."], "durations": null, "exact_frames_per_prompt": [22], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-5ZUMpdBPyw.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 58, "npz_gt_video_start_frame": 58, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 58, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_742_1_1"}, {"texts": ["A man wearing dark blue clothes is standing on the left side while holding a camel's harness while a man and a woman are riding the camel."], "durations": null, "exact_frames_per_prompt": [40], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-5ZUMpdBPyw.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 44, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_742_2_0"}, {"texts": ["The man starts walking with the first camel on the soil surface while another man wearing a black t-shirt is riding another camel."], "durations": null, "exact_frames_per_prompt": [36], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-5ZUMpdBPyw.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 44, "npz_gt_video_start_frame": 44, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 44, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_742_2_1"}, {"texts": ["A light brown camel is sitting on the soil surface."], "durations": null, "exact_frames_per_prompt": [24], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-5ZUMpdBPyw.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 28, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_742_3_0"}, {"texts": ["The light brown camel is standing up and starts walking."], "durations": null, "exact_frames_per_prompt": [52], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-5ZUMpdBPyw.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 28, "npz_gt_video_start_frame": 28, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 28, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_742_3_1"}, {"texts": ["A camel whose head is visible, is standing on the soil surface behind the first camel while a man and woman are sitting on the first camel and another man leads the first camel to get up."], "durations": null, "exact_frames_per_prompt": [33], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-5ZUMpdBPyw.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 37, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_742_5_0"}, {"texts": ["The camel starts walking.  while carrying a person in its back and the first camel also starts walking."], "durations": null, "exact_frames_per_prompt": [43], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-5ZUMpdBPyw.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 37, "npz_gt_video_start_frame": 37, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 37, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_742_5_1"}, {"texts": ["A man whom upper body is visible wearing a striped blue shirt and a golden color watch in his left hand is tying the necktie while wearing a necktie around his neck."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/09E5UTSE_-8.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_743_0_0"}, {"texts": ["A boy wearing a school uniform is sitting in the middle of the two girls and eating food."], "durations": null, "exact_frames_per_prompt": [49], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/3r4kLXjZ4_E.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 53, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_744_0_0"}, {"texts": ["The boy is wiping his mouth with a blue handkerchief while two girls sits beside him in the bus."], "durations": null, "exact_frames_per_prompt": [26], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/3r4kLXjZ4_E.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 53, "npz_gt_video_start_frame": 53, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 53, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_744_0_1"}, {"texts": ["A girl wearing a school uniform is sitting on the right side of the boy, holding a school bag and looking down while a girl wearing white-blue school uniform is sitting on the left is looking down."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/3r4kLXjZ4_E.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_744_1_0"}, {"texts": ["A boy wearing a white school uniform is sitting and eating something while a girl wearing white clothes is sitting on the right side of the boy and a person wearing white-blue dress is sitting on the left side of the boy."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/3r4kLXjZ4_E.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_745_0_0"}, {"texts": ["A girl wearing a white school uniform is sitting on the right while a boy wearing a white shirt is sitting in the middle, eating something, and wiping his mouth with a towel and a girl is sitting on the left."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/3r4kLXjZ4_E.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_745_2_0"}, {"texts": ["A person on the left side wearing a white shirt is sitting with a boy inside a moving vehicle while a girl wearing a white shirt is also sitting on the right side."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/3r4kLXjZ4_E.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_746_2_0"}, {"texts": ["A man wearing a bee protection suit leans down to put a basket."], "durations": null, "exact_frames_per_prompt": [20], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/4QDwWnhjm7k.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 24, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_747_0_0"}, {"texts": ["The man cuts the branch of a tree with the beehive."], "durations": null, "exact_frames_per_prompt": [56], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/4QDwWnhjm7k.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 24, "npz_gt_video_start_frame": 24, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 24, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_747_0_1"}, {"texts": ["A person on the right side, wearing printed cloth is drawing something on a paper using a brown pencil while a person whose only hands are visible on the left side is holding the hand of the another person and trying to help him."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-Ka7cGELbPg.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_74_0_0"}, {"texts": ["A person on the left side is holding the first person's hand and helping him to draw with a pencil."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-Ka7cGELbPg.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_74_1_0"}, {"texts": ["A person whose hands are visible is taking out stuff from the box."], "durations": null, "exact_frames_per_prompt": [64], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/11yfm5aKVW4.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 68, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_751_0_0"}, {"texts": ["The person whose hands are visible is putting the box aside.\n"], "durations": null, "exact_frames_per_prompt": [12], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/11yfm5aKVW4.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 68, "npz_gt_video_start_frame": 68, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 68, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_751_0_1"}, {"texts": ["A person whose hand is visible is holding a box."], "durations": null, "exact_frames_per_prompt": [11], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/11yfm5aKVW4.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 15, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_752_0_0"}, {"texts": ["The person picking out a transparent poly-bag and user guide."], "durations": null, "exact_frames_per_prompt": [65], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/11yfm5aKVW4.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 15, "npz_gt_video_start_frame": 15, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 15, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_752_0_1"}, {"texts": ["A person whose hands and lower body is visible is sitting on a bed and folding clothes using a brown cardboard."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-U2IQ5qOGmQ.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_753_0_0"}, {"texts": ["A person wearing a pink-black clothes is sitting on the bed and folding the clothes with the help of brown cardboard."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-U2IQ5qOGmQ.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_754_0_0"}, {"texts": ["A boy wearing a maroon t-shirt is standing in the front."], "durations": null, "exact_frames_per_prompt": [33], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/29LRyxlnxJg.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 37, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_756_0_0"}, {"texts": ["The boy is eating food, and then goes right and comes back while another boy wearing a stripe t-shirt is comes from the left side from the behind of the first boy and waving his hand."], "durations": null, "exact_frames_per_prompt": [44], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/29LRyxlnxJg.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 37, "npz_gt_video_start_frame": 37, "npz_gt_video_end_frame": 81, "skip_frames_after_generation": 37, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_756_0_1"}, {"texts": ["A boy wearing a white, gray, and red striped t-shirt is walking in the back towards the front and speaking while another boy is sitting infront and biting a food."], "durations": null, "exact_frames_per_prompt": [35], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/29LRyxlnxJg.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 34, "npz_gt_video_start_frame": 34, "npz_gt_video_end_frame": 69, "skip_frames_after_generation": 34, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_756_1_0"}, {"texts": ["The boy then goes back."], "durations": null, "exact_frames_per_prompt": [11], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/29LRyxlnxJg.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 69, "npz_gt_video_start_frame": 69, "npz_gt_video_end_frame": 81, "skip_frames_after_generation": 69, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_756_1_1"}, {"texts": ["A girl wearing grey jeans is sitting on a chair while a man wearing black jeans is sitting on the right side and making tattoo on the woman's leg."], "durations": null, "exact_frames_per_prompt": [36], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-ruRaj74zns.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 40, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_757_0_0"}, {"texts": ["The girl wearing grey jeans is getting a tattoo on her feet by a man."], "durations": null, "exact_frames_per_prompt": [40], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-ruRaj74zns.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 40, "npz_gt_video_start_frame": 40, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 40, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_757_0_1"}, {"texts": ["A man wearing a black t-shirt is sitting on the right side and making a tattoo on the feet of the girl with a tattoo making machine."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-ruRaj74zns.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_757_1_0"}, {"texts": ["A woman wearing a black shirt is standing and is keeping the tray of cookies on to the black kitchen counter, while speaking while a woman wearing a blue top stands with both hands on the black kitchen counter, speaking to the other woman."], "durations": null, "exact_frames_per_prompt": [29], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/1Mjxn54pu64.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 33, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_758_0_0"}, {"texts": ["A woman wearing a blue top is also standing, looking at the cookies.  while a woman on the left side in a green top is standing, speaking and putting a tray on the black countertop."], "durations": null, "exact_frames_per_prompt": [16], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/1Mjxn54pu64.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 20, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_758_1_0"}, {"texts": ["The woman wearing a blue top is speaking."], "durations": null, "exact_frames_per_prompt": [60], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/1Mjxn54pu64.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 20, "npz_gt_video_start_frame": 20, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 20, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_758_1_1"}, {"texts": ["A man wearing a dark bluish dress is sitting on the back of the camel and riding in the pasture with strong fences while another brown bactrian camel is walking on the left."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/0paGUniTKIo.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_759_0_0"}, {"texts": ["A brown camel is walking in the pasture with strong fences while carrying a man on his back.\n while another brown camel is walking on the left side."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/0paGUniTKIo.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_759_1_0"}, {"texts": ["A man wearing a black shirt and pants is sitting on the back of the camel on the right side and another camel is also walking on the left side."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/0paGUniTKIo.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_760_0_0"}, {"texts": ["A brown camel on the right side is walking from left to right while a man is sitting on its back while another brown camel on the left side is walking on the from the left to right."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/0paGUniTKIo.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_760_1_0"}, {"texts": ["A boy wearing a blue shirt is standing and tying a gray-black tie while speaking."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-55he7NGmDA.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_761_0_0"}, {"texts": ["A boy wearing a blue shirt is standing and tying a tie."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-55he7NGmDA.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_762_0_0"}, {"texts": ["A brown-black goat is standing inside a cage while a girl on the left side, wearing a red top, is standing and picking something from a plastic bowl."], "durations": null, "exact_frames_per_prompt": [35], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/06FXV1CM0Ck.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 39, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_763_1_0"}, {"texts": ["The goat is eating food given by the girl."], "durations": null, "exact_frames_per_prompt": [41], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/06FXV1CM0Ck.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 39, "npz_gt_video_start_frame": 39, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 39, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_763_1_1"}, {"texts": ["A person whose hand is visible is filling the water into the printed teapot."], "durations": null, "exact_frames_per_prompt": [30], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/1Wwn_LHYfp4.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 34, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_764_0_0"}, {"texts": ["The person whose hand is visible is putting a cap on the teapot."], "durations": null, "exact_frames_per_prompt": [25], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/1Wwn_LHYfp4.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 34, "npz_gt_video_start_frame": 34, "npz_gt_video_end_frame": 60, "skip_frames_after_generation": 34, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_764_0_1"}, {"texts": ["The person whose hand is visible is opening the brown wooden cupboard."], "durations": null, "exact_frames_per_prompt": [17], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/1Wwn_LHYfp4.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 60, "npz_gt_video_start_frame": 60, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 60, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_764_0_2"}, {"texts": ["A person wearing a blue protection suit is holding a hive frame."], "durations": null, "exact_frames_per_prompt": [28], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/7jGMbbXwur0.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 32, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_765_0_0"}, {"texts": ["The person is scratching the hive frame with a metal object."], "durations": null, "exact_frames_per_prompt": [13], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/7jGMbbXwur0.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 32, "npz_gt_video_start_frame": 32, "npz_gt_video_end_frame": 45, "skip_frames_after_generation": 32, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_765_0_1"}, {"texts": ["A boy wearing a light-purple t-shirt is standing on the multicoloured printed mat."], "durations": null, "exact_frames_per_prompt": [24], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/6o3NsfdlKNY.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 28, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_766_0_0"}, {"texts": ["The boy wearing a light-purple t-shirt is flipping the newspaper and looking at the newspaper."], "durations": null, "exact_frames_per_prompt": [51], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/6o3NsfdlKNY.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 28, "npz_gt_video_start_frame": 28, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 28, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_766_0_1"}, {"texts": ["A woman wearing a red-black dress is sitting."], "durations": null, "exact_frames_per_prompt": [54], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-Zbu2_p35m8.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 58, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_769_0_0"}, {"texts": ["The woman is picking a white-brown object from the table."], "durations": null, "exact_frames_per_prompt": [22], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-Zbu2_p35m8.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 58, "npz_gt_video_start_frame": 58, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 58, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_769_0_1"}, {"texts": ["A person whose hand is visible is holding a tong and tapping on the bowl."], "durations": null, "exact_frames_per_prompt": [25], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-PHSdnps9BA.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 14, "npz_gt_video_start_frame": 14, "npz_gt_video_end_frame": 39, "skip_frames_after_generation": 14, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_770_0_0"}, {"texts": ["A woman wearing black t-shirt is sitting."], "durations": null, "exact_frames_per_prompt": [17], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/5FJeX8Um9pw.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 21, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_772_0_0"}, {"texts": ["The woman is moving her hands in sign language."], "durations": null, "exact_frames_per_prompt": [59], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/5FJeX8Um9pw.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 21, "npz_gt_video_start_frame": 21, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 21, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_772_0_1"}, {"texts": ["A boy wearing a red t-shirt is standing and folding a paper."], "durations": null, "exact_frames_per_prompt": [48], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/2pyk-4pfMO0.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 52, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_773_0_0"}, {"texts": ["The boy wearing a red t-shirt sits down."], "durations": null, "exact_frames_per_prompt": [28], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/2pyk-4pfMO0.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 52, "npz_gt_video_start_frame": 52, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 52, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_773_0_1"}, {"texts": ["A woman wearing a white top and a black skirt is standing in front of a screen and presenting weather forecast report on a big screen."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/0PZucUHByXQ.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_774_0_0"}, {"texts": ["A woman standing in front of a screen wearing a black skirt is presenting the weather forecast report."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/0PZucUHByXQ.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_775_0_0"}, {"texts": ["A girl wearing a pink t-shirt and dark blue pants is sitting on a gray seat and eating the food."], "durations": null, "exact_frames_per_prompt": [38], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-MoSli_S6hM.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 42, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_776_0_0"}, {"texts": ["A person wearing a purple cloth standing behind the counter-top is chopping the pineapple."], "durations": null, "exact_frames_per_prompt": [54], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/23I7VJAJqcA.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 58, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_783_0_0"}, {"texts": ["A man wearing a black t-shirt and brown trousers is standing while a group of people is playing basketball at the back."], "durations": null, "exact_frames_per_prompt": [10], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/0lyAtixAMGE.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 14, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_786_0_0"}, {"texts": ["The man is wrapping tape on the foot of the other man."], "durations": null, "exact_frames_per_prompt": [66], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/0lyAtixAMGE.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 14, "npz_gt_video_start_frame": 14, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 14, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_786_0_1"}, {"texts": ["A man wearing a gray vest and gray shorts is sitting on the bed while a man wearing a black t-shirt is standing on a brown floor, holding a tape, and a group of people are walking at the  back."], "durations": null, "exact_frames_per_prompt": [10], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/0lyAtixAMGE.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 14, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_786_1_0"}, {"texts": ["The man is getting tape applied on his foot by the first man."], "durations": null, "exact_frames_per_prompt": [66], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/0lyAtixAMGE.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 14, "npz_gt_video_start_frame": 14, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 14, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_786_1_1"}, {"texts": ["A man wearing a black t-shirt is standing while a person wearing a grey vest is sitting on the right side."], "durations": null, "exact_frames_per_prompt": [10], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/0lyAtixAMGE.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 14, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_787_0_0"}, {"texts": ["The man is tapping a person's ankle with a blue tape while a group of people are standing and moving at the back."], "durations": null, "exact_frames_per_prompt": [66], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/0lyAtixAMGE.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 14, "npz_gt_video_start_frame": 14, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 14, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_787_0_1"}, {"texts": ["A man wearing a grey tank t-shirt is sitting while another man wearing black and grey outfit, holding an ankle tape is standing on the left side of first man and saying something."], "durations": null, "exact_frames_per_prompt": [10], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/0lyAtixAMGE.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 14, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_787_1_0"}, {"texts": ["The man is getting his ankle taped while group of people are standing on the backside and one person wearing white outfit is running towards right side."], "durations": null, "exact_frames_per_prompt": [66], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/0lyAtixAMGE.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 14, "npz_gt_video_start_frame": 14, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 14, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_787_1_1"}, {"texts": ["A man wearing blue gloves is kneeling on the floor near the car's wheel, and touching the wheel while doing hand gestures while another man wearing a blue jacket and white gloves is opening the back part of the car."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/46snzh87QeY.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_788_0_0"}, {"texts": ["A man wearing blue clothes is sitting on the car's trunk."], "durations": null, "exact_frames_per_prompt": [15], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/46snzh87QeY.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 47, "npz_gt_video_start_frame": 47, "npz_gt_video_end_frame": 62, "skip_frames_after_generation": 47, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_788_1_0"}, {"texts": ["The man opens the boot cover of the car."], "durations": null, "exact_frames_per_prompt": [18], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/46snzh87QeY.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 62, "npz_gt_video_start_frame": 62, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 62, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_788_1_1"}, {"texts": ["A golden fish is swimming in the small aquarium."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/0mR_JpazrtA.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_789_1_0"}, {"texts": ["A man wearing a white t-shirt is standing and drinking from a glass while two men sitting in the chair are watching him."], "durations": null, "exact_frames_per_prompt": [46], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/7nu0mVwhxLc.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 12, "npz_gt_video_start_frame": 12, "npz_gt_video_end_frame": 58, "skip_frames_after_generation": 12, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_790_0_0"}, {"texts": ["The man wearing a white t-shirt puts it down."], "durations": null, "exact_frames_per_prompt": [3], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/7nu0mVwhxLc.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 58, "npz_gt_video_start_frame": 58, "npz_gt_video_end_frame": 61, "skip_frames_after_generation": 58, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_790_0_1"}, {"texts": ["A man wearing a blue t-shirt is standing and drinking from a glass."], "durations": null, "exact_frames_per_prompt": [19], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/7nu0mVwhxLc.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 61, "npz_gt_video_start_frame": 61, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 61, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_790_1_0"}, {"texts": ["A man wearing a white blue striped t-shirt is sitting on a chair and watching the first man and claps while a man wearing glasses is sitting on the left side and watching the first man."], "durations": null, "exact_frames_per_prompt": [48], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/7nu0mVwhxLc.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 13, "npz_gt_video_start_frame": 13, "npz_gt_video_end_frame": 61, "skip_frames_after_generation": 13, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_790_2_0"}, {"texts": ["A woman wearing a white t-shirt is cleaning the white horse with a brown brush."], "durations": null, "exact_frames_per_prompt": [51], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/3VoRIFAw4so.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 55, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_791_0_0"}, {"texts": ["The woman wearing a white t-shirt is rubbing the two brown brushes against each other."], "durations": null, "exact_frames_per_prompt": [25], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/3VoRIFAw4so.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 55, "npz_gt_video_start_frame": 55, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 55, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_791_0_1"}, {"texts": ["A woman is wearing a purple sweater and blue jeans is riding a horse."], "durations": null, "exact_frames_per_prompt": [39], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/4kjSczWgOS8.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 41, "npz_gt_video_start_frame": 41, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 41, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_792_0_0"}, {"texts": ["A man whose only hands are visible is tying a knot in a rope."], "durations": null, "exact_frames_per_prompt": [20], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/1dzj0lySg-c.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 24, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_796_0_0"}, {"texts": ["The man is untying the knot."], "durations": null, "exact_frames_per_prompt": [17], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/1dzj0lySg-c.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 24, "npz_gt_video_start_frame": 24, "npz_gt_video_end_frame": 41, "skip_frames_after_generation": 24, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_796_0_1"}, {"texts": ["A man wearing a black-white outfit is standing, holding a cloth to stop engine oil from coming out of a car while a person wearing a white-black jacket is pouring something into a bucket, another person whose hands are visible is clapping on the right side, and a man wearing a black hoodie is walking towards the man and tickling him."], "durations": null, "exact_frames_per_prompt": [68], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-PIE6PrQ66s.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 72, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_797_0_0"}, {"texts": ["The man removes the cloth."], "durations": null, "exact_frames_per_prompt": [8], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-PIE6PrQ66s.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 72, "npz_gt_video_start_frame": 72, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 72, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_797_0_1"}, {"texts": ["A man wearing a white t-shirt is playing golf on the green mat."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-NnynSMg5Xg.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_799_0_0"}, {"texts": ["A woman wearing a red sweatshirt and white pants is sitting on the back of the horse and  holding a rope attached with a gray object."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/04G0mXfS9Ck.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_802_0_0"}, {"texts": ["A dark brown horse is walking while carrying a woman on his back."], "durations": null, "exact_frames_per_prompt": [42], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/04G0mXfS9Ck.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 46, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_802_1_0"}, {"texts": ["A dark brown horse stops and the woman is moving a purple object tied to a rope around the horse."], "durations": null, "exact_frames_per_prompt": [34], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/04G0mXfS9Ck.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 46, "npz_gt_video_start_frame": 46, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 46, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_802_1_1"}, {"texts": ["A woman wearing a red hoodie and white trousers is riding a horse."], "durations": null, "exact_frames_per_prompt": [32], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/04G0mXfS9Ck.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 36, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_803_0_0"}, {"texts": ["The woman wearing a red hoodie and white trousers is swinging a gray thing while a brown horse is standing on the green grass surface while carrying a woman on its back."], "durations": null, "exact_frames_per_prompt": [44], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/04G0mXfS9Ck.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 36, "npz_gt_video_start_frame": 36, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 36, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_803_0_1"}, {"texts": ["A brown horse is walking on a green grass field being ridden by a woman."], "durations": null, "exact_frames_per_prompt": [43], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/04G0mXfS9Ck.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 47, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_803_1_0"}, {"texts": ["A woman whose hands are visible is putting the card on another woman's face."], "durations": null, "exact_frames_per_prompt": [26], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-lYpwvV3Wos.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 30, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_804_0_0"}, {"texts": ["The woman is then doing eyebrow make-up with the help of make-up brush."], "durations": null, "exact_frames_per_prompt": [50], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-lYpwvV3Wos.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 30, "npz_gt_video_start_frame": 30, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 30, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_804_0_1"}, {"texts": ["A woman whose face is visible is getting his eyebrow make-up done by the first woman."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-lYpwvV3Wos.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_804_1_0"}, {"texts": ["A woman wearing a blue t-shirt and grey pants is standing on the left side eating food while a man standing in the middle in a green t-shirt is also eating food and another man also wearing a green t-shirt is standing on the right holding a cup and eating something."], "durations": null, "exact_frames_per_prompt": [33], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/2GBRqXhNWts.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 37, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_807_0_0"}, {"texts": ["The woman moves forward to pick the food."], "durations": null, "exact_frames_per_prompt": [10], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/2GBRqXhNWts.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 37, "npz_gt_video_start_frame": 37, "npz_gt_video_end_frame": 47, "skip_frames_after_generation": 37, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_807_0_1"}, {"texts": ["The woman turns around and the first man picks food up and eating it."], "durations": null, "exact_frames_per_prompt": [33], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/2GBRqXhNWts.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 47, "npz_gt_video_start_frame": 47, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 47, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_807_0_2"}, {"texts": ["A man wearing a blue t-shirt and grey shorts standing is eating food while another man and a woman are also eating food."], "durations": null, "exact_frames_per_prompt": [41], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/2GBRqXhNWts.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 45, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_807_1_0"}, {"texts": ["The man leans forward to pick the food."], "durations": null, "exact_frames_per_prompt": [35], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/2GBRqXhNWts.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 45, "npz_gt_video_start_frame": 45, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 45, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_807_1_1"}, {"texts": ["A boy wearing a black t-shirt is sitting."], "durations": null, "exact_frames_per_prompt": [19], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-997f74Tn8Q.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 23, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_808_0_0"}, {"texts": ["The boy tapping noodles on a plate with a fork."], "durations": null, "exact_frames_per_prompt": [44], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-997f74Tn8Q.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 23, "npz_gt_video_start_frame": 23, "npz_gt_video_end_frame": 67, "skip_frames_after_generation": 23, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_808_0_1"}, {"texts": ["The boy starts eating noodles."], "durations": null, "exact_frames_per_prompt": [13], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-997f74Tn8Q.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 67, "npz_gt_video_start_frame": 67, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 67, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_808_0_2"}, {"texts": ["A man wearing black t-shirt is standing in front of a table and performing flair bartending."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/2eJdtG9blsc.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_80_0_0"}, {"texts": ["A man whose only hand is visible is ironing a blue cloth with a white iron."], "durations": null, "exact_frames_per_prompt": [62], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-YD8IT6Htqk.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 66, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_810_0_0"}, {"texts": ["A man wearing a grey t-shirt is standing, holding a golf stick and playing golf."], "durations": null, "exact_frames_per_prompt": [22], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/0fPAmoTZcdE.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 26, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_812_0_0"}, {"texts": ["The ball is going towards the flag stick."], "durations": null, "exact_frames_per_prompt": [52], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/0fPAmoTZcdE.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 26, "npz_gt_video_start_frame": 26, "npz_gt_video_end_frame": 78, "skip_frames_after_generation": 26, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_812_0_1"}, {"texts": ["A man wearing a gray suit and white shirt is standing in front of a desk and speaking."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-L074U8sOpU.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_814_0_0"}, {"texts": ["A woman on the left side wearing a black t-shirt is standing, holding a trophy in her hands and a woman on the right side wearing a black t-shirt is standing, holding a trophy in her hands."], "durations": null, "exact_frames_per_prompt": [51], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/0gMiOR0LNhw.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 55, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_815_0_0"}, {"texts": ["The woman is dancing while another woman in a black t-shirt also starts dancing."], "durations": null, "exact_frames_per_prompt": [25], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/0gMiOR0LNhw.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 55, "npz_gt_video_start_frame": 55, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 55, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_815_0_1"}, {"texts": ["A woman wearing a black t-shirt is standing on the right side while another woman on the left side wearing a black t-shirt is standing while holding an award and speaking."], "durations": null, "exact_frames_per_prompt": [52], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/0gMiOR0LNhw.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 56, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_815_1_0"}, {"texts": ["The woman wearing a black t-shirt is also dancing while another woman on the left side wearing a black t-shirt is dancing while doing beat-boxing."], "durations": null, "exact_frames_per_prompt": [24], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/0gMiOR0LNhw.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 56, "npz_gt_video_start_frame": 56, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 56, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_815_1_1"}, {"texts": ["A lady wearing a black top is standing on the brown soil surface and holding three budgerigar and feeding them grains in her hand."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-qEiyS8Zx2s.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_818_0_0"}, {"texts": ["A girl wearing a red frock takes out the white dress out of the pink bag."], "durations": null, "exact_frames_per_prompt": [15], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/0cRYsT9pHOM.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 13, "npz_gt_video_start_frame": 13, "npz_gt_video_end_frame": 28, "skip_frames_after_generation": 13, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_819_0_0"}, {"texts": ["The girl wearing a red frock looks at the dress."], "durations": null, "exact_frames_per_prompt": [24], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/0cRYsT9pHOM.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 28, "npz_gt_video_start_frame": 28, "npz_gt_video_end_frame": 52, "skip_frames_after_generation": 28, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_819_0_1"}, {"texts": ["The girl wearing a red frock then takes the dress towards the couch."], "durations": null, "exact_frames_per_prompt": [13], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/0cRYsT9pHOM.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 52, "npz_gt_video_start_frame": 52, "npz_gt_video_end_frame": 65, "skip_frames_after_generation": 52, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_819_0_2"}, {"texts": ["A man wearing a black t-shirt is standing, juggling bottles, and performing tricks."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/2eJdtG9blsc.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_81_0_0"}, {"texts": ["A girl moving on a brown surface takes a pink box and puts it on something."], "durations": null, "exact_frames_per_prompt": [15], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/0cRYsT9pHOM.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 19, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_820_0_0"}, {"texts": ["The girl picks up a white cloth."], "durations": null, "exact_frames_per_prompt": [46], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/0cRYsT9pHOM.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 19, "npz_gt_video_start_frame": 19, "npz_gt_video_end_frame": 65, "skip_frames_after_generation": 19, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_820_0_1"}, {"texts": ["A kid on the left wearing a light blue top is feeding the brown mountain goat and is looking at the back while another kid wearing blue jeans is also feeding it, a person is sitting on the right and holding food and a white-black goat is walking from left right in the backside."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/0NwHUzWjfIE.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_822_0_0"}, {"texts": ["A kid on the right is also standing while a girl wearing a blue top is feeding the goat then starts jumping and smiling, and a person whose hand is visible is sitting on the right and is holding something in his hands."], "durations": null, "exact_frames_per_prompt": [39], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/0NwHUzWjfIE.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 43, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_822_1_0"}, {"texts": ["The kid is feeding the brown mountain goat while the girl is holding the fence and is jumping and looking back, and the person holds the hand of the kid."], "durations": null, "exact_frames_per_prompt": [37], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/0NwHUzWjfIE.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 43, "npz_gt_video_start_frame": 43, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 43, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_822_1_1"}, {"texts": ["A person whose hands and legs are visible is sitting and is holding the food for the goat while a girl wearing a blue top is feeding food to the goat and then starts jumping on the grey surface, a kid wearing a grey t-shirt is taking food from the person's hand and feeding it to the goat, and the goat starts to lick the kid's hand, and another black-and-white goat is moving in the right direction."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/0NwHUzWjfIE.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_822_2_0"}, {"texts": ["A brown mountain goat is standing and is being fed by kid one and two while a person is sitting on the right side and holding food in the left hand while pulling the hand of the kid standing in the middle."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/0NwHUzWjfIE.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_822_3_0"}, {"texts": ["A kid wearing a gray t-shirt and blue jeans is standing on the right while another girl on the left is feeding the goat and a woman is holding goat food in a paper."], "durations": null, "exact_frames_per_prompt": [37], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/0NwHUzWjfIE.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 41, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_823_1_0"}, {"texts": ["The kid is feeding the brown mountain goat and the woman holds the hand of the kid."], "durations": null, "exact_frames_per_prompt": [18], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/0NwHUzWjfIE.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 41, "npz_gt_video_start_frame": 41, "npz_gt_video_end_frame": 59, "skip_frames_after_generation": 41, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_823_1_1"}, {"texts": ["A brown mountain goat is standing and being fed by the kids while a person whose half body is visible wearing black cloth is sitting and holding a paper bowl with some stuff and then he touches the hand of the kid."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/0NwHUzWjfIE.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_823_3_0"}, {"texts": ["A person whose hands are visible is holding a tea bag and a spoon."], "durations": null, "exact_frames_per_prompt": [21], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/0DphXEsQAbU.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 25, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_824_0_0"}, {"texts": ["The person is opening the tea bag."], "durations": null, "exact_frames_per_prompt": [54], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/0DphXEsQAbU.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 25, "npz_gt_video_start_frame": 25, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 25, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_824_0_1"}, {"texts": ["A man wearing a black t-shirt is standing and talking."], "durations": null, "exact_frames_per_prompt": [77], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/0sBTBSDbxAU.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 81, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_825_0_0"}, {"texts": ["A woman wearing a black t-shirt is standing, picking the clothes from the white bucket and putting the clothes on the floor."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/3O0CDPBnLJ8.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_826_0_0"}, {"texts": ["A man whose upper half-body is visible wearing a white shirt is tucking the bow tie around his neck and tightening the bow tie with his hands."], "durations": null, "exact_frames_per_prompt": [59], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/42aQOA9oFPM.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 63, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_827_0_0"}, {"texts": ["The man wearing a white shirt is putting the bow tie down the collar of the shirt."], "durations": null, "exact_frames_per_prompt": [17], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/42aQOA9oFPM.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 63, "npz_gt_video_start_frame": 63, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 63, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_827_0_1"}, {"texts": ["A girl wearing a pink jacket is standing, pulling the branch."], "durations": null, "exact_frames_per_prompt": [26], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/0fAJtEFuaMM.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 30, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_828_0_0"}, {"texts": ["The girl is holding fruit while a person wearing a black jacket and a kid wearing a red jacket are standing behind the girl."], "durations": null, "exact_frames_per_prompt": [50], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/0fAJtEFuaMM.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 30, "npz_gt_video_start_frame": 30, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 30, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_828_0_1"}, {"texts": ["A person whose hand is visible is holding the same fruit while a girl in a pink jacket is holding that fruit in her hands, then loosens her grip on the fruit but again holds it, and two people at the back are standing."], "durations": null, "exact_frames_per_prompt": [70], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/0fAJtEFuaMM.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 10, "npz_gt_video_start_frame": 10, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 10, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_828_1_0"}, {"texts": ["A girl wearing a pink jacket is standing."], "durations": null, "exact_frames_per_prompt": [8], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/0fAJtEFuaMM.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 12, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_829_0_0"}, {"texts": ["The girl wearing a pink jacket is trying to pick a fruit from the tree."], "durations": null, "exact_frames_per_prompt": [68], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/0fAJtEFuaMM.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 12, "npz_gt_video_start_frame": 12, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 12, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_829_0_1"}, {"texts": ["A child wearing a white dress is moving backward and giving food to the pigeons."], "durations": null, "exact_frames_per_prompt": [21], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-VuAeKfNf18.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 25, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_833_0_0"}, {"texts": ["The child stops."], "durations": null, "exact_frames_per_prompt": [47], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-VuAeKfNf18.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 25, "npz_gt_video_start_frame": 25, "npz_gt_video_end_frame": 72, "skip_frames_after_generation": 25, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_833_0_1"}, {"texts": ["The child bends downwards."], "durations": null, "exact_frames_per_prompt": [8], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-VuAeKfNf18.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 72, "npz_gt_video_start_frame": 72, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 72, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_833_0_2"}, {"texts": ["A man wearing a striped shirt is holding a knife and picking a honeycomb from a beehive box."], "durations": null, "exact_frames_per_prompt": [41], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/4Z-k0tR8E7g.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 45, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_838_0_0"}, {"texts": ["The man is blowing air on it and removing the bees."], "durations": null, "exact_frames_per_prompt": [17], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/4Z-k0tR8E7g.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 45, "npz_gt_video_start_frame": 45, "npz_gt_video_end_frame": 62, "skip_frames_after_generation": 45, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_838_0_1"}, {"texts": ["The man is moving and flipping the honeycomb."], "durations": null, "exact_frames_per_prompt": [18], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/4Z-k0tR8E7g.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 62, "npz_gt_video_start_frame": 62, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 62, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_838_0_2"}, {"texts": ["A hive of bees is sitting upon the honeycombs and a person wearing a shirt is removing honeycomb with his right hand while holding a knife in his left hand."], "durations": null, "exact_frames_per_prompt": [40], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/4Z-k0tR8E7g.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 44, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_838_1_0"}, {"texts": ["A boy wearing an orange t-shirt is riding on an elephant while looking here and there while a gray elephant is walking on the green grass surface."], "durations": null, "exact_frames_per_prompt": [39], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-5O_xxIpooE.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 43, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_839_0_0"}, {"texts": ["An elephant is giving a ride to the boy in an orange t-shirt as another elephant is walking in front of the boy on a green grass surface."], "durations": null, "exact_frames_per_prompt": [38], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-5O_xxIpooE.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 42, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_839_1_0"}, {"texts": ["An elephant is giving a ride to the group of people sitting on it then one person in a green top jumps down on the ground."], "durations": null, "exact_frames_per_prompt": [33], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-5O_xxIpooE.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 47, "npz_gt_video_start_frame": 47, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 47, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_839_3_0"}, {"texts": ["A person whose hands are visible is peeling a watermelon with a knife."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/ac8VY4zXIao.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_843_0_0"}, {"texts": ["A girl wearing a multi-color monokini is sitting in a chair and talking."], "durations": null, "exact_frames_per_prompt": [21], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/2FrJ5gswZ0Y.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 25, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_844_0_0"}, {"texts": ["The girl is eating a chip."], "durations": null, "exact_frames_per_prompt": [38], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/2FrJ5gswZ0Y.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 25, "npz_gt_video_start_frame": 25, "npz_gt_video_end_frame": 63, "skip_frames_after_generation": 25, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_844_0_1"}, {"texts": ["The girl is showing a thumb signal."], "durations": null, "exact_frames_per_prompt": [16], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/2FrJ5gswZ0Y.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 63, "npz_gt_video_start_frame": 63, "npz_gt_video_end_frame": 79, "skip_frames_after_generation": 63, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_844_0_2"}, {"texts": ["A person, whose only hand is visible, is wrapping with a red gift paper."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/1vuCbghkVXE.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_846_0_0"}, {"texts": ["A person wearing blue clothes is standing near the grey bed and holding the dog while a person wearing white gloves standing on the left side is putting a bandage on the dog's eye."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-6rd6BQQM_g.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_847_0_0"}, {"texts": ["A person whose hands are visible is wearing white gloves and bandaging the dog while another person on the right side a blue shirt is standing and holding the dog."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-6rd6BQQM_g.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_847_1_0"}, {"texts": ["An off-white colored dog is lying on a grey bed and getting itself bandaged by the person in the left side while a person on the right is holding the dog."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-6rd6BQQM_g.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_847_2_0"}, {"texts": ["A woman wearing a purple t-shirt and a pink apron is standing and dancing."], "durations": null, "exact_frames_per_prompt": [31], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/1b30W2nL9qI.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 35, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_848_0_0"}, {"texts": ["The woman lifts a pan, and flip something in it."], "durations": null, "exact_frames_per_prompt": [46], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/1b30W2nL9qI.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 35, "npz_gt_video_start_frame": 35, "npz_gt_video_end_frame": 81, "skip_frames_after_generation": 35, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_848_0_1"}, {"texts": ["A woman wearing blue clothes is standing near the horse and massaging the horse."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/3mQPKkRWfRA.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_849_0_0"}, {"texts": ["A horse is standing and getting massaged by the woman."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/3mQPKkRWfRA.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_849_1_0"}, {"texts": ["A boy wearing a multi-grey t-shirt is sitting on the right."], "durations": null, "exact_frames_per_prompt": [65], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/1ujBjL4WdaI.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 69, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_852_0_0"}, {"texts": ["The boy is getting teased by another boy and smiling."], "durations": null, "exact_frames_per_prompt": [11], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/1ujBjL4WdaI.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 69, "npz_gt_video_start_frame": 69, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 69, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_852_0_1"}, {"texts": ["A boy wearing a black t-shirt sitting on the left is teasing the first boy and starts talking."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/1ujBjL4WdaI.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_852_1_0"}, {"texts": ["A person whose hand is visible is caressing the black cat with his hand."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-SH4SlhSki0.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_853_0_0"}, {"texts": ["A black cat is sleeping on the dark pink cushion and being caressed by the person."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-SH4SlhSki0.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_853_1_0"}, {"texts": ["A black cat is sleeping on the dark pink cushion, and its body is being caressed by the person."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-SH4SlhSki0.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_854_1_0"}, {"texts": ["A man wearing a light blue shirt is demonstrating how to set a tie and setting the collar."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/03kTvd80PPc.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_855_0_0"}, {"texts": ["A person whose hands are visible is touching a barbie doll set."], "durations": null, "exact_frames_per_prompt": [43], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/0NFCUv_2yCg.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 47, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_856_0_0"}, {"texts": ["The person starts opening the thread knots."], "durations": null, "exact_frames_per_prompt": [33], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/0NFCUv_2yCg.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 47, "npz_gt_video_start_frame": 47, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 47, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_856_0_1"}, {"texts": ["A black dog is moving on a gray surface along with a person whose shoes are visible."], "durations": null, "exact_frames_per_prompt": [27], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/4751IAv_ZJM.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 31, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_858_0_0"}, {"texts": ["The dog sits down."], "durations": null, "exact_frames_per_prompt": [49], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/4751IAv_ZJM.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 31, "npz_gt_video_start_frame": 31, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 31, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_858_0_1"}, {"texts": ["A person wearing blue cloth is walking behind the dog."], "durations": null, "exact_frames_per_prompt": [46], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/4751IAv_ZJM.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 50, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_858_1_0"}, {"texts": ["A boy wearing a grey-blue jacket is sitting."], "durations": null, "exact_frames_per_prompt": [22], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/0kj550MXu-8.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 26, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_859_0_0"}, {"texts": ["The boy touches his nose with a finger."], "durations": null, "exact_frames_per_prompt": [24], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/0kj550MXu-8.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 26, "npz_gt_video_start_frame": 26, "npz_gt_video_end_frame": 50, "skip_frames_after_generation": 26, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_859_0_1"}, {"texts": ["The boy licks the finger."], "durations": null, "exact_frames_per_prompt": [15], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/0kj550MXu-8.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 50, "npz_gt_video_start_frame": 50, "npz_gt_video_end_frame": 65, "skip_frames_after_generation": 50, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_859_0_2"}, {"texts": ["The boy starts eating an ice cream cone."], "durations": null, "exact_frames_per_prompt": [14], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/0kj550MXu-8.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 65, "npz_gt_video_start_frame": 65, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 65, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_859_0_3"}, {"texts": ["A girl wearing white clothes is holding a baby snake in her hands and is looking at the snake."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/3hpek607Uv0.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_863_0_0"}, {"texts": ["A baby snake is moving from one hand to another of the girl."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/3hpek607Uv0.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_863_1_0"}, {"texts": ["A man wearing a maroon t-shirt is standing, lifting the food from a glass bowl and pouring the food with a spoon on the black-brown food."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-NKy_8ynrKs.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_866_0_0"}, {"texts": ["A man in a purple t-shirt stands."], "durations": null, "exact_frames_per_prompt": [10], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-NKy_8ynrKs.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 14, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_867_0_0"}, {"texts": ["The man sprinkles some mix on a mushroom from the bowl."], "durations": null, "exact_frames_per_prompt": [66], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-NKy_8ynrKs.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 14, "npz_gt_video_start_frame": 14, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 14, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_867_0_1"}, {"texts": ["A person wearing a white shirt is making eggs in a black pan with the help of a spatula."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-NM9jWuf4hM.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_870_0_0"}, {"texts": ["A kid wearing a brown cloth is standing while a man in a white t-shirt is holding a cup of food and looking at him."], "durations": null, "exact_frames_per_prompt": [26], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-PL3Ncuylo0.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 30, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_871_0_0"}, {"texts": ["The kid is fed by a man."], "durations": null, "exact_frames_per_prompt": [50], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-PL3Ncuylo0.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 30, "npz_gt_video_start_frame": 30, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 30, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_871_0_1"}, {"texts": ["A man wearing a white shirt is sitting on the grey floor."], "durations": null, "exact_frames_per_prompt": [26], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-PL3Ncuylo0.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 30, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_871_1_0"}, {"texts": ["The man is feeding food to the baby."], "durations": null, "exact_frames_per_prompt": [50], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-PL3Ncuylo0.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 30, "npz_gt_video_start_frame": 30, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 30, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_871_1_1"}, {"texts": ["A woman wearing camouflage is standing and putting the blue blanket."], "durations": null, "exact_frames_per_prompt": [36], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/j7nD-S7pIV8.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 40, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_873_0_0"}, {"texts": ["The woman is tucking in the blanket into the bed."], "durations": null, "exact_frames_per_prompt": [40], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/j7nD-S7pIV8.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 40, "npz_gt_video_start_frame": 40, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 40, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_873_0_1"}, {"texts": ["A person whom a hand is visible is at first opens the lid of a tetra pack."], "durations": null, "exact_frames_per_prompt": [59], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-ZfEt3R5GeA.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 63, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_874_0_0"}, {"texts": ["The person pours the egg white in white bowl."], "durations": null, "exact_frames_per_prompt": [17], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-ZfEt3R5GeA.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 63, "npz_gt_video_start_frame": 63, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 63, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_874_0_1"}, {"texts": ["A man wearing blue clothes is applying something on his face."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/0PgfOKP1Ow0.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_879_0_0"}, {"texts": ["A man wearing a blue shirt is touching the collar of the shirt and touching the white tie."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/0nuCSRSpBJs.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_87_0_0"}, {"texts": ["A person whose hand is visible lifts the clear glass, puts it into a microwave."], "durations": null, "exact_frames_per_prompt": [16], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/2HaBPLuf2jk.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 50, "npz_gt_video_start_frame": 50, "npz_gt_video_end_frame": 66, "skip_frames_after_generation": 50, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_880_0_0"}, {"texts": ["The person closes the microwave."], "durations": null, "exact_frames_per_prompt": [14], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/2HaBPLuf2jk.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 66, "npz_gt_video_start_frame": 66, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 66, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_880_0_1"}, {"texts": ["A person whose hand is visible, puts a fork and then picks a bowl and puts in the microwave."], "durations": null, "exact_frames_per_prompt": [25], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/2HaBPLuf2jk.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 50, "npz_gt_video_start_frame": 50, "npz_gt_video_end_frame": 75, "skip_frames_after_generation": 50, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_881_0_0"}, {"texts": ["A man wearing a blue t-shirt and denim jeans is walking around in the room."], "durations": null, "exact_frames_per_prompt": [37], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/1TmE8hPBAZY.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 41, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_882_0_0"}, {"texts": ["The man wearing a blue t-shirt is folding the clothes."], "durations": null, "exact_frames_per_prompt": [40], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/1TmE8hPBAZY.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 41, "npz_gt_video_start_frame": 41, "npz_gt_video_end_frame": 81, "skip_frames_after_generation": 41, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_882_0_1"}, {"texts": ["A person whose hand is visible is holding a tool."], "durations": null, "exact_frames_per_prompt": [23], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/4kjreM5z-Mo.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 27, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_883_0_0"}, {"texts": ["The person puts the tool down."], "durations": null, "exact_frames_per_prompt": [26], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/4kjreM5z-Mo.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 27, "npz_gt_video_start_frame": 27, "npz_gt_video_end_frame": 53, "skip_frames_after_generation": 27, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_883_0_1"}, {"texts": ["The person lifts another tool."], "durations": null, "exact_frames_per_prompt": [18], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/4kjreM5z-Mo.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 53, "npz_gt_video_start_frame": 53, "npz_gt_video_end_frame": 71, "skip_frames_after_generation": 53, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_883_0_2"}, {"texts": ["The person puts it into the rim."], "durations": null, "exact_frames_per_prompt": [9], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/4kjreM5z-Mo.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 71, "npz_gt_video_start_frame": 71, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 71, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_883_0_3"}, {"texts": ["A boy wearing blue clothes and a blue cap is holding a fishing rod."], "durations": null, "exact_frames_per_prompt": [46], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/24Ula6s8kNA.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 50, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_885_0_0"}, {"texts": ["The boy is getting down from the rocky surface towards the shore."], "durations": null, "exact_frames_per_prompt": [30], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/24Ula6s8kNA.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 50, "npz_gt_video_start_frame": 50, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 50, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_885_0_1"}, {"texts": ["A man wearing a red t-shirt is training a brown dog."], "durations": null, "exact_frames_per_prompt": [66], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/4O-73GOrq9U.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 70, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_886_0_0"}, {"texts": ["The man is treating him with food."], "durations": null, "exact_frames_per_prompt": [10], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/4O-73GOrq9U.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 70, "npz_gt_video_start_frame": 70, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 70, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_886_0_1"}, {"texts": ["A brown dog is being trained by the man."], "durations": null, "exact_frames_per_prompt": [67], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/4O-73GOrq9U.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 71, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_886_1_0"}, {"texts": ["The brown dog is being treated with food by the man."], "durations": null, "exact_frames_per_prompt": [9], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/4O-73GOrq9U.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 71, "npz_gt_video_start_frame": 71, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 71, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_886_1_1"}, {"texts": ["A man whose only upper body is visible wearing a black hoodie and a black cap is holding a screen and showing the dog."], "durations": null, "exact_frames_per_prompt": [41], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-2-2p5R_ujo.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 45, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_887_0_ms_0"}, {"texts": ["The man whose only upper body is visible starts walking on the green grass surface."], "durations": null, "exact_frames_per_prompt": [35], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-2-2p5R_ujo.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 45, "npz_gt_video_start_frame": 45, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 45, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_887_0_ms_1"}, {"texts": ["A brown dog whose only face is visible is watching the screen."], "durations": null, "exact_frames_per_prompt": [12], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-2-2p5R_ujo.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 34, "npz_gt_video_start_frame": 34, "npz_gt_video_end_frame": 46, "skip_frames_after_generation": 34, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_887_1_ms_0"}, {"texts": ["A boy wearing a red t-shirt is sitting on a sofa and picking a watermelon slice from a plate."], "durations": null, "exact_frames_per_prompt": [28], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/3a5ogQ_wKlw.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 32, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_889_0_0"}, {"texts": ["The boy wearing a red t-shirt is eating the watermelon slice."], "durations": null, "exact_frames_per_prompt": [33], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/3a5ogQ_wKlw.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 32, "npz_gt_video_start_frame": 32, "npz_gt_video_end_frame": 65, "skip_frames_after_generation": 32, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_889_0_1"}, {"texts": ["The boy wearing a red t-shirt is speaking."], "durations": null, "exact_frames_per_prompt": [15], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/3a5ogQ_wKlw.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 65, "npz_gt_video_start_frame": 65, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 65, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_889_0_2"}, {"texts": ["A baby wearing white clothes is sitting on the chair and putting his hands in his mouth while a man is clapping, another man is eating, a woman is recording and a girl is eating."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/2eYMcKwGo3A.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_88_0_0"}, {"texts": ["A man wearing black clothes is on the right side of the baby and clapping his hands while looking here and there while a woman wearing a black top is sitting, holding a camera, and recording the baby, a man wearing a black t-shirt is sitting and eating cupcakes, and a baby girl is sitting and eating cupcakes."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/2eYMcKwGo3A.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_88_1_0"}, {"texts": ["A woman wearing black clothes is sitting at the front side of the baby and holding a camera and taking the picture of the baby while a man is eating and another man is clapping and singing a birthday song and a girl is holding a cupcake."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/2eYMcKwGo3A.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_88_3_0"}, {"texts": ["A girl is sitting on the right side of the woman while holding the food in her hand while a woman is clicking pictures, a man is eating something, and a baby is sitting on a high chair eating food, a man sitting beside a baby is clapping."], "durations": null, "exact_frames_per_prompt": [54], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/2eYMcKwGo3A.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 58, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_88_4_0"}, {"texts": ["The girl is licking her finger she dipped in the food."], "durations": null, "exact_frames_per_prompt": [22], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/2eYMcKwGo3A.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 58, "npz_gt_video_start_frame": 58, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 58, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_88_4_1"}, {"texts": ["A person wearing a graphic gray t-shirt, red jacket and blue jeans is sitting on the light blue carpeted floor and wrapping a gift then holding a sellotape."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/1y6c8lqyvas.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_892_0_0"}, {"texts": ["A person wearing a white-black t-shirt holding the camel leash is walking on the beach while a group of people is sitting on a camel, and one person is tapping their feet on the sandy surface along with another person."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/15JRTW-QAYA.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_895_1_0"}, {"texts": ["A camel is walking on the beach while carrying four people on its back, a man is holding camel's rope while walking ahead on the beach, and two people are playing on the beach on the right."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/15JRTW-QAYA.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_895_2_0"}, {"texts": ["A baby wearing a red t-shirt is sitting, clapping and shaking hands with a person."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-H1ScxTwBlw.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_897_0_0"}, {"texts": ["A person whose only hands are visible is shaking hands with a baby while a person whose only legs are visible on the right side is lying on the bed."], "durations": null, "exact_frames_per_prompt": [22], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-H1ScxTwBlw.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 58, "npz_gt_video_start_frame": 58, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 58, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_897_1_0"}, {"texts": ["A baby wearing a pink dress is sitting on a bed, and laughing a person whose legs are visible is laying on the bed and a person whose hands are visible is moving its hand."], "durations": null, "exact_frames_per_prompt": [68], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-H1ScxTwBlw.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 72, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_898_0_0"}, {"texts": ["The baby is shaking hands with a person."], "durations": null, "exact_frames_per_prompt": [8], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-H1ScxTwBlw.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 72, "npz_gt_video_start_frame": 72, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 72, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_898_0_1"}, {"texts": ["A girl wearing a green top is standing on an orange carpet floor, eating pepperoni"], "durations": null, "exact_frames_per_prompt": [42], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/0R684IGlXxg.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 46, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_899_0_0"}, {"texts": ["The girl is putting pepperoni on a pizza."], "durations": null, "exact_frames_per_prompt": [7], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/0R684IGlXxg.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 46, "npz_gt_video_start_frame": 46, "npz_gt_video_end_frame": 53, "skip_frames_after_generation": 46, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_899_0_1"}, {"texts": ["The girl is speaking."], "durations": null, "exact_frames_per_prompt": [27], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/0R684IGlXxg.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 53, "npz_gt_video_start_frame": 53, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 53, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_899_0_2"}, {"texts": ["A man in black clothing is holding a mike and speaking on it while moving his head here and there and doing hand gestures while another man and woman is standing infront of him holding something and laughing."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/1vGlHgc2v2I.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_89_0_0"}, {"texts": ["A man wearing a black sweatshirt is standing in front of the man holding a mike. He is at first points his hand in front of the man holding a mike while a woman in green dress is standing in front on the right and laughing."], "durations": null, "exact_frames_per_prompt": [22], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/1vGlHgc2v2I.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 26, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_89_1_0"}, {"texts": ["The man then puts his hand down and looking here and there while a woman in black tank top is standing at the back."], "durations": null, "exact_frames_per_prompt": [54], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/1vGlHgc2v2I.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 26, "npz_gt_video_start_frame": 26, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 26, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_89_1_1"}, {"texts": ["A woman wearing a green dress, is standing in the right side and holding an object while smiling and looking here and there while a man is talking on the mic and a man is standing infront of him and he is talking to the first woman and another woman wearing black clothes is standing on the left."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/1vGlHgc2v2I.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_89_2_0"}, {"texts": ["A woman wearing a black dress, is standing on the left side while holding an object in her hands while a person holding a mic is interacting with others."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/1vGlHgc2v2I.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_89_3_0"}, {"texts": ["A man wearing a black vest is sitting on a car wheel and touching the brake system area of the car."], "durations": null, "exact_frames_per_prompt": [27], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/0DxjTJJOkRU.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 31, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_900_0_0"}, {"texts": ["The man is applying pink gel grease on the brake drum."], "durations": null, "exact_frames_per_prompt": [43], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/0DxjTJJOkRU.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 31, "npz_gt_video_start_frame": 31, "npz_gt_video_end_frame": 74, "skip_frames_after_generation": 31, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_900_0_1"}, {"texts": ["The man is showing the brake drum."], "durations": null, "exact_frames_per_prompt": [6], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/0DxjTJJOkRU.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 74, "npz_gt_video_start_frame": 74, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 74, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_900_0_2"}, {"texts": ["A person wearing black clothes is holding a tattoo pen and making a tattoo on the waist of the another person."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-gVtqTsHlkk.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_902_0_0"}, {"texts": ["A person wearing black blue clothes is lying and getting tattooed on his waist while a person wearing black clothes is making tattoo."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-gVtqTsHlkk.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_902_1_0"}, {"texts": ["A boy whose upper body is visible, wearing a brown t-shirt, is standing while holding a red water glass, then drinks from it. "], "durations": null, "exact_frames_per_prompt": [24], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/4FtrC-SgqfE.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 28, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_904_1_0"}, {"texts": ["The boy is putting something in his mouth."], "durations": null, "exact_frames_per_prompt": [16], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/4FtrC-SgqfE.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 28, "npz_gt_video_start_frame": 28, "npz_gt_video_end_frame": 44, "skip_frames_after_generation": 28, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_904_1_1"}, {"texts": ["The boy is drinking water from the glass."], "durations": null, "exact_frames_per_prompt": [36], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/4FtrC-SgqfE.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 44, "npz_gt_video_start_frame": 44, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 44, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_904_1_2"}, {"texts": ["A woman wearing a red cloth and a white bracelet is standing behind the table and wrapping a box with blue wrapping paper."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/0h45uztur-o.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_905_0_0"}, {"texts": ["A man in the front wearing a white t-shirt is standing and playing golf on the ground while another man wearing a white t-shirt and back shorts is standing and moving golf balls with a golf club, and another man wearing a black t-shirt is walking toward the back."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-ol5AnuSsM0.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_909_0_0"}, {"texts": ["A man wearing a white t-shirt and black shorts is standing and pushing balls with a gold stick a person wearing white t-shirt is standing and holding golf stick and playing golf and a person wearing black outfit is walking on the green surface on the right side."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-ol5AnuSsM0.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_909_1_0"}, {"texts": ["A man wearing black clothes is walking while a man wearing a white t-shirt is standing on a green grass surface and playing a golf shot, another man wearing a white t-shirt is moving on a green grass surface, holding a golf stick."], "durations": null, "exact_frames_per_prompt": [31], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-ol5AnuSsM0.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 35, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_909_2_0"}, {"texts": ["The man is putting something on the ground."], "durations": null, "exact_frames_per_prompt": [45], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-ol5AnuSsM0.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 35, "npz_gt_video_start_frame": 35, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 35, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_909_2_1"}, {"texts": ["A man wearing a white t-shirt, black coat and black pants is standing behind the stool and speaking while holding a mic while another man and two women are standing and watching the man."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/1vGlHgc2v2I.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_90_0_0"}, {"texts": ["A man wearing a black sweatshirt and dark brown pants is standing on the left side of the first man and watching the second woman."], "durations": null, "exact_frames_per_prompt": [50], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/1vGlHgc2v2I.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 54, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_90_1_0"}, {"texts": ["The man wearing a black sweatshirt and dark brown pants is then watching the first woman."], "durations": null, "exact_frames_per_prompt": [26], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/1vGlHgc2v2I.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 54, "npz_gt_video_start_frame": 54, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 54, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_90_1_1"}, {"texts": ["A woman wearing a seafoam color dress is standing and watching the men while a man wearing a black coat is standing while holding some object in his left hand and talking to another man who is wearing a black hoodie, a woman wearing black clothing is standing to the left and looking at the men."], "durations": null, "exact_frames_per_prompt": [24], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/1vGlHgc2v2I.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 28, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_90_2_0"}, {"texts": ["The woman is watching in front while smiling and holding some flowers while a woman wearing black clothing standing on the left side is holding something in her hand, she turns her face to the left."], "durations": null, "exact_frames_per_prompt": [52], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/1vGlHgc2v2I.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 28, "npz_gt_video_start_frame": 28, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 28, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_90_2_1"}, {"texts": ["A person whose only a hand is visible is caressing the cat sitting on a green surface."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/1M93VXg55Fk.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_910_0_0"}, {"texts": ["A brown and white colored cat is sitting on a green surface, and being caressed by the person."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/1M93VXg55Fk.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_910_1_0"}, {"texts": ["A baby wearing a red vest is sitting on the lap of the woman and eating an ice cream."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/0W-Enb4ARIw.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_911_0_0"}, {"texts": ["A woman wearing a blue top and pink tights is holding the baby."], "durations": null, "exact_frames_per_prompt": [8], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/0W-Enb4ARIw.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 12, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_911_1_0"}, {"texts": ["The woman is holding the baby and cleaning its face."], "durations": null, "exact_frames_per_prompt": [68], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/0W-Enb4ARIw.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 12, "npz_gt_video_start_frame": 12, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 12, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_911_1_1"}, {"texts": ["A man wearing a black t-shirt and blue jeans is standing and holding a tire, pointing on its treads while speaking."], "durations": null, "exact_frames_per_prompt": [10], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/2hng0y10Vy4.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 14, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_913_0_0"}, {"texts": ["The man is putting it back."], "durations": null, "exact_frames_per_prompt": [10], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/2hng0y10Vy4.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 14, "npz_gt_video_start_frame": 14, "npz_gt_video_end_frame": 24, "skip_frames_after_generation": 14, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_913_0_1"}, {"texts": ["The man is going backward."], "durations": null, "exact_frames_per_prompt": [13], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/2hng0y10Vy4.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 24, "npz_gt_video_start_frame": 24, "npz_gt_video_end_frame": 37, "skip_frames_after_generation": 24, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_913_0_2"}, {"texts": ["The man is bending."], "durations": null, "exact_frames_per_prompt": [13], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/2hng0y10Vy4.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 37, "npz_gt_video_start_frame": 37, "npz_gt_video_end_frame": 50, "skip_frames_after_generation": 37, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_913_0_3"}, {"texts": ["The man is touching the other tire of the car while speaking."], "durations": null, "exact_frames_per_prompt": [29], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/2hng0y10Vy4.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 50, "npz_gt_video_start_frame": 50, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 50, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_913_0_4"}, {"texts": ["A baby wearing a green t-shirt is sitting in a baby stroller."], "durations": null, "exact_frames_per_prompt": [21], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/0G3yniDCOWk.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 25, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_916_0_0"}, {"texts": ["The baby is eating the ice cream."], "durations": null, "exact_frames_per_prompt": [56], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/0G3yniDCOWk.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 25, "npz_gt_video_start_frame": 25, "npz_gt_video_end_frame": 81, "skip_frames_after_generation": 25, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_916_0_1"}, {"texts": ["A man wearing a half-black jacket is standing on the green grass surface and playing golf."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/0s2Jzk6yBVk.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_917_0_0"}, {"texts": ["A man wearing a black jacket is standing while holding a golf stick."], "durations": null, "exact_frames_per_prompt": [63], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/0s2Jzk6yBVk.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 67, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_918_0_0"}, {"texts": ["The man is hitting the golf ball."], "durations": null, "exact_frames_per_prompt": [13], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/0s2Jzk6yBVk.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 67, "npz_gt_video_start_frame": 67, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 67, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_918_0_1"}, {"texts": ["A man wearing a white t-shirt puts pizza on a pizza peel."], "durations": null, "exact_frames_per_prompt": [44], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/0tEySAHKtcU.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 48, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_919_0_0"}, {"texts": ["The man lifts the peel."], "durations": null, "exact_frames_per_prompt": [13], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/0tEySAHKtcU.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 48, "npz_gt_video_start_frame": 48, "npz_gt_video_end_frame": 61, "skip_frames_after_generation": 48, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_919_0_1"}, {"texts": ["The man puts the pizza into the oven."], "durations": null, "exact_frames_per_prompt": [19], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/0tEySAHKtcU.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 61, "npz_gt_video_start_frame": 61, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 61, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_919_0_2"}, {"texts": ["A big girl whose upper half-body is visible wearing a dark gray shirt is standing and wearing a necktie on her neck while knitting the necktie with her hands."], "durations": null, "exact_frames_per_prompt": [35], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/1ofynNksJRU.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 39, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_920_0_0"}, {"texts": ["The big girl is showing the name-batch of the necktie."], "durations": null, "exact_frames_per_prompt": [40], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/1ofynNksJRU.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 39, "npz_gt_video_start_frame": 39, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 39, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_920_0_1"}, {"texts": ["A woman wearing a white shirt is standing and knotting a blue tie."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/1h343ev0Hgw.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_921_0_0"}, {"texts": ["A person whose hand is visible, holding a blue spoon and feeding a baby."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/1rQaP5Kf5Xw.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_922_0_0"}, {"texts": ["A baby is lying on a baby chair and eating some food which is being fed by the person whose hand is visible and holding the green spoon."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/1rQaP5Kf5Xw.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_922_1_0"}, {"texts": ["A woman wearing a pink top and jeans is standing, holding a pan and tossing the omelet in the pan."], "durations": null, "exact_frames_per_prompt": [28], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/28lxquLr49w.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 32, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_923_0_0"}, {"texts": ["A person wearing white blue clothes is standing and holding a golf stick and hitting a ball with his stick."], "durations": null, "exact_frames_per_prompt": [43], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/24ZOxWDShDo.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 47, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_925_0_ms_0"}, {"texts": ["A man wearing black color clothes is standing, holding a golf stick and speaking."], "durations": null, "exact_frames_per_prompt": [32], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/24ZOxWDShDo.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 48, "npz_gt_video_start_frame": 48, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 48, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_925_1_ms_0"}, {"texts": ["A black elephant is walking while another group of people is sitting on another elephant, a person wearing a black outfit is standing and helping the kids, and a group of people is sitting and moving at the back."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-w2cwjTx-ew.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_928_1_0"}, {"texts": ["A woman wearing a black t-shirt and black trousers is walking with the first elephant while a person wearing black outfit is standing with an elephant and a group of people are sitting and trying to laying off from the elephant and another group of people are sitting and looking at the center area."], "durations": null, "exact_frames_per_prompt": [73], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-w2cwjTx-ew.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 7, "npz_gt_video_start_frame": 7, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 7, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_928_2_0"}, {"texts": ["A person is holding a white paper. A person is inserting the paper into the paper shredder machine."], "durations": null, "exact_frames_per_prompt": [9], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/7jv3d26a6EI.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 13, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_929_0_0"}, {"texts": ["A man wearing a gray t-shirt and dark blue jeans is standing while holding a python snake on his shoulder while another man wearing a brown t-shirt is taking the snake from the first man."], "durations": null, "exact_frames_per_prompt": [11], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/4JIZzHOzV7w.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 15, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_92_0_0"}, {"texts": ["The man wearing a gray t-shirt and dark blue jeans is passing the python snake to the second man while some people are standing around."], "durations": null, "exact_frames_per_prompt": [66], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/4JIZzHOzV7w.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 15, "npz_gt_video_start_frame": 15, "npz_gt_video_end_frame": 81, "skip_frames_after_generation": 15, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_92_0_1"}, {"texts": ["A man wearing a dark green t-shirt and dark blue jeans is standing and holding the python snake given by the first man, and then holding it on his shoulder while some people is standing near the man."], "durations": null, "exact_frames_per_prompt": [77], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/4JIZzHOzV7w.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 81, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_92_1_0"}, {"texts": ["A dark green python snake is being holded by the first man and the second man."], "durations": null, "exact_frames_per_prompt": [77], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/4JIZzHOzV7w.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 81, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_92_4_0"}, {"texts": ["A person whose only hand is visible is putting a white paper in a shredding machine."], "durations": null, "exact_frames_per_prompt": [8], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/7jv3d26a6EI.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 12, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_930_0_0"}, {"texts": ["A man wearing a gray t-shirt, and black pants is sitting on a chair on the right side, holding a glass in his left hand and drinking beer from the glass while speaking."], "durations": null, "exact_frames_per_prompt": [42], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-xFiThR2JE4.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 46, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_933_1_0"}, {"texts": ["A baby wearing a white romper is walking on a white floor and caressing the cat."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-pLylBWuHLc.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_935_0_0"}, {"texts": ["A white black cat is sitting on a brown sofa as a baby in a white t-shirt is caressing the black cat."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-pLylBWuHLc.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_935_2_0"}, {"texts": ["A man wearing a brown t-shirt is sitting on the right side holding a glass of drink while another man wearing a gray hoodie is sitting and moving its hand and speaking."], "durations": null, "exact_frames_per_prompt": [9], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/0ghNZluZEg8.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 13, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_936_0_0"}, {"texts": ["The man wearing a brown t-shirt is drinking it and moving his right hand while another man wearing a gray hoodie is sitting and drinking."], "durations": null, "exact_frames_per_prompt": [66], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/0ghNZluZEg8.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 13, "npz_gt_video_start_frame": 13, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 13, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_936_0_1"}, {"texts": ["A man wearing a green t-shirt is sitting on the left side holding a glass of drink while another man, sitting on the right side, is holding a glass of drink and talking."], "durations": null, "exact_frames_per_prompt": [39], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/0ghNZluZEg8.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 43, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_936_1_0"}, {"texts": ["The man is shaking the glass while the other man is drinking from the glass."], "durations": null, "exact_frames_per_prompt": [12], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/0ghNZluZEg8.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 43, "npz_gt_video_start_frame": 43, "npz_gt_video_end_frame": 55, "skip_frames_after_generation": 43, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_936_1_1"}, {"texts": ["The man is drinking the drink while the other man puts the glass down, touches a container, and picks up a marker."], "durations": null, "exact_frames_per_prompt": [25], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/0ghNZluZEg8.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 55, "npz_gt_video_start_frame": 55, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 55, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_936_1_2"}, {"texts": ["A baby wearing a red t-shirt is standing and playing with toilet paper."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/3yM75tJhsdQ.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_939_0_0"}, {"texts": ["A man wearing a white t-shirt and black shorts is standing on the left side of the countertop and peeling a potato attached to a drill machine."], "durations": null, "exact_frames_per_prompt": [41], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/0ogdoF3ayO8.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 45, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_93_0_0"}, {"texts": ["A person whose fingers are visible is holding a spatula and making chowmein in a cooking pan."], "durations": null, "exact_frames_per_prompt": [43], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-QoffNuV_H0.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 28, "npz_gt_video_start_frame": 28, "npz_gt_video_end_frame": 71, "skip_frames_after_generation": 28, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_940_0_0"}, {"texts": ["A person of whom only hands are visible is counting banknotes with his hands."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/2beJ4Ey9moQ.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_941_0_0"}, {"texts": ["A man whose hands are visible is wearing a hand watch and counting money."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/2beJ4Ey9moQ.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_942_0_0"}, {"texts": ["A person whose only hands are visible is ironing a white paper on a brown board."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/1TPtY6_EVlo.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_943_0_0"}, {"texts": ["A person of whom only hands are visible is ironing a paper."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/1TPtY6_EVlo.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_944_0_0"}, {"texts": ["A woman wearing a white top is unscrewing the cork from a cork opener."], "durations": null, "exact_frames_per_prompt": [52], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/S0UHTQNB9Cg.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 56, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_945_0_0"}, {"texts": ["The woman starts opening a bottle."], "durations": null, "exact_frames_per_prompt": [24], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/S0UHTQNB9Cg.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 56, "npz_gt_video_start_frame": 56, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 56, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_945_0_1"}, {"texts": ["A woman takes a white paper then fold it in half."], "durations": null, "exact_frames_per_prompt": [60], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/3-Sp_d3cnSs.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 64, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_946_0_0"}, {"texts": ["The woman starts pressing the paper with a bone folder."], "durations": null, "exact_frames_per_prompt": [16], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/3-Sp_d3cnSs.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 64, "npz_gt_video_start_frame": 64, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 64, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_946_0_1"}, {"texts": ["A man whose hand is visible is mixing sauce and food in a black pan with a brown spatula."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/0wb1qpN5Pnw.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_947_0_0"}, {"texts": ["A man wearing a black t-shirt is sitting on the right side, eating a burger while another man on the left is sitting and picking food from a basket and eating it."], "durations": null, "exact_frames_per_prompt": [14], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/0LuJFYtclQA.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 18, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_949_0_0"}, {"texts": ["The man wearing a black t-shirt is putting it back on the plate while another man starts eating the burger."], "durations": null, "exact_frames_per_prompt": [50], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/0LuJFYtclQA.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 18, "npz_gt_video_start_frame": 18, "npz_gt_video_end_frame": 68, "skip_frames_after_generation": 18, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_949_0_1"}, {"texts": ["The man wearing a black t-shirt is picking it up and eating it."], "durations": null, "exact_frames_per_prompt": [12], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/0LuJFYtclQA.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 68, "npz_gt_video_start_frame": 68, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 68, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_949_0_2"}, {"texts": ["A man wearing a grey shirt is sitting on the left side, placing something in the basket while another man in a black outfit is eating a burger."], "durations": null, "exact_frames_per_prompt": [22], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/0LuJFYtclQA.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 26, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_949_1_0"}, {"texts": ["The man is picking the burger bun, putting it back, picking the burger and eating it."], "durations": null, "exact_frames_per_prompt": [54], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/0LuJFYtclQA.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 26, "npz_gt_video_start_frame": 26, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 26, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_949_1_1"}, {"texts": ["A man wearing a black t-shirt is sitting on the right and eating a burger while a man on the left side in a blue shirt is sitting, eating and picking something from a wooden basket."], "durations": null, "exact_frames_per_prompt": [22], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/0LuJFYtclQA.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 26, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_950_0_0"}, {"texts": ["The man is putting it on the plate while a man on the left side in a blue shirt starts eating a burger."], "durations": null, "exact_frames_per_prompt": [54], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/0LuJFYtclQA.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 26, "npz_gt_video_start_frame": 26, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 26, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_950_0_1"}, {"texts": ["A man wearing a white denim shirt sitting on the left is picking something from the basket while a man wearing black clothes on the right is eating a burger."], "durations": null, "exact_frames_per_prompt": [57], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/0LuJFYtclQA.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 61, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_950_1_0"}, {"texts": ["The man is eating a burger."], "durations": null, "exact_frames_per_prompt": [19], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/0LuJFYtclQA.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 61, "npz_gt_video_start_frame": 61, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 61, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_950_1_1"}, {"texts": ["A bare-chested man is sitting inside the tub filled with water and eating potato chips."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/2mCBkcarADg.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_951_0_0"}, {"texts": ["A bare-chested man is sitting inside the tub filled with water, holding a yellow packet of potato chips and eating potato chips."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/2mCBkcarADg.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_952_0_0"}, {"texts": ["A person whose hand is visible is opening a cap of a blue bottle."], "durations": null, "exact_frames_per_prompt": [43], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/6q1MyI4yEco.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 23, "npz_gt_video_start_frame": 23, "npz_gt_video_end_frame": 66, "skip_frames_after_generation": 23, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_956_0_0"}, {"texts": ["The person is putting the liquid inside the washing machine."], "durations": null, "exact_frames_per_prompt": [14], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/6q1MyI4yEco.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 66, "npz_gt_video_start_frame": 66, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 66, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_956_0_1"}, {"texts": ["A person whom hand is visible is frying the food in oil in the pan with a wooden spatula."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/02QplMWeNhU.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_957_0_0"}, {"texts": ["A person whose hands are visible is picking up a container with chicken in it while holding gloves."], "durations": null, "exact_frames_per_prompt": [45], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/1xI1lrRuDkQ.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 35, "npz_gt_video_start_frame": 35, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 35, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_958_0_0"}, {"texts": ["A person of whom only hands are visible holding baking gloves, picks the chicken and shows it."], "durations": null, "exact_frames_per_prompt": [45], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/1xI1lrRuDkQ.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 35, "npz_gt_video_start_frame": 35, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 35, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_959_0_0"}, {"texts": ["A man wearing a striped black suit is sitting on a chair on the left side and is looking at the woman and then looking in front while putting his hands on the counter while a man wearing a white shirt is sitting behind the monitor."], "durations": null, "exact_frames_per_prompt": [30], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/3Dm9M_F1f0w.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 50, "npz_gt_video_start_frame": 50, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 50, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_95_2_0"}, {"texts": ["A person whose hands are only visible is wearing a white dress and yellow gloves, and is mixing the liquid in the white mug with a spoon."], "durations": null, "exact_frames_per_prompt": [26], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/2a32cGaDUSs.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 30, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_960_0_0"}, {"texts": ["The person is mixing the white liquid in the two transparent mugs."], "durations": null, "exact_frames_per_prompt": [50], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/2a32cGaDUSs.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 30, "npz_gt_video_start_frame": 30, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 30, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_960_0_1"}, {"texts": ["A person whose hand is visible is starting the washing machine."], "durations": null, "exact_frames_per_prompt": [24], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/7KnzBBIwgGg.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 28, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_963_0_ms_0"}, {"texts": ["A girl is sitting and getting her eyebrows done by a woman."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/0RhN11PDhCs.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_964_0_0"}, {"texts": ["A woman of whom only hands are visible is doing the girl's eyebrow makeup."], "durations": null, "exact_frames_per_prompt": [73], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/0RhN11PDhCs.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 7, "npz_gt_video_start_frame": 7, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 7, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_964_1_0"}, {"texts": ["A person whose hands are visible is putting the ingredients into a bowl and whisking them."], "durations": null, "exact_frames_per_prompt": [69], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/22pyq67SU2c.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 11, "npz_gt_video_start_frame": 11, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 11, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_965_0_ms_0"}, {"texts": ["A woman wearing a brown top and purple scarf is standing."], "durations": null, "exact_frames_per_prompt": [24], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/2L7biV1djJc.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 28, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_968_0_0"}, {"texts": ["The woman is taking off the tape from the folder."], "durations": null, "exact_frames_per_prompt": [32], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/2L7biV1djJc.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 28, "npz_gt_video_start_frame": 28, "npz_gt_video_end_frame": 60, "skip_frames_after_generation": 28, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_968_0_1"}, {"texts": ["The woman is sticking the card on the green table."], "durations": null, "exact_frames_per_prompt": [20], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/2L7biV1djJc.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 60, "npz_gt_video_start_frame": 60, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 60, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_968_0_2"}, {"texts": ["A woman wearing a pink shirt is standing and pointing her hand at the food ingredients while speaking."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/1E-SZA4urOQ.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_972_0_0"}, {"texts": ["A woman wearing a white t-shirt is at first taking out the clothes from the washing machine."], "durations": null, "exact_frames_per_prompt": [26], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/6VNgBc9fnKg.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 9, "npz_gt_video_start_frame": 9, "npz_gt_video_end_frame": 35, "skip_frames_after_generation": 9, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_973_0_0"}, {"texts": ["The woman is putting it in the dryer."], "durations": null, "exact_frames_per_prompt": [14], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/6VNgBc9fnKg.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 35, "npz_gt_video_start_frame": 35, "npz_gt_video_end_frame": 50, "skip_frames_after_generation": 35, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_973_0_1"}, {"texts": ["The woman is sitting on a chair and talking."], "durations": null, "exact_frames_per_prompt": [29], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/6VNgBc9fnKg.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 50, "npz_gt_video_start_frame": 50, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 50, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_973_0_2"}, {"texts": ["A woman wearing a white t-shirt and blue jeans is taking clothes out of the washing machine."], "durations": null, "exact_frames_per_prompt": [26], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/6VNgBc9fnKg.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 9, "npz_gt_video_start_frame": 9, "npz_gt_video_end_frame": 35, "skip_frames_after_generation": 9, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_974_0_0"}, {"texts": ["The woman wearing a white t-shirt and blue jeans is putting them into the dryer."], "durations": null, "exact_frames_per_prompt": [14], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/6VNgBc9fnKg.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 35, "npz_gt_video_start_frame": 35, "npz_gt_video_end_frame": 50, "skip_frames_after_generation": 35, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_974_0_1"}, {"texts": ["The woman wearing a white t-shirt and blue jeans is sitting and talking."], "durations": null, "exact_frames_per_prompt": [30], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/6VNgBc9fnKg.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 50, "npz_gt_video_start_frame": 50, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 50, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_974_0_2"}, {"texts": ["A man wearing a black clothes is standing and performing a flair bar-tending."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-n2sDzOvLeY.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_975_0_0"}, {"texts": ["A man wearing a green jacket is fixing a wheel."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/50cpkmQoP5M.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_976_0_0"}, {"texts": ["A man wearing a black suit is sitting and he is talking."], "durations": null, "exact_frames_per_prompt": [25], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-rsT9HSBkyI.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 29, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_982_0_0"}, {"texts": ["A woman wearing black clothes takes the chair while other people are sitting on chairs and one woman wearing white blazer is standing."], "durations": null, "exact_frames_per_prompt": [33], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-rsT9HSBkyI.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 29, "npz_gt_video_start_frame": 29, "npz_gt_video_end_frame": 62, "skip_frames_after_generation": 29, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_982_1_0"}, {"texts": ["The woman sits."], "durations": null, "exact_frames_per_prompt": [14], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-rsT9HSBkyI.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 62, "npz_gt_video_start_frame": 62, "npz_gt_video_end_frame": 76, "skip_frames_after_generation": 62, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_982_1_1"}, {"texts": ["A woman wearing grey clothes is taking a black chair while a group of men are already sitting at the chairs and a man in a blue suit is moving forward."], "durations": null, "exact_frames_per_prompt": [32], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-rsT9HSBkyI.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 29, "npz_gt_video_start_frame": 29, "npz_gt_video_end_frame": 61, "skip_frames_after_generation": 29, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_982_2_0"}, {"texts": ["The woman wearing grey clothes is about to sit while another woman wearing dark grey clothes sat on the chair."], "durations": null, "exact_frames_per_prompt": [14], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-rsT9HSBkyI.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 61, "npz_gt_video_start_frame": 61, "npz_gt_video_end_frame": 75, "skip_frames_after_generation": 61, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_982_2_1"}, {"texts": ["A person whose hand are visible, is holding a pineapple and a knife."], "durations": null, "exact_frames_per_prompt": [33], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-zNzRkmzoT0.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 37, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_983_0_0"}, {"texts": ["The person starts cutting the pineapple with a knife."], "durations": null, "exact_frames_per_prompt": [43], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-zNzRkmzoT0.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 37, "npz_gt_video_start_frame": 37, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 37, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_983_0_1"}, {"texts": ["A person whose only hand is visible is filling air in the tyre with a tire inflation nozzle."], "durations": null, "exact_frames_per_prompt": [59], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/1ED19BBz1Lw.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 63, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_984_0_0"}, {"texts": ["The person whose only hand is visible holds a deflator tool."], "durations": null, "exact_frames_per_prompt": [17], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/1ED19BBz1Lw.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 63, "npz_gt_video_start_frame": 63, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 63, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_984_0_1"}, {"texts": ["A girl wearing a white top is riding a brown camel on a green grass surface while a man wearing a black t-shirt is walking on a green grass surface, holding the camel's leash."], "durations": null, "exact_frames_per_prompt": [46], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-cUGz7HAGBg.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 50, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_989_0_0"}, {"texts": ["A man wearing a black t-shirt holding the camel's leash and walking on a grass surface in front of the camel while a girl wearing a white top is riding the camel."], "durations": null, "exact_frames_per_prompt": [44], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-cUGz7HAGBg.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 6, "npz_gt_video_start_frame": 6, "npz_gt_video_end_frame": 50, "skip_frames_after_generation": 6, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_989_1_0"}, {"texts": ["A brown camel, taking a girl on its back is moving on a green grass surface while a man wearing a black t-shirt is is holding the rope of the camel and moving on the grass surface."], "durations": null, "exact_frames_per_prompt": [46], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-cUGz7HAGBg.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 50, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_989_2_0"}, {"texts": ["A person wearing a blue shirt is ironing clothes on an ironing table."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/3cxPpNfts8s.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_98_0_0"}, {"texts": ["A yellow bus is moving in the left direction."], "durations": null, "exact_frames_per_prompt": [65], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/156VYYahmoo.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 15, "npz_gt_video_start_frame": 15, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 15, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_990_1_0"}, {"texts": ["A person whose hand is visible is holding a spatula is mixing vegetables in a frying pan."], "durations": null, "exact_frames_per_prompt": [55], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/3o_0U5JSSjg.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 59, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_992_0_0"}, {"texts": ["A person of whom only hands are visible is inserting capsicum pieces into the wok."], "durations": null, "exact_frames_per_prompt": [20], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/3o_0U5JSSjg.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 24, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_993_0_0"}, {"texts": ["The person of whom only hands are visible is mixing them with other vegetables with a wooden spoon."], "durations": null, "exact_frames_per_prompt": [56], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/3o_0U5JSSjg.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 24, "npz_gt_video_start_frame": 24, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 24, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_993_0_1"}, {"texts": ["A man wearing a blue t-shirt is standing and holding a camera while three men are standing near the table and talking, a man wearing a black t-shirt and a cap is holding a pair of tongs and cooking something, some people are standing at the back, and a man wearing a white t-shirt is sitting near the table."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/1l3uTCM7NPo.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_995_1_0"}, {"texts": ["A man wearing a white shirt is sitting on his knee next to the first man in a group of people, some are talking, some are cooking, and some are standing, and a man in a blue t-shirt is filming with a camera."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/1l3uTCM7NPo.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_995_2_0"}, {"texts": ["A man wearing a black t-shirt is standing and doing barbecue while a man in white shirt is taking an interview with the man in a blue t-shirt, a man in a red t-shirt is standing behind him, two men are recording the interview, and a man in black t-shirt is standing at the back near the barbecue."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/1l3uTCM7NPo.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_995_3_0"}, {"texts": ["A person whose hands are visible is feeding the fishes."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/0vlrwlay0c4.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_998_0_0"}, {"texts": ["A brown disabled dog is walking with the dog wheelchair."], "durations": null, "exact_frames_per_prompt": [76], "background": null, "npz_video": "storybench/npy_96x160pix_8fps/uvo-test/videos/-5kButbj9OE.npy", "npz_video_start_frame": 0, "npz_video_end_frame": 4, "npz_gt_video_start_frame": 4, "npz_gt_video_end_frame": 80, "skip_frames_after_generation": 4, "storybench_mode": "action_exe", "indices_to_select": null, "comment": "UVO_sparse_val_99_0_0"}]